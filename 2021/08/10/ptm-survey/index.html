<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Pre-trained Models for Natural Language Processing A Survey - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="原文内容很丰富，慢慢学习更新。 摘要这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。 1.Introduction随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，G"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:description" content="原文内容很丰富，慢慢学习更新。 摘要这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。 1.Introduction随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，G"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:published_time" content="2021-08-10T14:55:19.000Z"><meta property="article:modified_time" content="2022-06-11T09:41:43.391Z"><meta property="article:author" content="Lavine Hu"><meta property="article:tag" content="PTM"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com/2021/08/10/ptm-survey/"},"headline":"Lavine Hu","image":[],"datePublished":"2021-08-10T14:55:19.000Z","dateModified":"2022-06-11T09:41:43.391Z","author":{"@type":"Person","name":"Lavine Hu"},"description":"原文内容很丰富，慢慢学习更新。 摘要这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。 1.Introduction随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，G"}</script><link rel="canonical" href="http://example.com/2021/08/10/ptm-survey/"><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-10  <a class="commentCountImg" href="/2021/08/10/ptm-survey/#comment-container"><span class="display-none-class">fa4f8744578ff1a660a64a4ac67da7c2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="fa4f8744578ff1a660a64a4ac67da7c2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>13 m  <i class="fas fa-pencil-alt"> </i>2.0 k</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>&nbsp;visits</span></div></div><h1 class="title is-3 is-size-4-mobile">Pre-trained Models for Natural Language Processing A Survey</h1><div class="content"><p>原文内容很丰富，慢慢学习更新。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，GNN以及attention。</p>
<p>尽管NLP任务的取得很大成功，但是和CV比较，性能提高可能不是非常明显。这主要是因为NLP任务的数据集都非常小（除了机器翻译），然而深度网络参数非常多，没有足够的数据支撑网络训练会导致过拟合问题。</p>
<p>最近，大量工作表明，预先训练的模型（PTMs），在大型语料库上可以学习通用语言表示，这有利于下游NLP任务可以避免从零开始训练新模型。随着算力的发展，深度模型（例如，transformer）的出现和训练技巧的不断调高，PTM的结构从浅层发展成深层。<strong>第一代PTM</strong>被用于Non-contextual  word Embedding。由于下游任务不需要这些模型本身，只需要训练好的词向量矩阵，因此对于现在的算力，这些模型非常浅层，比如Skip-Gram和GloVe。虽然这些预训练词向量可以捕获词语的语义，但它们不受上下文限制，无法捕获上下文中的高级含义，某些任务会失效，例如多义词，句法结构，语义角色、回指。<strong>第二代PTM</strong>关注Contextual word embeddings，比如BERT，GPT等。这些编码器任然需要通过下游任务在上下文中表示词语。</p>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2.Background"></a>2.Background</h2><h3 id="2-1-Language-Representation-Learning"><a href="#2-1-Language-Representation-Learning" class="headerlink" title="2.1 Language Representation Learning"></a>2.1 Language Representation Learning</h3><p>The core idea of distributed representation is to describe the meaning of a piece of text by low-dimensional real-valued vectors. And each dimension of the vector has no corresponding sense, while the whole represents a concrete concept.</p>
<p><img src="/2021/08/10/ptm-survey/ptm1.JPG" alt></p>
<p><strong>Non-contextual Embeddings</strong></p>
<p>这一步主要是将分割的字符，比如图中的$x$，变成向量表达$e_x \in \mathbb{R}^{D_e}$，$D_e$是词向量维度。向量化过程就是基于一个离线训练的词向量矩阵$E\in \mathbb{R}^{D_e\times |\mathcal{V}|} $做查找，$\mathcal{V}$是词汇表。</p>
<p>这个过程主要有两个问题。第一个是这个词向量是静态的，没有考虑上下文含义，无法处理多义词。第二个是oov问题，许多算法可以缓解这个问题，比如基于character level，比如基于subword，subword算法有BPE，CharCNN等。</p>
<p><strong>Contextual Embeddings</strong></p>
<p>To address the issue of polysemous and the context-dependent nature of words, we need distinguish the semantics of words in different contexts：</p>
<script type="math/tex; mode=display">
[\textbf{h}_1,\textbf{h}_2,...,\textbf{h}_T]=f_{enc}(x_1,x_2,...,x_T)</script><p>其中$f_{enc}(\cdot)$为深度编码器。$\textbf{h}_t$就是contextual embedding或者dynamical embedding。</p>
<h3 id="2-2-Neural-Contextual-Encoders"><a href="#2-2-Neural-Contextual-Encoders" class="headerlink" title="2.2 Neural Contextual Encoders"></a>2.2 Neural Contextual Encoders</h3><p><img src="/2021/08/10/ptm-survey/ptm3.JPG" alt></p>
<p>可以分成两类，sequence models and non-sequence models。</p>
<h4 id="2-2-1-sequence-models"><a href="#2-2-1-sequence-models" class="headerlink" title="2.2.1 sequence models"></a>2.2.1 sequence models</h4><p>sequence models 分为两类，Convolutional Models和Recurrent Models，见上图。</p>
<p><strong>Convolutional</strong> </p>
<p>Convolutional models take the embeddings of words in the input sentence and capture the meaning of a word by aggregating the <strong>local information</strong> from its neighbors by convolution operations</p>
<p><strong>Recurrent</strong> </p>
<p>Recurrent models capture the contextual representations of words with short memory, such as LSTMs and GRUs . In practice, bi-directional LSTMs or GRUs are used to collect information from both sides of a word, but its performance is often affected by the <strong>long-term dependency problem</strong>.</p>
<h4 id="2-2-2-non-sequence-models"><a href="#2-2-2-non-sequence-models" class="headerlink" title="2.2.2 non-sequence models"></a>2.2.2 non-sequence models</h4><p>transformer： model the relation of every two words</p>
<h4 id="2-2-3-Analysis"><a href="#2-2-3-Analysis" class="headerlink" title="2.2.3 Analysis"></a>2.2.3 Analysis</h4><p><strong>Sequence models：</strong></p>
<p>1.Sequence models learn the contextual representation of the word with locality bias and are hard to capture the long-range interactions between words. </p>
<p>2.Nevertheless, sequence models are usually easy to train and get good results for various NLP tasks.</p>
<p><strong>fully-connected self-attention model：</strong></p>
<p>1.can directly model the dependency between every two words in a sequence, which is more powerful and suitable to model long range dependency of language</p>
<p>2.However, due to its heavy structure and less model bias, the Transformer usually requires a large training corpus and is easy to overfit on small or modestly-sized datasets</p>
<p><strong>结论</strong>：the Transformer has become the mainstream architecture of PTMs due to its powerful capacity.</p>
<h3 id="2-3-Why-Pre-training"><a href="#2-3-Why-Pre-training" class="headerlink" title="2.3 Why Pre-training?"></a>2.3 Why Pre-training?</h3><ol>
<li>Pre-training on the huge text corpus can <strong>learn universal language representation</strong>s and help with the downstream tasks.</li>
<li>Pre-training provides a <strong>better model initialization</strong>,which usually leads to a better generalization performance and speeds up convergence on the target task.</li>
<li>Pre-training can be <strong>regarded as a kind of regularization</strong> to avoid overfitting on small data</li>
</ol>
<h2 id="3-Overview-of-PTMs"><a href="#3-Overview-of-PTMs" class="headerlink" title="3 Overview of PTMs"></a>3 Overview of PTMs</h2><h3 id="3-1-Pre-training-Tasks"><a href="#3-1-Pre-training-Tasks" class="headerlink" title="3.1 Pre-training Tasks"></a>3.1 Pre-training Tasks</h3><p>预训练任务对于学习通用语言表示至关重要。通常，这些预训练任务应具有挑战性，并拥有大量训练数据。在本节中，我们将预训练任务分成三个类别：Supervised learning、Unsupervised learning和Self-Supervised learning。</p>
<p><strong>Self-Supervised learning</strong>： is a blend of supervised learning and unsupervised learning. The learning paradigm of SSL is entirely the same as supervised learning, but the labels of training data are generated automatically. The key idea of SSL is to predict any part of the input from other parts in some form. For example, the masked language model (MLM) is a self-supervised task that attempts to predict the masked words in a sentence given the rest words.</p>
<p>接下来基于介绍常用的基于Self-Supervised learning的预训练任务。</p>
<h4 id="3-1-1-Language-Modeling-LM"><a href="#3-1-1-Language-Modeling-LM" class="headerlink" title="3.1.1 Language Modeling (LM)"></a>3.1.1 Language Modeling (LM)</h4><h4 id="3-1-2-Masked-Language-Modeling-MLM"><a href="#3-1-2-Masked-Language-Modeling-MLM" class="headerlink" title="3.1.2 Masked Language Modeling (MLM)"></a>3.1.2 Masked Language Modeling (MLM)</h4><h4 id="3-1-3-Permuted-Language-Modeling-PLM"><a href="#3-1-3-Permuted-Language-Modeling-PLM" class="headerlink" title="3.1.3 Permuted Language Modeling (PLM)"></a>3.1.3 Permuted Language Modeling (PLM)</h4><h4 id="3-1-4-Denoising-Autoencoder-DAE"><a href="#3-1-4-Denoising-Autoencoder-DAE" class="headerlink" title="3.1.4 Denoising Autoencoder (DAE)"></a>3.1.4 Denoising Autoencoder (DAE)</h4><h4 id="3-1-5-Contrastive-Learning-CTL"><a href="#3-1-5-Contrastive-Learning-CTL" class="headerlink" title="3.1.5 Contrastive Learning (CTL)"></a>3.1.5 Contrastive Learning (CTL)</h4><p>nsp也属于CTL</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360892229">https://zhuanlan.zhihu.com/p/360892229</a></p>
<h4 id="3-1-6-Others"><a href="#3-1-6-Others" class="headerlink" title="3.1.6 Others"></a>3.1.6 Others</h4><h3 id="3-2-Taxonomy-of-PTMs"><a href="#3-2-Taxonomy-of-PTMs" class="headerlink" title="3.2 Taxonomy of PTMs"></a>3.2 Taxonomy of PTMs</h3><p><img src="/2021/08/10/ptm-survey/ptm5.JPG" alt></p>
<p>作者从以下四个角度，即Representation Type，Architectures，Pre-Training Task Types，Extensions，对现有的PTM分类，分类结果如上。图和这里有一点不统一，是作者没注意？图里有5个类别，多了Tuning Strategies，而且Representation Type在图中为Contextual?。</p>
<h3 id="3-3-Model-Analysis"><a href="#3-3-Model-Analysis" class="headerlink" title="3.3 Model Analysis"></a>3.3 Model Analysis</h3><h2 id="4-Extensions-of-PTMs"><a href="#4-Extensions-of-PTMs" class="headerlink" title="4 Extensions of PTMs"></a>4 Extensions of PTMs</h2><h3 id="4-1-Knowledge-Enriched-PTMs"><a href="#4-1-Knowledge-Enriched-PTMs" class="headerlink" title="4.1 Knowledge-Enriched PTMs"></a>4.1 Knowledge-Enriched PTMs</h3><h3 id="4-2-Multilingual-and-Language-Specific-PTMs"><a href="#4-2-Multilingual-and-Language-Specific-PTMs" class="headerlink" title="4.2 Multilingual and Language-Specific PTMs"></a>4.2 Multilingual and Language-Specific PTMs</h3><h3 id="4-3-Multi-Modal-PTMs"><a href="#4-3-Multi-Modal-PTMs" class="headerlink" title="4.3 Multi-Modal PTMs"></a>4.3 Multi-Modal PTMs</h3><h3 id="4-4-Domain-Specific-and-Task-Specific-PTMs"><a href="#4-4-Domain-Specific-and-Task-Specific-PTMs" class="headerlink" title="4.4 Domain-Specific and Task-Specific PTMs"></a>4.4 Domain-Specific and Task-Specific PTMs</h3><h3 id="4-5-Model-Compression"><a href="#4-5-Model-Compression" class="headerlink" title="4.5 Model Compression"></a>4.5 Model Compression</h3><h2 id="5-Adapting-PTMs-to-Downstream-Tasks"><a href="#5-Adapting-PTMs-to-Downstream-Tasks" class="headerlink" title="5 Adapting PTMs to Downstream Tasks"></a>5 Adapting PTMs to Downstream Tasks</h2><p>虽然PTM学习了很多通用知识，但是如何将这些知识有效应用到下游任务是个挑战。</p>
<h3 id="5-1-Transfer-Learning"><a href="#5-1-Transfer-Learning" class="headerlink" title="5.1 Transfer Learning"></a>5.1 Transfer Learning</h3><p>Transfer learning is to adapt the knowledge from a source task (or domain) to a target task (or domain).如下图。</p>
<p><img src="/2021/08/10/ptm-survey/ptm6.JPG" alt></p>
<h3 id="5-2-How-to-Transfer"><a href="#5-2-How-to-Transfer" class="headerlink" title="5.2 How to Transfer?"></a>5.2 How to Transfer?</h3><h4 id="5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus"><a href="#5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus" class="headerlink" title="5.2.1 Choosing appropriate pre-training task, model architecture and corpus"></a>5.2.1 Choosing appropriate pre-training task, model architecture and corpus</h4><h4 id="5-2-2-Choosing-appropriate-layers"><a href="#5-2-2-Choosing-appropriate-layers" class="headerlink" title="5.2.2 Choosing appropriate layers"></a>5.2.2 Choosing appropriate layers</h4><p>使用哪些层参与下游任务</p>
<p>选择的层model1+下游任务model2</p>
<p>对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。</p>
<p>用$\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\cdot)$为特定的任务模型。有以下几种方法选择representation:</p>
<p><strong>a) Embedding Only</strong></p>
<p>choose only the pre-trained static embeddings，即$g(\textbf{H}^{1})$</p>
<p><strong>b) Top Layer</strong></p>
<p>选择顶层的representation，然后接入特定的任务模型，即$g(\textbf{H}^{L})$</p>
<p><strong>c) All Layers</strong></p>
<p>输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下</p>
<script type="math/tex; mode=display">
g(\textbf{r}_t)=g(\gamma \sum_{l=1}^{L}\alpha_l\textbf{H}^{(l)})</script><p>其中$\alpha$ is the softmax-normalized weight for layer $l$ and $\gamma$ is a scalar to scale the vectors output by pre-trained model</p>
<h4 id="5-2-3-To-tune-or-not-to-tune"><a href="#5-2-3-To-tune-or-not-to-tune" class="headerlink" title="5.2.3 To tune or not to tune?"></a>5.2.3 To tune or not to tune?</h4><p>总共有两种常用的模型迁移方式：<strong>feature extraction</strong> (where the pre-trained parameters are frozen), and <strong>fine-tuning</strong> (where the pre-trained parameters are unfrozen and fine-tuned).</p>
<p><img src="/2021/08/10/ptm-survey/ptm7.JPG" alt></p>
<p>选择的层model1参数是否固定，model2一定要训练</p>
<p>bert 只有top  layer finetune？？？？</p>
<h3 id="5-3-Fine-Tuning-Strategies"><a href="#5-3-Fine-Tuning-Strategies" class="headerlink" title="5.3 Fine-Tuning Strategies"></a>5.3 Fine-Tuning Strategies</h3><p><strong>Two-stage fine-tuning</strong></p>
<p>第一阶段为中间任务，第二阶段为目标任务</p>
<p><strong>Multi-task fine-tuning</strong></p>
<p>multi-task learning and pre-training are complementary technologies.</p>
<p><strong>Fine-tuning with extra adaptation modules</strong></p>
<p>The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed.</p>
<p><strong>Others</strong></p>
<p>self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Pre-trained Models for Natural Language Processing A Survey</p><p><a href="http://example.com/2021/08/10/ptm-survey/">http://example.com/2021/08/10/ptm-survey/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><a href="http://example.com"><p>Lavine Hu</p></a></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-08-10</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-06-11</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="recommend-area"><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># Related Post</span><br><span>  1.<a class="is-size-6" href="/2022/06/30/micro-Grained/" target="_blank">细粒度NLP任务</a><br></span><span>  2.<a class="is-size-6" href="/2022/06/17/chinesebert/" target="_blank">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</a><br></span><span>  3.<a class="is-size-6" href="/2022/06/11/finetune/" target="_blank">finetune</a><br></span><span>  4.<a class="is-size-6" href="/2022/05/29/pretrain-task/" target="_blank">预训练任务</a><br></span><span>  5.<a class="is-size-6" href="/2022/03/28/ptm-relation/" target="_blank">ptm之间的联系</a><br></span><span>  6.<a class="is-size-6" href="/2021/11/04/bert-wwm/" target="_blank">Pre-Training with Whole Word Masking for Chinese BERT</a><br></span><span>  7.<a class="is-size-6" href="/2021/11/04/albert/" target="_blank">ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a><br></span><span>  8.<a class="is-size-6" href="/2021/10/26/ch-word-bert/" target="_blank">中文词粒度BERT</a><br></span></div></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/css/share.min.css"><div class="social-share"></div><script src="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="https://github.com/hlw95/blog_img/blob/main/ali.jpg" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/source%5Cimg%5Cwechat.jpg" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/08/16/L2R/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">排序学习</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/08/10/leetcode_algorith-tech/"><span class="level-item">leetcode常见套路</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--><div class="card"><div class="card-content"><div class="title is-5">Comments</div><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.0/gitalk.css"><script> $.getScript('/js/gitalk.min.js', function () { 
            var gitalk = new Gitalk({
            language:'zh-CN',
            id: 'fa4f8744578ff1a660a64a4ac67da7c2',
            repo: 'hlw95.github.io',
            owner: 'hlw95',
            clientID: 'a0eb7de1c4dc4908943b',
            clientSecret: 'b466946cbbe843a4a6ef249eb54ae5956d2f91c3',
            admin: "hlw95",
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            proxy: 'https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token',
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">429</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">136</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">409</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T14:07:07.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/catboot/">catboost</a></p><p class="categories"><a href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a> / <a href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting/">boosting</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T14:00:04.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/http/">http</a></p><p class="categories"><a href="/categories/c/">c++</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T13:49:28.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/lucene/">lucene</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%80%92%E6%8E%92/">倒排</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T13:46:59.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/reverse-index/">倒排引擎</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%80%92%E6%8E%92/">倒排</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:56:21.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/jar/">jar</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">41</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(true){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script type="text/javascript">function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };loadMathJax();</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>