{"pages":[{"title":"","text":"个人简介 分享很喜欢的老罗的一段话： “每一个生命来到世间都注定改变世界，别无选择。要么变得好一点，要么变得坏一点。你如果走进社会为了生存为了什么不要脸的理由，变成了一个恶心的成年人社会中的一员，那你就把这个世界变得恶心了一点点。如果你一生刚正不阿，如果你一生耿直，没有做任何恶心的事情，没做对别人有害的事情，一辈子拼了老命勉强把自己身边的几个人照顾好了，没有成名没有发财，没有成就伟大的事业，然后耿着脖子一生正直，到了七八十岁耿着脖子去世了。你这一生是不是没有改变世界？你还是改变世界了，你把这个世界变得美好了一点点。因为世界上又多了一个好人。“ 善恶终有报,天道好轮回。不信抬头看,苍天饶过谁。无论何时何地，我们都要保持一颗积极乐观、善良感恩的心。但行好事莫问前程，永远年轻，永远热内盈眶，永远保持正能量。💪💪💪💪💪💪冲鸭！！！！ -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;个人信息：计算机科学与技术专业从事JAVA后端开发码畜一枚坚信代码改变世界 博客信息 网站采用的Icarus主题 追求尽可能的简洁，清晰，易用。 在Icarus主题之上进行了部分修改。 更新日志：—2020.09.20：icarus4.0适配—2020.01.18：icarus3.0适配—2019.11.17：增加深色主题开关—2019.10.30：去图，精简卡片—2019.10.22：改版部分显示，优化速度—2019.10.16：文章列表加上评论数显示—2019.10.13：改版评论—2019.09.25：图片、资源接入CDN免费jsDelivr、文章加入置顶—2019.09.19：开源博客代码—2019.09.19：修改布局，拉伸布局，更宽的展示—2019.09.18：修改友链ui为一行三个，并适配移动端，暗黑模式文章增加评论链接，增加留言链接—2019.09.14：增加精简next主题—2019.09.14：利用中秋节放假，重做了首页的热门推荐、加个widget最新评论框、归档页加入文章贡献概览面板 本站推荐索引 博客主题相关 github Issue 作为博客微型数据库的应用 github page网站cdn优化加速 博客源码分享 博客换肤的一种实现方式思路 博客中gitalk最新评论的获取 博客图片上传picgo工具github图传使用 安装、部分配置icarus主题中文版 技术知识点 Java并发知识点 法律法规 法律法规数据库 中华人民共和国国旗法 中华人民共和国宪法 中华人民共和国消费者权益保护法 中华人民共和国刑事诉讼法 中华人民共和国婚姻法 中华人名共和国网络安全法 中华人民共和国劳动法 其他 网易云音乐歌单分享 计划2020计划 2019.12.31 2020-GOALS [ ] 跑两三场马拉松 2019计划 2018.12.31/21:59:00-&gt;更新于2019.12.31 2019-GOALS [x] 购买的专业书籍至少看完一遍（并发、重构、设计模式…）-&gt; 95%额外： [x] 追了很多剧总结： 有优点有缺点，没坚持下来的还是太多，追了太多剧。以后多学习，多思考！ 时间轴记录","link":"/about/index.html"},{"title":"","text":"&nbsp;&nbsp;听听音乐 音乐播放器由mePlayer提供，布局参照网友博客所作，感谢作者的辛勤付出。更多音乐分享请查看歌单。 &nbsp;&nbsp;看看视频 ->点击以下条目开始播放视频,向下滑动查看更多","link":"/media/index.html"},{"title":"","text":"🎈🎈微笑墙🎈🎈 彭小苒 &lt;/div&gt; 唐艺昕 &lt;/div&gt; 李一桐 &lt;/div&gt; gakki &lt;/div&gt; 图片搜集于互联网，侵权请留言，马上处理😊。","link":"/album/index.html"},{"title":"","text":"来而不往非礼也畅所欲言，有留必应","link":"/message/index.html"},{"title":"","text":"申请友链须知 原则上只和技术类博客交换，但不包括含有和色情、暴力、政治敏感的网站。 不和剽窃、侵权、无诚信的网站交换，优先和具有原创作品的网站交换。 申请请提供：站点名称、站点链接、站点描述、logo或头像（不要设置防盗链）。 排名不分先后，刷新后重排，更新信息后请留言告知。 会定期清理很久很久不更新的、不符合要求的友链，不再另行通知。 本站不存储友链图片，如果友链图片换了无法更新。图片裂了的会替换成默认图，需要更换的请留言告知。 本站友链信息如下，申请友链前请先添加本站信息： 网站图标：https://removeif.github.io/images/avatar.jpg 网站名称：辣椒の酱 网站地址：https://removeif.github.io 网站简介：后端开发，技术分享 加载中，稍等几秒...","link":"/friend/index.html"},{"title":"音乐歌单收藏","text":"&lt;/meting-js&gt; 温馨提示：选择喜欢的音乐双击播放，由于版权原因部分不能播放。如果喜欢歌单收藏一下，去网易云都能播放哟！","link":"/music/index.html"},{"title":"","text":"碎碎念 tips：github登录后按时间正序查看、可点赞加❤️、本插件地址..「+99次查看」&lt;/span&gt;&lt;/div&gt; 碎碎念加载中，请稍等...&lt;/div&gt; $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '46a9f3481b46ea0129d8', clientSecret: '79c7c9cb847e141757d7864453bcbf89f0655b24', id: '666666', repo: 'issue_database', owner: 'removeif', admin: \"removeif\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/self-talking/index.html"}],"posts":[{"title":"神经网络进行二分类时，输出层使用两个神经元和只使用一个神经元，模型的性能有何差异，为什么？","text":"https://www.zhihu.com/question/397625619","link":"/2021/11/04/2-classify/"},{"title":"即席查询(Ad hoc)","text":"定义即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。 举例说明 以电商的数仓分析项目为例，有一些应用侧/业务侧的分析指标：每日活跃用户数（日活），每日留存用户数（留存），新注册用户有多少下了单（转换率），因为计算方法固定，变化的是每天的数据，因此这些指标的查询/计算SQL是提前写好的，到店被调度（Azkaban）执行即可； 但有一些指标或者临时增加的指标、临时增加的一些分析需求，是无法预知其计算逻辑的，所以要现写查询SQL，并且希望能很快拿到查询/计算结果，这就是即席查询 工具Kylin、druid、presto、impala https://zhuanlan.zhihu.com/p/266695601","link":"/2022/02/05/Ad-hoc/"},{"title":"Atlas","text":"1.概述Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理+团队，提供围绕这些数据资产的协作功能。 2.Atlas的具体功能 元数据分类 支持对元数据进行分类管理，例如个人信息，敏感信息等 元数据检索 可按照元数据类型、元数据分类进行检索，支持全文检索 血缘依赖 支持表到表和字段到字段之间的血缘依赖，便于进行问题回溯和影响分析等 1）表与表之间的血缘依赖 2）字段与字段之间的血缘依赖 3.Atlas架构原理 4.使用4.1 Hive元数据初次导入操作： Atlas提供了一个Hive元数据导入的脚本，直接执行该脚本，即可完成Hive元数据的初次全量导入。 /opt/module/atlas/hook-bin/import-hive.sh 问题： Failed to import Hive Meta Data!!! 注意：hive —service metastore &amp; 4.2 Hive元数据增量同步Hive元数据的增量同步，无需人为干预，只要Hive中的元数据发生变化（执行DDL语句），Hive Hook就会将元数据的变动通知Atlas。除此之外，Atlas还会根据DML语句获取数据之间的血缘关系。","link":"/2022/02/07/Atlas/"},{"title":"Azkaban","text":"https://blog.csdn.net/wtzhm/article/details/89220508 1 为什么需要工作流调度系统1）一个完整的数据分析系统通常都是由大量任务单元组成：Shell脚本程序，Java程序，MapReduce程序、Hive脚本等 2）各任务单元之间存在时间先后及前后依赖关系 3）为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行 2 常见工作流调度系统1）简单的任务调度 直接使用Linux的Crontab来定义； 2）复杂的任务调度 开发调度平台或使用现成的开源调度系统，比如Ooize、Azkaban、 Airflow、DolphinScheduler等。 3）Azkaban Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows. Azkaban是一个开源的任务调度系统，用于负责任务的调度运行（如数据仓库调度），用以替代linux中的crontab。 和Oozie对比 总体来说，Ooize相比Azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器Azkaban是很不错的候选对象。 3 使用 1 使用流程1.数据准备 2.编写Azkaban工作流程配置文件 ​ a.编写azkaban.project ​ b.编写gmall.flow文件 2 多Executor模式下注意事项方案一：指定特定的Executor（hadoop102）去执行任务。 ​ a.在MySQL中azkaban数据库executors表中，查询hadoop102上的Executor的id。 ​ b.在执行工作流程时加入useExecutor属性 方案二：在Executor所在所有节点部署任务所需脚本和应用。 推荐使用方案二","link":"/2022/02/04/Azkaban/"},{"title":"冷启动","text":"推荐系统冷启动 mark https://zhuanlan.zhihu.com/p/79950668","link":"/2021/10/25/Cold-start/"},{"title":"DGL","text":"https://docs.dgl.ai/api/python/nn.html 实现了常见的图卷积和图池化 以graphconv为例，不仅给出了接口和代码还有论文介绍","link":"/2021/12/27/DGL/"},{"title":"Wide&amp;Deep和DeepFM","text":"https://zhuanlan.zhihu.com/p/66928413 https://blog.csdn.net/sinat_29819401/article/details/91359217","link":"/2021/11/05/DeepFM/"},{"title":"Factorization Machines","text":"原文地址 https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf https://zhuanlan.zhihu.com/p/50426292 a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models I. INTRODUCTIONIn total, the advantages of our proposed FM are:1) FMs allow parameter estimation under very sparse data where SVMs fail.2) FMs have linear complexity, can be optimized in the primal and do not rely on support vectors like SVMs.3) FMs are a general predictor that can work with any real valued feature vector. II. PREDICTION UNDER SPARSITY III. FACTORIZATION MACHINES (FM)A. Factorization Machine Model1) 模型: \\hat{y}(x):=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^n\\sum_{j=i+1}^n \\bbox[border: 2px solid red]{w_{i,j}}x_ix_j$x_i$表示第$i$个特征，但是针对上式，一个很大的问题，用户交互矩阵往往是比较稀疏的，这样就会导致对$w_{i,j}$的估算存在很大的问题。举个例子，假如想要估计Alice(A)和Star Trek(ST)的交互参数$w_{A,ST}$，由于训练集中没有实例同时满足$x_A$和$x_{ST}$非零，这会造成$w_{A,ST}=0$。因此这里使用了矩阵分解的思想： \\textbf{W} = \\textbf{V}\\textbf{V}^T,\\textbf{V}=\\begin{pmatrix} \\textbf{v}_1 \\\\ \\textbf{v}_2\\\\...\\\\\\textbf{v}_n \\end{pmatrix} \\in {R}^{n\\times k} \\\\\\bbox[border: 2px solid red]{w_{i,j}==\\sum_{f=1}^k v_{i,f}\\cdot v_{j,f}} \\\\\\hat{y}(x):=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{i=1}^n\\sum_{j=i+1}^n \\bbox[border: 2px solid red]{}x_ix_j2) 提升效率: 直接计算上面的公式求解$\\hat{y}(x)$的时间复杂度为$O ( k n^2 ) $，因为所有的特征交叉都需要计算。但是可以通过公式变换，将时间复杂度减少到$O(kn)$，如下公式推导 \\begin{align*} \\\\\\sum_{i=1}^n\\sum_{j=i+1}^n x_ix_j&=\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n x_ix_j-\\frac{1}{2}\\sum_{i=1}^n\\ x_ix_i \\\\&=... \\\\&=\\frac{1}{2}\\sum_{f=1}^k((\\sum_{i=1}^nv_{i,f}x_i)^2-\\sum_{i=1}^nv_{i,f}^2x_i^2) \\end{align*}B. Factorization Machines as PredictorsFM can be applied to a variety of prediction tasks. Among them are: Regression，Binary classification，Ranking C. Learning Factorization Machinesthe model parameters of FMs can be learned efficiently by gradient descent methods – e.g. stochastic gradient descent (SGD).The gradient of the FM model is: \\\\\\begin{equation} \\frac{\\partial \\hat{y}(x)}{\\partial \\theta}=\\left\\{ \\begin{array}{rcl} 1& & {if \\ \\theta \\ is \\ w_0 }\\\\ x_i & & {if \\ \\theta \\ is \\ w_i}\\\\ x_i\\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 & & {if \\ \\theta \\ is \\ v_{i,f}} \\end{array} \\right. \\end{equation}D. d-way Factorization MachineThe 2-way FM described so far can easily be generalized to a d-way FM: \\hat{y}(x):=w_0+\\sum_{i=1}^nw_ix_i+\\sum_{l=2}^d\\sum_{i_l=1}^n ...\\sum_{i_l=i_{l-1}+1}^n (\\prod \\limits_{j=1}^lx_{i_j})(\\sum_{f=1}^{k_l}\\prod \\limits_{j=1}^lv_{i_j,f}^{(l)})直接计算上式的时间复杂度为$O(k_dn^d)$，利用类似上面的公式变形也可以将其降低为$O(k_d n )$ E. SummaryFMs model all possible interactions between values in the feature vector $x$ using factorized interactions instead of full parametrized ones. This has two main advantages: 1) The interactions between values can be estimated even under high sparsity. Especially, it is possible to generalize to unobserved interactions.2) The number of parameters as well as the time for prediction and learning is linear. IV. FMS VS. SVMSV. FMS VS. OTHER FACTORIZATION MODELS参考https://blog.csdn.net/qq_26822029/article/details/103993243","link":"/2021/09/30/FM/"},{"title":"ConvGNNs","text":"ConvGNNs fall into two categories, spectral-based and spatial-based. Spectral based approaches define graph convolutions by introducing filters from the perspective of graph signal processing [82] where the graph convolutional operation is interpreted as removing noises from graph signals. Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation. spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality. 谱域图卷积是空域图卷积的特例 https://zhuanlan.zhihu.com/p/139682302 https://zhuanlan.zhihu.com/p/122968925 https://blog.csdn.net/weixin_45901519/article/details/106388964 https://blog.csdn.net/weixin_45901519/article/details/106436591 https://blog.csdn.net/weixin_45901519/article/details/106492963","link":"/2021/12/27/GCN/"},{"title":"Grafana","text":"Operational dashboards for your data here, there, or anywhere Grafana是一款用Go语言开发的开源数据可视化工具，可以做数据监控和数据统计，带有告警功能。","link":"/2022/02/05/Grafana/"},{"title":"GNN核心构成","text":"GNN种类很多，包括GCN，GAEs，RecGNNs等，他们的差异在于图结构，消息传递 1.图结构同构图，异构图，结点和边的设计等 同构图：只有一种类型的节点和边 异构图：可以有不同类型的节点和边 2.消息传递消息传递是实现GNN的一种通用框架和编程范式。包含以下两个过程： 1 Message Propagation 聚合邻居节点的特征，形成一个消息向量 2 Representation Updating 更新当前时刻的节点表示 参考https://docs.dgl.ai/guide/message.html# https://zhuanlan.zhihu.com/p/352510643 https://aclanthology.org/2020.acl-main.547.pdf https://zhuanlan.zhihu.com/p/350900048 https://docs.dgl.ai/guide_cn/graph-heterogeneous.html#guide-cn-graph-heterogeneous https://zhuanlan.zhihu.com/p/376062090","link":"/2022/01/17/GNN-component/"},{"title":"Kerberos","text":"1.定义Kerberos是一种计算机网络认证协议，用来在非安全网络中，对个人通信以安全的手段进行身份认证。这个词又指麻省理工学院为这个协议开发的一套计算机软件。软件设计上采用客户端/服务器结构，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止重放攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统。 1）KDC（Key Distribute Center）：密钥分发中心，负责存储用户信息，管理发放票据。 2）Realm：Kerberos所管理的一个领域或范围。 3）Rrincipal：可以理解为Kerberos中保存的一个账号，其格式通常如下：primary/instance@realm 4）keytab：密钥文件。 有个疑问 ，对谁认证？是对不同用户吗（root，user1，user2）？ 2.认证原理 https://cloud.tencent.com/developer/article/1496451 https://blog.csdn.net/jewes/article/details/20792021 3.基本操作https://blog.csdn.net/Happy_Sunshine_Boy/article/details/102801386 1 创建管理员用户 2 注册 3 认证 4.HADOOP配置 https://www.cnblogs.com/yjt1993/p/11769515.html https://makeling.github.io/bigdata/39395030.html 访问HDFS集群文件 Shell命令 kinit admin/admin klist web页面 1.安装Kerberos客户端 2.配置火狐浏览器 3.认证 5.HIVE配置 https://zhuanlan.zhihu.com/p/137424234 客服端访问 beeline 0.首先需使用hive用户启动hiveserver2 [root@hadoop102 ~]# sudo -i -u hive hiveserver2 1.认证，执行以下命令，并按照提示输入密码 [atguigu@hadoop102 ~]$ kinit atguigu 2.使用beeline客户端连接hiveserver2 [atguigu@hadoop102 ~]$ beeline 3.使用如下url进行连接 1!connect jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM DataGrip客户端 https://blog.csdn.net/github_39319229/article/details/112692897 经常连接不稳定，连接失败可以尝试重启DataGrip 6.数仓此处统一将数仓的全部数据资源的所有者设为hive用户，全流程的每步操作均认证为hive用户。 7.即席查询8.sparkhttps://www.cnblogs.com/bainianminguo/p/12639887.html 9 hbase1 hbase shell kerberos认证错误 1root:kinit atguigu 参考https://blog.csdn.net/jewes/article/details/20792021","link":"/2022/02/06/Kerberos/"},{"title":"Kylin","text":"架构 Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。 1）REST Server REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。 2）查询引擎（Query Engine） 当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 3）路由器（Routing） 在最初设计时曾考虑过将Kylin不能执行的查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。 4）元数据管理工具（Metadata） Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 5）任务引擎（Cube Build Engine） 这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。 Kylin Cube构建原理，构建优化https://jishuin.proginn.com/p/763bfbd2bb9c BI工具集成可以与Kylin结合使用的可视化工具很多，例如： ODBC：与Tableau、Excel、PowerBI等工具集成 JDBC：与Saiku、BIRT等Java工具集成 RestAPI：与JavaScript、Web网页集成 Kylin开发团队还贡献了Zepplin的插件，也可以使用Zepplin来访问Kylin服务。","link":"/2022/02/05/Kylin/"},{"title":"排序学习","text":"Literature survey for Learning to rank https://www.eecis.udel.edu/~vijay/fall13/snlp/lit-survey/LearningToRank.pdf a short introduction to learning to rank（李航） https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en Learning to Rank for Information Retrieval — By Tie-Yan Liu http://didawikinf.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/ir/ir13/1_-_learning_to_rank.pdf Feature Selection For Ranking https://dl.acm.org/doi/10.1145/1277741.1277811 A Deep Look into Neural Ranking Models for Information Retrieval 中科院 https://par.nsf.gov/servlets/purl/10277191 相关参考： https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en https://blog.csdn.net/anshuai_aw1/article/details/86018105 https://blog.csdn.net/pearl8899/article/details/102920628 https://blog.csdn.net/lipengcn/article/details/80373744 分类","link":"/2021/08/16/L2R/"},{"title":"极大似然估计","text":"1.定义就是利用已知的样本结果信息，反推最具有可能导致这些样本结果出现的模型参数值。换句话说，即：“模型已定，结果已知，反推参数”。 2.极大似然构造损失函数大多数常见的损失函数就是基于极大似然推导的。例子参考 https://www.cnblogs.com/hello-ai/p/11000899.html 判别模型下的极大似然估计 最大似然估计很容易扩展到估计条件概率$P\\left (y|x;\\theta \\right)$，从而给定$x$预测$y$。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。如果$X$表示所有的输入，$Y$表示我们观测到的目标，那么条件最大似然估计是： \\theta_{ML} = \\mathop\\arg\\max_{\\theta}P\\left(Y|X;\\theta \\right)如果假设样本是独立同分布的，那么这可以分解成 \\theta_{ML} = \\mathop\\arg\\max_{\\theta}\\sum_{i=1}^m logP\\left(y^{(i)}|x^{(i)};\\theta \\right)生成模型下的极大似然估计 考虑一组含有m个样本的数据集$X = \\left \\{ x^{(1)}, …, x^{(m)} \\right \\}$，由$p_{data}(x)$生成，独立同分布 对独立同分布的样本，生成样本集$X$的概率如下: p_{model}(X; \\theta)= \\prod _{i=1}^m p_{model}\\left (x^{(i)}; \\theta \\right )对$\\theta$的最大似然估计被定义为： \\theta_{ML} = \\mathop{\\arg\\max}_{\\theta}p_{model}\\left (X;\\theta \\right ) = \\mathop{\\arg\\max}_{\\theta}\\prod _{i=1}^m p_{model}\\left (x^{(i)}; \\theta \\right )多个概率的乘积公式会因很多原因不便于计算。例如，计算中很可能会因为多个过小的数值相乘而出现数值下溢。为了得到一个便于计算的等价优化问题，两边取对数： \\theta_{ML} = \\mathop{\\arg\\max}_{\\theta}\\sum_{i=1}^{m}logp_{model}\\left (x^{(i)};\\theta\\right ) 可以发现，使用极大似然估计时，每个样本$x^{(i)}$都希望拉高它所对应的模型概率值$p_{model}(x^{(i)};\\theta)$，如上图所示，但是由于所有样本的密度函数$p_{model}(x^{(i)};\\theta)$的总和必须是1，所以不可能将所有样本点都拉高到最大的概率，一个样本点的概率密度函数值被拉高将不可避免的使其他点的函数值被拉低，最终的达到一个平衡态。我们也可以将上式除以$m$，便可以看到极大似然法最大化的目标是在经验分布$\\widehat{p}_{data}$下样本概率对数的期望值，即 \\theta_{ML} = \\mathop{\\arg\\max}_{\\theta}E_{x\\sim \\widehat{p}_{data}}logp_{model}\\left (x^{(i)};\\theta \\right )参考https://zhuanlan.zhihu.com/p/26614750 https://www.cnblogs.com/hello-ai/p/11000899.html https://blog.csdn.net/hustqb/article/details/77168436 https://zhuanlan.zhihu.com/p/273246971","link":"/2021/08/29/MLE/"},{"title":"多路召回","text":"1.定义 所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后再把这些候选集混合在一起后供后续排序模型使用的策略。值得注意的是，每一路召回需要尽可能的保持独立性与互斥性，从而在保证各链路能够并行召回的同时，增加召回的多样性。 2.多路召回融合策略（可以算是粗排） 平均法：C的计算方法：(0.7 + 0.5 + 0.3)/3 加权平均：假设三种策略的权重指定为0.4、0.3、0.2（人为给定或者算法拟合），则B的权重为（0.4 0.8 + 0.3 0.6 + 0.2* 0）/ （0.4+0.3+0.2） 动态加权法:计算三种召回策略的CTR，作为每天更新的动态权重。但是只考虑了点击率，并不全面。 3.例子https://tianchi.aliyun.com/notebook-ai/detail?postId=144452 参考https://zhuanlan.zhihu.com/p/388601198","link":"/2021/10/08/Multiple-recall/"},{"title":"NER","text":"Named Entity Recognition，命名实体识别 旨在从文本中抽取出命名实体，比如人名、地名、机构名等 分类 文本数据标注为什么标注？说白了就是标签 https://blog.csdn.net/scgaliguodong123_/article/details/121303421 举个例子： 1234567891011121314151617181920212223BIO-三位序列标注法(B-begin，I-inside，O-outside)B-X代表实体X的开头 x:PER(person) , ORG(orgnization),LOC(location)I-X代表实体X的中间或结尾O代表不属于任何类型的样例： 我 O 是 O 李 B-PER 果 I-PER 冻 I-PER ， O 我 O 爱 O 中 B-ORG 国 I-ORG ， O 我 O 来 O 自 O 四 B-LOC 川 I-LOC 。 O 参考https://www.cnblogs.com/huangyc/p/10064853.html https://www.cnblogs.com/YoungF/p/13488220.htmlhttps://www.cnblogs.com/YoungF/p/13488220.html https://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html https://zhuanlan.zhihu.com/p/156914795 https://blog.csdn.net/scgaliguodong123_/article/details/121303421","link":"/2021/11/26/NER/"},{"title":"Negative Sampling 负采样","text":"首先区别于欠采样 ( under sampling )和过采样 (oversampling) 作用： 减少计算量，调高训练效率 是什么 负采样，顾名思义，就是从一堆负样本中采样出一部分负样本，用于模型的训练。 1 作用在训练时候 也就是说在训练的时候采样 一个全连接网络为100X10X100,多分类，100选1，也就是说输出层只有一个正样本，99个负样本，为了减少计算量，每次只选部分负样本，比如5个，那么梯度更新的时候，只更新正样本和5个负样本的，这样还剩94个就不更新了 2 作用在训练前面 也就是说在训练前，样本已经采好了 分类 在负采样过程中，有几个问题需要重点考虑：（1）这么多负样本中，到底需要采出哪一部分作为负样本呢（2）需要采出多大数量的负样本？ https://kaiyuan.blog.csdn.net/article/details/122264543 https://zhuanlan.zhihu.com/p/456088223 参考https://kaiyuan.blog.csdn.net/article/details/122264543 https://blog.csdn.net/ningyanggege/article/details/87869393 https://zhuanlan.zhihu.com/p/456088223","link":"/2022/05/21/Negative-sample/"},{"title":"NLP子任务的评价指标","text":"1.文本分类采用分类任务的评价指标，比如accuracy，recall，F1等 2.文本匹配重点说下一些paper的sts（Semantic Textual Similarity）任务，为什么采用相关系数（Pearson correlation或者spearman correlation）来衡量，比如 S-bert https://arxiv.org/abs/1908.10084 ，consert https://arxiv.org/abs/2105.11741 。 这是因为S-bert和consert 都是文本表示的方法，最后计算文本相似度是利用余弦相似度计算的，相似度的值域为0-1，但是sts数据集的相似度值域为0-5。值域范围不同，不能直接进行比较，用相关系数来间接评价。 3.文本生成https://zhuanlan.zhihu.com/p/144182853 https://arxiv.org/pdf/2006.14799.pdf 文本改写（算是特殊的生成） https://aclanthology.org/2020.findings-emnlp.111.pdf https://arxiv.org/pdf/1909.01187.pdf Exact score: percentage of exactly correctly predicted fusions SARI: the average F1 scores of the added, kept, and deleted n-grams 4.文本表示https://arxiv.org/pdf/1908.10084.pdf SentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings.","link":"/2021/10/20/NLP-task/"},{"title":"深度学习中的五种归一化（BN、LN、IN、GN和SN）方法简介","text":"mark https://blog.csdn.net/u013289254/article/details/99690730 https://blog.csdn.net/qq_35290785/article/details/95879067 https://zhuanlan.zhihu.com/p/34879333","link":"/2021/10/18/Normalization/"},{"title":"Presto","text":"概念 架构 Presto优缺点 Presto、Impala性能比较https://blog.csdn.net/u012551524/article/details/79124532 测试结论：Impala性能稍领先于Presto，但是Presto在数据源支持上非常丰富，包括Hive、图数据库、传统关系型数据库、Redis等。","link":"/2022/02/05/Presto/"},{"title":"Pre-train, Prompt, and Predict A Systematic Survey of Prompting Methods in Natural Language Processing","text":"0 和pre-train，finetune区别 prompt感觉是一种特殊的finetune方式，还是先pre-train然后prompt tuning 目的：prompt narrowing the gap between pre-training and fine-tuning 1 怎么做3步 1 Prompt Addition $x^{‘}=f_{prompt}(x)$ x是input text Apply a template, which is a textual string that has two slots: an input slot [X] for input x and an answer slot[Z] for an intermediate generated answer text z that will later be mapped into y. Fill slot [X] with the input text x. 2 Answer Search f：fills in the location [Z] in prompt $x^{‘}$ with the potential answer z Z：a set of permissible values for z 3 Answer Mapping因为上面的 $\\hat{z}$ 还不是 $\\hat{y}$，比如情感分析，“excellent”, “fabulous”, “wonderful” -》positive go from the highest-scoring answer $\\hat{z}$ to the highest-scoring output $\\hat{y}$ 4 举个例子，文本情感分类的任务原来 “ I love this movie.” -》 positive 现在 1 $x=$ “ I love this movie.” -》模板为： “ [x] Overall, it was a [z] movie.” -》$x^{‘}$为”I love this movie. Overall ,it was a [z] movie.” 2 下一步会进行答案搜索，顾名思义就是LM寻找填在[z] 处可以使得分数最高的文本 $\\hat{z}$(比如”excellent”, “great”, “wonderful” ) 3 最后是答案映射。有时LM填充的文本并非任务需要的最终形式(最终为positive，上述为”excellent”, “great”, “wonderful”)，因此要将此文本映射到最终的输出$\\hat{y}$ 2 Prompt方法分类 3 Prompt Engineering1 one must first consider the prompt shape, 2 then decide whether to take a manual or automated approach to create prompts of the desired shape 1 Prompt ShapePrompt的形状主要指的是[X]和[Z]的位置和数量。 如果在句中，一般称这种prompt为cloze prompt；如果在句末，一般称这种prompt为prefix prompt。 在实际应用过程中选择哪一种主要取决于任务的形式和模型的类别。cloze prompts和Masked Language Model的训练方式非常类似，因此对于使用MLM的任务来说cloze prompts更加合适；对于生成任务来说，或者使用自回归LM解决的任务，prefix prompts就会更加合适；Full text reconstruction models较为通用，因此两种prompt均适用。另外，对于文本对的分类，prompt模板通常要给输入预留两个空，[x1]和[x2]。 2 create prompts1 Manual Template Engineering2 Automated Template Learning1 Discrete Promptsthe prompt 作用在文本上 D1: Prompt Mining D2: Prompt Paraphrasing D3: Gradient-based Search D4: Prompt Generation D5: Prompt Scoring 2 Continuous Promptsthe prompt 直接作用到模型的embedding空间 C1: Prefix Tuning C2: Tuning Initialized with Discrete Prompts C3: Hard-Soft Prompt Hybrid Tuning 4 Answer Engineeringtwo dimensions that must be considered when performing answerengineering:1 deciding the answer shape and 2 choosing an answer design method. 1 Answer Shape和Prompt Shape啥区别？？？ 2 Answer Space Design Methods1 Manual Design2 automatic automatic1 Discrete Answer Search2 Continuous Answer Search5 Multi-Prompt Learning之前在讨论single prompt，现在介绍multiple prompts 6 Training Strategies for Prompting Methods1 Training Settings full-data few-shot /zero-shot 2 Parameter Update Methods 参考https://arxiv.org/abs/2107.13586 刘鹏飞博士 https://zhuanlan.zhihu.com/p/395115779 https://zhuanlan.zhihu.com/p/399295895 https://zhuanlan.zhihu.com/p/440169921 https://zhuanlan.zhihu.com/p/399295895","link":"/2021/12/12/Prompt_survey/"},{"title":"T5","text":"T5 原文 https://arxiv.org/pdf/1910.10683.pdf https://zhuanlan.zhihu.com/p/88438851 mT5 原文 https://arxiv.org/pdf/2010.11934.pdf https://zhuanlan.zhihu.com/p/302380842 Sentence-T5（文本表示新SOTA） 原文 https://arxiv.org/pdf/2108.08877.pdf https://zhuanlan.zhihu.com/p/403153114","link":"/2021/10/26/T5/"},{"title":"Zabbix","text":"概述Zabbix是一款能够监控各种网络参数以及服务器健康性和完整性的软件。Zabbix使用灵活的通知机制，允许用户为几乎任何事件配置基于邮件的告警。这样可以快速反馈服务器的问题。基于已存储的数据，Zabbix提供了出色的报告和数据可视化功能。 基础架构","link":"/2022/02/05/Zabbix/"},{"title":"对比学习在NLP应用","text":"https://zhuanlan.zhihu.com/p/435367182 1.应用在预训练 Pre-trained Models for Natural Language Processing A Survey https://arxiv.org/pdf/2003.08271v4.pdf 2.应用在finetune simcse，consert","link":"/2021/11/22/acl-contrasive-nlp/"},{"title":"常见激活函数","text":"作用：激活函数是来向神经网络中引入非线性因素的，通过激活函数，神经网络就可以拟合各种曲线 1.sigmoid \\begin{align*} y&=\\frac{1}{1+e^{-x}} \\\\y^{'}&=\\frac{1}{1+e^{-x}}(1-\\frac{1}{1+e^{-x}})=y(1-y) \\end{align*}一般应用在二分类的输出层 缺点： ​ 1.sigmoid 极容易导致梯度消失问题，可以从导数曲线可以看出，绝大多数的导数值为0 ​ 2.Sigmoid 函数的输出不是以零为中心的（non-zero-centered），这会导致神经网络收敛较慢，详细原因请参考 https://liam.page/2018/04/17/zero-centered-active-function/ 2.softmax S_i=\\frac{e^i}{\\sum_je^j}和sigmoid关系：Softmax函数是二分类函数Sigmoid在多分类上的推广 https://zhuanlan.zhihu.com/p/356976844 3.tanh \\begin{align*} y&=tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} \\\\y^{'}&=1-(tanh(x))^{2} \\end{align*}优点: ​ 1.tanh解决了sigmoid中的 zero-centered 问题 缺点： ​ 2.对于梯度消失问题依旧无能为力。 4.Relu系列4.1 Relu \\begin{align*} y&=max(0,x) \\\\ y^{'}&=\\left\\{ \\begin{array}{cl} 1 & \\ x \\ge 0 \\\\ 0 & \\ x < 0 \\\\ \\end{array} \\right. \\end{align*}优点: ​ 1.可以缓解梯度消失，因为导数在正数部分是恒等于1的 缺点： ​ 1.Relu的输出不是zero-centered ​ 2.由于负数部分导数恒为0，会导致一些神经元无法激活，叫做Dead ReLU Problem 4.2 leaky Reluleaky Relu就是为了解决Relu的0区间带来的影响，其数学表达为： \\begin{align*} y&=\\left\\{ \\begin{array}{cl} x & \\ x \\ge 0 \\\\ kx & \\ x < 0 \\\\ \\end{array} \\right. \\\\ y^{'}&=\\left\\{ \\begin{array}{cl} 1 & \\ x \\ge 0 \\\\ k & \\ x < 0 \\\\ \\end{array} \\right. \\end{align*}其中$k$是为超参数，一般数值较小，比如0.01 4.3 Elu Elu激活函数也是为了解决Relu的0区间带来的影响，其数学表达为： \\begin{align*} y&=\\left\\{ \\begin{array}{cl} x & \\ x \\ge 0 \\\\ \\alpha(e^{x}-1) & \\ x < 0 \\\\ \\end{array} \\right. \\\\ y^{'}&=\\left\\{ \\begin{array}{cl} 1 & \\ x \\ge 0 \\\\ \\alpha e^{x} & \\ x < 0 \\\\ \\end{array} \\right. \\end{align*}Elu相对于leaky Relu来说，计算要更耗时间一些 参考https://zhuanlan.zhihu.com/p/44398148 https://liam.page/2018/04/17/zero-centered-active-function/ https://www.cnblogs.com/tornadomeet/p/3428843.html https://www.cnblogs.com/chamie/p/8665251.html https://zhuanlan.zhihu.com/p/33006526?from_voters_page=true","link":"/2021/09/06/activate-func/"},{"title":"ad hoc和routing","text":"0 区别https://blog.csdn.net/memray/article/details/41149633 ad hoc类似于图书馆里的书籍检索，即书籍库(数据库)相对稳定不变，不同用户的查询要求是千变万化的。 routing的情况与ad hoc相对，用户的查询要求相对稳定。在routing中，查询常常称为pro file，也就是通常所说的兴趣，用户的兴趣在一段时间内是稳定不变的，但是数据库(更确切的说，是数据流)是不断变化的。这种任务很像我们所说的新闻定制什么的，比如用户喜欢体育，这个兴趣在一段时间内是不变的，而体育新闻在不断变化。 1 ad hochttps://ils.unc.edu/courses/2017_fall/inls509_002/lectures/03-IntroductionToAdhocRetrieval.pdf Ad-hoc Retrieval 是什么 Given a query and a corpus, find the relevant items ‣ query: textual description of information need ‣ corpus: a collection of textual documents ‣ relevance: satisfaction of the user’s information need 2 routing","link":"/2022/04/01/ad-doc-routing/"},{"title":"loss不下降的解决方法","text":"https://blog.csdn.net/zongza/article/details/89185852","link":"/2021/11/10/adjust-loss/"},{"title":"调参","text":"1. 调什么参数1 训练层面 0 权重初始化 1 学习率 2 batch size 3 epoch 4 dropout 5 正则化 6 优化算法 2 模型层面 1 激活函数 2 网络尺寸 2. 超参数怎么调1.手动调参 经验值 2.自动化调参 ​ a.网格搜索 超参数排序组合，如果有n个参数，每个参数都有m个候选值，那么网格搜索中就要训练m的n次方个模型。 ​ b.随机搜索 比起网格搜索：1、搜索次数少，快 2. 因为有偶然性，可能不是最优 ​ c.贝叶斯优化 https://zhuanlan.zhihu.com/p/146633409 Bayesian optimization algorithm，简称BOA 网格搜索和随机搜索，每次都是相互独立的，贝叶斯优化利用之前已搜索点的信息确定下一个搜索点 参考https://zhuanlan.zhihu.com/p/340578370 https://www.jianshu.com/p/92d8943fb0ba https://zhuanlan.zhihu.com/p/146633409 https://blog.csdn.net/weixin_45884316/article/details/109828084 https://www.cnblogs.com/zingp/p/11352012.html#_label8 https://www.jianshu.com/p/71f39c2ea512","link":"/2021/11/04/adjust-papameter/"},{"title":"ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","text":"There are three main contributions that ALBERT makes over the design choices of BERT： 1 Factorized embedding parameterization 原来embedding层是一个矩阵$M_{emb[V\\times H]} $,现在变为两个$M_{emb1[V\\times E]}$和$M_{emb2[E\\times H]}$,参数量从VH变为VE+EH（This parameter reduction is significant when H &gt;&gt; E.） 2 Cross-layer parameter sharing The default decision for ALBERT is to share all parameters across layers（attention，FFN)） 3 Inter-sentence coherence loss 原来的NSP改为现在的sop，正例的构建和NSP是一样的，不过负例则是将两句话反过来。 参考 https://zhuanlan.zhihu.com/p/88099919 https://blog.csdn.net/weixin_37947156/article/details/101529943 https://openreview.net/pdf?id=H1eA7AEtvS","link":"/2021/11/04/albert/"},{"title":"answer select","text":"1 问题定义 Given a question and a set of candidate sentences, the task is to identify candidate sentences that contain the correct answer to the question. From the definition, the problem can be formulated as a ranking problem, where the goal is to give better rank to the candidate sentences that are relevant to the question. 2 博客： https://zhuanlan.zhihu.com/p/39920446 3 A Review on Deep Learning Techniques Applied to Answer Selection https://aclanthology.org/C18-1181.pdf 4 BAS: An Answer Selection Method Using BERT Language Model https://arxiv.org/ftp/arxiv/papers/1911/1911.01528.pdf","link":"/2021/11/26/anser-select/"},{"title":"it的技术场景","text":"前后端 指的web前后端 移动端 ios和安卓 服务端 ，客户端 客户端（Client）是指与服务器相对应并为客户提供本地服务的程序。 除了仅在本地运行的某些应用程序外，它们通常安装在普通客户端上，并且需要与服务器一起使用。 服务端：顾名思义是服务的，客户端发送的请求交给服务器端处理，是以response对象存在，服务端处理完毕后反馈给客户端。","link":"/2022/03/06/application/"},{"title":"attention总结","text":"1.Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 提出了两种 attention 模式，即 hard attention 和 soft attention 2.Effective Approaches to Attention-based Neural Machine Translation 文章提出了两种 attention 的改进版本，即 global attention 和 local attention。 3.Attention Is All You Need 提出self attention 4.Hierarchical Attention Networks for Document Classification 提出了Hierarchical Attention用于文档分类 5.Attention-over-Attention Neural Networks for Reading Comprehension 提出了Attention Over Attention的Attention机制 6.Convolutional Sequence to Sequence Learning 论文中还采用了 Multi-step Attention","link":"/2021/10/12/attention/"},{"title":"AutoTokenizer和BertTokenizer区别","text":"https://github.com/huggingface/transformers/issues/5587","link":"/2021/12/09/autotokenizer/"},{"title":"auxiliary loss(辅助损失)","text":"多任务学习中经常可以看到 参考https://blog.nowcoder.net/n/600d5e96508d4b818009227fdc9b8cbc","link":"/2022/05/21/auxiliary-loss/"},{"title":"basic algorithm","text":"快排核心思想：取一个数作为基准，比这个数小的放在左边，大的放在右边，然后左边重复，右边重复。 基准元素，一般来说选取有几种方法 取第一个元素 取最后一个元素 取第中间位置元素 实现方式：一般递归+双指针 伪代码 12341：首先取序列第一个元素为基准元素pivot=R[low]。i=low,j=high。2：从后向前扫描，找小于等于pivot的数，如果找到，R[i]与R[j]交换，i++。3：从前往后扫描，找大于pivot的数，如果找到，R[i]与R[j]交换，j--。 4: 重复2~3，直到i=j,返回该位置mid=i,该位置正好为pivot元素。 完成一趟排序后，以mid为界，将序列分为两部分，左序列都比pivot小，有序列都比pivot大，然后再分别对这两个子序列进行快速排序。 时间复杂度（0nlogn） 参考： https://zhuanlan.zhihu.com/p/63227573","link":"/2022/05/18/basic-algorithm/"},{"title":"Bert系列之句向量生成","text":"https://zhuanlan.zhihu.com/p/444346578 1 sentence-bertsts任务，数据分为sts无标签数据，sts有标签数据，nli有标签 无监督,有监督loss一样，文中有3种loss，区别在于数据集 无监督:nli有标签;有监督:sts有标签数据 2 simcsests任务，数据分为sts无标签数据，sts有标签数据 无监督，有监督区别在于：样本构造不同 无监督样本正负来源于sts无标签数据数据增强，有监督样本正负来源于sts有标签数据 3 consertsts任务，数据分为sts无标签数据，sts有标签数据，还有nli数据集（有标签） 相同 和simcse相同之处：都是在finetune引入对比 不同 1 无监督 和simces loss一样为NT-Xent，不同在于sts无标签数据数据增强方式不同 2 有监督 区别在于loss和数据源 simcse loss为NT-Xent，数据源为sts有标签数据 consert loss为 NT-Xent + 别的有监督loss（比如cross entropy），数据源为sts无标签数据和nli数据集（有标签），+表示融合 ，论文有3种融合方式","link":"/2021/12/21/bert-emb/"},{"title":"分类回归模型总结","text":"很多的深度模型都属于表示学习，是为了得到好的特征表示，比如文本表示之类的模型，有了好的特征表示，才能增强分类或者回归的效果。某些端到端的模型其实可以拆解成几个部分，比如前置的环节包括了特征提取，特征表示，然后顶层是分类或者回归层。下文总结的是纯粹的分类和回归的模型。 1.分类https://www.jianshu.com/p/169dc01f0589 2.回归https://blog.csdn.net/ChenVast/article/details/82107490","link":"/2021/10/29/basic-regression-classfify/"},{"title":"bert_serving","text":"bert词向量服务，生成词向量并聚类可视化 参考https://www.jianshu.com/p/61323d366f7c","link":"/2022/07/01/bert-serving/"},{"title":"BertGCN Transductive Text Classification by Combining GCN and BERT","text":"origin paper： https://arxiv.org/abs/2105.05727 ori code git： https://github.com/ZeroRin/BertGCN 官方知乎： https://zhuanlan.zhihu.com/p/378798855 TextGCN： https://arxiv.org/abs/1809.05679 图结构 we construct a heterogeneous graph containing both word nodes and document nodes following TextGCN. 如下图 node ：word nodes and document nodes edge ： We build edges among nodes based on word occurrence in documents (document-word edges) and word co-occurrence in the whole corpus (word-word edges) edge weight 也和TextGCN一样 node data 不同 重点是解决了TextGCN和BERT一起联调的收敛问题","link":"/2022/01/17/bert-gcn/"},{"title":"bert(Pre-training of Deep Bidirectional Transformers for Language Understanding)","text":"https://arxiv.org/abs/1810.04805 1 结构 整体结构如上图，基本单元为Transformer 的encoder部分。作者对结构的描述为：BERT’s model architecture is a multi-layer bidirectional Transformer encoder。 2 Input/Output Representations [CLS]表征句子开始，[SEP]表示句子结束以及分割两个句子 Token Embedding为词向量的表示，Position Embedding为位置信息，Segment Embedding表示A，B两句话，最后的输入向量为三者相加。比起transformer多一个Segment Embedding。 具体例子：https://www.cnblogs.com/d0main/p/10447853.html 3 预训练任务 1 Masked LM standard conditional language models can only be trained left-to-right or right-to-left , since bidirectional conditioning would allow each word to indirectly “see itself”.In order to train a deep bidirectional representation,MLM The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. 2 Next Sentence Prediction (NSP) In order to train a model that understands sentence relationships choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext). 4 Fine-tuning BERT For each task, we simply plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end. 输入: 可以为句子对或者单句，取决于特定任务 输出：At the output, the token representations are fed into an output layer for token level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis. 5 常见问题1 bert为什么双向，gpt单向？ 1.结构的不同 因为BERT用了transformer的encoder，在编码某个token的时候同时利用了其上下文的token，但是gptT用了transformer的decoder，只能利用上文 2.预训练任务的不同 2 为什么bert长度固定？ 因为bert是基于transformer encoder的，不同位置的词语都是并行的，所以长度要提前固定，不可变 bert的输入输出长度为max_length,大于截断，小于padding，max_length的最大值为512 3 为什么bert需要补充位置信息？ 因为是并行，不像迭代，没有天然的位置信息，需要补充position embedding。","link":"/2021/07/20/bert/"},{"title":"Pre-Training with Whole Word Masking for Chinese BERT","text":"BERT-wwm-ext wwm：whole word mask ext： we also use extended training data (mark with ext in the model name) 预训练1 改变mask策略 Whole Word Masking，wwm cws: Chinese Word Segmentation 对比四种mask策略 参考Pre-Training with Whole Word Masking for Chinese BERT https://arxiv.org/abs/1906.08101v3 Revisiting Pre-trained Models for Chinese Natural Language Processing https://arxiv.org/abs/2004.13922 github：https://hub.fastgit.org/ymcui/Chinese-BERT-wwm","link":"/2021/11/04/bert-wwm/"},{"title":"bertviz:attention可视化工具","text":"看不同layer，不同head的attention 注意： 12345678910111213141516from bertviz.neuron_view import showfrom bertviz.transformers_neuron_view import BertModel, BertTokenizermodel1=BertModel.from_pretrained(path)model_type = 'bert'show(model1, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)可以###########################from bertviz.neuron_view import showfrom transformers import BertTokenizer, BertModelmodel1=BertModel.from_pretrained(path)model_type = 'bert'show(model1, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)报错 参考https://zhuanlan.zhihu.com/p/457043243","link":"/2022/06/30/bertviz/"},{"title":"大数据组件","text":"大数据组件分类 发行版本 免费：Apache 收费：（CDH，HDP，二合一），阿里云，亚马逊云，华为云等","link":"/2022/01/18/bigdata-component/"},{"title":"大数据","text":"mark https://github.com.cnpmjs.org/heibaiying/BigData-Notes","link":"/2021/10/14/bigdata/"},{"title":"bm25（best matching）","text":"是TD-IDF的优化版本 https://zhuanlan.zhihu.com/p/79202151 https://blog.csdn.net/weixin_42486623/article/details/121498706","link":"/2021/11/26/bm25/"},{"title":"GB, GBDT, XGBoost, LightGBM","text":"GB：boosting算法，是一种算法思想 GBDT ：说白了就是gradient boosting基学习器为cart回归树 XGBoost：GBDT的高效实现 LightGBM：GBDT的高效实现，对XGBoost改进 参考https://www.jianshu.com/p/765efe2b951a","link":"/2022/06/09/boost-relation/"},{"title":"bp算法","text":"Error Back Propagation 两个过程：1.信号的正向传播 2.误差的反向传播","link":"/2022/05/28/bp/"},{"title":"离线数仓搭建例子(电商为例)","text":"0 架构 1.数据来源1.1 用户行为数据用户在使用产品过程中，通过埋点收集与客户端产品交互过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。用户行为数据通常存储在日志文件中。 我们的日志结构大致可分为两类，一是普通页面埋点日志，二是启动日志。 普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。 启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。 1.2 业务数据就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在MySQL、Oracle等数据库中。 总共47张表，选了27张业务表 2 数据采集1 用户行为数据 jar-》log（日志服务器）-》flume-》kafka-》flume-》hdfs 2 业务数据 jar-》mysql-》sqoop-》hdfs 3.ODShdfs（ori）-》hdfs（ods） 1 用户行为数据1张表，topic_log -&gt; ods_log a.建表 就一个字段”line“ b 分区 首日，每日都是全量 c .数据装载 2 业务数据27张表 0 整体 ​ b 同步 ​ c .数据装载 1.活动信息表 ​ activity_info -&gt; ods_activity_info ​ a 建表 ​ 少了个“activity_desc”，多了”dt” ​ b 同步 ​ 首日，每日都是全量 ​ c .数据装载 4.DIMhdfs（ods）-》hdfs（dim） 构建6张维度表 1 商品维度表 ​ a 建表 ​ b 分区 ​ 首日，每日都是全量 ​ c 数据装载 2 优惠券维度表 3 活动维度表 4 地区维度表 ​ b 分区 ​ 地区维度表数据相对稳定，变化概率较低，故无需每日装载，首日全量 5 时间维度表 ​ b 分区 ​ 通常情况下，时间维度表的数据并不是来自于业务系统，而是手动写入，并且时间维度表数据具有可预见性，无须每日导入，一般首日可一次性导入一年的数据 ​ c 数据装载 ​ 1）创建临时表tmp_dim_date_info ​ 2）将数据文件上传到HFDS上临时表指定路径/warehouse/gmall/tmp/tmp_dim_date_info/ ​ 3）执行以下语句将其导入时间维度表 1insert overwrite table dim_date_info select * from tmp_dim_date_info; 6 用户维度表 ​ b 分区 ​ 拉链表 https://cloud.tencent.com/developer/article/1752848# ​ c 数据装载 5.DWDhdfs（ods）-》hdfs（dwd） 1 用户行为日志0 日志数据拆解 ods_log由两部分构成，分别为页面日志和启动日志，拆解成5张表 1 启动日志表 ​ b 分区 ​ 首日，每日全量 ​ c 数据装载 2 页面日志表 3 动作日志表 4 曝光日志表 5 错误日志表 2 业务数据1 评价事实表（事务型事实表） ​ b 分区 ​ c 数据装载 ​ 首日，动态分区；每日，静态分区 2 订单明细事实表（事务型事实表） 3 退单事实表（事务型事实表） 4 加购事实表（周期型快照事实表） ​ b 分区 ​ c 数据加载 5 收藏事实表（周期型快照事实表） 6 优惠券领用事实表（累积型快照事实表） ​ b 分区 ​ c 数据加载 ​ （1）首日 123456789101112131415insert overwrite table dwd_coupon_use partition(dt)select id, coupon_id, user_id, order_id, coupon_status, get_time, using_time, used_time, expire_time, coalesce(date_format(used_time,'yyyy-MM-dd'),date_format(expire_time,'yyyy-MM-dd'),'9999-99-99')from ods_coupon_usewhere dt='2020-06-14'; ​ （2）每日 7 支付事实表（累积型快照事实表） 8 退款事实表（累积型快照事实表） 9 订单事实表（累积型快照事实表） 6.DWShdfs（dwd）-》hdfs（dws） 0 整体 ​ b 分区 ​ ​ c 数据装载 ​ 1 访客主题 2 用户主题 3 商品主题 4 优惠券主题 5 活动主题 6 地区主题 7.DWThdfs（dws）-》hdfs（DWT） 0 整体 c 数据装载 只保留当天和前一天的分区，过时的需要清理掉i 1 访客主题 2 用户主题 3 商品主题 4 优惠券主题 5 活动主题 6 地区主题 8.ADShdfs（DWT）-》hdfs（ADS）-》mysql 1 访客主题1 访客统计 a 建表 指标 说明 对应字段 访客数 统计访问人数 uv_count 页面停留时长 统计所有页面访问记录总时长，以秒为单位 duration_sec 平均页面停留时长 统计每个会话平均停留时长，以秒为单位 avg_duration_sec 页面浏览总数 统计所有页面访问记录总数 page_count 平均页面浏览数 统计每个会话平均浏览页面数 avg_page_count 会话总数 统计会话总数 sv_count 跳出数 统计只浏览一个页面的会话个数 bounce_count 跳出率 只有一个页面的会话的比例 bounce_rate b 分区 c 数据装载 第一步：对所有页面访问记录进行会话的划分。 第二步：统计每个会话的浏览时长和浏览页面数。 第三步：统计上述各指标。 2 路径分析 用户访问路径的可视化通常使用桑基图 2 用户主题3 商品主题4 订单主题5 优惠券主题6 活动主题9.Azkaban全流程调度 就是将原来写的脚本文件串起来","link":"/2022/02/03/build-datawarehouse/"},{"title":"CDC（Change Data Capture）工具对比","text":"https://blog.csdn.net/u013411339/article/details/120917907","link":"/2022/04/23/cdc/"},{"title":"中文词粒度BERT","text":"1 Is Word Segmentation Necessary for Deep Learning of Chinese Representations? we find that charbased（字粒度） models consistently outperform wordbased （词粒度）models. We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. 2 腾讯中文词模型 词模型在公开数据集的表现逊于字模型 参考https://arxiv.org/pdf/1905.05526.pdf https://www.jiqizhixin.com/articles/2019-06-27-17","link":"/2021/10/26/ch-word-bert/"},{"title":"协同过滤","text":"https://www.jianshu.com/p/5463ab162a58 https://www.jianshu.com/p/20041e72e9ec https://www.cnblogs.com/pinard/p/6349233.html","link":"/2021/10/21/cf/"},{"title":"chatbot","text":"https://zhuanlan.zhihu.com/p/55201625# https://zhuanlan.zhihu.com/p/88546027 https://blog.csdn.net/m0_37565948/article/details/81582585","link":"/2021/11/26/chatbot/"},{"title":"checkpoints_iterator","text":"12for ckpt in tf.contrib.training.checkpoints_iterator( FLAGS.output_dir, timeout=FLAGS.eval_timeout) 持续捕捉最新的checkpoint，两次捕捉之间的最大等待时间为 eval_timeout 什么用？ 用于训练验证并行，之前是交替 一个设备训练，产生checkpoint，一个设备捕捉验证","link":"/2022/06/06/checkpoints-iterator/"},{"title":"ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information","text":"考虑字形和拼音的中文PTM 1 模型结构 变动在bert的输入 原来Char embedding+Position embedding+segment embedding-&gt; 现在 Fusion embedding+Position embedding （omit the segment embedding） Char embedding +Glyph ( 字形 ) embedding +Pinyin （拼音）embedding -》Fusion embedding 2 预训练任务 Whole Word Masking (WWM) and Char Masking (CM) 3 使用1234567891011121314151617181920&gt;&gt;&gt; from datasets.bert_dataset import BertDataset&gt;&gt;&gt; from models.modeling_glycebert import GlyceBertModel&gt;&gt;&gt; tokenizer = BertDataset([CHINESEBERT_PATH])&gt;&gt;&gt; chinese_bert = GlyceBertModel.from_pretrained([CHINESEBERT_PATH])&gt;&gt;&gt; sentence = '我喜欢猫'&gt;&gt;&gt; input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)&gt;&gt;&gt; length = input_ids.shape[0]&gt;&gt;&gt; input_ids = input_ids.view(1, length)&gt;&gt;&gt; pinyin_ids = pinyin_ids.view(1, length, 8)&gt;&gt;&gt; output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[0]&gt;&gt;&gt; print(output_hidden)tensor([[[ 0.0287, -0.0126, 0.0389, ..., 0.0228, -0.0677, -0.1519], [ 0.0144, -0.2494, -0.1853, ..., 0.0673, 0.0424, -0.1074], [ 0.0839, -0.2989, -0.2421, ..., 0.0454, -0.1474, -0.1736], [-0.0499, -0.2983, -0.1604, ..., -0.0550, -0.1863, 0.0226], [ 0.1428, -0.0682, -0.1310, ..., -0.1126, 0.0440, -0.1782], [ 0.0287, -0.0126, 0.0389, ..., 0.0228, -0.0677, -0.1519]]], grad_fn=&lt;NativeLayerNormBackward&gt;) 参考https://github.com/ShannonAI/ChineseBert https://arxiv.org/pdf/2106.16038.pdf","link":"/2022/06/17/chinesebert/"},{"title":"分类任务的衡量指标","text":"一、二分类1.1 confusion matrix 1.2 accuracy accuracy={\\frac{TP+TN}{TP+TN+FP+FN}}accuracy 衡量全局分类正确的数量占总样本的比例 1.3 precision precision={\\frac{TP}{TP+FP}}precision为预测正确正样本数占预测的全部正样本数的比例，即系统判定为正样本的正确率。通俗地说，假如医生给病人检查，医生判断病人有疾病，然后医生判断的正确率有多少。 1.4 recall recall={\\frac{TP}{TP+FN}}recall为预测正确的正样本数量占真实正样本数量的比例，即衡量正样本的召回比例。通俗说，假如有一批病人，医生能从中找出病人的比例 1.5 F1由于precision和recall往往是矛盾的，因此为了综合考虑二者，引入F1，即为precision和recall的调和平均 F_{1}={2\\frac{precision\\cdot recall}{precision+recall}}当$precision$和$recall$的任一个值为0，$F_1$都为0 之所以采用调和平均，是因为调和平均数受极端值影响较大，更适合评价不平衡数据的分类问题 通用的F值表达式： F_{\\beta}={(1+\\beta^2)\\frac{precision\\cdot recall}{\\beta^2\\cdot precision+recall}}除了$F_1$分数之外，$F_2$ 分数和$F_{0.5}$分数在统计学中也得到大量的应用。其中，$F_2$分数中，召回率的权重高于精确率，而$F_{0.5}$分数中，精确率的权重高于召回率。 1.6 ROC roc曲线：接收者操作特征(receiver operating characteristic), roc曲线上每个点反映某个阈值下的FPR和TPR的组合。 横轴：$FPR$，叫做假正类率，表示预测为正例但真实情况为反例的占所有真实情况中反例的比率，公式为$FPR=\\frac{FP}{TN+FP}$。 纵轴：$TPR$ ，叫做真正例率，表示预测为正例且真实情况为正例的占所有真实情况中正例的比率，公式为​ $TPR=\\frac{TP}{TP+FN}$。 1.7 AUC$AUC$(Area under Curve)：ROC曲线下的面积，数值可以直观评价分类器的好坏，值越大越好，对于二分类，结果介于0.5和1之间，1为完美分类器，0.5是因为二分类分类效果最差也是0.5。 二、多分类2.1 混淆矩阵 2.2 accuracy accuracy=\\frac{分类正确的样本数,即对角线上的数}{总样本数，即矩阵全部元素相加}2.3 某个类别的precision，recall，F1与二分类公式一样 precision_{pig}=\\frac{20}{20+(10+40)}=\\frac{2}{7} \\\\recall_{pig}=\\frac{20}{20+10}=\\frac{2}{3} \\\\F_{1pig}={2\\frac{precision_{pig}\\cdot recall_{pig}}{precision_{pig}+recall_{pig}}}2.4 系统的precision，recall，F1系统的precision，recall，$F_1$需要综合考虑所有类别，即同时考虑猫、狗、猪的precision，recall，$F_1$。有如下几种方案： 2.4.1 Macro average Macro-precision=\\frac{precision_{cat}+precision_{dog}+precision_{pig}}{3} \\\\Macro-recall=\\frac{recall{cat}+recall{dog}+recall{pig}}{3} \\\\Macro-F_{1}=\\frac{F_{1cat}+F_{1dog}+F_{1pig}}{3}2.4.2 Weighted average对macro的推广 Weighted-precision=W_{cat}\\cdot precision_{cat}+W_{dog}\\cdot precision_{dog}+W_{pig}\\cdot precision_{pig} \\\\Weighted-recall=W_{cat}\\cdot recall{cat}+W_{dog}\\cdot recall{dog}+W_{pig}\\cdot recall{pig} \\\\Weighted-F_{1}=W_{cat}\\cdot F_{1cat}+W_{dog}\\cdot F_{1dog}+W_{pig}\\cdot F_{1pig} \\\\W_{cat}:W_{dog}:W_{pig}=N_{cat}:N_{dog}:N_{pig},其中N为样本数量，W为权重2.4.3 Micro average Micro-precision={\\frac{TP_{总}}{TP_{总}+FP_{总}}}={\\frac{\\sum_{i=1}^{n}TP_{i}}{\\sum_{i=1}^{n}TP_{i}+\\sum_{i=1}^{n}FP_{i}}} \\\\Micro-recall={\\frac{TP_{总}}{TP_{总}+FN_{总}}}={\\frac{\\sum_{i=1}^{n}TP_{i}}{\\sum_{i=1}^{n}TP_{i}+\\sum_{i=1}^{n}FN_{i}}} \\\\Micro-F_{1}={2\\frac{Micro-precision\\cdot Micro-recall}{Micro-precision+Micro-recall}}2.5 ROC 对于多分类分类器整体效果的ROC如上micro或者macro曲线，其余3条描述单个类别的分类效果。对于多分类，ROC上的点，同样是某个阈值下的FPR和TPR的组合。 对于多分类的$FPR$,$TPR$，有几种计算方式 a. micro average FPR_{micro } =\\frac{FP_总}{TN_总+FP_总}=\\frac{\\sum_{i=1}^{n}FP_{i}}{\\sum_{i=1}^{n}TN_{i}+\\sum_{i=1}^{n}FP_{i}}\\\\ TPR_{micro }=\\frac{TP_总}{TP_总+FN_总}=\\frac{\\sum_{i=1}^{n}TP_{i}}{\\sum_{i=1}^{n}TP_{i}+\\sum_{i=1}^{n}FN_{i}} \\\\n表示类别数量，FP_i，TN_i，TP_i，FN_i为某个类别的FP，TN，TP，FNb. macro average FPR_{macro}=\\frac{1}{n}\\sum_{i=1}^{n}FPR_{i}\\\\ TPR_{macro}=\\frac{1}{n}\\sum_{i=1}^{n}TPR_{i}，其中FPR_i，TPR_i为某个类别的FPR和TPR2.6 AUC$AUC$依旧为ROC曲线下的面积，对于多分类个人认为取值范围为[0,1]。 三.代码accuracy，precision，recall，F1 12345from sklearn.metrics import precision_recall_fscore_support, accuracy_scoredef eval_acc_f1(y_true, y_pred): acc = accuracy_score(y_true, y_pred) prf = precision_recall_fscore_support(y_true, y_pred, average=&quot;macro&quot;) return acc, prf ROC和AUC 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 引入必要的库import numpy as npimport matplotlib.pyplot as pltfrom itertools import cyclefrom sklearn import svm, datasetsfrom sklearn.metrics import roc_curve, aucfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import label_binarizefrom sklearn.multiclass import OneVsRestClassifierfrom scipy import interp# 加载数据iris = datasets.load_iris()X = iris.datay = iris.target# 将标签二值化y = label_binarize(y, classes=[0, 1, 2])# 设置种类n_classes = y.shape[1]# 训练模型并预测random_state = np.random.RandomState(0)n_samples, n_features = X.shape# shuffle and split training and test setsX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)# Learn to predict each class against the otherclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))y_score = classifier.fit(X_train, y_train).decision_function(X_test)# 计算每一类的ROCfpr = dict()tpr = dict()roc_auc = dict()for i in range(n_classes): fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i]) roc_auc[i] = auc(fpr[i], tpr[i])# Compute micro-average ROC curve and ROC area（方法二）fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])# Compute macro-average ROC curve and ROC area（方法一）# First aggregate all false positive ratesall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))# Then interpolate all ROC curves at this pointsmean_tpr = np.zeros_like(all_fpr)for i in range(n_classes): mean_tpr += interp(all_fpr, fpr[i], tpr[i])# Finally average it and compute AUCmean_tpr /= n_classesfpr[&quot;macro&quot;] = all_fprtpr[&quot;macro&quot;] = mean_tprroc_auc[&quot;macro&quot;] = auc(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;])# Plot all ROC curveslw=2plt.figure()plt.plot(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], label='micro-average ROC curve (area = {0:0.2f})' ''.format(roc_auc[&quot;micro&quot;]), color='deeppink', linestyle=':', linewidth=4)plt.plot(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;], label='macro-average ROC curve (area = {0:0.2f})' ''.format(roc_auc[&quot;macro&quot;]), color='navy', linestyle=':', linewidth=4)colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])for i, color in zip(range(n_classes), colors): plt.plot(fpr[i], tpr[i], color=color, lw=lw, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i]))plt.plot([0, 1], [0, 1], 'k--', lw=lw)plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel('False Positive Rate')plt.ylabel('True Positive Rate')plt.title('Some extension of Receiver operating characteristic to multi-class')plt.legend(loc=&quot;lower right&quot;)plt.show() 参考资料：https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839 https://zhuanlan.zhihu.com/p/147663370 https://zhuanlan.zhihu.com/p/81202617 https://zhuanlan.zhihu.com/p/266386193","link":"/2021/07/18/classify-performance/"},{"title":"clickhouse","text":"https://zhuanlan.zhihu.com/p/139948625 关系型，分布式，完全列式 列式指得是列存储","link":"/2022/04/23/clickhouse/"},{"title":"闭包","text":"调用函数A，返回函数B给你，函数B就是闭包，A的参数就是自由变量","link":"/2022/09/12/closure/"},{"title":"spark部署方式","text":"https://blog.csdn.net/qq_37163925/article/details/106260434 https://spark.apache.org/docs/latest/cluster-overview.html https://book.itheima.net/course/1269935677353533441/1270998166728089602/1270999667882074115 通过设置mater来选择部署方式。该属性没有默认值。这是Spark程序需要连接的集群管理器所在的URL地址。如果这个属性在提交应用程序的时候没设置，程序将会通过System.getenv(“MASTER”)来获取MASTER环境变量；但是如果MASTER环境变量没有设定，那么程序将会把master的值设定为local[*]，之后程序将在本地启动。 local为单机 standalone是Spark自身实现资源调度 yarn为使用hadoop yarn来实现资源调度 1 local本地模式就是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时环境 local【N】：N为线程数量，通常N为cpu的core的数量 local【*】：cpu的core数量 跑local可以不依赖hadoop https://blog.csdn.net/wangmuming/article/details/37695619 https://blog.csdn.net/bettesu/article/details/68512570 2 Standalonehttps://sfzsjx.github.io/2019/08/26/spark-standalone-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86 2.1 client 执行流程 client 模式提交任务后，会在客户端启动Driver进程。 Driver 会向Master申请启动Application启动资源。 资源申请成功后，Driver端会将task发送到worker端执行。 worker端执行成功后将执行结果返回给Driver端 2.2 cluster 执行流程： 客户端使用命令spark-submit –deploy-mode cluster 后会启动spark-submit进程 此进程为Driver向Master 申请资源。 Master会随机在一台Worker节点来启动Driver进程。 Driver启动成功后，spark-submit关闭，然后Driver向Master申请资源。 Master接收到请求后，会在资源充足的Worker节点上启动Executor进程。 Driver分发Task到Executor中执行。 2.3 高可用HA 3 Mesosa general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated) 4 YARN为什么要YARN？ Spark On YARN是有两种运行模式的,一种是Cluster模式一种是Client模式.这两种模式的区别就是Driver运行的位置. Cluster模式即:Driver运行在YARN容器内部, 和ApplicationMaster在同一个容器内 Client模式即:Driver运行在客户端进程中, 比如Driver运行在spark-submit程序的进程中 4.1 cluster 具体流程步骤如下：1）、任务提交后会和ResourceManager通讯申请启动ApplicationMaster;2）、随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver；3）、Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程;4）、Executor进程启动后会向Driver反向注册;5）、Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行; 4.2 client 具体流程步骤如下：1）、Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；2）、随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存；3）、ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；4）、Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；5）、之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。 5 Kubernetesan open-source system for automating deployment, scaling, and management of containerized applications.","link":"/2022/01/18/cluster-clinet/"},{"title":"colab","text":"没有GPU怎么办，薅大户！！！ https://blog.csdn.net/zhang_li_ke/article/details/89704682 https://www.jianshu.com/p/a42d69568966 https://blog.csdn.net/oldmao_2001/article/details/90737735?","link":"/2021/11/21/colab/"},{"title":"表字段的数据类型","text":"https://www.html.cn/qa/other/20219.html","link":"/2022/02/15/column_type/"},{"title":"ConSERT A Contrastive Framework for Self-Supervised Sentence Representation Transfer","text":"https://arxiv.org/abs/2105.11741 https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html 1.背景首先，BERT其自身导出的句向量（不经过Fine-tune，对所有词向量求平均）会出现“坍缩（Collapse）”现象，即所有的句子都倾向于编码到一个较小的空间区域内，如图。为了解决这个问题，将对比学习结合到finetune过程，借助无标签数据来提升模型的能力。 2.原理给定一个类似BERT的预训练语言模型$\\textbf{M}$，以及从目标领域数据分布中收集的无标签文本语料库$\\mathcal{D}$，我们希望通过构建自监督任务在$\\mathcal{D}$上对$\\textbf{M}$进行Fine-tune，使得Fine-tune后的模型能够在目标任务（文本语义匹配）上表现最好。 2.1 整体框架 模型整体结构如上图所示，主要由三个部分组成 A data augmentation module that generates different views for input samples at the token embedding layer. A shared BERT encoder that computes sentence representations for each input text. During training, we use the average pooling of the token embeddings at the last layer to obtain sentence representations. A contrastive loss layer on top of the BERT encoder. It maximizes the agreement between one representation and its corresponding version that is augmented from the same sentence while keeping it distant from other sentence representations in the same batch. 对于任意一个句子输入$x$，得到其对应的两个增强向量$e_i=T_1(x),e_j=T_2(x),e_i,e_j\\in \\mathbb{R}^{L\\times d}$，然后经过shared BERT encoder编码为$r_i,r_j$,其中$T_1,T_2$为不同的数据增强方式，$L$为句子$x$的长度，$d$为隐藏单元的数量。对于每个train step，从$\\mathcal{D}$随机选取$N$个样本作为mini-batch，然后得到$2N$个增强样本，使用NT-Xent构造loss为 \\mathcal{L}_{i,j}=-log\\frac{exp(sim(r_i,r_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k\\neq i]}exp(sim(r_i,r_k)/\\tau)} \\\\\\mathcal{L}_{con}=\\frac{1}{2N}\\sum_{(i,j)}\\mathcal{L}_{i,j}其中$sim(.)$为余弦相似度计算，$\\tau$表示temperature，是一个超参数，实验中取0.1,$\\mathbb{1}$是指示器，当$k=i$时，值为0。上式分子为正样本，分母为全部（但是基本为负样本，所以可以看成负样本），所以loss变小就是让分子变大，分母变小，也就是让正样本相似度变大，负样本相似度变小 2.2 数据增强策略显式生成增强样本的方法包括：回译、同义词替换、意译等，然而这些方法一方面不一定能保证语义一致。所以考虑了在Embedding层隐式生成增强样本的方法。 对抗攻击（Adversarial Attack）：这一方法通过梯度反传生成对抗扰动，将该扰动加到原本的Embedding矩阵上，就能得到增强后的样本。由于生成对抗扰动需要梯度反传，因此这一数据增强方法仅适用于有监督训练的场景。 打乱词序（Token Shuffling）：这一方法扰乱输入样本的词序。由于Transformer结构没有“位置”的概念，模型对Token位置的感知全靠Embedding中的Position Ids得到。因此在实现上，我们只需要将Position Ids进行Shuffle即可。 裁剪（Cutoff） ：又可以进一步分为两种： Token Cutoff：随机选取Token，将对应Token的Embedding整行置为零。 Feature Cutoff：随机选取Embedding的Feature，将选取的Feature维度整列置为零。 Dropout：Embedding中的每一个元素都以一定概率置为零，与Cutoff不同的是，该方法并没有按行或者按列的约束。 2.3 融合监督信号除了无监督训练以外，作者给出3种进一步融合监督信号的策略，以NLI任务为例： f=Concat(r_1,r_2,|r_1-r_2|) \\\\\\mathcal{L}_{ce}=CrossEntropy(Wf+b,y)Joint training (joint): \\mathcal{L}_{joint}=\\mathcal{L}_{ce}+\\alpha\\mathcal{L}_{con}\\ \\# on\\ NLI \\ datasetSupervised training then unsupervised transfer (sup-unsup): first train the model with $\\mathcal{L}_{ce}$on NLI dataset, then use $\\mathcal{L}_{con}$to finetune it on the target dataset. Joint training then unsupervised transfer (joint-unsup): first train the model with the $\\mathcal{L}_{joint}$on NLI dataset, then use $\\mathcal{L}_{con }$to fine-tune it on the target dataset. 3.定性分析后又发现BERT句向量表示的坍缩和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，坍缩现象可以在一定程度上得到缓解（如图2蓝色曲线所示）。 4 实验结果4.1 Unsupervised Results 4.2 Supervised Results","link":"/2021/08/27/consert/"},{"title":"使用DGL构造图","text":"https://www.cnblogs.com/liyinggang/p/13360917.html https://zhuanlan.zhihu.com/p/406833147 https://blog.csdn.net/wufeil7/article/details/107106299","link":"/2022/01/11/construct-graph-dgl/"},{"title":"SparkSession、SparkContext、HiveContext、SQLContext","text":"https://blog.csdn.net/weixin_43648241/article/details/108917865 SparkSession包含SparkContext SparkContext包含HiveContext HiveContext包含SQLContext SparkSession &gt; SparkContext &gt; HiveContext &gt; SQLContext 12345678910111213141516171819SparkSession.builder.\\config(&quot;hive.metastore.uris&quot;, &quot;thrift://xxx.xx.x.xx:xxxx&quot;).\\config(&quot;spark.pyspark.python&quot;, &quot;/opt/dm_python3/bin/python&quot;).\\config('spark.default.parallelism ', 10 ).\\config('spark.sql.shuffle.partitions', 200 ).\\config(&quot;spark.driver.maxResultSize&quot;, &quot;16g&quot;).\\config(&quot;spark.port.maxRetries&quot;, &quot;100&quot;).\\config(&quot;spark.driver.memory&quot;,&quot;16g&quot;).\\config(&quot;spark.yarn.queue&quot;, &quot;dcp&quot; ).\\config(&quot;spark.executor.memory&quot;, &quot;16g&quot; ).\\config( &quot;spark.executor.cores&quot;, 20).\\config(&quot;spark.files&quot;, addfile).\\config( &quot;spark.executor.instances&quot;, 6 ).\\config(&quot;spark.speculation&quot;, False).\\config( &quot;spark.submit.pyFiles&quot;, zipfile).\\appName(&quot;testing&quot;).\\master(&quot;yarn&quot;).\\enableHiveSupport().\\getOrCreate()","link":"/2022/02/23/context/"},{"title":"Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting","text":"原文：https://arxiv.org/abs/1907.00235 作者利用transformer做时间序列预测，发现了两个问题，然后提出了改进。一个问题是locality-agnostics，the point-wise dot product self-attention in canonical Transformer architecture is insensitive to local context。另外一个问题是memory bottleneck ：space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible.为了解决这两个问题，作者提出了convolutional self-attention和LogSparse Transformer。 3.背景问题定义 其中$\\Phi$是参数，$\\textbf{X}$是辅助输入，就是除了观测值以外的输入，$\\textbf{Z}_{i,t}$表示序列$i$在时刻$t$的值 为了简化式子，定义了 目标就变成了$\\textbf{z}_t \\sim f(\\textbf{Y}_t)$ Transformer $h$表示某个头，$M$表示mask matrix 4.方法论4.1 Enhancing the locality of Transformer 改进思想如上图所示，原版的transformer，利用了point-wise之间的相似度，万一存在异常点，就会造成偏差。改进方向就是将点和点之间的相似度变为local context based，也就是先利用卷积得到local的表示，然后基于local做Q和K的相似度。当卷积核的尺寸为1就退化为原版的transformer。 4.2 Breaking the memory bottleneck of Transformer 原来的transformer需要$O(L^2)$的空间复杂度，每层每个cell为$O(L)$，每层所有cell为$O(L^2)$，然后堆叠$h$层，$h$为常数，所以为$O(L^2)$，如图(a)。作者提出了LogSparse Transformer，如图（b）,空间复杂度为$O(L(logL)^2)$。首先每层每一个cell需要$logL$，每层所有cell就是$LlogL$，然后堆叠$logL$层，最后为$O(L(logL)^2)$。 对于LogSparse Transformer，筛选规则为： 图（c）和图（d）是对（b）的改进。 5.实验评级指标:p-quantile loss $R_p$ with $p\\in(0,1)$ 参考https://zhuanlan.zhihu.com/p/412800154","link":"/2021/11/01/convtrans/"},{"title":"CRF和HMM","text":"大佬写的很详细，Mark一下 CRFhttps://www.cnblogs.com/pinard/p/7048333.html https://zhuanlan.zhihu.com/p/148813079# HMMhttps://www.cnblogs.com/pinard/p/6945257.html","link":"/2021/10/12/crf/"},{"title":"常见问题","text":"CTR模型为什么普遍采用二分类建模而不是回归建模 https://zhuanlan.zhihu.com/p/372110635 CTR 预估和推荐系统有什么区别？？？ https://blog.csdn.net/qiqi123i/article/details/105259351 CTR 预估和排序学习有什么区别？？？ CTR预估模型属于point-wise模型 https://www.zhihu.com/question/359973444#","link":"/2021/10/14/ctr/"},{"title":"Deep Match to Rank Model for Personalized Click-Through Rate Prediction","text":"1 输入特征由4个部分构成，分别为User Profile, User Behavior, Target Item and Context，每个特征都包含子特征，比如User Profile contains user ID, consumption level and so on。最初的表示为one-hot形式，经过embedding层，转成高纬向量，通过查找表来实现。最后4个特征分别表示为$\\textbf{x}_p,\\textbf{x}_b,\\textbf{x}_t,\\textbf{x}_c$，以$\\textbf{x}_b$来举例，$\\textbf{x}_b=[e_1，e_2，…，e_T]\\in \\mathbb{R}^{T\\times d_e}$ 2 User-to-Item Networkwe apply attention mechanism with positional encoding as query to adaptively learn the weight for each behavior，where the position of user behavior is the serial number in the behavior sequence ordered by occurred time 其中$\\textbf{z}\\in \\mathbb{R}^{d_h}$是学习的参数，$\\textbf{p}_t\\in \\mathbb{P}^{d_p}$是位置$t$的embedding 为什么不用$\\textbf{x}_t$,而用$\\textbf{v}^{‘}$表示Target Item。作者的意思是对于Target Item，有两个查找表，we call $\\textbf{V}$ the input representation and $\\textbf{V}^{‘}$ the output representation of Target Item。we apply inner product operation to represent the user-to-item relevance 3 Item-to-Item Network 4 finalAnd the final input of MLP is represented by $\\textbf{c}=[\\textbf{x}_p,\\textbf{x}_t,\\textbf{x}_c,\\hat{\\textbf{u}},r,\\hat{r}]$ 5 losstarget The loss for input feature vector $\\textbf{x}=[\\textbf{x}_p,\\textbf{x}_b,\\textbf{x}_t,\\textbf{x}_c]$ and click label $ y \\in \\{0, 1\\} $is: auxiliary match network 主要是提高$r$对于user-to-item relevance的表现能力而引入。 The probability that user with the first $T −1$ behaviors click item $j$ next can be formulated with the softmax function as: 其中$\\textbf{v}^{‘}_j$表示第$j$个商品的output representation。With cross-entropy as loss function, we have the loss as follows: However, the cost of computing $p_j$ in Equation (6) is huge,引入负采样，然后loss为 final 参考阿里2020年发表在AAAI上的关于CTR的paper，原文链接 https://sci-hub.se/10.1609/aaai.v34i01.5346","link":"/2021/10/15/dMR/"},{"title":"DAG","text":"https://blog.csdn.net/qq_16669583/article/details/106026722 https://blog.csdn.net/u011564172/article/details/70172060 作用根据rdd的依赖关系构建dag，根据dag划分stage 啥样 1个action=1个job=1个dag 调度流程","link":"/2022/01/05/dag/"},{"title":"数据采集","text":"1 离线 1.用户行为数据jar-》log-》flume-》kafka-》flume-》hdfs 用户行为数据存储在日志服务器，以.log文件存在，log-》flume-》kafka-》flume-》hdfs 2 业务数据jar-》mysql-》sqoop-》hdfs 业务数据存储在mysql，使用sqoop导入hdfs 2 实时 1.用户行为数据前端-》Nginx-》日志服务器-》）log，Kafka（ods 1 前端埋点数据 通过jar包模拟 2 Nginx https://blog.csdn.net/qq_40036754/article/details/102463099 负载均衡 3 日志服务器 spring boot搭建 首先，Spring 就是一个java框架，spring boot在 Spring 的基础上演进 4 落盘，整合 Kafka 落盘指的是存在日志服务器 生产者-》kafka-》消费者 ​ 生产 消费 2 业务数据jar-》mysql-》flinkcdc-》kafka（ods） 不能使用sqoop，因为sqoop底层为mapreduce，太慢了，改用canal，maxwell或者flinkcdc 数据从mysql读到kafka，不是hdfs flink-cdc https://cloud.tencent.com/developer/article/1801766 Change Data Capture(变更数据获取）","link":"/2022/02/13/data-collect/"},{"title":"数据字典","text":"https://www.cnblogs.com/arxive/p/9673830.html https://blog.panoply.io/how-to-create-a-data-dictionary https://www.secoda.co/blog/how-to-create-a-data-dictionary-a-step-by-step-guide","link":"/2022/02/10/data-dict/"},{"title":"数据集成","text":"https://zhuanlan.zhihu.com/p/269780713 https://help.aliyun.com/document_detail/137663.html 异构数据集成","link":"/2022/04/23/data-ensemble/"},{"title":"数据不平衡如何解决","text":"1.基于数据a.过采样和欠采样 对少数数据进行有放回的过采样，使原本的数据变的均衡，这样就是对少数数据进行了复制，容易造成过拟合。 对多数数据进行有放回/无放回的欠采样，这样会丢失一些样本，损失信息，模型只学会整体模式的一部分，容易欠拟合。 b.SMOTE算法 c.数据增强 通过人为或算法增加少数数据的数量 2.基于loss使用代价函数时，可以增加小类样本的权值，降低大类样本的权值 参考https://zhuanlan.zhihu.com/p/62877337 https://blog.csdn.net/asialee_bird/article/details/83714612","link":"/2021/09/04/data-imbalance/"},{"title":"数据质量","text":"The Challenges of Data Quality and Data Quality Assessment in the Big Data Era https://pdfs.semanticscholar.org/0fb3/7330a4170ec63d60eec7dbb2b86e6829a3de.pdf A Data Quality in Use model for Big Data Automating LargeScale Data Quality Verification Data Sets and Data Quality in Software Engineering Discovering Data Quality Rules Context-aware Data Quality Assessment for Big Data","link":"/2022/02/09/data-quality/"},{"title":"数据安全","text":"分为两步： ​ 1.用户认证 ​ Kerberos ​ 2.权限管理 ​ ranger","link":"/2022/02/06/data-safety/"},{"title":"数据集划分","text":"训练 验证: 调参 测试","link":"/2022/05/28/data-split/"},{"title":"数据结构汇总","text":"https://blog.csdn.net/m0_37568814/article/details/81288756 https://zhuanlan.zhihu.com/p/138523723 1 分类 数据结构包括：逻辑结构，存储结构，数据运算 数据的逻辑结构主要分为线性结构和非线性结构。 线性结构：数据结构的元素之间存在一对一线性关系，所有结点都最多只有一个直接前趋结点和一个直接后继结点。常见的有数组、队列、链表、栈。 非线性结构：各个结点之间具有多个对应关系，一个结点可能有多个直接前趋结点和多个直接后继结点。常见的有多维数组、广义表、树结构和图结构等。 数据的物理结构（以后我都统一称存储结构），表示数据元素之间的逻辑关系，一种数据结构的逻辑结构根据需要可以表示成多种存储结构，常用的存储结构有： 顺序存储：存储顺序是连续的，在内存中用一组地址连续的存储单元依次存储线性表的各个数据元素。 链式存储：在内存中的存储元素不一定是连续的，用任意地址的存储单元存储元素，元素节点存放数据元素以及通过指针指向相邻元素的地址信息。 索引存储：除建立存储结点信息外，还建立附加的索引表来标识节点的地址。索引表由若干索引项组成。 散列存储：又称Hash存储，由节点的关键码值决定节点的存储地址。 2 常用的数据结构 数组（Array） 队列（Queue） 链表（Linked List） 栈（Stack） 树（Tree） 散列表（Hash） 堆（Heap） 图（Graph）","link":"/2022/05/01/data-struct/"},{"title":"数据同步","text":"1.数据同步策略 1 全量存储完整的数据。 2 增量存储新增加的数据。 3 新增及变化存储新增加的数据和变化的数据。 4 特殊某些特殊的表，可不必遵循上述同步策略。 1.例如某些不会发生变化的表 地区表，省份表，民族表等，可以只存一份固定值。 2.拉链表 在第一天同步拉链表的时候，需要同步全量数据，并且设置endtime = 9999；首日过后，每日同步数据到拉链表中就是新增及变化的数据，可以采用分区策略，以999为一个分区表示有效的数据，加上以过期时间为分区； 2.首日，每日首日同步，视情况，不一定全量 每日同步，视情况","link":"/2022/02/09/data-synchronization/"},{"title":"数据仓库书单","text":"https://www.douban.com/doulist/1202941/?start=0&amp;sort=seq&amp;playable=0&amp;sub_type=","link":"/2022/01/05/data-warehouse/"},{"title":"数据库分类","text":"常见关系型数据库： Oracle MySql Microsoft SQL Server SQLite PostgreSQL IBM DB2 常见的非关系型数据库： 键值数据库：Redis、Memcached、Riak 列族数据库：Bigtable、HBase、Cassandra 文档数据库：MongoDB、CouchDB、MarkLogic 图形数据库：Neo4j、InfoGrid","link":"/2022/02/15/database-con/"},{"title":"数据库建模","text":"https://blog.csdn.net/L_zhai/article/details/118650439 https://cloud.tencent.com/developer/article/1644918","link":"/2022/02/10/database-design/"},{"title":"分库分表","text":"https://www.zhihu.com/question/4487756131 https://zhuanlan.zhihu.com/p/137368446","link":"/2022/03/30/database-split-table-base/"},{"title":"datastream","text":"1 转换算子 Transformationfunction分类：普通的，rich 怎么写function： 自定义 匿名类 lambda表达式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package com.atguigu.chapter05;/** * Copyright (c) 2020-2030 尚硅谷 All Rights Reserved * &lt;p&gt; * Project: FlinkTutorial * &lt;p&gt; * Created by wushengran */import org.apache.flink.api.common.functions.MapFunction;import org.apache.flink.api.common.typeinfo.Types;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStream;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;public class TransReturnTypeTest { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); DataStreamSource&lt;Event&gt; clicks = env.fromElements( new Event(&quot;Mary&quot;, &quot;./home&quot;, 1000L), new Event(&quot;Bob&quot;, &quot;./cart&quot;, 2000L) ); // 想要转换成二元组类型，需要进行以下处理 // 1) 使用显式的 &quot;.returns(...)&quot; DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream3 = clicks .map( event -&gt; Tuple2.of(event.user, 1L) ) .returns(Types.TUPLE(Types.STRING, Types.LONG)); stream3.print(); // 2) 使用类来替代Lambda表达式 clicks.map(new MyTuple2Mapper()) .print(); // 3) 使用匿名类来代替Lambda表达式 clicks.map(new MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() { @Override public Tuple2&lt;String, Long&gt; map(Event value) throws Exception { return Tuple2.of(value.user, 1L); } }).print(); env.execute(); } // 自定义MapFunction的实现类 public static class MyTuple2Mapper implements MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;{ @Override public Tuple2&lt;String, Long&gt; map(Event value) throws Exception { return Tuple2.of(value.user, 1L); } }} max maxby 区别 1234567891011121314151617181920212223242526272829303132DataStreamSource&lt;Event&gt; stream = env.fromElements( new Event(&quot;Mary&quot;, &quot;./home&quot;, 5000L), new Event(&quot;Bob&quot;, &quot;./cart&quot;, 2000L), new Event(&quot;Mary&quot;, &quot;./cart&quot;, 3000L), new Event(&quot;ss&quot;, &quot;./fav&quot;, 4000L), new Event(&quot;Mary&quot;, &quot;./fav&quot;, 10000L) ); stream.keyBy(e -&gt; e.user)// .maxBy(&quot;timestamp&quot;) .maxBy(&quot;timestamp&quot;) // 指定字段名称 .print(&quot;maxBy:&quot;); stream.keyBy(e -&gt; e.user)// .(&quot;timestamp&quot;) .max(&quot;timestamp&quot;) // 指定字段名称 .print(&quot;max:&quot;); max:&gt; Event{user='Mary', url='./home', timestamp=1970-01-01 08:00:05.0}maxBy:&gt; Event{user='Mary', url='./home', timestamp=1970-01-01 08:00:05.0}max:&gt; Event{user='Bob', url='./cart', timestamp=1970-01-01 08:00:02.0}maxBy:&gt; Event{user='Bob', url='./cart', timestamp=1970-01-01 08:00:02.0}max:&gt; Event{user='Mary', url='./home', timestamp=1970-01-01 08:00:05.0}max:&gt; Event{user='ss', url='./fav', timestamp=1970-01-01 08:00:04.0}maxBy:&gt; Event{user='Mary', url='./home', timestamp=1970-01-01 08:00:05.0}max:&gt; Event{user='Mary', url='./home', timestamp=1970-01-01 08:00:10.0}maxBy:&gt; Event{user='ss', url='./fav', timestamp=1970-01-01 08:00:04.0}maxBy:&gt; Event{user='Mary', url='./fav', timestamp=1970-01-01 08:00:10.0}max部分替换，maxby全部替换 2 窗口1 简介 窗口[0-10）中有11,12,但是11,12并不在窗口[0-10）处理，而是在对应的窗口[10,20)处理 2 窗口的分类 按照驱动类型分类 （1）时间窗口 Time Window （2）计数窗口 Count Window 按照窗口分配数据的规则分类 （1）滚动窗口 Tumbling Windows （2）滑动窗口 Sliding Windows （3）会话窗口 Session Windows （4）全局窗口 Global Windows 3 使用 4 迟到数据的处理 窗口中 的迟到数据默认会被丢弃，这导致计算结果不够准确 1 设置水位线延迟时间 1234567assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(2)) .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() { @Override public long extractTimestamp(Event element, long recordTimestamp) { return element.timestamp; } })); 2 允许窗口处理迟到数据 1.allowedLateness(Time.minutes(1)) 3 将迟到数据放入侧输出流 收集关窗之后的迟到数据，然后手动处理 1.sideOutputLateData(outputTag)","link":"/2022/04/24/datastream/"},{"title":"大数据工具","text":"1 数据库管理工具Navicat 破解 https://cdmana.com/2022/114/202204240555321952.html DataGrip 需要用到JDBC协议连接到Hive，故需要启动HiveServer2。 DataGrip 版是由JetBrains 公司（就是那个出品Intellij IDEA 的公司）推出的数据库管理软件。 2 数据库设计工具EZDML:来辅助我们梳理复杂的业务表关系，效果如下","link":"/2022/02/01/datawarehouse-develop-tool/"},{"title":"数仓分层","text":"https://blog.csdn.net/BeiisBei/article/details/105723188#_19 https://www.saoniuhuo.com/article/detail-72.html https://www.dianjilingqu.com/20890.html https://www.i4k.xyz/article/wjt199866/115184169#ODS__100 1.为什么要分层1.清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。2.数据血缘追踪：简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的张业务表，但是它的来源有很多，如果有一张来 源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。3.减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。4.把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。5.屏蔽原始数据的异常：屏蔽业务的影响，不必改一次业务就需要重新接入数据。 2.分层结构 2.1 ODS层作用 https://blog.csdn.net/xuebo_911/article/details/8156016# 起到备份数据的作用 构建 1 直接加载原始日志、数据，保持原貌不做处理 2.2 DIM层dim存放维度表，dwd存放事实表 作用 维度表可以看作是用户来分析数据的窗口，维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，维度表包含帮助汇总数据的特性的层次结构。 构建 https://help.aliyun.com/document_detail/137615.html 1 构建维度表，主要是对业务事实的描述信息，例如何人，何时，何地等 2.3 DWD层作用 保存业务事实明细，一行信息代表一次业务行为，例如一次下单。 构建 1 对ODS层数据进行清洗（去除空值，脏数据，超过极限范围的数据、脱敏等） 2 构建事实表 https://help.aliyun.com/document_detail/114457.html 2.4 DWS作用 避免重复计算 1）问题引出 两个需求，统计每个省份订单的个数、统计每个省份订单的总金额，都是将省份表和订单表进行join，group by省份，然后计算。同样数据被计算了两次，实际上类似的场景还会更多。 2） 那怎么设计能避免重复计算呢？ 针对上述场景，可以设计一张地区宽表，其主键为地区ID，字段包含为：下单次数、下单金额、支付次数、支付金额等。上述所有指标都统一进行计算，并将结果保存在该宽表中，这样就能有效避免数据的重复计算。 构建 https://help.aliyun.com/document_detail/126913.html 0 分主题 1 构建宽表 2 以DWD为基础，按天进行轻度汇总。DWS层存放的所有主题对象当天的汇总行为，例如每个地区当天的下单次数，下单金额等 2.5 DWT和DWS区别 DWS按天进行轻度汇总，DWT累积汇总 作用 避免重复计算 构建 0 分主题 1 构建宽表 2 以DWS为基础，对数据进行累积汇总。DWT层存放的是所有主题对象的累积行为，例如每个地区最近７天（１５天、３０天、６０天）的下单次数、下单金额等 2.6 ADS层作用 为各种统计报表提供数据 构建 1 由于这层的数据量不大，所有没有分区，列式存储，压缩 3.构建过程借助hive，主要就是写SQL，核心步骤就是： 1.建表 2.分区规划 ​ 按什么分区，比如说按天，按月 ​ 注意数据同步策略，全量，增量等 3.数据装载 ​ 注意首日，每日 联系业务和不同层的要求 4.全流程调度Azkaban","link":"/2022/01/13/datawarehouse-multi-layer/"},{"title":"数仓架构","text":"数仓架构 https://notomato.blog.csdn.net/article/details/110790403 离线数据仓库到实时数据仓库 https://blog.csdn.net/fuyipingwml1976124/article/details/105571193","link":"/2022/01/20/datawarehouse-struct/"},{"title":"数仓主题和主题域","text":"https://blog.csdn.net/qq_22473611/article/details/116702667 1 主题域、主题、实体的关系 主题域下面可以有多个主题，主题还可以划分成更多的子主题，主题和主题之间的建设可能会有交叉现象，而实体则是不可划分的最小单位 2 主题域划分1 按照业务系统划分 2 按照业务过程划分 3 按照部门划分","link":"/2022/04/02/datawarehouse-subject/"},{"title":"时间序列预测滞后现象","text":"https://www.dazhuanlan.com/bb_principessa/topics/1021683","link":"/2021/11/01/decay-time-series/"},{"title":"决策树vs逻辑回归","text":"最大的差异上图就可以看出，左边为逻辑回归的决策面，右边为决策树的决策面 参考https://www.zhihu.com/question/319481283","link":"/2021/10/10/deci-tree-LR/"},{"title":"决策树","text":"两幅图意思一样 1.构建算法常见方法的有ID3，C4.5，CART，总结如下 1.1 ID3假设训练数据集为 $D$,∣$D∣$表示其大小。设有$K$个分类$ C_1,C_2,…,C_K$。设特征集为$\\textbf{A}$,假设某个特征$ A$ 有$ n$ 个不同的取值 $\\{a_1,a_2,…,a_n\\}$,根据特征$A$的取值将 $D$ 划分成$n$个子集。记子集 $D_i$中属于类$ C_k$的样本集合为 $D_{ik}$。 数据集$D$的经验熵$H(D)$: H(D) = - \\sum_{j=1}^K \\frac {|C_j|}{|D|} \\log \\frac {|C_j|}{|D|}特征$A$对数据集$ D$的经验条件熵$H(D|A)$ % 信息增益$g(D,A)$ g(D,A)=H(D)-H(D|A)算法流程: 若$D$中所有实例都属于同一类 $C_k$,则 $T$ 为单节点树,并将类 $C_k$作为该节点的类标记,返回$T$. 若$\\textbf{A}=\\phi$,则$T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$. 否则,按照信息增益的算法,计算每个特征对$D$的信息增益,取信息增益最大的特征 $A_g$. 如果$A_g&lt; \\varepsilon$,则置 $T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$. 否则,对$A_g$的每一可能值 $a_i$,依$A_g=a_i$将$D$分成若干非空子集$D_i$ 以$D_i$为训练集,以$\\textbf{A}- A_g $为特征集,递归地调用步骤1到步骤5,得到子树 $T_i$,全部 $T_i$构成$T$,返回$T$. 1.2 C4.5 C4.5算法流程与ID3相类似，只不过将信息增益改为信息增益比，以解决偏向取值较多的属性的问题，另外它可以处理连续型属性。 分裂信息 $SplitInformation(D,A)$ SplitInformation(D,A) = -\\sum_{i=1}^n \\frac {|D_i|}{|D|} \\log \\frac {|D_i|}{|D|}信息增益比 $GainRatio(D, A)$ GainRatio(D, A) = \\frac {g(D, A)} {SplitInformation(D, A)}1.3 CART1.3.1 CART分类树CART分类树算法使用基尼系数来代替信息增益（比），基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。 对于样本$D$，个数为$|D|$，假设$K$个类别，第$k$个类别的数量为$|C_k|$，则样本$D$的基尼系数表达式： Gini(D)=1-\\sum_{k=1}^{K}(\\frac{|C_k|}{|D|})^2对于样本$D$，个数为$|D|$，根据特征$A$的某个值$a$，把$D$分成$|D_1|$和$|D_2|$，则在特征$A=a$的条件下，样本$D$的基尼系数表达式为： Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)算法流程：算法输入训练集$D$，基尼系数的阈值，样本个数阈值。输出的是决策树$T$。 (1)、对于当前节点的数据集为$D$，如果样本个数小于阈值或没有特征，则当前节点停止递归，返回决策树。 (2)、计算样本集$D$的基尼系数，如果基尼系数小于阈值，则当前节点停止递归，返回决策树。 (3)、计算当前节点所有特征的各个特征值对数据集$D$的基尼系数 (4)、在计算出来的所有基尼系数中，选择基尼系数最小的特征$A$和对应的特征值$a$，并把数据集划分成两部分$D_1$和$D_2$，同时建立当前节点的左右节点，左节点的数据集$D$为$D_1$，右节点的数据集$D$为$D_2$。 (5)、对左右的子节点递归的调用1-4步，生成决策树。 1.3.2 CART回归树对回归树用平方误差最小化准则 算法流程：输入为训练数据$D$，输出为回归树$f(x)$ (1) 选择最优的切分变量$j$和切分点$s$，遍历$j$，对固定的$j$遍历$s$，求解 \\min \\limits_{j,s} \\ [\\min \\limits_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min \\limits_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2](2) 用选定的$(j,s)$划分区域并决定输出值 R_1(j,s)=\\{x|x^{(j)}\\le s\\},R_2(j,s)=\\{x|x^{(j)}> s\\} \\\\\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i,m=1,2(3) 继续对两个子区域调用步骤（1）（2），直至满足停止条件 (4) 将输入空间划分成$M$个区域$R_1,R_2,…,R_M$，生成回归树 f(x)=\\sum_{m=1}^{M}\\hat{c}_mI(x\\in R_m)1.4 多变量决策树无论ID3，C4.5，CART都是选择一个最优的特征做分类决策，但大多数，分类决策不是由某一个特征决定，而是一组特征。这样得到的决策树更加准确，这种决策树叫多变量决策树(multi-variate decision tree)。在选择最优特征的时，多变量决策树不是选择某一个最优特征，而是选择一个最优的特征线性组合做决策。 代表算法OC1。 2.剪枝剪枝(pruning)是解决决策树过拟合的主要手段，通过剪枝可以大大提升决策树的泛化能力。通常，剪枝处理可分为：预剪枝，后剪枝。 预剪枝：通过启发式方法，在生成决策树过程中对划分进行考察，若当前结点的划分影响决策树的泛化性能，则停止划分，并将其标记为叶节点 后剪枝：对已有的决策树，自底向上的对非叶结点进行考察，若该结点对应的子树替换为叶结点能提升决策树的泛化能力，则将改子树替换为叶结点 参考https://zhuanlan.zhihu.com/p/89607509 https://www.cnblogs.com/wxquare/p/5379970.html https://blog.csdn.net/qq_43468807/article/details/105969232 http://leijun00.github.io/2014/09/decision-tree/ https://zhuanlan.zhihu.com/p/89607509 https://www.cnblogs.com/wxquare/p/5379970.html https://blog.csdn.net/qq_43468807/article/details/105969232 http://leijun00.github.io/2014/09/decision-tree/ https://www.cnblogs.com/wqbin/p/11689709.html https://www.cnblogs.com/keye/p/10564914.html https://www.cnblogs.com/keye/p/10601501.html https://cloud.tencent.com/developer/article/1486712 https://blog.csdn.net/weixin_44132485/article/details/106502422","link":"/2021/09/23/decison-tree/"},{"title":"装饰器","text":"装饰器@ 本质是闭包 https://www.runoob.com/w3cnote/python-func-decorators.html 1234567891011121314151617181920212223242526272829303132333435363738394041421.本质###装饰的函数，本质是闭包def a_new_decorator(a_func): def wrapTheFunction(): print(&quot;I am doing some boring work before executing a_func()&quot;) a_func() print(&quot;I am doing some boring work after executing a_func()&quot;) return wrapTheFunction####被装饰的函数def a_function_requiring_decoration(): print(&quot;I am the function which needs some decoration to remove my foul smell&quot;) ####使用a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)#now a_function_requiring_decoration is wrapped by wrapTheFunction() a_function_requiring_decoration()#outputs:I am doing some boring work before executing a_func()# I am the function which needs some decoration to remove my foul smell# I am doing some boring work after executing a_func()2.@@a_new_decorator ###a_new_decorator装饰的函数，本质是闭包 def a_function_requiring_decoration(): ####被装饰的函数 &quot;&quot;&quot;Hey you! Decorate me!&quot;&quot;&quot; print(&quot;I am the function which needs some decoration to &quot; &quot;remove my foul smell&quot;)####使用a_function_requiring_decoration() #outputs: I am doing some boring work before executing a_func()# I am the function which needs some decoration to remove my foul smell# I am doing some boring work after executing a_func() 一次修饰两个装饰器，顺序为从下至上 常见装饰器@property将方法转换为相同名称的只读属性 12345678910class dataset(object): @property def fun1(self): return 13 def fun2(self): return 13a=dataset()print(a.fun1)print(a.fun2()) @staticmethod@classmthodhttps://zhuanlan.zhihu.com/p/35643573","link":"/2022/09/12/decorator/"},{"title":"维度退化","text":"https://blog.csdn.net/wzy0623/article/details/49797421 https://www.google.com/search?q=Degenerate+Dimension&amp;sourceid=chrome&amp;ie=UTF-8 https://www.jamesserra.com/archive/2011/11/degenerate-dimensions/ https://blog.csdn.net/yezonghui/article/details/120131320 当一个维度没有数据仓库需要的任何数据的时候就可以退化此维度，需要把退化的相关数据迁移到事实表中，然后删除退化的维度。","link":"/2022/04/04/degenetate-dimension/"},{"title":"dfs，bfs","text":"https://www.jianshu.com/p/a753d5c733ec dfs1 递归实现 123def dfs(当前节点,子节点) for node in 子节点集合 dfs(node,new 子节点) 310. 最小高度树12#### root 当前节点 path子节点 depth 深度dfs(root,path,depth) 695. 岛屿的最大面积12###grid全部格点 cur_i，cur_j 当前节点的位置，用+1-1可以确定子节点dfs(self, grid, cur_i, cur_j) : bfs1 迭代实现 root进队，root出队，root的子节点【a,b,c】进队，a出队，a的子节点进队，b出队，b的子节点进队。。。 访问顺序（出队顺序）：root a b c 。。。 1234567891011121314151617class Solution: def levelOrder(self, root: 'Node') -&gt; List[List[int]]: if root==None: return [] ans = [] q = [root] while q: # length = len(q) # arr = [] # for _ in range(length): node = q.pop(0) # arr.append(node.val) for chd in node.children: q.append(chd) # if arr: ans.append(node.val) return ans 429. N 叉树的层序遍历123456789101112131415161718class Solution: def levelOrder(self, root: 'Node') -&gt; List[List[int]]: ans = [] ##结果 q = [root] ##队列 while q: length = len(q) arr = [] #出队 for _ in range(length): node = q.pop(0) if not node: continue arr.append(node.val) for chd in node.children: q.append(chd) if arr: ans.append(arr) return ans 应用选择时间复杂度都为O（n）,n为全部节点 BFS是用来搜索最短径路的解是比较合适的，比如求最少步数的解，最少交换次数的解，因为BFS搜索过程中遇到的解一定是离根最近的，所以遇到一个解，一定就是最优解，此时搜索算法可以终止。这个时候不适宜使用dfs，因为DFS搜索到的解不一定是离根最近的，只有全局搜索完毕，才能从所有解中找出离根的最近的解。（当然这个DFS的不足，可以使用迭代加深搜索ID-DFS去弥补） DFS适合搜索全部的解，而正因为要搜索全部的解，那么BFS搜索过程中，遇到离根最近的解，并没有什么用，也必须遍历完整棵搜索树；DFS搜索会搜索全部，但是相比之下 DFS不用记录过多信息，所以搜素全部解的问题，DFS显然更加合适。空间优劣上，DFS是有优势的，DFS不需要保存搜索过程中的状态，而BFS在搜索过程中需要保存搜索过的状态，而且一般情况需要一个队列来记录。 和回溯的关系回溯是算法思想，dfs和bfs是搜索解空间的手段 参考https://www.jianshu.com/p/e81a16a6f771 https://blog.csdn.net/weixin_41894030/article/details/95317440","link":"/2022/04/06/dfs/"},{"title":"DGL notice","text":"any graph transformation will result in the loss of batch information. for a workaround , you may should use set_batch_num_nodes and set_batch_num_edges","link":"/2022/01/12/dgl-notice/"},{"title":"list，dict，set的时间复杂度","text":"1 x in slist：O(n) dict：O(1) set：O(1) 参考https://blog.csdn.net/jmh1996/article/details/78481365 https://www.cnblogs.com/tintinsoft/articles/9743765.html https://blog.csdn.net/weixin_48629601/article/details/107532754 https://blog.csdn.net/ACBattle/article/details/97012800","link":"/2022/05/24/dic-set-list/"},{"title":"词表特殊词的含义","text":"[PAD]：要将句子处理为特定的长度，就要在句子前或后补[PAD] [CLS]：句子的开始 [SEP]：分开两个输入句子 [mask] ：遮盖句子中的一些单词 [UNK]：标记词典内没有的词","link":"/2022/05/31/dict-word/"},{"title":"Deep Interest Evolution Network for Click-Through Rate Prediction","text":"1.概述对din的改进 din：强调用户兴趣是多样的，并使用基于注意力模型来捕获用户的兴趣 dien：不但要找到用户的兴趣，还要抓住用户兴趣的变化过程 2.结构 1 behavior layerFeature RepresentationUser Profile, User Behavior, Ad and Context one-hot vector Embeddingtransforms the large scale sparse feature into lowdimensional dense feature 2 Interest Extractor Layer利用GRU作为基本单元 3 Interest Evolving Layer主要两个部分，一个是attention一个是AUGRU attention 用公式表示为： \\alpha_t=\\frac{exp(\\textbf{h}_tW_{\\textbf{e}_a})}{\\sum_{j=1}^Texp(\\textbf{h}_jW_{\\textbf{e}_a})}AUGRU 结构如上图，用式子表达如下： 3 losstarget 为了提高准确率引入Auxiliary loss L_{aux}=-\\frac{1}{N}\\sum_{i=1}^N\\sum_t[log\\sigma(\\textbf{h}_t^i,\\textbf{e}_b^i[t+1])+log(1-\\sigma(\\textbf{h}_t^i,\\tilde{\\textbf{e}}_b^i[t+1]))]其中$\\sigma$为sigmoid global loss： L=L_{target}+\\alpha L_{aux}参考原文地址 https://arxiv.org/pdf/1809.03672.pdf","link":"/2021/10/13/dien/"},{"title":"降维","text":"1.意义1.高纬空间样本具有稀疏性，容易欠拟合 2.可视化 3.维度过大导致训练时间长，预测慢 2.分类大致分为线形降维度和非线性降维，线形降维包括PCA，LDA等，非线性降维包括LLE，t-SNE，auto encoder等。 3.PCA系列3.1 PCA假设矩阵$x\\in \\mathbb{R}^{ n}$，假设有$M$个样本，将原始数据按列组成$M$ 行$ n $列矩阵$ X\\in \\mathbb{R}^{M\\times n}$，PCA的使用过程为： 1.计算协方差矩阵$G_t \\in \\mathbb{R}^{n \\times n}$ G_t=\\frac{1}{M}\\sum_{j=1}^{M}(X-\\overline{X})^T(X-\\overline{X})注意，其中$\\overline{X}\\in \\mathbb{R}^{n}$为列的均值，$X-\\overline{X}$表示将$ X$ 的每一列进行零均值化，即减去这一列的均值 2.求出协方差矩阵的特征值及对应的特征向量 (U,\\sum,V)=SVD(G_t)3.将特征向量按对应的特征值大小排列，取前 $k$ 列组成矩阵 $P\\in \\mathbb{R}^{n \\times k} $ P=U(:,1:k)4.实现数据降维 y_{[1\\times k]}=x_{[1\\times n ]}P_{[n\\times k]}局限： ​ a. PCA只能针对1D的向量，对于2D的矩阵而言，比如图片数据，需要先flatten成向量 3.2 2DPCA将2D的矩阵flatten成向量其实丢失了行列的位置信息，为了直接在2D的矩阵上实现降维，提出了2DPCA。 假设有原始矩阵$A\\in\\mathbb{R}^{m \\times n }$，使用过程如下： 1.计算协方差矩阵 G_t=\\frac{1}{M}\\sum_{j=1}^{M}(A_j-\\overline{A})^{T}(A_j-\\overline{A}) \\\\\\overline{A}=\\frac{1}{M}\\sum_{j=1}^{M}A_j2.求出协方差矩阵的特征值及对应的特征向量 (U,\\sum,V)=SVD(G_t)3.将特征向量按对应的特征值大小排列，取前 $k$ 列组成矩阵 X=U(:,1:k)4.实现数据降维 Y_{[m\\times k]}=A_{[m\\times n ]}X_{[n\\times k]}3.3 （2D）2PCA作者证明了2DPCA只是在行上工作，然后提出了Alternative 2DPCA可以工作在列上，最后将其结合得到（2D）2PCA，使其可以同时在行列工作 假设有原始矩阵$A\\in\\mathbb{R}^{m \\times n }$，使用过程如下： 1.计算协方差矩阵 G_x=\\frac{1}{M}\\sum_{k=1}^{M}\\sum_{i=1}^{m}(A_k^{(i)}-\\overline{A}^{(i)})^{T}(A_k^{(i)}-\\overline{A}^{(i)})其中$A_k=[(A_k^{(1)})^{T} \\ (A_k^{(2)})^{T} \\ …\\ (A_k^{(m)})^{T}]^{T},\\overline{A}=[(\\overline{A}^{(1)})^{T} \\ (\\overline{A}^{(2)})^{T} \\ …\\ (\\overline{A}^{(m)})^{T}]^{T}, \\ A_k^{(i)}, \\overline{A}^{(i)}$表示$A_k,\\overline{A}$的第$i$行 \\\\G_z=\\frac{1}{M}\\sum_{k=1}^{M}\\sum_{j=1}^{n}(A_k^{(j)}-\\overline{A}^{(j)})(A_k^{(j)}-\\overline{A}^{(j)})^{T}其中$A_k=[(A_k^{(1)}) \\ (A_k^{(2)}) \\ …\\ (A_k^{(n)})],\\overline{A}=[(\\overline{A}^{(1)}) \\ (\\overline{A}^{(2)}) \\ …\\ (\\overline{A}^{(n)})],A_k^{(j)},\\overline{A}^{(j)}$表示$A_k,\\overline{A}$的第$j$列 2.求出协方差矩阵的特征值及对应的特征向量 (U_x,\\sum x,V_x)=SVD(G_x) \\\\(U_z,\\sum z,V_z)=SVD(G_z)3.将特征向量按对应的特征值大小排列 X=U_x(:,1:k) \\in \\mathbb{R}^{n\\times k} \\\\Z=U_z(:,1:q) \\in \\mathbb{R}^{m\\times q}4.实现数据降维 C_{[q\\times k]}=Z^{T}_{[q\\times m]}A_{[m\\times n ]}X_{[n\\times k]}4.t-SNE4.1 原理可以参考 https://blog.csdn.net/scott198510/article/details/76099700 4.2代码123456789101112131415161718from sklearn.manifold import TSNEfrom matplotlib import pylabimport torchimport pandas as pdembdding=torch.load(path1)words = pd.read_csv(path2)words_embedded = TSNE(n_components=2).fit_transform(embdding)pylab.figure(figsize=(20, 20))for i, label in enumerate(words): x, y = words_embedded[i, :] pylab.scatter(x, y) pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')pylab.show() 5.auto encoder AutoEncoder在优化过程中无需使用样本的label，通过最小化重构误差希望学习到样本的抽象特征表示z，这种无监督的优化方式大大提升了模型的通用性。 参考https://zhuanlan.zhihu.com/p/68903857 https://blog.csdn.net/scott198510/article/details/76099700","link":"/2021/09/08/dimension-reduction/"},{"title":"数仓建模","text":"关系建模和维度建模是两种数据仓库的建模技术。关系建模由Bill Inmon所倡导，维度建模由Ralph Kimball所倡导。目前主流为维度建模。 https://zhuanlan.zhihu.com/p/362991213 1.关系建模（范式建模）1.1 范式1 目的 降低数据的冗余性 2 目前业界范式 第一范式(1NF)、第二范式(2NF)、第三范式(3NF)、巴斯-科德范式(BCNF)、第四范式(4NF)、第五范式(5NF)。逐个遵循，一般要求遵循第一，第二，第三范式，也就是三范式。 https://blog.csdn.net/Dream_angel_Z/article/details/45175621 1.2 建模 1 建模 关系建模将复杂的数据抽象为两个概念——实体和关系（实体表，关系表），并使用规范化（三范式）的方式表示出来 2 特点 关系模型严格遵循第三范式（3NF），数据冗余程度低，数据的一致性容易得到保证。 由于数据分布于众多的表中，查询会相对复杂，在大数据的场景下，查询效率相对较低。 2.维度建模https://www.jianshu.com/p/daab50a23c56 https://cloud.tencent.com/developer/article/1772027 2.1 事实表和维度表1 事实表 存储业务事实，事实表中的每行数据代表一个业务事件（下单、支付、退款、评价等）。 事实表的特征： ​ 内容相对的窄：列数较少（主要是外键id和度量值） ​ 非常的大 ​ 经常发生变化，每天会新增加很多。 分类：事务型事实表，周期型快照事实表，累积型快照事实表 2 维度表 维度表：一般是对事实的描述信息。每一张维表对应现实世界中的一个对象或者概念。 例如：用户、商品、日期、地区等。 维表的特征： ​ 维表的范围很宽（具有多个属性、列比较多） ​ 跟事实表相比，行数相对较小：通常&lt; 10万条 ​ 内容相对固定：编码表 2.2 维度模型分类在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。 星座模型是多个星型模型交织 2.3 建模 1 建模 维度模型面向业务，将业务用事实表和维度表呈现出来。 步骤： https://www.cnblogs.com/suheng01/p/13522677.html 选择业务过程→声明粒度→确认维度→确认事实 2 特点 维度模型以数据分析作为出发点，不遵循三范式，故数据存在一定的冗余。 表结构简单，故查询简单，查询效率较高。","link":"/2022/01/25/dimention-modeling/"},{"title":"Deep Interest Network for Click-Through Rate Prediction","text":"1.DEEP INTEREST NETWORK 1.1 特征表示特征可以表示为$\\textbf{x}=[t_1^T,t_2^T,…,t_M^T]^T$，one hot表示，举个例子如下 1.2 embedding层对于$t_i \\in \\mathbb{R}^{K_i}$，$W^i=[w_1^i,…,w_j^i,…,w_{K_i}^i] \\in \\mathbb{R}^{D\\times K_i} $ 1.3 Pooling layer and Concat layer \\textbf{e}_i=pooling(\\textbf{e}_{i_1},\\textbf{e}_{i_2},...,\\textbf{e}_{i_k})Two most commonly used pooling layers are sum pooling and average pooling, which apply element-wise sum/average operations to the list of embedding vectors. 1.4 Activation unitDIN就是在base的基础上加入local activation unit，作用是对用户行为特征的不同商品给与不同权重，其余保持不变，式子表示如下 \\mathcal{V}_{U}(A)=f(\\mathcal{V}_{A},\\textbf{e}_1,\\textbf{e}_2,...,\\textbf{e}_H)=\\sum_{j=1}^Ha(\\textbf{e}_j,\\mathcal{V}_{A})\\textbf{e}_j=\\sum_{j=1}^H\\textbf{w}_j\\textbf{e}_j其中$a(\\cdot)$为上图中activate unit,与attention很像，原文是Local activation unit of Eq.(3) shares similar ideas with attention methods which are developed in NMT task[1]. 1.5 MLP1.6 Loss交叉熵表示为： L=-\\frac{1}{N}\\sum_{(\\textbf{x},y) \\in \\textbf{S}}(ylogp(\\textbf{x})+(1-y)log(1-p(\\textbf{x})))2.训练技巧Practically, training industrial deep networks with large scale sparse input features is of great challenge. 引入Mini-batch Aware Regularization和Data Adaptive Activation Function，具体不在此介绍 参考原文 https://arxiv.org/pdf/1706.06978.pdf","link":"/2021/10/13/din/"},{"title":"距离&#x2F;相似度","text":"https://zhuanlan.zhihu.com/p/138107999 https://blog.csdn.net/txwh0820/article/details/51791739","link":"/2022/04/13/distance/"},{"title":"HIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICATION","text":"原版BERT的最大输入为512，为了使得BERT能解决超长文本的问题，作者在finetune阶段提出了两种策略来弥补这个问题，即利用BERT+LSTM或者BERT+transformer。 核心步骤： 1.split the input sequence into segments of a fixed size with overlap. 2.For each of these segments, we obtain H or P from BERT model. 3.We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer.//replacing the LSTM recurrent layer in favor of a small Transformer model 4.Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.","link":"/2021/09/17/doc-bert/"},{"title":"docker容器与虚拟机有什么区别？","text":"https://www.zhihu.com/question/48174633 一、物理机是这样的 二、虚拟机是这样的 三、容器是这样的","link":"/2022/03/16/docker-vm/"},{"title":"DSSM双塔模型系列","text":"简单介绍微软出品的DSSM,CNN-DSSM,LSTM-DSSM 原文分别为： 《Learning Deep Structured Semantic Models for Web Search using Clickthrough Data》 《A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval》 《SEMANTIC MODELLING WITH LONG-SHORT-TERM MEMORY FOR INFORMATION RETRIEVAL》 首先为什么叫做双塔，query塔做在线serving，doc塔离线计算embeding建索引，推到线上即可。 注意， DSSM中query和不同的doc是共享参数的， https://flashgene.com/archives/72820.html 一.DSSM1.1 模型整体结构 模型的整体结构如上图所示，$Q$为query，$D_i$为文档。 文本的初始词袋表示为$x$，因为参数过多，不利于训练，所以降低维度，就提出了word hashing l_1=W_1xword hashing其实就是利于char n-gram分词，然后用向量表示（只是这里依然用词袋表示向量，而不是稠密向量），如下所示 这里有个顾虑为是否存在不同的词使用相同的向量表示。关于这个作者做了实验，结果如下。 对于词汇数量500K大小的词表，采用3-gram后，此表压缩到30k，而且重复表示的仅为22个。重复表示率为0.0044%，维度压缩到原来6%，可以说非常有效。 然后为多层的非线性映射，每层都为全连接网络，得到 l_i=f(W_il_{i-1}+b_{i}),i=2,...,N-1\\\\非线性映射层的最后一层得到语义特征$y$为 y=f(W_Nl_{N-1}+b_N)\\\\ f(x)=tanh(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}利用余弦相似度衡量$Q$和$D$相似度得到 R(Q,D)=cosine(y_Q,y_D)=\\frac{y_Q^Ty_D}{||y_Q||||y_D||}最后的概率输出为 P(D|Q)=\\frac{e^{\\gamma R(Q,D)}}{\\sum_{D^{'}\\in \\textbf{D}}e^{\\gamma R(Q,D^{'})}}其中$\\gamma$为smoothing factor。 1.2 训练样本集构造，对每个正样本$(Q,D^+)$，搭配4个随机负样本$(Q,D_j^-;j=1,..,4)$ 损失函数为： L(\\wedge)=-log \\prod \\limits_{(Q,D^+)}P(D^+|Q)其中$\\wedge$为模型参数。 二.CNN-DSSM2.1 CLSM结构 模型包括几个部分：(1) a word-n-gram layer obtained by running a contextual sliding window over the input word sequence (2) a letter-trigram layer that transforms each word-trigram into a letter-trigram representation vector (3) a convolutional layer that extracts contextual features for each word with its neighboring words defined by a window (4) a max-pooling layer that discovers and combines salient word-n-gram features to form a fixed-length sentence-level feature vector (5) a semantic layer that extracts a high-level semantic feature vector for the input word sequence. 2.2 Letter-trigram based Word-n-gram Representation在DSSM的Letter-trigram的基础上加了Word-n-gram，Word-n-gram就是对原始输入文本做滑窗，对于第$t$个word-n-gram可以表示为： l_t=[f^T_{t-d},...,f^T_{t},...,f^T_{t+d}]^T,\\ t=1,2,...,T其中$n=2d+1,f_t$为的第$t$个词语的letter-trigram。一个letter-trigram的维度为$30K$，那么一个word-n-gram维度为$n\\times30K$ 举个例子，如上图，输入文本为$(s) \\ online \\ auto\\ body \\ (s)$，滑动窗口大小为n=3，可得$(s)\\ online \\ auto，\\ online \\ auto \\ body ，auto\\ body \\ (s) $，那么 $l_1=[f^T((s)),f^T(online ),f^T(auto)]^T,\\\\l_2=[f^T(online ),f^T(auto),f^T(body)]^T,\\\\l_3=[f^T(auto),f^T(body),f^T((s))]^T$ 2.3 Modeling Word-n-gram-Level Contextual Features at the Convolutional Layer语境相关特征向量$h_t$可以表示为： h_t=tanh(W_c\\cdot l_t),\\ t=1,...,T其中$W_c$为特征转换矩阵，也就是卷积矩阵，对于全部的word n-grams，$W_c$共享。有小伙伴肯定好奇，这不就是全连接吗，和卷积什么关系，俺也疑惑？ 下图为作者做的一个实验。 2.4 Modeling Sentence-Level Semantic Features Using Max Pooling获取局部的语境相关的特征向量后，我们需要把它们合在一起组合句子级别的特征向量。由于语句中某些词语不重要，我们可以忽略它，有些词语很重要，要保留。为了达到这个目的，使用了max pooling，用式子描述如下 v(i)= \\mathop{\\max}_{t=1,..,T} \\{h_t(i)\\},\\ i=1,...,K其中$v(i)$表示池化层输出$v$的第$i$个元素，$K$为$v$的维度和$h_t$的维度一样，$h_t(i)$是第$t$个局部语境特征向量的第$i$个元素。举个例子如下， 2.5 Latent Semantic Vector Representations语义向量表示$y$，用公式描述如下 y=tanh(W_s\\cdot v)2.6 Using the CLSM for IR和DSSM都一样， R(Q,D)=cosine(y_Q,y_D)=\\frac{y_Q^Ty_D}{||y_Q||||y_D||} \\\\P(D|Q)=\\frac{e^{\\gamma R(Q,D)}}{\\sum_{D^{'}\\in \\textbf{D}}e^{\\gamma R(Q,D^{'})}}2.7 损失函数 L(\\wedge)=-log \\prod \\limits_{(Q,D^+)}P(D^+|Q)三.LSTM-DSSMcnn-dssm只能捕获局部的文本信息，lstm对于长序列的信息捕获能力强于lstm，因此使用lstm改进dssm。 3.1 模型结构整体结构如下图，注意红色的部分为残差传递的方向。 图中的LSTM单元是LSTM的变种，加入了peep hole的 LSTM，具体结构如下。 参考https://www.cnblogs.com/guoyaohua/p/9229190.html","link":"/2021/08/18/dssm/"},{"title":"电商业务","text":"1 SPU，SKU SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。 SKU = Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。 例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。","link":"/2022/04/04/e-commerce-business-knowleage/"},{"title":"early stop","text":"","link":"/2022/06/01/early-stop/"},{"title":"ELMo(Deep contextualized word representations)","text":"引入了新的深度考虑上下文的词语表示，模型考虑了两个方面：（1）词语的复杂特性，包括语法和语义，（2）在语境中的不同含义。模型使用了深度双向语言模型，并且在大预料库上做了预训练。这个模型可以很方便地和现有的模型结合，并且在NLP的6个任务上取得了SOTA。作者还揭露了预训练网络的深层构件是关键，这使得下游模型能够混合不同类型的半监督信号。 3 ELMo: Embeddings from Language Models 模型的整体机构如上所示，由左右两个单向的多层LSTM网络构成，左边为正向，右边为反向。 3.1 Bidirectional language models（预训练）假定一个句子有$N$个token，分别为$(t_1,t_2,…,t_N)$，正向的语言模型的句子概率为： p(t_1,t_2,...,t_N)=\\prod_{k=1}^{N}p(t_k|t_1,t_2,...,t_{k-1})反向的语言模型的句子概率为： p(t_1,t_2,...,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},...,t_{N})得到正向和反向的语言后，将其结合可以得到双向的语言模型，这里取对数表示为： \\sum_{k=1}^N(log\\ p(t_k|t_1,t_2,...,t_{k-1};\\Theta_x,\\overrightarrow{\\Theta}_{LSTM} ,\\Theta_s )+log \\ p(t_k|t_{k+1},t_{k+2},...,t_{N};\\Theta_x,\\overleftarrow{\\Theta}_{LSTM} ,\\Theta_s) )\\\\其中$\\Theta_x$为token表示的参数，$\\Theta_s$为softmax层的参数，$\\overrightarrow{\\Theta}_{LSTM}$表示前向语言模型的参数，$\\overleftarrow{\\Theta}_{LSTM}$表示反向语言模型的参数。 3.2 ELMo（如何表示词向量）得到$L$层的预训练双向深度语言模型后，对于token $t_k$，一共包含了$2L+1$个相关的表示，集合如下 R_k=\\{x_{k}^{LM},\\overrightarrow{h^{LM}_{k,j}},\\overleftarrow{h^{LM}_{k,j}}|j=1,2,...,L \\}\\\\=\\{h_{k,j}^{LM} | j=0,...,L\\}注意$h_{k,0}^{LM}=x_{k}^{LM}，h_{k,j}^{LM}=[\\overrightarrow{h^{LM}_{k,j}};\\overleftarrow{h^{LM}_{k,j}}]$,其中$x_{k}^{LM}$为token表示，$\\overrightarrow{h^{LM}_{k,j}},\\overleftarrow{h^{LM}_{k,j}}$分别为正反向语言模型的表示 对于下游任务，需要将$2L+1$个表示压缩到一个向量$ELmo_k^{task}$，最简单的做法是只取顶层的表示，即 ELmo_k^{task}=E(R_k)=h_{k,L}^{LM}更加通用的做法为线形组合输出，如下图，公式表达为 ELmo_k^{task}=E(R_k,\\Theta^{task})=\\gamma^{task}\\sum_{j=0}^{L}s_{j}^{task}h_{k,j}^{LM}其中$\\gamma^{task}$用于缩放向量，$s_{j}^{task}$表示权重，通过下游任务学习。 3.3 Using biLMs for supervised NLP tasks（fine tune）对于下游任务模型，可以得到不考虑上下文的静态词向量$x_k$和考虑上下文的向量表示$h_k$ 对于一部分任务，将$x_k$和$ ELMo_k^{task}$ 拼接作为下游任务的特征：$[x_k;ELMo_k^{task}]$ 对于一部分任务，将 $h_k$和 $ ELMo_k^{task}$ 拼接可提升效果：$[h_k;ELMo_k^{task}]$ 参考https://blog.csdn.net/linchuhai/article/details/97170541 https://zhuanlan.zhihu.com/p/63115885 https://zhuanlan.zhihu.com/p/88993965 https://arxiv.org/abs/1802.05365","link":"/2021/08/19/elmo/"},{"title":"Enhanced-RCNN An Efficient Method for Learning Sentence Similarity","text":"特点：非预训练，参数量少 1 input encoding得到两个encoding，RNN Encoding，RCNN Encoding 1 BiGRU $\\textbf{a}=\\{a_1,a_2,…,a_{l_a}\\},\\textbf{a}$ 是句子，$l_a$ 是句子1的长度 得到RNN Encoding，$\\overline{\\textbf{p}}_i$统一表示$\\overline{\\textbf{a}}_i,\\overline{\\textbf{b}}_i$ 2 CNN 在 BiGRU 编码的基础上，使用 CNN 来进行二次编码 结构如下，“newtork in network”,k 是卷积核的kernel size，比如k=1,卷积核为$1 \\times 1$ 对于每个 CNN 单元，具体的计算过程如下: 得到 RCNN Encoding $\\widetilde{\\textbf{p}}_i$ 2 Interactive Sentence Representation1 Soft-attention Alignment attention： 加了attention的rnn encoding： 2 Interaction Modeling $\\overline{\\textbf{p}}$是rnn encoding $\\hat{}$是加了attention的rnn encoding $\\widetilde{}$是rcnn encoding 最终得到Interactive Sentence Representation为$\\textbf{o}_a,\\textbf{o}_b$ 3 Similarity Modeling1 Fusion Layer g是门控函数 2 Label Prediction 全连接层 4 loss交叉熵 参考https://sci-hub.st/10.1145/3366423.3379998 https://zhuanlan.zhihu.com/p/138061003","link":"/2021/12/14/enhanced-rcnn/"},{"title":"特征向量化","text":"特征-&gt;one hot-&gt; embedding 目的：one hot 表示特征太稀疏，不利于训练，而且参数过多，速度慢 举例子： https://arxiv.org/pdf/1706.06978.pdf DIN论文中的特征表示和embdding层","link":"/2021/11/22/emb/"},{"title":"熵，KL散度，交叉熵，JS散度","text":"GAN需要KL散度和JS散度，所以先预热。 1.熵信息量为： \\begin{align} I(x) &= - \\log(p(x)) \\tag{1} \\end{align}熵为信息量的算术平均： H(x) = - \\sum_{i=1}^{n}p(x_i)log(p(x_i)) \\tag{2}2.交叉熵交叉熵为 H(P,Q) = -\\sum_{i=1}^np(x_i)logq(x_i)\\tag{3} 3.KL散度对于同一个随机变量有两个单独的概率分布，我们可以使用KL散度(Kullback-Leibler divergence)来衡量两个分布的差异。在机器学习的损失函数的计算中，我们可以假设$P$为样本的真实分布，$Q$用来表示模型所预测的分布，使用KL散度来衡量两个分布之间的差异。KL散度等于交叉熵减去熵 \\begin{align} D_{KL}(P||Q) &= \\sum_{i=1}^np(x_i)log(\\frac{p(x_i)}{q(x_i)}) \\notag\\\\ &=\\sum_{i=1}^np(x_i)(logp(x_i)-logq(x_i)) \\notag\\\\ &=\\sum_{i=1}^n[p(x_i)logp(x_i)-p(x_i)logq(x_i)] \\notag\\\\ &=\\sum_{i=1}^np(x_i)logp(x_i)-\\sum_{i=1}^np(x_i)logq(x_i) \\\\ &=-H(P)+H(P,Q)\\tag{4} \\end{align}$P$和$Q$概率分布越接近，$D_{KL}(P||Q)$越小。 KL散度与交叉熵区别与联系 https://blog.csdn.net/Dby_freedom/article/details/83374650 KL散度主要有两个性质： （1）不对称性 尽管KL散度从直观上是个距离函数，但它并不是一个真正的度量，因为它不具有对称性，即$D_{KL}(P||Q)\\neq D_{KL}(Q||P)$。 （2）非负性 即$D_{KL}(P||Q) \\geq 0$。 4.JS散度JS散度也是用于度量两个概率分布的相似度，其解决了KL散度不对称的缺点 JS(P||Q) = \\frac{1}{2}KL(P||\\frac{P+Q}{2})+\\frac{1}{2}KL(Q||\\frac{P+Q}{2}) \\tag{5}不同于KL主要在两方面： （1）值域范围 JS散度的值域范围是[0,1]，相同则是0，相反为1。 （2）对称性 即$ JS(P||Q)=JS(Q||P)$，从数学表达式中就可以看出。 参考https://www.cnblogs.com/Mrfanl/p/11938139.html https://zhuanlan.zhihu.com/p/346518942 https://www.w3cschool.cn/article/83016451.html","link":"/2021/08/18/entropy/"},{"title":"集成学习","text":"目前常见的集成学习可以分类为：1.Bagging 2.Boosting 3.Stacking 4.Blending 1.Baggingbagging是解决variance问题。 2.Boostingboosting是解决bias问题。 Bagging，Boosting二者之间的区别 https://zhuanlan.zhihu.com/p/81340270 3.Stackingstacking和boosting的最大区别在于：boosting的基学习器是一个，stacking的基学习器是多个 4.Blending和stacking区别： https://www.jianshu.com/p/4380cd1def76 参考https://zhuanlan.zhihu.com/p/105038453 https://zhuanlan.zhihu.com/p/126968534 https://blog.csdn.net/starter_____/article/details/79328749","link":"/2021/09/06/ensemble/"},{"title":"ETL","text":"https://www.cnblogs.com/yjd_hycf_space/p/7772722.html https://blog.csdn.net/qq_33269009/article/details/90522087 https://blog.csdn.net/Stubborn_Cow/article/details/48420997 注意：很多人理解的ETL是在经过前两个部分之后，加载到数据仓库的数据库中就完事了。ETL不仅仅是在源数据—&gt;ODS这一步，ODS—&gt;DW, DW—&gt;DM包含更为重要和复杂的ETL过程。","link":"/2022/02/09/etl/"},{"title":"分类任务的类别数量很大(万以上)怎么处理？","text":"https://www.zhihu.com/question/387899184 Extreme Multi Label Classification，XML，可以提供一些启发 https://zhuanlan.zhihu.com/p/131584886","link":"/2021/10/25/extreme-num-classify/"},{"title":"Deep Learning Recommendation Model for Personalization and Recommendation Systems","text":"Facebook19年出品的推荐系统的paper，原文地址 https://arxiv.org/pdf/1906.00091.pdf DLRM 结构(deep learning recommendation model) 参考https://zhuanlan.zhihu.com/p/82839874","link":"/2021/10/11/facebook-recommend-sys/"},{"title":"fasttext","text":"1、文本分类1.1 n-gram由于Bag of words不考虑词语的顺序，因此引入bag of n-gram。针对英文，词内的是char n-gram，用于词向量；词之间的是word n-gram，用于分类；对于中文，存在词粒度和字粒度。 举个例子，句子A为”今天天气真不错”，这里以词粒度举例，先分词为[“今天”，”天气”，”真“，”不错“] uni-gram：今天 天气 真 不错 2-gram为：今天/天气 天气/真 真/不错 3-gram为：今天/天气/真 天气/真/不错 由于n-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采用了hashing trick的方式，如下图所示： 用哈希的方式既能保证查找时O(1)的效率，又可能把内存消耗控制在O(buckets * dim)范围内。不过这种方法潜在的问题是存在哈希冲突，不同的n-gram可能会共享同一个embedding。如果buckets取的足够大，这种影响会很小。 代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def build_dataset(config, ues_word): if ues_word: tokenizer = lambda x: x.split(' ') # word-level else: tokenizer = lambda x: [y for y in x] # char-level if os.path.exists(config.vocab_path): vocab = pkl.load(open(config.vocab_path, 'rb')) else: vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1) pkl.dump(vocab, open(config.vocab_path, 'wb')) print(f&quot;Vocab size: {len(vocab)}&quot;) def biGramHash(sequence, t, buckets): t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0 return (t1 * 14918087) % buckets def triGramHash(sequence, t, buckets): t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0 t2 = sequence[t - 2] if t - 2 &gt;= 0 else 0 return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets def load_dataset(path, pad_size=32): contents = [] with open(path, 'r', encoding='UTF-8') as f: for line in tqdm(f): lin = line.strip() if not lin: continue content, label = lin.split('\\t') words_line = [] token = tokenizer(content) seq_len = len(token) if pad_size: if len(token) &lt; pad_size: token.extend([PAD] * (pad_size - len(token))) else: token = token[:pad_size] seq_len = pad_size # word to id for word in token: words_line.append(vocab.get(word, vocab.get(UNK))) # fasttext ngram buckets = config.n_gram_vocab bigram = [] trigram = [] # ------ngram------ for i in range(pad_size): bigram.append(biGramHash(words_line, i, buckets)) trigram.append(triGramHash(words_line, i, buckets)) # ----------------- contents.append((words_line, int(label), seq_len, bigram, trigram)) return contents # [([...], 0), ([...], 1), ...] train = load_dataset(config.train_path, config.pad_size) dev = load_dataset(config.dev_path, config.pad_size) test = load_dataset(config.test_path, config.pad_size) return vocab, train, dev, test 1.2 网络结构 模型结构上word2vec的cbow模型很像 输入层：举个例子，输入文本”今天天气真不错”，词粒度的2-gram为 x_2=\\begin{bmatrix} emb_{今天/天气}，emb_{天气/真}，emb_{ 真/不错} \\end{bmatrix},emb为词向量矩阵 \\\\x_{1},x_{2},...,x_{N}最后输入到中间层的形式为: mean(\\begin{bmatrix}x_1 \\\\ x_2 \\\\...\\\\x_N \\end{bmatrix}),其中mean为对每个x的列求平均中间层：线形层+relu作为激活函数 输出层：为简单的线形层 代码： 123456789101112131415161718192021222324252627class Model(nn.Module): def __init__(self, config): super(Model, self).__init__() if config.embedding_pretrained is not None: self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False) else: self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1) self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed) self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed) self.dropout = nn.Dropout(config.dropout) self.fc1 = nn.Linear(config.embed * 3, config.hidden_size) # self.dropout2 = nn.Dropout(config.dropout) self.fc2 = nn.Linear(config.hidden_size, config.num_classes) def forward(self, x): out_word = self.embedding(x[0]) out_bigram = self.embedding_ngram2(x[2]) out_trigram = self.embedding_ngram3(x[3]) out = torch.cat((out_word, out_bigram, out_trigram), -1) out = out.mean(dim=1) out = self.dropout(out) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) return out 1.3 分层softmax对于分类问题，神经网络的输出结果需要经过softmax将其转为概率分布后才可以利用交叉熵计算loss 由于普通softmax的计算效率比较低，计算效率为$O(Kd)$使用分层的softmax时间复杂度可以达到$dlogK$，$K$为分类的数量，$d$为向量的维度 1.3.1 普通softmax假设输出为$Y_{pred}=[y_1,y_2,…,y_K]$,则$P_{y_i}$为 P_{y_i}=\\frac{e_{y_i}}{\\sum_{j=0}^Ke^{y_j}}其中$y_i$的维度为$d$，从公式可以看出计算效率为$O(Kd)$ 1.3.2 分层softmax霍夫曼树可以参考 https://zhuanlan.zhihu.com/p/154356949 为什么要霍夫曼，普通的不行？ 分层softmax核心思想为利用训练样本构建霍夫曼树，如下 树的结构是根据不同类在样本中出现的频次构造的，即频次越大的节点距离根节点越近。$K$个不同的类组成所有的叶子节点，$K-1个$内部节点作为参数。从根节点到某个叶子节点$y_i$经过的节点和边形成一条路径，路径长度表示为 $L_{y_i}$,$n_{(y_i,j)}$表示路径上的节点，那么 P_{y_i}=\\prod \\limits_{j=1}^{L_{y_i}}P_{(n(y_{i},j),left\\ or\\ right)} \\\\=\\prod \\limits_{j=0}^{L_{y_i}-1}\\sigma(f(n(y_i,j+1)==LC(n(y_i,j))){\\theta_{n(y_i,j)}^T} Y) \\\\其中LC(n(y_i,j)表示n(y_i,j)的左孩子，\\sigma 为SIGMOD函数，f(m)=\\begin{equation}\\left\\{ \\begin{aligned} 1 && if \\ m==true \\\\ -1 & & \\ else \\\\ \\end{aligned} \\right. \\end{equation}从公式可以看出时间复杂度降低至$dlogK$。 以图中$y_2$为例： P_{y_2}=P_{(n(y_{2},1),left)}\\cdot P_{(n(y_{2},2),left)}\\cdot P_{(n(y_{2},3),right)} \\\\=\\sigma({\\theta_{n(y_2,1)}^T} Y)\\cdot \\sigma({\\theta_{n(y_2,2)}^T} Y) \\cdot \\sigma({-\\theta_{n(y_2,3)}^T} Y)从根节点走到叶子节点 $y_2$ ，实际上是在做了3次逻辑回归。 2.训练词向量https://arxiv.org/abs/1607.04606 参考https://arxiv.org/abs/1607.01759 https://zhuanlan.zhihu.com/p/32965521 https://blog.csdn.net/qq_27009517/article/details/80676022 http://alex.smola.org/papers/2009/Weinbergeretal09.pdf https://arxiv.org/abs/1607.04606 fasttext工具 https://github.com/facebookresearch/fastText","link":"/2021/07/19/fasttext/"},{"title":"容错机制","text":"在分布式架构中，当某个节点出现故障，其他节点基本不受影响。这时只需要重启应用，恢复之前某个时间点的状态继续处理就可以了。这一切看似简单，可是在实时流处理中，我们不仅需要保证故障后能够重启继续运行，还要保证结果的正确性、故障恢复的速度、对处理性能的影响，这就需要在架构上做出更加精巧的设计。在Flink中，有一套完整的容错机制（ fault tolerance）来保证故障后的恢复，其中最重要的就是检查点（ checkpoint）。在第九章中，我们已经介绍过检查点的基本概念和用途，接下来我 们就深入探讨一下检查点的原理和 Flink的容错机制。","link":"/2022/03/27/fault-tolerance/"},{"title":"特征工程","text":"https://zhuanlan.zhihu.com/p/111296130","link":"/2021/10/21/feature-en/"},{"title":"特征提取器","text":"大致分为三类：CNN，LSTM，transformer block 1.CNN 滑动部分为卷积核 2.LSTM 3.transformer block 参考https://www.cnblogs.com/sandwichnlp/p/11612596.html#transformer","link":"/2021/07/24/feature-extractor/"},{"title":"feature scale","text":"","link":"/2022/08/04/feature-scale/"},{"title":"Felix Flexible Text Editing Through Tagging and Insertion","text":"google继lasertagger之后的又一篇text edit paper In contrast to conventional sequence-to-sequence (seq2seq) models, FELIX is efficient in low-resource settings and fast at inference time, while being capable of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input. 1 Introduction In particular, we have designed FELIX with the following requirements in mind: Sample efficiency, Fast inference time, Flexible text editing 2 Model descriptionFELIX decomposes the conditional probability of generating an output sequence $y$ from an input$x$ as follows: p(\\textbf{y}|\\textbf{x})=p_{ins}(\\textbf{y}|\\textbf{y}^m)p_{tag}(\\textbf{y}^t,\\pi|\\textbf{x})2.1 Tagging Modeltrained to optimize both the tagging and pointing loss: \\mathcal{L}=\\mathcal{L}_{pointing }+\\lambda\\mathcal{L}_{tagging }Tagging : tag sequence $\\textbf{y}^t$由3种tag组成：$KEEP$，$DELETE$，$INSERT (INS)$ Tags are predicted by applying a single feedforward layer $f$ to the output of the encoder $\\textbf{h}^L$ (the source sentence is first encoded using a 12-layer BERT-base model). $\\textbf{y}^t_i=argmax(f(\\textbf{h}^L_i))$ Pointing: Given a sequence $\\textbf{x}$ and the predicted tags $\\textbf{y}^t$ , the re-ordering model generates a permutation $\\pi$ so that from $\\pi$and $\\textbf{y}^t$ we can reconstruct the insertion model input $\\textbf{y}^m$. Thus we have: p(\\textbf{y}^m|\\textbf{x}) \\approx \\prod \\limits_{i}p(\\pi(i)|\\textbf{x},\\textbf{y}^t,i)p(\\textbf{y}_i^t|\\textbf{x})Our implementation is based on a pointer network. The output of this model is a series of predicted pointers (source token → next target token) The input to the Pointer layer at position $i$: \\textbf{h}^{L+1}_{i}=f([\\textbf{h}^{L}_{i};e(\\textbf{y}_i^t);e(\\textbf{p}_i)])其中$e(\\textbf{y}_i^t)$is the embedding of the predicted tag，$e(\\textbf{p}_i)$ is the positional embedding The pointer network attends over all hidden states, as such: p(\\pi(i)|\\textbf{h}_i^{L+1})=attention(\\textbf{h}_i^{L+1},\\textbf{h}_{\\pi(i)}^{L+1})其中$\\textbf{h}_i^{L+1}$ as $Q $, $\\textbf{h}_{\\pi(i)}^{L+1}$ as $K$ When realizing the pointers, we use a constrained beam search 2.2 Insertion Model To represent masked token spans we consider two options: masking and infilling. In the former case the tagging model predicts how many tokens need to be inserted by specializing the $INSERT$ tag into $INS_k$, where $k$ translates the span into $ k$ $MASK$ tokens. For the infilling case the tagging model predicts a generic $INS$ tag. Note that we preserve the deleted​ span in the input to the insertion model by enclosing it between $[REPL]$ and $[/REPL]$ tags. our insertion model is also based on a 12-layer BERT-base and we can directly take advantage of the BERT-style pretrained checkpoints. 参考https://aclanthology.org/2020.findings-emnlp.111.pdf","link":"/2021/09/30/felix/"},{"title":"Generalizing from a Few Examples A Survey on Few-Shot Learning","text":"paper： https://arxiv.org/abs/1904.05046 git: https://github.com/tata1661/FSL-Mate/tree/master/FewShotPapers#Applications 原文按应用对FSL做了总结，与NLP相关的有： High-risk learning: Acquiring new word vectors from tiny data, in EMNLP, 2017. A. Herbelot and M. Baroni. paper MetaEXP: Interactive explanation and exploration of large knowledge graphs, in TheWebConf, 2018. F. Behrens, S. Bischoff, P. Ladenburger, J. Rückin, L. Seidel, F. Stolp, M. Vaichenker, A. Ziegler, D. Mottin, F. Aghaei, E. Müller, M. Preusse, N. Müller, and M. Hunger. paper code Few-shot representation learning for out-of-vocabulary words, in ACL, 2019. Z. Hu, T. Chen, K.-W. Chang, and Y. Sun. paper Learning to customize model structures for few-shot dialogue generation tasks, in ACL, 2020. Y. Song, Z. Liu, W. Bi, R. Yan, and M. Zhang. paper Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network, in ACL, 2020. Y. Hou, W. Che, Y. Lai, Z. Zhou, Y. Liu, H. Liu, and T. Liu. paper Meta-reinforced multi-domain state generator for dialogue systems, in ACL, 2020. Y. Huang, J. Feng, M. Hu, X. Wu, X. Du, and S. Ma. paper Few-shot knowledge graph completion, in AAAI, 2020. C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V. Chawla. paper Universal natural language processing with limited annotations: Try few-shot textual entailment as a start, in EMNLP, 2020. W. Yin, N. F. Rajani, D. Radev, R. Socher, and C. Xiong. paper code Simple and effective few-shot named entity recognition with structured nearest neighbor learning, in EMNLP, 2020. Y. Yang, and A. Katiyar. paper code Discriminative nearest neighbor few-shot intent detection by transferring natural language inference, in EMNLP, 2020. J. Zhang, K. Hashimoto, W. Liu, C. Wu, Y. Wan, P. Yu, R. Socher, and C. Xiong. paper code Few-shot learning for opinion summarization, in EMNLP, 2020. A. Bražinskas, M. Lapata, and I. Titov. paper code Adaptive attentional network for few-shot knowledge graph completion, in EMNLP, 2020. J. Sheng, S. Guo, Z. Chen, J. Yue, L. Wang, T. Liu, and H. Xu. paper code Few-shot complex knowledge base question answering via meta reinforcement learning, in EMNLP, 2020. Y. Hua, Y. Li, G. Haffari, G. Qi, and T. Wu. paper code Self-supervised meta-learning for few-shot natural language classification tasks, in EMNLP, 2020. T. Bansal, R. Jha, T. Munkhdalai, and A. McCallum. paper code Uncertainty-aware self-training for few-shot text classification, in NeurIPS, 2020. S. Mukherjee, and A. Awadallah. paper code Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction, in NeurIPS, 2020:. J. Baek, D. B. Lee, and S. J. Hwang. paper code MetaNER: Named entity recognition with meta-learning, in TheWebConf, 2020. J. Li, S. Shang, and L. Shao. paper Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters &amp; less data, in ICLR, 2021. J. Pilault, A. E. hattami, and C. Pal. paper code Revisiting few-sample BERT fine-tuning, in ICLR, 2021. T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y. Artzi. paper code Few-shot conversational dense retrieval, in SIGIR, 2021. S. Yu, Z. Liu, C. Xiong, T. Feng, and Z. Liu. paper code Relational learning with gated and attentive neighbor aggregator for few-shot knowledge graph completion, in SIGIR, 2021. G. Niu, Y. Li, C. Tang, R. Geng, J. Dai, Q. Liu, H. Wang, J. Sun, F. Huang, and L. Si. paper Few-shot language coordination by modeling theory of mind, in ICML, 2021. H. Zhu, G. Neubig, and Y. Bisk. paper code Graph-evolving meta-learning for low-resource medical dialogue generation, in AAAI, 2021. S. Lin, P. Zhou, X. Liang, J. Tang, R. Zhao, Z. Chen, and L. Lin. paper KEML: A knowledge-enriched meta-learning framework for lexical relation classification, in AAAI, 2021. C. Wang, M. Qiu, J. Huang, and X. He. paper Few-shot learning for multi-label intent detection, in AAAI, 2021. Y. Hou, Y. Lai, Y. Wu, W. Che, and T. Liu. paper code SALNet: Semi-supervised few-shot text classification with attention-based lexicon construction, in AAAI, 2021. J.-H. Lee, S.-K. Ko, and Y.-S. Han. paper Learning from my friends: Few-shot personalized conversation systems via social networks, in AAAI, 2021. Z. Tian, W. Bi, Z. Zhang, D. Lee, Y. Song, and N. L. Zhang. paper code Relative and absolute location embedding for few-shot node classification on graph, in AAAI, 2021. Z. Liu, Y. Fang, C. Liu, and S. C.H. Hoi. paper Few-shot question answering by pretraining span selection, in ACL-IJCNLP, 2021. O. Ram, Y. Kirstain, J. Berant, A. Globerson, and O. Levy. paper code A closer look at few-shot crosslingual transfer: The choice of shots matters, in ACL-IJCNLP, 2021. M. Zhao, Y. Zhu, E. Shareghi, I. Vulic, R. Reichart, A. Korhonen, and H. Schütze. paper code Learning from miscellaneous other-classwords for few-shot named entity recognition, in ACL-IJCNLP, 2021. M. Tong, S. Wang, B. Xu, Y. Cao, M. Liu, L. Hou, and J. Li. paper code Distinct label representations for few-shot text classification, in ACL-IJCNLP, 2021. S. Ohashi, J. Takayama, T. Kajiwara, and Y. Arase. paper code Entity concept-enhanced few-shot relation extraction, in ACL-IJCNLP, 2021. S. Yang, Y. Zhang, G. Niu, Q. Zhao, and S. Pu. paper code On training instance selection for few-shot neural text generation, in ACL-IJCNLP, 2021. E. Chang, X. Shen, H.-S. Yeh, and V. Demberg. paper code Unsupervised neural machine translation for low-resource domains via meta-learning, in ACL-IJCNLP, 2021. C. Park, Y. Tae, T. Kim, S. Yang, M. A. Khan, L. Park, and J. Choo. paper code Meta-learning with variational semantic memory for word sense disambiguation, in ACL-IJCNLP, 2021. Y. Du, N. Holla, X. Zhen, C. Snoek, and E. Shutova. paper code Multi-label few-shot learning for aspect category detection, in ACL-IJCNLP, 2021. M. Hu, S. Z. H. Guo, C. Xue, H. Gao, T. Gao, R. Cheng, and Z. Su. paper TextSETTR: Few-shot text style extraction and tunable targeted restyling, in ACL-IJCNLP, 2021. P. Rileya, N. Constantb, M. Guob, G. Kumarc, D. Uthusb, and Z. Parekh. paper Few-shot text ranking with meta adapted synthetic weak supervision, in ACL-IJCNLP, 2021. S. Sun, Y. Qian, Z. Liu, C. Xiong, K. Zhang, J. Bao, Z. Liu, and P. Bennett. paper code PROTAUGMENT: Intent detection meta-learning through unsupervised diverse paraphrasing, in ACL-IJCNLP, 2021. T. Dopierre, C. Gravier, and W. Logerais. paper code AUGNLG: Few-shot natural language generation using self-trained data augmentation, in ACL-IJCNLP, 2021. X. Xu, G. Wang, Y.-B. Kim, and S. Lee. paper code Meta self-training for few-shot neural sequence labeling, in KDD, 2021. Y. Wang, S. Mukherjee, H. Chu, Y. Tu, M. Wu, J. Gao, and A. H. Awadallah. paper code Knowledge-enhanced domain adaptation in few-shot relation classification, in KDD, 2021. J. Zhang, J. Zhu, Y. Yang, W. Shi, C. Zhang, and H. Wang. paper code Few-shot text classification with triplet networks, data augmentation, and curriculum learning, in NAACL-HLT, 2021. J. Wei, C. Huang, S. Vosoughi, Y. Cheng, and S. Xu. paper code Few-shot intent classification and slot filling with retrieved examples, in NAACL-HLT, 2021. D. Yu, L. He, Y. Zhang, X. Du, P. Pasupat, and Q. Li. paper Non-parametric few-shot learning for word sense disambiguation, in NAACL-HLT, 2021. H. Chen, M. Xia, and D. Chen. paper code Towards few-shot fact-checking via perplexity, in NAACL-HLT, 2021. N. Lee, Y. Bang, A. Madotto, and P. Fung. paper ConVEx: Data-efficient and few-shot slot labeling, in NAACL-HLT, 2021. M. Henderson, and I. Vulic. paper Few-shot text generation with natural language instructions, in EMNLP, 2021. T. Schick, and H. Schütze. paper Towards realistic few-shot relation extraction, in EMNLP, 2021. S. Brody, S. Wu, and A. Benton. paper code Few-shot emotion recognition in conversation with sequential prototypical networks, in EMNLP, 2021. G. Guibon, M. Labeau, H. Flamein, L. Lefeuvre, and C. Clavel. paper code Learning prototype representations across few-shot tasks for event detection, in EMNLP, 2021. V. Lai, F. Dernoncourt, and T. H. Nguyen. paper Exploring task difficulty for few-shot relation extraction, in EMNLP, 2021. J. Han, B. Cheng, and W. Lu. paper code Honey or poison? Solving the trigger curse in few-shot event detection via causal intervention, in EMNLP, 2021. J. Chen, H. Lin, X. Han, and L. Sun. paper code Nearest neighbour few-shot learning for cross-lingual classification, in EMNLP, 2021. M. S. Bari, B. Haider, and S. Mansour. paper Knowledge-aware meta-learning for low-resource text classification, in EMNLP, 2021. H. Yao, Y. Wu, M. Al-Shedivat, and E. P. Xing. paper code Few-shot named entity recognition: An empirical baseline study, in EMNLP, 2021. J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng, J. Gao, and J. Han. paper MetaTS: Meta teacher-student network for multilingual sequence labeling with minimal supervision, in EMNLP, 2021. Z. Li, D. Zhang, T. Cao, Y. Wei, Y. Song, and B. Yin. paper Meta-LMTC: Meta-learning for large-scale multi-label text classification, in EMNLP, 2021. R. Wang, X. Su, S. Long, X. Dai, S. Huang, and J. Chen. paper","link":"/2021/12/12/few-shot/"},{"title":"前馈神经网络","text":"Feedforward neural network，FNN 和全连接神经网络的区别？？？","link":"/2022/08/20/ffn/"},{"title":"finetune","text":"1 使用哪些层参与下游任务使用哪些层参与下游任务 选择的层model1+下游任务model2 对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。 用$\\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\\cdot)$为特定的任务模型。有以下几种方法选择representation: a) Embedding Only choose only the pre-trained static embeddings，即$g(\\textbf{H}^{1})$ b) Top Layer 选择顶层的representation，然后接入特定的任务模型，即$g(\\textbf{H}^{L})$ c) All Layers 输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下 g(\\textbf{r}_t)=g(\\gamma \\sum_{l=1}^{L}\\alpha_l\\textbf{H}^{(l)})其中$\\alpha$ is the softmax-normalized weight for layer $l$ and $\\gamma$ is a scalar to scale the vectors output by pre-trained model 2 参数是否固定总共有两种常用的模型迁移方式：feature extraction (where the pre-trained parameters are frozen), and fine-tuning (where the pre-trained parameters are unfrozen and fine-tuned). 3 Fine-Tuning StrategiesTwo-stage fine-tuning 第一阶段为中间任务，第二阶段为目标任务 Multi-task fine-tuning multi-task learning and pre-training are complementary technologies. Fine-tuning with extra adaptation modules The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed. Others self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing 参考https://arxiv.org/pdf/2003.08271v4.pdf","link":"/2022/06/11/finetune/"},{"title":"flask","text":"python web应用框架，后端 编写逻辑 123456789101112from flask import Flask, request, jsonify##创建应用app1 = Flask(__name__)###绑定函数@app1.route('URL rule', methods=['post'])def process_element_data(): return##启动应用app1.run(....) 执行逻辑1 启动应用2 前端发送请求3 根据URL rule找函数，返回结果给前端","link":"/2022/06/08/flask/"},{"title":"flink分层api","text":"Flink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。","link":"/2022/03/20/flink-api/"},{"title":"流批选择","text":"之前版本 1234//流StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();//批ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 现在版本 通过执行模式 execution mode选择 1 流处理 streaming 默认 2 批处理 batch 3 自动 automatic （1） 通过命令行 1flink run -Dexecution.runtime-mode=BATCH/../.. （2）代码 123env.setRuntimeMode(RuntimeExecutionMode.STREAMING); env.setRuntimeMode(RuntimeExecutionMode.BATCH); env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);","link":"/2022/05/07/flink-batch-stream/"},{"title":"flink cdc","text":"CDC是 Change Data Capture(变更数据获取 )的简称。 核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入 、 更新 以及 删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。 Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、 PostgreSQL 等数据库直接 读取全量数据 和 增量变更数据 的 source 组件。 说白了就是连接数据库，然后实时监控变化","link":"/2022/05/15/flink-cdc/"},{"title":"flink cep","text":"0 简介类似的多个事件的组合，我们把它叫作“复杂事件”。对于复杂时间的处理，由于涉及到事件的严格顺序，有时还有时间约束，我们很难直接用 SQL或DataStream API来完成。于是只好放大招 派底层的处理函数（ process function）上阵了。处理函数确实可以搞定这些需求，不过对于非常复杂的组合事件，我们可能需要设置很多状态、定时器，并在代码中定义各种条件分支（ if else）逻辑来处理，复杂度会非常高，很可能会使代码失去可读性。怎 样处理这类复杂事件呢？ Flink为我们提供了专门用于处理复杂事件的库 CEP，可以让我们更加轻松地解决这类棘手的问题。这在企业的实时风险控制中有非常重要的作用。 Complex Event Processing，flink 专门用来处理复杂事件的库 1 原理cep底层是状态机 复杂事件可以通过设计状态机来处理，用户自己写容易出错，cep帮我们封装好，用户写顶层逻辑就可以了 2 核心步骤 总结起来，复杂事件处理（CEP）的流程可以分成三个步骤（1）定义一个匹配规则（2）将匹配规则应用到事件流上，检测满足规则的复杂事件（3）对检测到的 复杂事件进行处理，得到结果进行输出","link":"/2022/03/27/flink-cep/"},{"title":"flink部署","text":"Flink的部署方式是灵活的，跟Spark一样，支持Local，Standalone，Yarn，Mesos，Kubernetes 代码中好像不能指定部署方式，和spark不同 https://blog.csdn.net/qq_33689414/article/details/90671685 1 Local最简单的启动方式，其实是不搭建集群，直接本地启动。本地部署非常简单，直接解压安装包就可以使用，不用进行任何配置；一般用来做一些简单的测试。 2 Standalone 会话模式，应用模式 区别在于jobmaster的启动时间点，会话预先启动，应用在作业提交启动 3 Yarn1 会话 session 在会话模式下，我们需要先启动一个YARN session，这个会话会创建一个 Flink集群。 2 单作业 per-job flink不会预先启动，在提交作业，才启动新的jobmanager 3 应用application 与单作业很相似 区别在于提交给yarn资源管理器的不是具体作业，而是整个应用（包含了多个作业） 4 Mesos5 Kubernetes","link":"/2022/03/12/flink-deploy/"},{"title":"算子链","text":"多个算子合并 合并条件：1 并行度相同的算子 2 一对一 one to one 好处：1. 减少线程之间的切换和缓存区的数据交换 2 减少时延 3 提高吞吐量","link":"/2022/04/25/flink-operator-chain/"},{"title":"flink优化","text":"https://shopify.engineering/optimizing-apache-flink-applications-tips https://cloud.tencent.com/developer/article/1897249 1 广播 https://blog.csdn.net/weixin_44318830/article/details/107678101 广播是一种操作 如果不使用广播，每一个 Task 都会拷贝一份数据集，造成内存资源浪费 ; 广播后，每个节点存一份,不同的Task 都可以在节点上获取到 1 广播变量 https://blog.csdn.net/yang_shibiao/article/details/118662134 2 广播流 BroadcastStream 3 广播状态 BroadcastState","link":"/2022/04/23/flink-optimization/"},{"title":"并行度设置","text":"https://blog.csdn.net/hongzhen91/article/details/90812686 一个任务的并行实例(线程)数目就被称为该任务的并行度 并行度设置层次 1 Operator Level（算子层次） 12345678910111213141516171819202122232425262728293031323334353637383940setParallelismreduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() { @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception { // 将累加器更新为当前最大的pv统计值，然后向下游发送累加器的值 return value1.f1 &gt; value2.f1 ? value1 : value2; } }).setParallelism(5) .print(); (Mary,1)(Bob,1)(Mary,2)(Bob,2)(Mary,3)(Bob,3)(Mary,4)(Bob,4)keyBy(r -&gt; true) // 为每一条数据分配同一个key，将聚合结果发送到一条流中去 .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() { @Override public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception { // 将累加器更新为当前最大的pv统计值，然后向下游发送累加器的值 return value1.f1 &gt; value2.f1 ? value1 : value2; } }) .print().setParallelism(5); 2&gt; (Bob,2)1&gt; (Mary,2)1&gt; (Bob,4)3&gt; (Mary,3)5&gt; (Bob,1)5&gt; (Mary,4)4&gt; (Mary,1)4&gt; (Bob,3) 2Execution Environment Level（执行环境层次） 3Client Level（客户端层次） 4System Level（系统层次） 优先级1&gt;2&gt;3&gt;4","link":"/2022/05/11/flink-parallel/"},{"title":"处理函数(process funtion)","text":"处理函数位于底层，操作麻烦，但是使用更加灵活，是flink的“核武器”，轻易不用，但是一定行。 在处理函数中，我们直面的就是数据流中最基本的元素：数据事件（event）、状态 state以及时间（ time）。 https://blog.51cto.com/u_15349018/3698518 1 分类8种不同的处理函数 每个处理函数使得的时候注意两个关键函数 1 processElement 必须 元素基本处理 2 onTimer() 非必须 就是设置定时器，然后触发操作 2 侧输出流（ Side Output）1 主流 collect 2 分流 处理函数的processElement或者onTimer中使用.output （outputTag，数据） 获取侧输出流 1Stream.getSideOutput(outputTag)","link":"/2022/03/22/flink-processfunction/"},{"title":"Flink程序构成部分","text":"⚫ 获取执行环境（ execution environment）⚫ 读取数据源（ source）⚫ 定义基于数据的转换操作（ transformations）⚫ 定义计算结果的输出位置（ sink） 1 source1 从集合中读取数据最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。 2 从文件读取数据真正的实际应用中，自然不会直接将数据写在代码中。通常情况下，我们会从存储介质中获取数据，一个比较常见的方式就是读取日志文件。这也是批处理中最常见的读取方式。 1DataStream&lt;String&gt; stream = env.readTextFile(&quot;clicks .csv“); 3 sockethttps://www.jianshu.com/p/cb26a0f6c622 socket文本流的读取需要配置两个参数：发送端主机名和端口 文本流数据的发送，可以通过 Linux系统自带的 netcat工具进行模拟。 1nc -lk 7777 4 kafka5 自定义 Source3 sink1 输出到文件2 输出到 Kafka3 输出到 Redis4 输出到 Elasticsearch5 输出到 MySQL (JDBC)6 自定义 Sink输出","link":"/2022/03/20/flink-program-struct/"},{"title":"flink vs spark","text":"数据模型1 spark 采用 RDD 模型， spark streaming 的 DStream 实际上也就是一组 组小批数据 RDD 的集合2 flink 基本数据模型是数据流，以及事件（ Event ）序列运行时架构1 spark 是批计算，将 DAG 划分为不同的 stage ，一个完成后才可以计算下一个2 flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理","link":"/2022/03/12/flink-spark/"},{"title":"Flink架构原理","text":"1.Flink组件 1 JobManager 2 任务管理器（ TaskManager） 2.任务调度流程","link":"/2022/03/12/flink-srtuct/"},{"title":"状态编程","text":"0 状态管理机制1 算子任务分类1 无状态 2 有状态 2 状态分类Flink 有两种状态：托管状态（Managed State）和原始状态（Raw State）。一般情况使用托管状态，只有在托管状态无法实现特殊需求，才会使用原始转态，一般情况不使用。 托管状态分类：算子状态（Operator State）和按键分区状态（Keyed State） 1 按键分区状态 2 算子状态 3 广播状态 Broadcast State 特殊的算子状态 3 状态持久化 对状态进行持久化（ persistence）保存，这样就可以在发生故障后进行重启恢复。 flink状态持久化方式：写入一个“检查点”（ checkpoint）或者保存点 savepoint保存到外部存储系统中。具体的存储介质，一般是分布式文件系统（ distributed file system）。 4 状态后端 State Backends在Flink中，状态的存储、访问以及维护，都是由一个可插拔的组件决定的，这个组件就叫作状态后端（ state backend）。状态后端主要负责两件事：一是本地的状态管理，二是将检查点（ checkpoint）写入远程的 持久化存储。","link":"/2022/03/27/flink-state-program/"},{"title":"Table API和SQL","text":"https://blog.csdn.net/weixin_45366499/article/details/115449175 0 原理1 动态表 flink中的表是动态表 静态表：hive，mysql等 动态表：不断更新 2 持续查询 1 简介Apache Flink 有两种关系型 API 来做流批统一处理：Table API 和 SQL。 Table API 是用于 Scala 和 Java 语言的查询 API，它可以用一种非常直观的方式来 组合使用选取、过滤、join 等关系型算子。 123Table maryClickTable = eventTable.where($(&quot;user&quot;).isEqual(&quot;alice&quot;)).select($(&quot;url&quot;), $(&quot;user&quot;)); SQL 是基于 Apache Calcite 来实现的标准 SQL 123Table urlCountTable = tableEnv.sqlQuery(&quot;SELECT user, COUNT(url) FROM EventTable GROUP BY user&quot;); 2 框架表环境和流执行环境不同 3 流表相互转化stream 《——》table 123456tableEnv表环境// 将数据流eventstream转换成表eventTableTable eventTable = tableEnv.fromDataStream(eventstream);// 将表visitTable转换成数据流，打印输出tableEnv.toDataStream(visitTable).print(); 4 连接外部系统可以在创建表的时候用 WITH子句指定连接器connector 5 客户端./bin/sql client.sh 6 时间属性 事件事件、处理事件 在创建表的 DDL中定义 在数据流转换为表时定义 7 窗口","link":"/2022/03/27/flink-tableapi-sql/"},{"title":"任务生成和分配","text":"main代码 -》 数据流图（dataflow graph，logical streamgraph） -》 作业图（jobgraph）-》执行图（executiongraph）-&gt; 物理图（physical graph）","link":"/2022/04/25/flink-task-assign/"},{"title":"任务槽 task slots","text":"slot共享 并行度：算子的子任务个数 程序的并行度：最大算子并行度 假设： 设置全局并行度为6，保持sink为1 source，map(6) -》keyby。。。(6) -》sink(1) 总共有13个子任务 2 个taskmanger , 每个taskmanger 3个slot","link":"/2022/04/25/flink-task-slot/"},{"title":"flink提交任务","text":"https://codeantenna.com/a/Y6wpSYwfRL 1.web ui https://blog.csdn.net/godelgnis/article/details/106051751 2.命令行 可以指定部署方式 https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/cli/ https://blog.csdn.net/weixin_42993799/article/details/106566037 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051 参数必选 ： -n,--container &lt;arg&gt; 分配多少个yarn容器 (=taskmanager的数量) 2 参数可选 ： -D &lt;arg&gt; 动态属性 -d,--detached 独立运行 -jm,--jobManagerMemory &lt;arg&gt; JobManager的内存 [in MB] -nm,--name 在YARN上为一个自定义的应用设置一个名字 -q,--query 显示yarn中可用的资源 (内存, cpu核数) -qu,--queue &lt;arg&gt; 指定YARN队列. -s,--slots &lt;arg&gt; 每个TaskManager使用的slots数量 -tm,--taskManagerMemory &lt;arg&gt; 每个TaskManager的内存 [in MB] -z,--zookeeperNamespace &lt;arg&gt; 针对HA模式在zookeeper上创建NameSpace -id,--applicationId &lt;yarnAppId&gt; YARN集群上的任务id，附着到一个后台运行的yarn session中 3 run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; run操作参数: -c,--class &lt;classname&gt; 如果没有在jar包中指定入口类，则需要在这里通过这个参数指定 -m,--jobmanager &lt;host:port&gt; 指定需要连接的jobmanager(主节点)地址，使用这个参数可以指定一个不同于配置文件中的jobmanager -p,--parallelism &lt;parallelism&gt; 指定程序的并行度。可以覆盖配置文件中的默认值。 4 启动一个新的yarn-session,它们都有一个y或者yarn的前缀 例如：./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar 连接指定host和port的jobmanager： ./bin/flink run -m SparkMaster:1234 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1 启动一个新的yarn-session： ./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1 5 注意：命令行的选项也可以使用./bin/flink 工具获得。 6 Action &quot;run&quot; compiles and runs a program. Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;run&quot; action options: -c,--class &lt;classname&gt; Class with the program entry point (&quot;main&quot; method or &quot;getPlan()&quot; method. Only needed if the JAR file does not specify the class in its manifest. -C,--classpath &lt;url&gt; Adds a URL to each user code classloader on all nodes in the cluster. The paths must specify a protocol (e.g. file://) and be accessible on all nodes (e.g. by means of a NFS share). You can use this option multiple times for specifying more than one URL. The protocol must be supported by the {@link java.net.URLClassLoader}. -d,--detached If present, runs the job in detached mode -n,--allowNonRestoredState Allow to skip savepoint state that cannot be restored. You need to allow this if you removed an operator from your program that was part of the program when the savepoint was triggered. -p,--parallelism &lt;parallelism&gt; The parallelism with which to run the program. Optional flag to override the default value specified in the configuration. -q,--sysoutLogging If present, suppress logging output to standard out. -s,--fromSavepoint &lt;savepointPath&gt; Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537). 7 Options for yarn-cluster mode: -d,--detached If present, runs the job in detached mode -m,--jobmanager &lt;arg&gt; Address of the JobManager (master) to which to connect. Use this flag to connect to a different JobManager than the one specified in the configuration. -yD &lt;property=value&gt; use value for given property -yd,--yarndetached If present, runs the job in detached mode (deprecated; use non-YARN specific option instead) -yh,--yarnhelp Help for the Yarn session CLI. -yid,--yarnapplicationId &lt;arg&gt; Attach to running YARN session -yj,--yarnjar &lt;arg&gt; Path to Flink jar file -yjm,--yarnjobManagerMemory &lt;arg&gt; Memory for JobManager Container with optional unit (default: MB) -yn,--yarncontainer &lt;arg&gt; Number of YARN container to allocate (=Number of Task Managers) -ynl,--yarnnodeLabel &lt;arg&gt; Specify YARN node label for the YARN application -ynm,--yarnname &lt;arg&gt; Set a custom name for the application on YARN -yq,--yarnquery Display available YARN resources (memory, cores) -yqu,--yarnqueue &lt;arg&gt; Specify YARN queue. -ys,--yarnslots &lt;arg&gt; Number of slots per TaskManager -yst,--yarnstreaming Start Flink in streaming mode -yt,--yarnship &lt;arg&gt; Ship files in the specified directory (t for transfer) -ytm,--yarntaskManagerMemory &lt;arg&gt; Memory per TaskManager Container with optional unit (default: MB) -yz,--yarnzookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode -z,--zookeeperNamespace &lt;arg&gt; Namespace to create the Zookeeper sub-paths for high availability mode","link":"/2022/03/14/flink-task/"},{"title":"时间语义(Notions of Time)","text":"https://blog.csdn.net/lomodays207/article/details/109642581 在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从 1.12版本 开始 Flink已经将 事件时间作为了默认的时间语义。 1.处理时间（ Processing Time)处理时间的概念非常简单，就是指执行处理操作的机器的系统时间。 2.事件时间（ Event Time)事件时间，是指每个事件在对应的设备上发生的时间，也就是数据生成的时间。 3 摄入时间（ Ingestion Time）它是指数据进入 Flink数据流的时间，也就是 Source算子读入数据的时间。","link":"/2022/03/21/flink-time/"},{"title":"watermark(水位线)","text":"https://blog.csdn.net/lmalds/article/details/52704170 https://blog.csdn.net/lightupworld/article/details/116697831 1 介绍在实际应用中，一般会采用事件时间语义 水位线可以看作在数据流中加入一个时钟标记，记录当前的事件时间；这个标记可以直接广播到下游，当下游任务收到这个标记，就可以更新自己的时钟了。 2 分类1 有序流的水位线 方框是事件时间，虚线是水位线，箭头表示数据到达的顺序，比如事件时间为2的数据第一个到，事件时间为5的数据第二个到 2 乱序流的水位线 比水位线小的数据在后面出现，也就是说本应该在水位线之前出现的数据晚到了，就是迟到数据，迟到数据是丢弃的，比如w（9）后面的8，9 3 如何生成水位线1 原则 我们知道，完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。而完美的东西总是可望不可即，我们只能尽量去保证水位线的正确。 “等” 或者说“延迟” 为了均衡实时性（少等，会引入大量迟到数据）和准确性（减少迟到数据，多等） 2 怎么写 https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/event-time/generating_watermarks/ 1 assignTimestampsAndWatermarks 1Datastream &lt;event&gt; withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(WatermarkStrategy) WatermarkStrategy 内置 12WatermarkStrategy.forMonotonousTimestampsWatermarkStrategy.forBoundedOutOfOrderness 自定义 先实现接口WatermarkGenerator，然后改写一些东西 2 自定义数据源中发送水位线 注意：自定义数据源也可以用assignTimestampsAndWatermarks，这里是指在自定义数据源中实现自己的水位线发送，不用assignTimestampsAndWatermarks 4 水位线传递 上游并行度为4，下游并行度为3 策略：木桶原理，取得是最小的 为什么木桶原则，以准为第一要义，举个例子图2，如果不是3，是4，那么3不就迟到了吗","link":"/2022/03/21/flink-watermark/"},{"title":"flume","text":"1.作用日志采集 Flume是流式日志采集工具，FLume提供对数据进行简单处理并且写到各种数据接收方（可定制）的能力，Flume提供从本地文件（spooling directory source）、实时日志（taildir、exec）、REST消息、Thift、Avro、Syslog、Kafka等数据源上收集数据的能力。 2.Flume架构https://jiandansuifeng.blog.csdn.net/article/details/118926483 https://www.jianshu.com/p/9a5c682b0551 三大核心组件： Source:数据源 Channel:临时存储数据的管道 Sink: 目的地 3.拦截器(interceptor)https://blog.51cto.com/u_15241496/2869403 拦截器是简单的插件式组件，设置在source和channel之间。source接收到的事件event，在写入channel之前，拦截器都可以进行转换或者删除这些事件。每个拦截器只处理同一个source接收到的事件。可以自定义拦截器。 1 jar包 2 将打包的jar放到XXX/flume/lib 3 配置文件 /opt/module/flume/conf vim file-flume-kafka.conf 123a1.sources.r1.interceptors = i1a1.sources.r1.interceptors.i1.type =com.atguigu.flume.interceptor.ETLInterceptor$Builder 4.为什么要集成Flume和Kafkahttps://blog.csdn.net/qq_37466640/article/details/103425555 5.例子 4.1 采集日志 1）Source （1）Taildir Source相比Exec Source、Spooling Directory Source的优势 TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。不会丢数据，但是有可能会导致数据重复。 Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。 Spooling Directory Source监控目录，支持断点续传。 （2）batchSize大小如何设置？ 答：Event 1K左右时，500-1000合适（默认为100） 2）Channel 采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。 注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。 3）Sink 省去了Sink，提高了效率 4）Interceptor ETLInterceptor 4.2 消费数据 1）Source 2）Channel MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。 金融类公司、对钱要求非常准确的公司通常会选择FileChannel 传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。 3）Sink HDFS Sink 4）Interceptor TimeStampInterceptor flume时间拦截器 获取日志中的实际时间","link":"/2022/01/30/flume/"},{"title":"pytorch中模型的forward方法是如何被自动调用的","text":"https://blog.csdn.net/weixin_41912543/article/details/108147378","link":"/2021/12/07/forward/"},{"title":"GBDT","text":"GBDT （Gradient Boosting Decison Tree）=Gradient Boosting+cart回归树 注意是cart回归树，不是cart分类树 说白了就是gradient boosting基学习器为cart回归树 gradient boosting算法流程:1.初始化：$f_0(x) = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{i=1}^N L(y_i, \\gamma)$ 2.for m=1 to M: (a). 计算负梯度： $\\tilde{y}_i = -\\frac{\\partial L(y_i,f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}, \\quad i = 1,2 \\cdots N$ (b). 通过最小化平方误差，用基学习器$h_m(x)$拟合$\\tilde{y_i}$，$w_m = \\mathop{\\arg\\min}\\limits_w \\sum\\limits_{i=1}^{N} \\left[\\tilde{y}_i - h_m(x_i\\,;\\,w) \\right]^2$ (c). 使用line search确定步长$ρ_m$，使$L$最小，$\\rho_m = \\mathop{\\arg\\min}\\limits_{\\rho} \\sum\\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \\rho h_m(x_i\\,;\\,w_m))$ (d). $f_m(x) = f_{m-1}(x) + \\rho_m h_m(x\\,;\\,w_m)$ 3.输出$f_M(x)$ GBDT算法流程： 初始化： $f_0(x) = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{i=1}^N L(y_i, \\gamma)$ for m=1 to M:(a). 计算负梯度： $\\tilde{y}_i = -\\frac{\\partial L(y_i,f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}, \\qquad i = 1,2 \\cdots N$(b). $\\left \\{ R_{jm} \\right\\}_1^J = \\mathop{\\arg\\min}\\limits_{\\left \\{ R_{jm} \\right\\}_1^J}\\sum\\limits_{i=1}^N \\left [\\tilde{y}_i - h_m(x_i\\,;\\,\\left \\{R_{jm},b_{jm} \\right\\}_1^J) \\right]^2$(c). $\\gamma_{jm} = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{x_i \\in R_{jm}}L(y_i,f_{m-1}(x_i)+\\gamma)$(d). $f_m(x) = f_{m-1}(x) + \\sum\\limits_{j=1}^J \\gamma_{jm}I(x \\in R_{jm})$ 输出$f_M(x)$ 参考https://blog.csdn.net/zpalyq110/article/details/79527653 https://zhuanlan.zhihu.com/p/86354141","link":"/2021/09/24/gbdt/"},{"title":"图神经工具","text":"PyG， DGL对比 https://www.zhihu.com/question/399802947","link":"/2021/12/16/gnn-trick/"},{"title":"gpt","text":"GPT三部曲宣告NLP的“预训练+微调”时代的崛起和走向辉煌。 原文分别为： 《Improving Language Understanding by Generative Pre-Training》 《Language Models are Unsupervised Multitask Learners》 《Language Models are Few-Shot Learners》 1.GPT1 模型的整体结构如上图所示。使用过程过程分为两步：第一步预训练，利用大量语料学习得到high-capacity的语言模型；第二步是fine_tuning，利用标签数据使其拟合到特定任务。 1.1 Unsupervised pre-training作者将transformer decoder中Encoder-Decoder Attention层去掉后作为基本单元，然后多层堆叠作为语言模型的主体，然后将输出经过一个softmax层，来得到目标词的输出分布： h_0=UW_e+W_p \\\\h_l=transformer\\_block(h_{l-1}),\\ \\forall l \\in [1,n] \\\\P(u|u_{-k},...,u_{-1}) =softmax(h_nW_e^T)\\其中$U=\\{u_{-k},…,u_{-1}\\}$ 是预测词$u $前$k$个token的独热编码序列，$n$是模型的层数，$W_e$是token embedding matrix，$W_p$是position embedding matrix。 给定一个无监督的语料库$\\mathcal{U}$，use a standard language modeling objective to maximize the following likelihood L_1(\\mathcal{U})=\\sum_ilog P(u_i|u_{i-k},...,u_{i-1})其中$k$ 是上下文窗口大小。 1.2 Supervised fine-tuning对于数据集$\\mathcal{C}$，有数据$(x^1,x^2,…,x^m,y)$ P(y|x^1,x^2,...,x^m)=softmax(h_l^mW_y) \\\\L_2(\\mathcal{C})=\\sum_{(x,y)}log P(y|x^1,x^2,...,x^m)其中$W_y$为全连接层的参数 作者发现，使用语言模型来辅助监督学习进行微调，有两个好处： 提高监督模型的泛化能力； 加速收敛。 所以，最终下游使用的监督模型损失函数为： L_3(\\mathcal{C})=L_2(\\mathcal{C})+\\lambda*L_1(\\mathcal{C})1.3 Task-specific input transformations 所有的输入文本都会加上开始和结合token$(s),(e)$ 分类 分类过程可如上1.2，输入表示为$[(s);Context;(e)]$ 文本蕴含 将输入拼接成$[(s); premise; ($) ; hypothesis ; (e)]$ 相似度 由于文本相似度与两个比较文本的前后顺序没有关系，因此将两种文本顺序都考虑进来，如上图所示 问答与常识推理 假设文档为$z$，问题为$q$，一系列答案为$\\{a_k\\}$，将其输入表示为$[(s); z; q; ($); a_k;(e)]$，然后多个回答组合的形式，如上图。 2.GPT2总结就是：多任务预训练+超大数据集+超大规模模型。通过一个超大数据集涵盖NLP的大多任务，然后使用一个超大规模模型进行多任务预训练，使其无需任何下游任务的finetune就可以做到多个NLP任务的SOTA。举个例子，拿高考为例，人的智力和脑容量可以理解为参数大小，由于个体差异，可以将不同的学生理解为不同参数量的模型，卷子可以理解为数据集，不同的学科可以理解为不同任务。GPT2有点类似学霸，就是有超高的智力和脑容量，然后刷大量不同学科的题目，因此对高考这个多任务的下游任务就可以取得好成绩。 GPT2相对于GPT1有哪些不同呢？ GPT2去掉了fine-tuning：不再针对不同任务分别进行微调建模，模型会自动识别出来需要做什么任务。这就好比一个人博览群书，你问他什么类型的问题，他都可以顺手拈来，GPT2就是这样一个博览群书的模型。 超大数据集：WebText，该数据集做了一些简单的数据清理，并且实验结果表明目前模型仍然处于一个欠拟合的情况。 增加网络参数：GPT2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。15亿什么概念呢，Bert的参数量也才只有3亿哦~当然，这样的参数量也不是说谁都能达到的，这也得取决于money的多少啊~ 调整transformer：将layer normalization放到每个sub-block之前，并在最后一个transformer后再增加一个layer normalization，如下图。 输入表示：GPT2采用了BPE这种subword的结构作为输入 其他：GPT2将词汇表数量增加到50257个；最大的上下文大小 (context size) 从GPT的512提升到了1024 tokens；batchsize增加到512。 GPT2的输入是完全的文本，什么提示都不加吗？ 当然不是，它也会加入提示词，比如：$TL;DR:$，GPT2模型就会知道是做摘要工作了，输入的格式就是 $文本+TL;DR:$，然后就等待输出就行了~ 3.GPT3GPT3，这是一种具有1750亿个参数的超大规模模型，比GPT2大100倍，感觉真是进入算力时代了。距离个人用户太远了，就不深挖了。 参考https://zhuanlan.zhihu.com/p/146719974 https://zhuanlan.zhihu.com/p/125139937 https://www.cnblogs.com/yifanrensheng/p/13167796.html#_label1_0 https://www.jianshu.com/p/96c5d5d5c468 https://blog.csdn.net/qq_35128926/article/details/111399679 https://zhuanlan.zhihu.com/p/96791725 https://terrifyzhao.github.io/2019/02/18/GPT2.0%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html https://zhuanlan.zhihu.com/p/56865533","link":"/2021/08/26/gpt/"},{"title":"Gradient Accumulation","text":"https://blog.csdn.net/u013546508/article/details/121157559 https://blog.csdn.net/Princeicon/article/details/108058822","link":"/2021/11/10/gradient-accumulate-steps/"},{"title":"梯度爆炸、梯度消失和解决方法","text":"1.梯度设二元函数$z=f(x,y)$ 在平面区域$D$上具有一阶连续偏导数，则对于每一个点$P(x，y)$的梯度为 grad \\ f(x,y)=\\nabla f(x,y)=f_x(x,y)\\vec{j}+ f_y(x,y)\\vec{j}2.BP算法图示 3.梯度消失和梯度爆炸梯度爆炸和梯度消失问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。 举个例子，现有如上链式连接的网络$(x\\rightarrow z \\rightarrow y)$ \\frac{\\partial C }{\\partial b_1}=\\frac{\\partial C }{\\partial y_4}\\frac{\\partial y_4 }{\\partial z_4}\\frac{\\partial z_4 }{\\partial x_4}\\frac{\\partial x_4 }{\\partial z_3}\\frac{\\partial z_3 }{\\partial x_3}\\frac{\\partial x_3 }{\\partial z_2}\\frac{\\partial z_2 }{\\partial x_2}\\frac{\\partial x_2 }{\\partial z_1}\\frac{\\partial z_1 }{\\partial b_1}=\\frac{\\partial C }{\\partial y_4}g^{'}(z_4)w_4g^{'}(z_3)w_3g^{'}(z_2)w_2g^{'}(z_1)w_1假设$g$为sigmoid，那么$g^{‘}(z)$最大值为$\\frac{1}{4}$，而我们初始化的网络权值通常都小于1，所以$g^{‘}(z)w \\le \\frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\\frac{\\partial C }{\\partial b_1}$越小，因而导致梯度消失的情况出现。 这样，梯度爆炸问题的出现原因就显而易见了，当$w$比较大的时候或者激活函数的梯度较大，即$g^{‘}(z)w &gt; 1$，层数越多，求导结果$\\frac{\\partial C }{\\partial b_1}$越大，直到爆炸。 4.梯度消失和梯度爆炸解决方法4.1 解决梯度消失1.用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。 2.用Batch Normalization。 3.LSTM的结构设计也可以改善RNN中的梯度消失问题。 4.残差网络 5.合适的初始化权重 4.2解决梯度爆炸1.梯度剪切：对梯度设定阈值 2.权重正则化(L1 和 L2 ) 3.合适的初始化权重 参考https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/ https://zhuanlan.zhihu.com/p/25631496 https://aijishu.com/a/1060000000100195","link":"/2021/09/02/gradient/"},{"title":"gradient boosting","text":"推导过程Gradient Boosting为boosting算法的一种，采用和AdaBoost同样的加法模型，在第m次迭代中，前m-1个基学习器都是固定的，即 f_m(x) = f_{m-1}(x) + \\rho_m h_m(x) \\tag{1}核心思想是得到基学习器$h_m(x)$和权重$p_m$ 参数空间的梯度下降很常见，即 \\theta = \\theta - \\alpha \\cdot \\frac{\\partial}{\\partial \\theta}L(\\theta)若将$f(x)$当成参数，则同样可以使用函数空间的梯度下降法： f_m(x) = f_{m-1}(x) - \\rho_m \\cdot \\frac{\\partial}{\\partial f_{m-1}(x)}L(y,f_{m-1}(x)) \\tag{2}对比（1）（2），我们发现$h_m(x) \\approx -\\frac{\\partial L(y,f_{m-1}(x))}{\\partial f_{m-1}(x)}$ 算法流程:1.初始化：$f_0(x) = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{i=1}^N L(y_i, \\gamma)$ 2.for m=1 to M: (a). 计算负梯度： $\\tilde{y}_i = -\\frac{\\partial L(y_i,f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}, \\quad i = 1,2 \\cdots N$ (b). 通过最小化平方误差，用基学习器$h_m(x)$拟合$\\tilde{y_i}$，$w_m = \\mathop{\\arg\\min}\\limits_w \\sum\\limits_{i=1}^{N} \\left[\\tilde{y}_i - h_m(x_i\\,;\\,w) \\right]^2$ (c). 使用line search确定步长$ρ_m$，使$L$最小，$\\rho_m = \\mathop{\\arg\\min}\\limits_{\\rho} \\sum\\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \\rho h_m(x_i\\,;\\,w_m))$ (d). $f_m(x) = f_{m-1}(x) + \\rho_m h_m(x\\,;\\,w_m)$ 3.输出$f_M(x)$ 参考https://www.cnblogs.com/zhubinwang/p/5170087.html https://www.cnblogs.com/massquantity/p/9174746.html","link":"/2021/09/23/gradient_boosting/"},{"title":"A Comprehensive Survey on Graph Neural Networks","text":"there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. 1.介绍虽然深度学习技术可以捕获欧式空间数据的隐藏模式，但是目前很多应用是基于图的，这是非欧空间的数据。图数据的复杂性给现有的技术带来了很大的挑战。这是因为图数据可以是不规则的，一个图可能有不同数量的无序结点，一个结点可能有不同数量的邻接结点。这会使得一些基本操作，比如卷积，在图领域无法很好的捕获特征。除此之外，目前机器学习算法有一个重要的假设，就是假设各个结点是相互独立的，然而，图中存在很多复杂的连接信息，主要用来表征结点间的互相关性。为了解决以上问题，衍生了很多图神经网络技术。举个例子，比如，图卷积。下图对比了传统的2D卷积和图卷积。二者最大的区别在于邻接结点，一个有序一个无序，一个尺寸固定一个尺寸可变。 2.背景和定义A. 背景Graph neural networks vs network embedding The main distinction between GNNs and network embedding is that GNNs are a group of neural network models which are designed for various tasks while network embedding covers various kinds of methods targeting the same task. Graph neural networks vs graph kernel methods The difference is that this mapping function of graph kernel methods is deterministic rather than learnable. GNNs are much more efficient than graph kernel methods. B. 定义 上表为本文的notation。 1.图 $ {G}=(V,E) $表示一个图。$N(v)=\\{u\\in V|(v,u)\\in E\\}$表示结点$v$的邻接结点。$\\textbf{A}$是邻接矩阵，如果$A_{ij}=1$,那么表示$e_{ij}\\in E$；如果$A_{ij}=0$,那么表示$e_{ij} \\notin E$。$\\textbf{X} \\in \\mathbb{R}^{n \\times d} $是结点特征矩阵，$\\textbf{X}^{e} \\in \\mathbb{R}^{m \\times c}$是边特征矩阵。 2.有向图 A graph is undirected if and only if the adjacency matrix is symmetric. 3.时空图 A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time. $G^{(t)}=(V,E,\\textbf{X}^{(t)})，\\textbf{X}^{(t)} \\in \\mathbb{R}^{n \\times d}$ 3.分类和框架3.1 GNN分类作者把GNN分成以下4类，分别为RecGNNs，ConvGNNs , GAEs, STGNNs。 RecGNNs（Recurrent graph neural networks） RecGNNs aim to learn node representations with recurrent neural architectures. They assume a node in a graph constantly exchanges information message with its neighbors until a stable equilibrium is reached. ConvGNNs（Convolutional graph neural networks ） The main idea is to generate a node $v$’s representation by aggregating its own features $\\textbf{x}_v$ and neighbors’ features $\\textbf{x}_u,u\\in N(v)$。Different from RecGNNs, ConvGNNs stack multiple graph convolutional layers to extract high-level node representations. GAEs（Graph autoencoders） are unsupervised learning frameworks which encode nodes/graphs into a latent vector space and reconstruct graph data from the encoded information. GAEs are used to learn network embeddings andgraph generative distributions. STGNNs（Spatial-temporal graph neural networks） aim to learn hidden patterns from spatial-temporal graphs. The key idea of STGNNs is to consider spatial dependency and temporal dependency at the same time. 3.2 框架With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms: Node-level outputs relate to node regression and node classification tasks. Edge-level outputs relate to the edge classification and link prediction tasks. Graph-level outputs relate to the graph classification task. Training Frameworks： 1.Semi-supervised learning for node-level classification 2.Supervised learning for graph-level classification 3.Unsupervised learning for graph embedding 4.RecGNNsRecGNNs apply the same set of parameters recurrently over nodes in a graph to extract high-level node representations. 接下来介绍几种RecGNNs 结构。 GNN* Based on an information diffusion mechanism, GNN* updates nodes’ states by exchanging neighborhood information recurrently until a stable equilibrium is reached. 结点的hidden state is recurrently updated by \\textbf{h}_v^{(t)}=\\sum_{u\\in N(v)}f(\\textbf{x}_v,\\textbf{x}^e_{(v,u)},\\textbf{x}_{u},\\textbf{h}_{u}^{(t-1)})$\\textbf{h}_v^0$随机初始化。$f(\\cdot)$是 parametric function，must be a contraction mapping, which shrinks the distance between two points after projecting them into a latent space. 训练过程分为两步，更新结点表示和更新参数，交替进行使得loss收敛。When a convergence criterion is satisfied, the last step node hidden states are forwarded to a readout layer. GraphESN GraphESN使用ESN提高GNN*的训练效率。GraphESN包含encoder和output output。encoder随机初始化并且不需要训练。It implements a contractive state transition function to recurrently update node states until the global graph state reaches convergence. Afterward, the output layer is trained by taking the fixed node states as inputs. Gated Graph Neural Networks (GGNNs) \\textbf{h}_{v}^t=GRU(\\textbf{h}_{v}^{t-1},\\sum_{u\\in N(v)}\\textbf{W}h_{u}^t)The adavantage is that it no longer needs to constrain parameters to ensure convergence. However, the downside of training by BPTT is that it sacrifices efficiency both in time and memory. GGNN RecGNNs 利用GRU作为循环函数 \\textbf{h}_v^{(t)}=GRU(\\textbf{h}_v^{(t-1)},\\sum_{u\\in N(v)}\\textbf{W}\\textbf{h}_u^{(t-1)})其中$\\textbf{h}_v^{(0)}=\\textbf{x}_v$。 GGNN uses the back-propagation through time (BPTT) algorithm to learn the model parameters. 对于大的图不适用。 SSE proposes a learning algorithm that is more scalable to large graphs \\textbf{h}_{v}^{(t)}=(1-\\alpha)\\textbf{h}_{v}^{（t-1）}+\\alpha \\textbf{W}_1 \\sigma(\\textbf{W}_2[\\textbf{x}_v,\\sum_{u\\in N(v)}[\\textbf{h}_u^{t-1},\\textbf{x}_u]])其中$\\alpha$为超参数，$\\sigma(\\cdot)$为sigmoid函数。 5.ConvGNNs ConvGNNs与RecGNNs 主要区别在于上图。 ConvGNNs fall into two categories, spectral-based and spatial-based. Spectral based approaches define graph convolutions by introducing filters from the perspective of graph signal processing [82] where the graph convolutional operation is interpreted as removing noises from graph signals. Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation. spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality. 5.1 Spectral-based ConvGNNs5.2 Spatial-based ConvGNNs罗列几个基本的结构。 NN4G \\textbf{h}_{v}^{(k)}=f(\\textbf{W}^{(k)^T}\\textbf{x}_v+\\sum_{i=1}^{k-1}\\sum_{u\\in N(v) }\\Theta^{(k)^{T}}\\textbf{h}_{u}^{(k-1)})其中$f(\\cdot)$是激活函数，$\\textbf{h}_{v}^{(0)}=0$，可以使用矩阵形式表达为： \\textbf{H}^{(k)}=f(\\textbf{X}\\textbf{W}^{(k)}+\\sum_{i=1}^{k-1}\\textbf{A}\\textbf{H}^{k-1}\\Theta^{(k)})DCNN regards graph convolutions as a diffusion process. \\textbf{H}^{(k)}=f(\\textbf{W}^{(k)}\\odot\\textbf{P}^k\\textbf{X} )其中$f(\\cdot)$是激活函数。probability transition matrix $\\textbf{P}\\in\\mathbb{R}^{n\\times n},\\textbf{P} = \\textbf{D}^{-1}\\textbf{A}$。 DCNN concatenates $\\textbf{H}^{(1)},\\textbf{H}^{(2)},…,\\textbf{H}^{(K)}$together as the final model outputs. PGC-DGCNN MPNN 5.3 Graph Pooling ModulesAfter a GNN generates node features, we can use them for the final task. But using all these features directly can be computationally challenging, thus, a down-sampling strategy is needed. Depending on the objective and the role it plays in the network, different names are given to this strategy: (1) the pooling operation aims to reduce the size of parameters by down-sampling the nodes to generate smaller representations and thus avoid overfitting, permutation invariance, and computational complexity issues; (2) the readout operation is mainly used to generate graph-level representation based on node representations. Their mechanism is very similar. In this chapter, we use pooling to refer to all kinds of down-sampling strategies applied to GNNs. mean/max/sum pooling is the most primitive and effective way ： \\textbf{h}_G=mean/max/sum(\\textbf{h}_1^{(K)},\\textbf{h}_2^{(K)},...,\\textbf{h}_n^{(K)})$K$ is the index of the last graph convolutional layer. some works [17], [27], [46] also use attention mechanisms to enhance the mean/sum pooling. [101] propose the Set2Set method to generate a memory that increases with the size of the input. 还有SortPooling，DiffPool等 6.GAEs7.STGNNs8.APPLICATIONS参考https://arxiv.org/abs/1901.00596v4","link":"/2021/08/10/graph-nn-survey/"},{"title":"Graph和Session","text":"graph定义了计算方式（计算流程），本身不会进行任何计算 session帮助graph计算 可以定义多个graph，例如一个graph实现z = x + y，另一个graph实现u = 2 * v 参考https://www.jianshu.com/p/b636de7c251a https://blog.csdn.net/qq_40242197/article/details/105315219","link":"/2022/06/02/graph-session/"},{"title":"hadoop架构","text":"1 总览https://blog.csdn.net/wangxudongx/article/details/104079998 2 MapReducehttps://zhuanlan.zhihu.com/p/377204048 3 hdfs0 读写原理https://blog.csdn.net/whdxjbw/article/details/81072207 1 文件格式https://www.cnblogs.com/wqbin/p/14635480.html Hadoop中的文件格式大致上分为面向行和面向列两类 面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。 面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。 2 上传文件1.页面 2.命令行 https://blog.csdn.net/tandelin/article/details/89514784 4 RPCRemote Procdure Call，中文名：远程过程调用 它允许一台计算机程序远程调用另外一台计算机的子程序，而不用去关心底层的网络通信细节 Hadoop的进程间交互都是通过RPC来进行的，比如Namenode与Datanode https://www.cnblogs.com/SmallBird-Nest/p/11430330.html","link":"/2022/01/28/hadoop-framewowk/"},{"title":"Hadoop部署方式","text":"https://blog.csdn.net/aohun0743/article/details/101702331 https://cloud.tencent.com/developer/article/1924241?from=article.detail.1336692 本地模式 单机运行，只是用来演示一下官方案例。生产环境不用。 伪分布式模式 也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。 完全分布式模式 多台服务器组成分布式环境。生产环境使用。 搭建： https://blog.csdn.net/a1786742005/article/details/104104983 高可用完全分布式模式 HA高可用是Hadoop2.x才开始引入的机制，是为了解决Hadoop的单点故障问题。主要有两种部署方式，一种是NFS（Network File System）方式，另外一种是QJM（Quorum Journal Manager）方式。用得较多的是QJM方式，稳定性更好。实际操作中，生产环境的Hadoop集群搭建一般都会做HA部署。","link":"/2022/01/26/hadoop-persudo/"},{"title":"hadoop SafeMode","text":"https://blog.csdn.net/bingduanlbd/article/details/51900512 https://developer.aliyun.com/article/566059# https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode During start up the NameNode loads the file system state from the fsimage and the edits log file. It then waits for DataNodes to report their blocks so that it does not prematurely start replicating the blocks though enough replicas already exist in the cluster. During this time NameNode stays in Safemode. Safemode for the NameNode is essentially a read-only mode for the HDFS cluster, where it does not allow any modifications to file system or blocks. Normally the NameNode leaves Safemode automatically after the DataNodes have reported that most file system blocks are available. If required, HDFS could be placed in Safemode explicitly using bin/hadoop dfsadmin -safemode command. NameNode front page shows whether Safemode is on or off. A more detailed description and configuration is maintained as JavaDoc for setSafeMode(). 安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求）","link":"/2022/02/06/hadoop-safe-mode/"},{"title":"Hadoop in Secure Mode","text":"https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#:~:text=When%20Hadoop%20is%20configured%20to,or%20%2Fetc%2Fhosts%20files. This document describes how to configure authentication for Hadoop in secure mode. By default Hadoop runs in non-secure mode in which no actual authentication is required.By configuring Hadoop runs in secure mode, each user and service needs to be authenticated by Kerberos in order to use Hadoop services.","link":"/2022/02/06/hadoop-security/"},{"title":"hadoop trick","text":"1 集群数据均衡1 节点间数据均衡 （1）开启数据均衡命令 start-balancer.sh -threshold 10 （2）停止数据均衡命令 stop-balancer.sh 2 磁盘间数据均衡 （1）生成均衡计划（我们只有一块磁盘，不会生成计划） hdfs diskbalancer -plan hadoop103 （2）执行均衡计划 hdfs diskbalancer -execute hadoop103.plan.json （3）查看当前均衡任务的执行情况 hdfs diskbalancer -query hadoop103 （4）取消均衡任务 hdfs diskbalancer -cancel hadoop103.plan.json 2 数据压缩https://cloud.tencent.com/developer/article/1417401 1 LZO压缩 LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。 1[atguigu@hadoop102 bin]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_log/dt=2020-06-14 3 Hadoop参数调优https://developer.aliyun.com/article/566013 1）HDFS参数调优 2）YARN参数调优","link":"/2022/01/28/hadoop-trick/"},{"title":"哈希表","text":"1 哈希函数 2 哈希冲突https://blog.51cto.com/u_15077556/3984082 https://www.jianshu.com/p/585f8882bbfb 3 效率插入和查找的时间复杂度都是为O(1) 参考https://zhuanlan.zhihu.com/p/144296454","link":"/2022/06/16/hashtable/"},{"title":"HBase","text":"0 简介Apache HBase is the Hadoop database, a distributed, scalable, big data store. HBase is a type of “NoSQL” database. HBase是一种构建在HDFS之上的分布式、面向列的存储系统。 Hadoop已经有了HDFS和MapReduce，为什么需要HBase 1 Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于HadoopMapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求。 2 传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）。传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间。 HBase与传统的关系数据库的区别 1、数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串。 2、数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系。 3、存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的。 4、数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来。 5、数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留。 6、可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩。 1 配置https://www.cnblogs.com/frankdeng/p/9310191.html 启动成功jps后可以看到hmaster ，hregionservice 2 hbase shell不支持sql，对表操作需要使用hbase shell命令或者hbase api 3 Phoenix在hbase上构建SQL层，使得hbase 能够使用标准SQL管理数据，Phoenix中的sql语句还是有些不同的 4 问题1 org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet https://cloud.tencent.com/developer/article/1812290 2 stop-hbase.sh关闭不了，一直处于等待状态 https://blog.csdn.net/weixin_45462732/article/details/106909501 3 hregionservice启动就挂了 看日志 参考https://www.cnblogs.com/wendyw/p/12691971.html#_label3 https://juejin.cn/post/6844903777347043336 https://www.jianshu.com/p/53864dc3f7b4 https://www.cnblogs.com/frankdeng/p/9310191.html https://blog.csdn.net/weixin_45462732/article/details/106909501 中文文档 http://hbase.org.cn/docs/166.html#regionserver.arch","link":"/2022/02/05/hbase/"},{"title":"Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling","text":"https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_EMNLP2019.pdf 2 HCAN: Hybrid Co-Attention Network three major components: (1) a hybrid encoder (2) a relevance matching module (3) a semantic matching module 2.1 Hybrid Encodershybrid encoder module that explores three types of encoders: deep, wide, and contextual query and context words :$\\{w_1^q,w_2^q,…,w_n^q\\},\\{w_1^c,w_2^c,…,w_m^c\\}$, embedding representations $\\textbf{Q}\\in \\mathbb{R}^{n\\times L},\\textbf{C}\\in \\mathbb{R}^{m\\times L}$ Deep Encoder $\\textbf{U}$表示$\\textbf{Q},\\textbf{C}$ Wide Encoder Unlike the deep encoder that stacks multiple convolutional layers hierarchically, the wide encoder organizes convolutional layers in parallel, with each convolutional layer having a different window size k Contextual Encoder 2.2 Relevance Matching 2.3 Semantic Matching 2.4 Final Classification","link":"/2021/12/02/hcan/"},{"title":"堆，优先队列","text":"堆 https://blog.csdn.net/weixin_45697774/article/details/104481087 优先队列 https://www.cnblogs.com/xzxl/p/7266404.html https://www.jianshu.com/p/b51ab28ca8dd 堆和优先队列的关系 https://blog.csdn.net/weixin_44337445/article/details/110508591","link":"/2021/11/30/heap/"},{"title":"hive数据导入导出","text":"https://www.cnblogs.com/xing901022/p/5801061.html","link":"/2022/03/17/hive-data/"},{"title":"hexo_intro","text":"1.部署123hexo clean hexo g hexo d 2.创建文章1hexo new &quot;XXX&quot; 3.常见问题Error: pandoc exited with code 7: pandoc: Unknown extension: smart 解决：卸载pandoc 1npm un hexo-renderer-pandoc —save error：spawn failed 1.删除.deploy_git文件夹 2.执行 1git config --global core.autocrlf false hexo 图片显示问题 1、在_config.yml设置post_asset_folder为true hexo new “paper_name”时会创建paper_name.md和paper_name的文件夹，将图片放在paper_name的文件夹 2、安装插件asset-imagenpm install https://github.com/CodeFalling/hexo-asset-image3、设置图片为相对路径 注意修改图片路径中的 \\ 为 / ,并且不带 . 或者 . /","link":"/2021/07/18/hexo-intro/"},{"title":"hive架构","text":"https://cwiki.apache.org/confluence/display/hive/design#Design-HiveArchitecture https://zhuanlan.zhihu.com/p/87545980 https://blog.csdn.net/oTengYue/article/details/91129850 https://jiamaoxiang.top/2020/06/27/Hive%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/ https://www.javatpoint.com/hive-architecture Hive ClientHive allows writing applications in various languages, including Java, Python, and C++. It supports different types of clients such as:- Thrift Server - It is a cross-language service provider platform that serves the request from all those programming languages that supports Thrift. JDBC Driver - It is used to establish a connection between hive and Java applications. The JDBC Driver is present in the class org.apache.hadoop.hive.jdbc.HiveDriver. ODBC Driver - It allows the applications that support the ODBC protocol to connect to Hive. Hive ServicesThe following are the services provided by Hive:- Hive CLI - The Hive CLI (Command Line Interface) is a shell where we can execute Hive queries and commands. Hive Web User Interface - The Hive Web UI is just an alternative of Hive CLI. It provides a web-based GUI for executing Hive queries and commands. Hive MetaStore - It is a central repository that stores all the structure information of various tables and partitions in the warehouse. It also includes metadata of column and its type information, the serializers and deserializers which is used to read and write data and the corresponding HDFS files where the data is stored. Hive Server - It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver. Hive Driver - It receives queries from different sources like web UI, CLI, Thrift, and JDBC/ODBC driver. It transfers the queries to the compiler. Hive Compiler - The purpose of the compiler is to parse the query and perform semantic analysis on the different query blocks and expressions. It converts HiveQL statements into MapReduce jobs. Hive Execution Engine - Optimizer generates the logical plan in the form of DAG of map-reduce tasks and HDFS tasks. In the end, the execution engine executes the incoming tasks in the order of their dependencies. 计算引擎Hive支持MapReduce、Tez、Spark https://cloud.tencent.com/developer/article/1893808 https://blog.csdn.net/kwu_ganymede/article/details/52223133 数据存储https://cloud.tencent.com/developer/article/1411821 Hive是基于hdfs的，它的数据存储在Hadoop分布式文件系统中。Hive本身是没有专门的数据存储格式，也没有为数据建立索引，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。 default数据库中的表的存储位置 /user/hive/warehouse其他数据库的表自己指定","link":"/2022/02/09/hive-framework/"},{"title":"Hive MetaStore","text":"1 描述Hive MetaStore - It is a central repository that stores all the structure information of various tables and partitions in the warehouse. It also includes metadata of column and its type information, the serializers and deserializers which is used to read and write data and the corresponding HDFS files where the data is stored. 2 Hive的元数据存储(Metastore三种配置方式)Embedded，Local，Remote https://blog.csdn.net/epitomizelu/article/details/117091656 https://zhuanlan.zhihu.com/p/473378621 https://blog.csdn.net/qq_40990732/article/details/80914873 3 Hive元数据库介绍https://blog.csdn.net/victorzzzz/article/details/81874674","link":"/2022/03/30/hive-metadatastore/"},{"title":"Hive与传统数据库对比","text":"Hive 传统数据库 查询语言 HQL SQL 数据存储 HDFS Raw Device或者 Local FS 数据格式 用户自定义 系统决定 数据更新 不支持 支持 执行 MapReduce Excutor 执行延迟 高 低 处理数据规模 大 小 索引 0.8版本后加入位图索引 有复杂的索引 可扩展性 高 低 https://cloud.tencent.com/developer/article/1785857","link":"/2022/03/01/hive-vs-database/"},{"title":"hive优化","text":"https://blog.csdn.net/yu0_zhang0/article/details/81776459 1 索引https://www.jianshu.com/p/28b825367ba1 https://www.jianshu.com/p/d53f528daca7 Hive索引的目标是提高对表的某些列进行查询查找的速度。 索引所能提供的查询速度的提高是以存储索引的磁盘空间为代价的。 Hive 3.0开始将 移除index的功能，取而代之的是Hive 2.3版本开始的物化视图，自动重写的物化视图替代了index的功能。 2 物化视图https://blog.csdn.net/u011447164/article/details/105790713 区别于普通视图 12345create materialized view view2asselect dept.deptno,dept.dname,emp.enamefrom emp,deptwhere emp.deptno=dept.deptno;","link":"/2022/04/06/hive-optimazation/"},{"title":"hive","text":"常见问题1.FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create Spark client for Spark session https://blog.csdn.net/qq_41504585/article/details/108064512 启动metastorehttps://blog.csdn.net/u010670689/article/details/41576647 hive —service metastore 2&gt;&amp;1 &gt;&gt; /var/log.log &amp; 启动hiveserver2hiveserver2 连接方式https://blog.csdn.net/qq_41851454/article/details/79833306 https://www.shuzhiduo.com/A/RnJW4Z2r5q/ 1.cli hive 2.beeline https://www.jianshu.com/p/97bbe79d88d2","link":"/2022/01/31/hive/"},{"title":"建表","text":"https://www.jianshu.com/p/4f60f3c923fe 0 CREATE TABLEhttps://blog.csdn.net/Thomson617/article/details/86153924 123456789101112131415161718192021CREATE EXTERNAL TABLE dim_sku_info ( `id` STRING COMMENT '商品id', `price` DECIMAL(16,2) COMMENT '商品价格', `sku_name` STRING COMMENT '商品名称', `sku_desc` STRING COMMENT '商品描述', `weight` DECIMAL(16,2) COMMENT '重量', `is_sale` BOOLEAN COMMENT '是否在售', `spu_id` STRING COMMENT 'spu编号', `spu_name` STRING COMMENT 'spu名称', `category3_id` STRING COMMENT '三级分类id', `category3_name` STRING COMMENT '三级分类名称', `category2_id` STRING COMMENT '二级分类id', `category2_name` STRING COMMENT '二级分类名称', `category1_id` STRING COMMENT '一级分类id', `category1_name` STRING COMMENT '一级分类名称', `tm_id` STRING COMMENT '品牌id', `tm_name` STRING COMMENT '品牌名称', `sku_attr_values` ARRAY&lt;STRUCT&lt;attr_id:STRING,value_id:STRING,attr_name:STRING,value_name:STRING&gt;&gt; COMMENT '平台属性', `sku_sale_attr_values` ARRAY&lt;STRUCT&lt;sale_attr_id:STRING,sale_attr_value_id:STRING,sale_attr_name:STRING,sale_attr_value_name:STRING&gt;&gt; COMMENT '销售属性', `create_time` STRING COMMENT '创建时间') COMMENT '商品维度表' 1 EXTERNAL 关键字可以让用户创建一个外部表，默认是内部表 2 字段的数据类型 https://blog.csdn.net/weixin_46941961/article/details/108551512 https://blog.csdn.net/weixin_43215250/article/details/90034169 集合数据类型：Array、Map和Struct 1.分区https://www.jianshu.com/p/5dbbaea8ff41 PARTITIONED BY (dt string) 0 分类 静态分区SP（static partition）动态分区DP（dynamic partition） 静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。 1 静态分区 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556--建表DROP TABLE IF EXISTS dwd_display_log;CREATE EXTERNAL TABLE dwd_display_log( `area_code` STRING COMMENT '地区编码', `brand` STRING COMMENT '手机品牌', `channel` STRING COMMENT '渠道', `is_new` STRING COMMENT '是否首次启动', `model` STRING COMMENT '手机型号', `mid_id` STRING COMMENT '设备id', `os` STRING COMMENT '操作系统', `user_id` STRING COMMENT '会员id', `version_code` STRING COMMENT 'app版本号', `during_time` BIGINT COMMENT 'app版本号', `page_item` STRING COMMENT '目标id ', `page_item_type` STRING COMMENT '目标类型', `last_page_id` STRING COMMENT '上页类型', `page_id` STRING COMMENT '页面ID ', `source_type` STRING COMMENT '来源类型', `ts` BIGINT COMMENT 'app版本号', `display_type` STRING COMMENT '曝光类型', `item` STRING COMMENT '曝光对象id ', `item_type` STRING COMMENT 'app版本号', `order` BIGINT COMMENT '曝光顺序', `pos_id` BIGINT COMMENT '曝光位置') COMMENT '曝光日志表'PARTITIONED BY (`dt` STRING)STORED AS PARQUETLOCATION '/warehouse/gmall/dwd/dwd_display_log'TBLPROPERTIES('parquet.compression'='lzo');--加载数据insert overwrite table dwd_display_log partition(dt='2020-06-14')select get_json_object(line,'$.common.ar'), get_json_object(line,'$.common.ba'), get_json_object(line,'$.common.ch'), get_json_object(line,'$.common.is_new'), get_json_object(line,'$.common.md'), get_json_object(line,'$.common.mid'), get_json_object(line,'$.common.os'), get_json_object(line,'$.common.uid'), get_json_object(line,'$.common.vc'), get_json_object(line,'$.page.during_time'), get_json_object(line,'$.page.item'), get_json_object(line,'$.page.item_type'), get_json_object(line,'$.page.last_page_id'), get_json_object(line,'$.page.page_id'), get_json_object(line,'$.page.source_type'), get_json_object(line,'$.ts'), get_json_object(display,'$.display_type'), get_json_object(display,'$.item'), get_json_object(display,'$.item_type'), get_json_object(display,'$.order'), get_json_object(display,'$.pos_id')from ods_log lateral view explode_json_array(get_json_object(line,'$.displays')) tmp as displaywhere dt='2020-06-14'and get_json_object(line,'$.displays') is not null; 2 动态分区 注意分区字段dt数据来源于date_format(create_time,’yyyy-MM-dd’) 和静态分区比较，建表的时候没区别，加载数据有区别 12345678910111213141516171819202122232425262728--建表DROP TABLE IF EXISTS dwd_comment_info;CREATE EXTERNAL TABLE dwd_comment_info( `id` STRING COMMENT '编号', `user_id` STRING COMMENT '用户ID', `sku_id` STRING COMMENT '商品sku', `spu_id` STRING COMMENT '商品spu', `order_id` STRING COMMENT '订单ID', `appraise` STRING COMMENT '评价(好评、中评、差评、默认评价)', `create_time` STRING COMMENT '评价时间') COMMENT '评价事实表'PARTITIONED BY (`dt` STRING)STORED AS PARQUETLOCATION '/warehouse/gmall/dwd/dwd_comment_info/'TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;lzo&quot;);--加载数据insert overwrite table dwd_comment_info partition (dt)select id, user_id, sku_id, spu_id, order_id, appraise, create_time, date_format(create_time,'yyyy-MM-dd')from ods_comment_infowhere dt='2020-0 2 LOCATIONLOCATION ‘/warehouse/gmall/ods/ods_log’ 指定数据在hdfs上的存储位置 3 ROW FORMAThttps://www.imooc.com/article/12213 https://blog.csdn.net/S_Running_snail/article/details/84258162 指定数据切分格式 ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\\t’ 4 STORED AShttps://blog.csdn.net/ZZQHELLO2018/article/details/106175887 指定存储方式 行式存储：TEXTFILE 、SEQUENCEFILE 列式存储： ORC、PARQUET 5 TBLPROPERTIEShttps://blog.csdn.net/yangguosb/article/details/83651073 https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable TBLPROPERTIES是表的一些属性，HIVE内置了一部分属性，使用者也可以在创建表时进行自定义； TBLPROPERTIES (“parquet.compression”=”lzo”);","link":"/2022/04/02/hql-build/"},{"title":"hql常见操作","text":"1 with…as…https://www.jianshu.com/p/d518e9f5d5f9 1 好处 ​ a. 提高代码可读性 ​ 结构清晰 ​ b. 优化执行速度 ​ 子查询结果存在内存中，不需要重复计算 2 用法 1with table_name as(子查询语句) 其他sql; 1234with temp as ( select * from xxx)select * from temp; 2 视图与基本表不同，它是一个虚表。在数据库中，存放的只是视图的定义，而不存放视图包含的数据项，这些项目仍然存放在原来的基本表结构中。 视图是只读的，不能向视图中插入或加载或改变数据 作用： 1 便捷 通过引入视图机制，用户可以将注意力集中在其关心的数据上（而非全部数据），这样就大大提高了用户效率与用户满意度，而且如果这些数据来源于多个基本表结构，或者数据不仅来自于基本表结构，还有一部分数据来源于其他视图，并且搜索条件又比较复杂时，需要编写的查询语句就会比较烦琐，此时定义视图就可以使数据的查询语句变得简单可行。 2 安全 定义视图可以将表与表之间的复杂的操作连接和搜索条件对用户不可见，用户只需要简单地对一个视图进行查询即可，故增加了数据的安全性，但不能提高查询效率。 创建 12345678CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ][COMMENT table_comment]AS SELECT ...hive&gt; CREATE VIEW emp_30000 AS &gt; SELECT * FROM employee &gt; WHERE salary&gt;30000; 删除 1DROP VIEW view_name","link":"/2022/04/02/hql-common-operation/"},{"title":"hql增删改查","text":"1增insert load 2删1 表 Drop 表结构都没有了 1DROP TABLE IF EXISTS employee; 2 记录 没有DELETE TRUNCATE 所有记录 truncate table employees; INSERT OVERWRITE 12INSERT OVERWRITE TABLE dpc_test SELECT * FROM dpc_test WHERE age is not null; 3改1 update 针对记录 1update student set id='444' where name='tom'; 2 Alter 表结构 4查select","link":"/2022/05/06/hql-crud/"},{"title":"函数","text":"1 系统函数https://www.studytime.xin/article/hive-knowledge-function.html 1 get_json_object https://blog.csdn.net/weixin_43412569/article/details/105290637 2 nvl 空值判断转换函数 https://blog.csdn.net/a850661962/article/details/101209028 3 coalesce https://blog.csdn.net/yilulvxing/article/details/86595725 1select coalesce(success_cnt,period,1) from tableA 当success_cnt不为null，那么无论period是否为null，都将返回success_cnt的真实值（因为success_cnt是第一个参数），当success_cnt为null，而period不为null的时候，返回period的真实值。只有当success_cnt和period均为null的时候，将返回1。 4 collect_list和collect_set https://blog.csdn.net/weixin_30230059/article/details/113324945 https://blog.csdn.net/qq_44104303/article/details/117551807 它们都是将分组中的某列转为一个数组返回，不同的是collect_list不去重而collect_set去重。 5 named_struct https://blog.csdn.net/weixin_43597208/article/details/117554838 做字段拼接 区别于struct，struct 是集合数据类型，一般用于建表，named_struct是字段拼接函数，一般用于查询 6 array_contains() 1array_contains(array，值) 判断array中是否包含某个值，包含返回true，不包含返回false 7 cast https://www.jianshu.com/p/999176fa2730 显式的将一个类型的数据转换成另一个数据类型 1Cast(字段名 as 转换的类型 ) 2 用户自定义函数UDF(User-Defined-Function)：单入单出UDTF(User-Defined Table-Generating Functions)：单入多出UDAF(User Defined Aggregation Function)：多入单出https://blog.csdn.net/qq_40579464/article/details/105903405 1.编写代码 jar不能随意编写，需要和hive对齐接口，可以借助工具import org.apache.hadoop.hive.ql.exec.UDF; 121 public class classname extends UDF2 编写evalute https://blog.csdn.net/eyeofeagle/article/details/83904147 2.打包3.导入hive复制到hdfs上Hive安装目录的lib目录下4.创建关联add jar hdfs://localhost:9000/user/root/hiveudf.jarcreate temporary function my_lower as ‘com.example.hive.udf.LowerCase’;5.使用 hql udf的使用和普通内置函数一样，比如有udf1 1select udf1（col1） from table1","link":"/2022/04/02/hql-function/"},{"title":"sql,hql区别","text":"https://www.geeksforgeeks.org/difference-between-sql-and-hiveql/","link":"/2022/03/18/hql-sql/"},{"title":"加载数据","text":"https://www.cnblogs.com/bjlhx/p/6946422.html https://blog.csdn.net/m0_49092046/article/details/109251015 1 load1load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] | into table tabName [partition (partcol1=val1,…)]; （1）load data:表示加载数据（2）local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表（3）inpath:表示加载数据的路径（4）overwrite:表示覆盖表中已有数据，否则表示追加（5）into table:表示加载到哪张表（6）tabName:表示具体的表（7）partition:表示上传到指定分区 例子： 1load data inpath '/origin_data/gmall/log/topic_log/2020-06-14' into table ods_log partition(dt='2020-06-14') 2 INSERThttps://help.aliyun.com/document_detail/73775.html insert into 和insert overwrite 1234561insert into table student partition(month='20201022') values(1,'zhangsan');2 insert overwrite table student partition(month='20201023')select id, name from student where month='20201023';","link":"/2022/04/02/hql-loaddata/"},{"title":"huggingface","text":"NLP小帮手，huggingface的transformer git： https://github.com/huggingface/transformers paper： https://arxiv.org/abs/1910.03771v5 整体结构 简单教程： https://blog.csdn.net/weixin_44614687/article/details/106800244 from_pretrained 底层为load_state_dict 12345678910Some weights of the model checkpoint at ../../../../test/data/chinese-roberta-wwm-ext were not used when initializing listnet_bert: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']- This IS expected if you are initializing listnet_bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing listnet_bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).Some weights of listnet_bert were not initialized from the model checkpoint at ../../../../test/data/chinese-roberta-wwm-ext and are newly initialized: ['Linear2.weight', 'Linear1.weight', 'Linear1.bias', 'Linear2.bias']You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.两部分：1 加载的预训练模型中有参数没有用到 2 自己的模型有参数没有初始化finetune的时候报这个 很正常predict的时候应该不会有 关于modelBertModel -&gt; our model 1 加载transformers中的模型 1from transformers import BertPreTrainedModel, BertModel,AutoTokenizer,AutoConfig 2 基于1中的模型搭建自己的结构","link":"/2021/12/07/huggingface/"},{"title":"IDEA","text":"激活https://www.heiz123.com/2022/02/242/#di_liu_bu_da_kai_IDEA_tian_ru_zhi_ding_ji_huo_ma_wan_cheng_ji_huo IDEA修改变量的值https://blog.csdn.net/qq_36925114/article/details/102484525 debughttps://www.cnblogs.com/chiangchou/p/idea-debug.html language levelhttps://blog.csdn.net/glpghz/article/details/107509987 当我们项目使用的是 JDK 8，但是代码却没有使用 JDK 8 的新特性，只需使用 JDK 7 的时候我们可以选择 7 - Diamonds，ARM，multi-catch etc","link":"/2022/03/08/idea/"},{"title":"克隆虚拟机修改静态IP不成功解决办法","text":"https://blog.csdn.net/nullnullago/article/details/122343454 https://blog.csdn.net/xiaozizai2015/article/details/88855915 https://blog.csdn.net/Panda_813/article/details/104606990?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=2 https://blog.csdn.net/qq_38616559/article/details/104949693 注意一个坑：修改ip的时候，vim ens33文件产生冲突，以为修改成功，其实没有","link":"/2022/01/29/ifconfig/"},{"title":"倒排索引","text":"","link":"/2022/07/15/indexing/"},{"title":"信息抽取 Information Extraction","text":"简介信息抽取是基于已有信息筛选出目标信息，不是无中生有，生成是有无中生有的能力 信息抽取主要包括三个子任务：命名实体识别、关系抽取、事件抽取。 实体抽取又称命名实体识别，其目的是从文本中抽取实体信息元素。想要从文本中进行实体抽取，首先需要从文本中识别和定位实体，然后再将识别的实体分类到预定义的类别中去。 关系抽取是知识抽取的重要子任务之一，面向非结构化文本数据， 关系抽取是从文本中抽取出两个或者多个实体之间的语义关系。关系抽取与实体抽取密切相关，一般在识别出文本中的实体后，再抽取实体之间可能存在的关系，也有很多联合模型同时将这两个任务一起做了的； 事件抽取是指 从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈 现出来，例如事件发生的时间、地点、发生原因、参与者等。跟关系抽取有重叠的地方，同样也可以分为流水线方法和联合抽取方法。 例子： 1.NER命名实体识别 （实体抽取）：从文本中检测出命名实体，并将其分类到预定义的类别中，例如人物、组织、地点、时间等。图中高灰色记的文字就是命名实体，在一般情况下，命名实体识别是知识抽取其他任务的基础。2.关系抽取 ：从文本中识别抽取实体及实体之间的关系。例如，从句子“[王思聪] 是万达集团董事长[王健林]的独子”中识别出实体“[王健林]”和“[王思 聪]”之间具有“父子”关系。3.事件抽取：识别文本中关于事件的信息，并以结构化的形式呈现。例如，从恐怖袭击事件的新闻报道中识别袭击发生的地点、时间、袭击目标和受害人等信息。 参考https://zhuanlan.zhihu.com/p/183966841 https://zhuanlan.zhihu.com/p/376898772 https://zhuanlan.zhihu.com/p/352513650","link":"/2022/04/07/information-extract/"},{"title":"Informer Beyond Efficient Transformer for Long Sequence Time-Series Forecasting","text":"paper： https://arxiv.org/abs/2012.07436 code github ： https://github.com/zhouhaoyi/Informer2020 https://zhuanlan.zhihu.com/p/363084133 https://blog.csdn.net/fluentn/article/details/115392229 https://blog.csdn.net/weixin_42838061/article/details/117361871 摘要there are several severe issues with Transformer that prevent it from being directly applicable to LSTF（Long sequence time-series forecasting）, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture Informer, with three distinctive characteristics：1. ProbSparse self-attention mechanism 2. the self-attention distilling 3.the generative style decoder 2 Preliminary 3 Methodology 3.1 The Uniform Input Representation 3.2 Efficient Self-attention MechanismQuery Sparsity Measurement Some previous attempts have revealed that the distribution of self-attention probability has potential sparsity. The “sparsity” self-attention score forms a long tail distribution,见上图,接下来就是要把不符合sparsity的query找出来,也就是uniform distribution，然后用KL散度衡量两种分布的距离，得出the i-th query’s sparsity measurement 但是直接用上面的式子存在几个问题：1. requires calculating each dot-product pairs 2.LogSumExp operation has the potential numerical stability issue 因此提出近似的计算 ProbSparse Self-attention The masked version can be achieved by applying positional mask on step 6 and using cmusum() in mean(\u0001) of step 7. In the practice, we can use sum() as the simpler implement of mean(). 3.3 Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation To enhance the robustness of the distilling operation, we build replicas of the main stack with halving inputs, and progressively decrease the number of self-attention distilling layers by dropping one layer at a time, like a pyramid in Fig.(2) 注意:encoder部分有两个stack，一个是主stack，一个是从stack，如图2，从stack的输入为主stack的一半， Self-attention Distilling 3.4 Decoder: Generating Long Sequential Outputs Through One Forward Procedure‘和原来transformer的区别：主要在于prediction，原来是step by step，现在是one forward procedure，怎么实现的呢？关键在于decoder的输入的构造 A fully connected layer acquires the final output, and its outsize dy depends on whether we are performing a univariate forecasting or a multivariate one. dy=1 uni，dy&gt;1 multi","link":"/2021/11/21/informer/"},{"title":"意图识别和槽位填充(Intent Detection and Slot Filling)","text":"1.简介出自 https://zhuanlan.zhihu.com/p/75228411# 在对话系统的NLU中，意图识别（Intent Detection，简写为ID）和槽位填充（Slot Filling，简写为SF）是两个重要的子任务。其中，意图识别可以看做是NLP中的一个分类任务，而槽位填充可以看做是一个序列标注任务，在早期的系统中，通常的做法是将两者拆分成两个独立的子任务。但这种做法跟人类的语言理解方式是不一致的，事实上我们在实践中发现，两者很多时候是具有较强相关性的，比如下边的例子： 1.我要听[北京天安门, song] — Intent：播放歌曲2.帮我叫个车，到[北京天安门, location] — Inent：打车3.播放[忘情水, song] — Intent：播放歌曲4.播放[复仇者联盟, movie] — Intent：播放视频 1和2中，可以看到同样是“北京天安门”，由于意图的不同，该实体具备完全不同的槽位类型。3和4中，由于槽位类型的不同，导致了最终意图的不同，这往往意味着，在对话系统中的后继流程中将展现出完全不同的行为——-打开网易音乐播放歌曲 or 打开爱奇艺播放电影。 随着对话系统的热度逐渐上升，研究的重点也逐渐倾向于将两个任务进行联合，以充分利用意图和槽位中的语义关联。那么，问题来了，我们该如何进行联合呢？从目前的趋势来看，大体上有两大类方法： 多任务学习：按Multi-Task Learning的套路，在学习时最终的loss等于两个任务的loss的weight sum，两者在模型架构上仍然完全独立，或者仅共享特征编码器。 交互式模型：将模型中Slot和Intent的隐层表示进行交互，引入更强的归纳偏置，最近的研究显示，这种方法的联合NLU准确率更高。 参考[1] https://zhuanlan.zhihu.com/p/165963264 [2] https://zhuanlan.zhihu.com/p/75228411#","link":"/2021/10/09/intent-detect-slot-fill/"},{"title":"意图识别","text":"本质是分类任务，多用在搜索引擎和智能问答中。 解决方法 1、基于规则模板意图识别 https://blog.csdn.net/qq_16555103/article/details/100767984 2、基于深度学习模型来对用户的意图进行判别 比如fasttext，LSTM+attention，BERT 参考https://blog.csdn.net/qq_37228811/article/details/104307144?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&amp;spm=1001.2101.3001.4242 https://blog.csdn.net/qq_16555103/article/details/100767984","link":"/2021/10/09/intent-detect/"},{"title":"java应用场景","text":"前端 后端 大数据 https://www.cnblogs.com/zlt9/p/7206238.html","link":"/2022/03/06/java-application/"},{"title":"java框架","text":"https://segmentfault.com/a/1190000016917114","link":"/2022/03/06/java-framework/"},{"title":"多线程编程","text":"进程 &gt; 线程","link":"/2022/05/03/java-multi-thread/"},{"title":"网络编程","text":"","link":"/2022/05/03/java-net-program/"},{"title":"java常见问题","text":"Could not find or load main classhttps://blog.csdn.net/gao_zhennan/article/details/112749742 https://www.html.cn/softprog/java/142271.html https://blog.csdn.net/qq_43189115/article/details/99856659 https://blog.csdn.net/zdash21/article/details/101310736 https://www.jianshu.com/p/bd5d07982699 java.lang.NoClassDefFoundErrorhttps://www.jianshu.com/p/8dcdb02f97f7 https://blog.csdn.net/jamesjxin/article/details/46606307 Warning: Class ‘com.xxx.xxx‘ not found in module ‘xxxx‘https://blog.csdn.net/diligent_jianhao/article/details/111515183 java: Compilation failed: internal java compiler errorhttps://blog.csdn.net/ximaiyao1984/article/details/114782006","link":"/2022/03/08/java-problem/"},{"title":"java学习网站","text":"https://www.pdai.tech/","link":"/2022/05/16/java-tech/"},{"title":"jdbc","text":"https://zhuanlan.zhihu.com/p/140885502 JDBC的全称是Java数据库连接(Java Database connect)，它是一套用于执行SQL语句的Java API。应用程序可通过这套API连接到关系数据库，并使用SQL语句来完成对数据库中数据的查询、更新和删除等操作。应用程序使用JDBC访问数据库的方式如下图所示。","link":"/2022/03/19/jdbc/"},{"title":"java","text":"1.Java平台https://www.cnblogs.com/HeavenZhi/p/14075331.html#jrejava-runtime-environment https://blog.csdn.net/ZGL_cyy/article/details/104081834 java是语言，这个是平台。Java 平台由 Java 虚拟机（Java Virtual Machine，JVM）和 Java 应用编程接口（Application Programming Interface，API））构成。 java SE Java SE（Java Platform Standard Edition，Java 平台标准版）以前称为 J2SE，它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 包含了支持 Java Web 服务开发的类，并为 Java EE 提供基础，如 Java 语言基础、JDBC 操作、I/O 操作、网络通信以及多线程等技术。 Java EE Java EE（Java Platform Enterprise Edition，Java 平台企业版）以前称为 J2EE。企业版本帮助开发和部署可移植、健壮、可伸缩且安全的服务器端 Java 应用程序。Java EE 是在 Java SE 基础上构建的，它提供 Web 服务、组件模型、管理和通信 API，可以用来实现企业级的面向服务体系结构（Service Oriented Architecture，SOA）和 Web 2.0 应用程序。 Java ME Java ME（Java Platform Micro Edition，Java 平台微型版）以前称为 J2ME，也叫 K-JAVA。 Java ME 为在移动设备和嵌入式设备（比如手机、PDA、电视机顶盒和打印机）上运行的应用程序提供一个健壮且灵活的环境。 Java ME 包括灵活的用户界面、健壮的安全模型、丰富的内置网络协议以及对可以动态下载的联网和离线应用程序。基于 Java ME 规范的应用程序 只需编写一次就可以用于许多设备，而且可以利用每个设备的本机功能。 2 run command123456789101112131415//he_test// lib// src// HelloWorld.java// binpackage com.hlw.test;public class HelloWorld { /* 第一个Java程序 * * 它将输出字符串 Hello World * */ public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); // 输出 Hello World }} he_test下 1.编译代码 javac -d bin/ ./src/HelloWorld.java bin/下生成com/hlw/test/HelloWorld.class 2.运行程序 java -classpath/-cp ./bin/ com/hlw/test/HelloWorld 3 Java 源文件的命名规则 通常情况下，Java 程序源文件的主文件名可以任意。 但如果其中定义了一个 public 类，则该源文件的文件名必须与该 public 类的类名相同。 一个Java 源文件可包含多个类定义，但最多只能包含一个public类定义。 4 jar包0.下载jar包https://www.cnblogs.com/Marydon20170307/p/9149256.html 1.环境导入jar包ide https://www.jianshu.com/p/9762b7098b76 https://www.cnblogs.com/mracale/p/10493823.html ubuntu https://blog.csdn.net/weixin_30681615/article/details/99745369 https://blog.csdn.net/chencaw/article/details/78884107 https://blog.csdn.net/j_bean/article/details/75095337 例子: ####目录结构test ​ lib ​ spark-assembly_2.10-1.6.0-cdh5.16.1.jar ​ src ​ demo.java ​ bin #在test路径下 1.编译 javac -cp ./lib/spark-assembly_2.10-1.6.0-cdh5.16.1.jar -d ./bin/ ./src/demo.java 2.运行 java -cp ./lib/spark-assembly_2.10-1.6.0-cdh5.16.1.jar:./bin/ com/yzy/spark/demo 注意：1 -cp 需要列出所有jar包，ide也是全部列出了 2 window上，用;分隔；linux上是:分隔 2.代码引用jar包import 3.怎么生成jarhttps://www.cnblogs.com/swordfall/p/11359370.html https://blog.csdn.net/smgsn01/article/details/108038046 maven，ide自带，jar命令 4.执行java -jar XXX.jar 5 packagehttps://blog.csdn.net/qq_41297896/article/details/90056534 https://www.runoob.com/java/java-package.html https://www.runoob.com/w3cnote/java-compile-with-package.html 为了更好地组织类，用于区别类名的命名空间 包的命名规范 https://blog.csdn.net/shi779276212/article/details/92795085 使用： 6 重写(Override)与重载(Overload)https://www.runoob.com/java/java-override-overload.html 重写：是子类对父类同名函数的二次实现 重载：一个类内存在多个重名函数，而参数不同 7 多态http://c.biancheng.net/view/1001.html# https://www.runoob.com/java/java-polymorphism.html 多态是同一个行为具有多个不同表现形式或形态的能力。 多态存在的三个必要条件 继承 重写 父类引用指向子类对象：Parent p = new Child(); 分类： https://blog.csdn.net/zhao_miao/article/details/83750898 1 向上转型 父类 父类对象 = 子类实例 只能调用父特有，子覆盖，不能子特有 2 向下转型 向下转型之前一定要进行向上转型！！ 子类 子类对象 = （子类）父类实例 可以父特有，子覆盖，子特有 8 函数入口https://blog.csdn.net/weixin_29740921/article/details/114249667 函数入口为main，就和c++的main函数一样，main方法的写法是固定的 12public static void main(String[] args) {}public static void main(String args[]) {} 一个java文件可以不只有一个main，不同类都可以有自己的main，然后选哪个main呢，也就是选哪个类呢？ 选公共类（至多包含一个）；若是没有就选用同名类，和文件同名 9 引用java没有指针 和C++的引用不一样，和C++指针有点像 java 4种引用 https://blog.csdn.net/linzhiqiang0316/article/details/88591907 10 数据类型和变量类型的区别https://blog.csdn.net/qq_61411852/article/details/123130531 1.数据类型 定义一个变量，每一种数据类型需要用到的存储空间都不同，这时需要用不同的数据类型来定义变量；例如：int float double等等 2.变量类型 局部变量：类的方法中的变量。 实例变量：独立于方法之外的变量，不过没有 static 修饰。 类变量：独立于方法之外的变量，用 static 修饰。 12 数据类型基本类型：boolean， char， int， byte，short，long， float，double（有值域范围） 为什么存在基本类型还要包装类？ 基本类型不是对象，包装类是一个对象，增加面向对象特性 包装类：Boolean，Character，Integer，Byte，Short，Long，Float，Double 自动装箱 123456Integer i = 100;等价于Integer i = Integer.valueOf(100);ArrayList intList = new ArrayList();intList.add(1); 自动拆箱 1234int t = i; 等价于int t = i.intValue(); 13 数组声明 12345dataType[] arrayRefVar; // 首选的方法 或 dataType arrayRefVar[]; // 效果相同，但不是首选方法 创建 123arrayRefVar=new dataType[arraySize];或者arrayRefVar = {value0, value1, ..., valuek}; 多维数组 1type[][] typeName = new type[typeLength1][typeLength2]; 14 foreach和普通for循环比较，在遍历数组、集合方面，foreach为开发人员提供了极大的方便 123456for(元素类型t 元素变量x : 遍历对象obj){ 引用了x的java语句; } 12345678910public class TestArray { public static void main(String[] args) { double[] myList = {1.9, 2.9, 3.4, 3.5}; // 打印所有数组元素 for (double element: myList) { System.out.println(element); } }} 15 从控制台读取数据1 BufferedReader 12341 throws IOException2 BufferedReader br = new BufferedReader(new InputStreamReader(System.in));3 c = (char) br.read();或者str = br.readLine(); 2 Scanner 16 类型转化http://c.biancheng.net/view/796.html 1自动类型转换 两种数据类型彼此兼容 目标类型的取值范围大于源数据类型（低级类型数据转换成高级类型数据） 2 强制类型转换 所以当两种数据类型不兼容，或目标类型的取值范围小于源类型时，自动转换将无法进行，这时就需要进行强制类型转换。 123int a = 3;double b = 5.0;a = (int)b; 17 异常处理http://c.biancheng.net/view/6751.html Exception 是异常的基类 1 语句抛出异常 try 123456789try{ // 程序代码}catch(异常类型1 异常的变量名1){ // 程序代码}catch(异常类型2 异常的变量名2){ // 程序代码}finally{ // 程序代码} throw 当 throw 语句执行时，它后面的语句将不执行，此时程序转向调用者程序，寻找与之相匹配的 catch 语句，执行相应的异常处理程序。如果没有找到相匹配的 catch 语句，则再转向上一层的调用程序。这样逐层向上，直到最外层的异常处理程序终止程序并打印出调用栈情况。 1throw ExceptionObject; 2 方法抛出异常 throws 函数声明异常，但是本身不处理异常，交给调用者处理 1returnType method_name(paramList) throws Exception 1,Exception2,…{…} 1234567891011121314151617181920212223import java.io.FileInputStream;import java.io.IOException;public class Test04 { public void readFile() throws IOException { // 定义方法时声明异常 FileInputStream file = new FileInputStream(&quot;read.txt&quot;); // 创建 FileInputStream 实例对象 int f; while ((f = file.read()) != -1) { System.out.println((char) f); f = file.read(); } file.close(); } public static void main(String[] args) { Throws t = new Test04(); try { t.readFile(); // 调用 readFHe()方法 } catch (IOException e) { // 捕获异常 System.out.println(e); } }} 12public static void main(String[] args) throws Exception异常谁处理，JVM处理 20 final，static，static和final一起使用https://blog.csdn.net/hust_yfang/article/details/79585696 1 final 类 变量 方法 2 static 变量 方法 老看见函数前面有static，可以没有，二者区别： 加了static，可以不用实例化类就能调用方法；没有加static，必须实例化类才可以用 3 static和final一起使用 变量 方法 22 泛型目的：为了兼容多种数据类型 泛型标记符 12345678E - Element (在集合中使用，因为集合中存放的是元素)；T - Type（Java 类）；K - Key（键）；V - Value（值）；N - Number（数值类型）；R - Result （返回结果，多用于函数式编程）；? - 表示不确定的java类型。O 1 泛型方法 12public void printArray( int [] inputArray ) -》public &lt; E &gt; void printArray( E[] inputArray )public &lt;T extends Comparable&lt;T&gt;&gt; T maximum(T x, T y, T z) //Comparable是一个类 2 泛型类 1public class Box -》 public class Box&lt;T&gt; 23 对象作为返回值123456789101112131415161718192021222324252627282930313233 class Phone { String brand;//品牌 double price; String color; public void call(String who){ System.out.println(&quot;给&quot;+who+&quot;打电话&quot;); } public void sendMessage(){ System.out.println(&quot;群发短信&quot;); }}public class test { public static void main(String[] args) { Phone two=getPhone(); System.out.println(two.price); System.out.println(two.brand); System.out.println(two.color); } public static Phone getPhone(){ Phone one=new Phone(); one.brand=&quot;苹果&quot;; one.price=8388.0; one.color=&quot;玫瑰金&quot;; return one; }} 24 正则java和python正则很像，但不是完全一样 https://blog.csdn.net/weixin_39574708/article/details/114958384 https://blog.csdn.net/henu_xiaohei/article/details/84765678 1 | 或者 2 固定匹配位数 12\\d{7,8} //7为或者8为[A-Z]{1}[0-9]{8} 注意matches()，find（） match是全部匹配，find是部分匹配 3 转义字符 转义字符\\ *：不同于原来的字母含义，比如\\n表示换行 java正则为什么要两个斜杆表示转义 https://blog.csdn.net/qq_37325947/article/details/107819945 25 java如何兼容不同类型的输入 函数重载 泛型 26 装饰器@Override 需要你重写 28 类this 和python self一样，指的是对象 #类直接调用方法 https://blog.csdn.net/qq_40136594/article/details/83996659 需要是静态方法 1 创建对象new 12345678910public class Puppy{ public Puppy(String name){ //这个构造器仅有一个参数：name System.out.println(&quot;小狗的名字是 : &quot; + name ); } public static void main(String[] args){ // 下面的语句将创建一个Puppy对象 Puppy myPuppy = new Puppy( &quot;tommy&quot; ); }} 2 类继承1 extends 123class 子类 extends 父类 {}//只可以单继承 2 implements 一般用于类继承接口 1234567891011public interface A { public void eat(); public void sleep();} public interface B { public void show();} public class C implements A,B {} super关键字：当前对象，但是用来调用父类成员 this关键字：当前对象，用来调用子类成员 关于构造函数： 1 进入子类对应的构造函数 2 若是没有显示调用父类的构造函数，则自动调用；若是显示的调用父类的构造函数（super()，必须写第一句，super.SuperClass（） 错误），则显示调用 3 先调用父类，然后调用子类 3 匿名类https://www.runoob.com/java/java-anonymous-class.html 4 抽象类，抽象方法作用：抽象类为所有子类提供了一个通用模板，子类可以在这个模板基础上进行扩展，可以避免子类设计的随意性 1 抽象类 特殊的类：抽象类不能实例化对象，会报错 12public abstract class Employee1 在普通类的基础上加abstract 怎么使用抽象类？ 通过类继承的方式使用 2 抽象方法 普通类不能有抽象方法，只有抽象类，或者接口 1234public abstract double computePay();1. abstract2. 只有函数声明，没有函数具体实现3 子类不是抽象类，需要给出抽象类中的抽象方法的具体实现；子类也是抽象类，那么可以不需要 5 接口接口并不是类，是抽象方法的集合 12345[可见度] interface 接口名称 [extends 其他的接口名， 其他的接口名， 其他的接口名] { // 声明变量,只能是 public static final 类型的 // 抽象方法}//接口只能继承接口，extends，可以是多继承 怎么使用接口？通过类来实现 123...implements 接口名称[, 其他接口名称, 其他接口名称..., ...] ...public class MammalInt implements Animal // Animal是接口 关系： 类和类，继承，只能单继承 类和接口，实现，可以多实现 接口和接口，继承，可以多继承 default https://blog.csdn.net/qq_35835624/article/details/80196932 配合接口使用 使得接口内的函数可以写方法体，原来全是抽象方法，不能有方法体 29 Lambda表达式声明的时候需要 -&gt; https://www.runoob.com/java/java8-lambda-expressions.html 12345MathOperation subtraction = (a, b) -&gt; a - b; //声明System.out.println(&quot;10 - 5 = &quot; + tester.operate(10, 5, subtraction));//调用一起 System.out.println(&quot;10 - 5 = &quot; + tester.operate(10, 5, (a, b) -&gt; a - b));","link":"/2022/01/05/java/"},{"title":"jupyter notebook","text":"配置https://blog.csdn.net/weixin_41149572/article/details/114640624 传参和.py唯一区别 jupyter：args=parser.pars_args(argew=[传入参数]) py：args=parser.pars_args() 参考https://blog.csdn.net/weixin_41149572/article/details/114640624","link":"/2022/06/22/jupyter-notebook/"},{"title":"jvm","text":"1. JDK、JRE、JVM0 汇总https://github.com/doocs/jvm 1 JDK、JRE、JVMhttps://its401.com/article/weixin_45797022/104963478 三者的区别与联系 JDK用于开发，是给开发人员用的 JRE 用于运行java程序，普通用户用 JVM是java实现跨平台的最核心的部分，普通用户用 2 java内存区域，java memory model二者不同 https://blog.csdn.net/javazejian/article/details/72772461 1 内存区域 https://cloud.tencent.com/developer/article/1748395 string 的 内存分配 https://blog.csdn.net/liufangbaishi2014/article/details/52238881 2 jmm","link":"/2022/05/01/jvm/"},{"title":"k-fold","text":"","link":"/2021/10/13/k-fold/"},{"title":"kafka常见计算","text":"Kafka机器数量计算经验公式：Kafka机器数量= 2 （峰值生产速度 副本数 / 100）+ 1 1）峰值生产速度 峰值生产速度可以压测得到。 2）副本数 副本数默认是1个，在企业里面2-3个都有，2个居多。 副本多可以提高可靠性，但是会降低网络传输效率。 例子： 先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。 比如我们的峰值生产速度是50M/s。副本数为2。 Kafka机器数量 = 2 （50 2 / 100）+ 1 = 3台 Kafka分区数计算（1）创建一个只有1个分区的topic （2）测试这个topic的producer吞吐量和consumer吞吐量。 （3）假设他们的值分别是Tp和Tc，单位可以是MB/s。 （4）然后假设总的目标吞吐量是Tt，那么分区数 = Tt / min（Tp，Tc） 例如：producer吞吐量 = 20m/s；consumer吞吐量 = 50m/s，期望吞吐量100m/s；分区数 = 100 / 20 = 5分区 https://blog.csdn.net/weixin_42641909/article/details/89294698 分区数一般设置为：3-10个","link":"/2022/03/12/kafka-cal/"},{"title":"kafka常见问题","text":"1 kafka启动后一段时间自动退出的解决方案https://blog.csdn.net/weixin_46303867/article/details/115256466 2 ERROR Shutdown broker because all log dirs in … have failedhttps://blog.csdn.net/szxiaohe/article/details/103639127 3 连接zookeeper超时https://www.jianshu.com/p/ce215e6ef203","link":"/2022/03/12/kafka-problem/"},{"title":"kafka常见命令","text":"启动命令 1/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties 关闭命令 1/opt/module/kafka/bin/kafka-server-stop.sh stop","link":"/2022/04/11/kafka-command/"},{"title":"kafka与Zookeeper的关系","text":"https://www.lilinchao.com/archives/1548.html https://developer.51cto.com/article/658581.html 过去 Apache Kafka的一个关键依赖是Apache Zookeeper，它是一个分布式配置和同步服务。 Zookeeper是Kafka代理和消费者之间的协调接口。 Kafka服务器通过Zookeeper集群共享信息。 Kafka在Zookeeper中存储基本元数据，例如关于主题，代理，消费者偏移(队列读取器)等的信息。 由于所有关键信息存储在Zookeeper中，并且它通常在其整体上复制此数据，因此Kafka代理/ Zookeeper的故障不会影响Kafka集群的状态。 Kafka将恢复状态，一旦Zookeeper重新启动。 这为Kafka带来了零停机时间。 Kafka代理之间的领导者选举也通过使用Zookeeper在领导者失败的情况下完成。 未来 Kafka 2.8.0，移除了对Zookeeper的依赖，通过KRaft进行自己的集群管理","link":"/2022/03/12/kafka-zoo/"},{"title":"Kafka原理结构","text":"Apache Kafka® is an event streaming platform. What does that mean? Kafka combines three key capabilities so you can implement your use cases for event streaming end-to-end with a single battle-tested solution: To publish (write) and subscribe to (read) streams of events, including continuous import/export of your data from other systems. To store streams of events durably and reliably for as long as you want. To process streams of events as they occur or retrospectively. And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors. 详细原理见：https://blog.csdn.net/weixin_45366499/article/details/106943229","link":"/2022/01/30/kafka/"},{"title":"主外键","text":"主键、外键 https://blog.csdn.net/weixin_31642161/article/details/113113942 有可能没有主键 联合主键，复合主键 联合主键：数据库表的主键由两个及以上的字段组成。 复合主键：有争议","link":"/2022/02/15/key-id/"},{"title":"key&#x2F;value","text":"某个字段为key，某个字段为value","link":"/2022/03/02/key-value/"},{"title":"KG-BERT BERT for Knowledge Graph Completion","text":"原文 https://arxiv.org/pdf/1909.03193.pdf 一.背景补充 知识图谱普遍存在不完备的问题。以上图为例，黑色的箭头表示已经存在的关系，红色的虚线则是缺失的关系。知识图谱补全是基于图谱里已有的关系去推理出缺失的关系。由于BERT在NLP取得的成绩，作者将其迁移到知识图谱补全的应用上。 二.结构作者设计了两种训练方式的KG - BERT, 可以运用到不同的知识图谱补全任务当中。 2.1 Illustrations of fine-tuning KG-BERT for predicting the plausibility of a triple 输入由三部分组成，$Head$，$Relation$，$Tail$。举个例子，$Head$可以是“Steven Paul Jobs was an American business magnate,entrepreneur and investor.” 或者“Steve Jobs”，$Relation$可以是“founded”，$Tail$可以是“Apple Inc. is an American multinational technology company headquartered in Cupertino, California.”或者“Apple Inc.”。用$[SEP]$分隔实体和关系。输入为3个向量的sum，即token, segment 和position embeddings。对于segment，实体的segment Embedding为$e_A$，而关系的segment Embedding为$e_B$。对于position ，相同position的不同token使用相同的position embedding。 对于输入的三元组$\\tau=(h,r,t)$，目标函数为： S_{\\tau}=f(h,r,t)=sigmoid(CW^T)，S_{\\tau} \\in \\mathbb{R}^2,S_{\\tau 0}, S_{\\tau 1} \\in [0,1]损失函数是$S$和$y$的交叉熵： L=-\\sum_{\\tau \\in D^{+}\\cup D^{-}}(y_{\\tau}log(S_{\\tau0})+(1-y_{\\tau}log(S_{\\tau1})))其中$y_{\\tau}\\in \\{0,1\\}$是标签。 关于负样本的构造，作者是将正样本的$Head$或者$Tail$变成随机替换成别的，如下 D^{-}=\\{(h^{'},r,t)|h^{'}\\in E\\cap h^{'}\\neq h \\cap(h^{'},r,t)\\notin D^{+} \\}\\\\\\cup\\{(h,r,t^{'})|t^{'}\\in E\\cap t^{'}\\neq t \\cap(h,r,t^{'})\\notin D^{+}\\}其中$E$为实体的集合。 2.2 Illustrations of fine-tuning KG-BERT for predicting the relation between two entities 作者发现直接使用两个实体去预测关系，效果优于使用两个实体和一个随机关系（这里本人认为一个随机的关系本来就是错误特征，感觉肯定会影响预测结果）。这里和2.1结构的差异在于：1.输入从实体加关系的三输入变成基于实体的双输入2.输出从二分类变成多分类 目标函数为： S_{\\tau}^{'}=f(h,r,t)=softmax(CW^{'T})损失函数为$S^{‘}$和$y^{‘}$的交叉熵： L^{'}=-\\sum_{\\tau \\in D^{+}}\\sum_{i=1}^{R}y_{\\tau i}^{'}log(s^{'}_{\\tau i})三.实验setting： We choose pre-trained BERT-Base model with 12 layers, 12 self-attention heads and H = 768 as the initialization of KG-BERT, then fine tune KG-BERT with Adam implemented in BERT. 参考https://github.com/yao8839836/kg-bert https://zhuanlan.zhihu.com/p/355391327","link":"/2021/08/06/kg-bert/"},{"title":"KNN","text":"KNN可以用来分类和回归，以分类为例。 一.算法流程 KNN分类算法的计算过程： 1）计算待分类点与已知类别的点之间的距离 2）按照距离递增次序排序 3）选取与待分类点距离最小的K个点 4）确定前K个点所在类别的出现次数 5）返回前K个点出现次数最高的类别作为待分类点的预测分类 如上图，举个例子： 如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。 二.距离度量选择1.闵可夫斯基距离 2.欧式距离 3.曼哈顿距离 三.K值的选择选择较小的K值，容易发生过拟合；选择较大的K值，则容易欠拟合。在应用中，通常采用交叉验证法来选择最优K值。 四.优缺点优点: 1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。 2）可用于非线性分类。 缺点： 1）需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。 2）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。","link":"/2021/10/10/knn/"},{"title":"知识图谱","text":"东南的课程： https://github.com/npubird/KnowledgeGraphCourse","link":"/2021/12/12/knowledgegraph/"},{"title":"From RankNet to LambdaRank to LambdaMART","text":"1.RankNetRankNet采用pairwise的方法进行模型训练。 loss推导 给定特定$query$下的两个文档$U_i$和$U_j$，其特征向量分别为$x_i$和$x_j$，经过RankNet进行前向计算得到对应的分数为$s_i=f(x_i)$和$s_j=f(x_j)$。用$U_i \\rhd U_j$表示$U_i$比$U_j$排序更靠前。继而可以用下面的公式来表示$U_i$应该比$U_j$排序更靠前的概率： P_{ij} \\equiv P(U_i \\rhd U_j) \\equiv \\frac{1}{1+e^{-\\sigma(s_i-s_j)}}定义$S_{ij} \\in \\{0,\\pm1\\}$为文档$i$和文档$j$被标记的标签之间的关联，即 S_{ij}=\\left\\{ \\begin{aligned} 1&& 文档i比文档j更相关\\\\ 0&& 文档i和文档j相关性一致\\\\ -1&& 文档j比文档i更相关 \\end{aligned} \\right.定义$\\overline{P}_{ij}=\\frac{1}{2}(1+S_{ij})$表示$U_i$应该比$U_j$排序更靠前的已知概率，则可以用交叉熵定义优化目标的损失函数： \\begin{align*} C&=-\\overline{P}_{ij}log{P_{ij}}-(1-\\overline{P}_{ij})log(1-P_{ij}) \\\\&=\\frac{1}{2}(1-S_{ij})\\sigma(s_i-s_j)+log(1+e^{-\\sigma(s_i-s_j)}) \\end{align*}注意：$\\sigma$是超参数 ranknet 加速 2.LambdaRankranket缺陷为只考虑pair的相对位置没有考虑二者在列表的整体位置 LambdaRank本质为ranknet基础上加入Listwise的指标，因此有人将LambdaRank归为listwise方法，也有归到pairwise方法 2.1 RankNet的局限 2.2 LambdaRank定义 \\begin{align*} \\frac{\\partial{C}}{\\partial{w_k}}&=\\frac{\\partial{C}}{\\partial{s_i}}\\frac{\\partial{s_i}}{\\partial{w_k}}+\\frac{\\partial{C}}{\\partial{s_j}}\\frac{\\partial{s_j}}{\\partial{w_k}} \\\\&=\\sigma\\left(\\frac{1}{2}(1-S_{ij})-\\frac{1}{1+e^{\\sigma(s_i-s_j)}}\\right)\\left(\\frac{\\partial{s_i}}{\\partial{w_k}}-\\frac{\\partial{s_j}}{\\partial{w_k}}\\right) \\\\&=\\lambda_{ij}\\left(\\frac{\\partial{s_i}}{\\partial{w_k}}-\\frac{\\partial{s_j}}{\\partial{w_k}}\\right) \\end{align*} \\\\其中\\lambda_{ij}=\\frac{\\partial{C}}{\\partial{s_i}}=-\\frac{\\partial{C}}{\\partial{s_j}}=\\sigma\\left(\\frac{1}{2}(1-S_{ij})-\\frac{1}{1+e^{\\sigma(s_i-s_j)}}\\right)上述公式可以进一步简化，即只考虑$S_{ij}=1$ （为什么可以？？？？？） 那么Lambda，$\\lambda$，就是梯度 \\lambda_{ij}=\\frac{-\\sigma}{1+e^{\\sigma(s_i-s_j)}}为了加强排序中前后顺序的重要性，Lambda在原基础上引入评价指标Z（如NDCG），把交换两个文档的位置引起的评价指标的变化$|\\Delta Z_{ij}|$作为其中一个因子： \\lambda_{ij}=\\frac{\\partial{C}}{\\partial{s_i}}=\\frac{-\\sigma}{1+e^{\\sigma(s_i-s_j)}}|\\Delta Z_{ij}|反推出 LambdaRank 的损失函数： C=log(1+e^{\\sigma (s_i-s_j)})|\\Delta Z_{ij}|3.LambdaMART属于listwise，也有说pairwise。 LambdaMART=lambda($\\lambda$)+mart(gbdt) $\\lambda$就是梯度，lambdarank就是一种loss，gbdt就是模型 lambdamart说白了就是利用gbdt计算lambdarank中s，或者说将lambdarank作为gbdt的loss gbdt，lambdamart算法流程差异在于step1 GBDT： 初始化： $f_0(x) = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{i=1}^N L(y_i, \\gamma)$ for m=1 to M:(a). 计算负梯度： $\\tilde{y}_i = -\\frac{\\partial L(y_i,f_{m-1}(x_i))}{\\partial f_{m-1}(x_i)}, \\qquad i = 1,2 \\cdots N$(b). $\\left \\{ R_{jm} \\right\\}_1^J = \\mathop{\\arg\\min}\\limits_{\\left \\{ R_{jm} \\right\\}_1^J}\\sum\\limits_{i=1}^N \\left [\\tilde{y}_i - h_m(x_i\\,;\\,\\left \\{R_{jm},b_{jm} \\right\\}_1^J) \\right]^2$(c). $\\gamma_{jm} = \\mathop{\\arg\\min}\\limits_\\gamma \\sum\\limits_{x_i \\in R_{jm}}L(y_i,f_{m-1}(x_i)+\\gamma)$(d). $f_m(x) = f_{m-1}(x) + \\sum\\limits_{j=1}^J \\gamma_{jm}I(x \\in R_{jm})$ 输出$f_M(x)$ LambdaMART: 参考https://blog.csdn.net/laolu1573/article/details/87372514 https://liam.page/uploads/slides/lambdamart.pdf https://blog.csdn.net/zpalyq110/article/details/79527653 https://zhuanlan.zhihu.com/p/86354141 https://www.cnblogs.com/genyuan/p/9788294.html https://blog.csdn.net/huagong_adu/article/details/40710305 https://zhuanlan.zhihu.com/p/270608987 https://www.cnblogs.com/bentuwuying/p/6690836.html paper原文： https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf https://www.jianshu.com/p/a78b3f52c221 https://blog.csdn.net/zhoujialin/article/details/46697409 https://blog.csdn.net/w28971023/article/details/45849659 https://zhuanlan.zhihu.com/p/270608987","link":"/2021/09/27/lambdamart/"},{"title":"lambda","text":"12345678def test01(a,b,c,d): return a*b*c*dprint(test01(1,2,3,4))#相当于下面这个函数f=lambda a,b,c,d:a*b*c*dprint(f(1,2,3,4)) 1234567891011121314151617181920211.f=[lambda x: i*x for i in range(1,5)]print(f)print(f[0](2))print([m(2) for m in f])[&lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x0000028503A613A0&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x0000028503A61430&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x0000028503A614C0&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x0000028503A61550&gt;]4[4, 4, 4, 4]2.f=[lambda x,i=i: i*x for i in range(1,5)]print(f)print(f[0](2))print([m(2) for m in f])[&lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x000001B4039813A0&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x000001B403981430&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x000001B4039814C0&gt;, &lt;function &lt;listcomp&gt;.&lt;lambda&gt; at 0x000001B403981550&gt;]2[2, 4, 6, 8]","link":"/2022/09/12/lamdba/"},{"title":"刷题指南","text":"https://github.com/halfrost/LeetCode-Go","link":"/2021/11/30/leet-coding/"},{"title":"LASERTAGGER","text":"一. 摘要对于某一些文本生成任务，输入和输出的文本有很多的重叠部分，如果还是采用encoder-decoder的文本生成模型去从零开始生成，其实是很浪费和没必要的，并且会导致两个问题：1：生成模型的幻觉问题(就是模型胡说八道) ；2：出现叠词(部分片段一致)。 基于上面的考虑，作者提出了lasertagger模型，通过几个常用的操作：keep token、delete token、 add token，给输入序列的每个token打上标签，使得文本生成任务转化为了序列标注任务。 通过这种方式，相较于encoder-decoder模型的优势有如下：1、推理的速度更快 2、在较小的数据集上性能优于seq2seq baseline，在大数据集上和baseline持平（因为输入和输出的文本有很多的重叠部分，对于这种情况，lasertagger的候选词库比较小，因为对于重叠部分的词，词库只需要添加keep，而传统encoder-decoder的候选词库依然很大，因为对于重叠部分的词，词库需要添加对应的词） 二.主要贡献1、通过输入和输出文本，自动去提取需要add的token 2、通过输入文本，输出文本和tag集，给训练的输入序列打上标签 3、提出了两个版本，$LASERTAGGER_{AR}$( bert+transformer decoder )和$LASERTAGGER_{FF}$( bert+desen+softmax ) 三. 整体流程 其实就是两个过程，一.将输入文本变编码成特殊标注，二.将标注解码成文本 四. 文本标注4.1 Tag集构建（也就是label集构建）一般情况，tag分为两个大类： base tag $B$和 add tag $P$。对于base tag，就是$KEEP$或者$DELETE$当前token；对于add tag，就是要添加一个词到token前面，添加的词来源于词表$V$。实际在工程中，将$B$和$P$结合来表示，即$^{P}B$，总的tag数量大约等于$B$的数量乘以$P$的数量，即$2|V|$。对于某些任务可以引入特定的tag，比如对于句子融合，可以引入$SWAP$,如下图。 4.1.1 词表V的构建构建目标： 最小化词汇表规模； 最大化目标词语的比例 限制词汇表的词组数量可以减少相应输出的决策量；最大化目标词语的比例可以防止模型添加无效词。 构建过程： 通过$LCS$算法（longest common sequence，最长公共子序列，注意和最长公共子串不是一回事），找出输入和输出序列的最长公共子序列，输出剩下的序列，就是需要$add$的token，添加到词表$V$，词表中的词基于词频排序,然后选择$l$个常用的。 举个例子：soruce为“12345678”，target为”1264591” ​ 最长公共子序列为[‘1’, ‘2’, ‘4’, ‘5’] ​ 需要$add$的token为 [‘6’, ‘91’] 源码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def _lcs_table(source, target): &quot;&quot;&quot;Returns the Longest Common Subsequence dynamic programming table.&quot;&quot;&quot; rows = len(source) cols = len(target) lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)] for i in range(1, rows + 1): for j in range(1, cols + 1): if source[i - 1] == target[j - 1]: lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1 else: lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1]) return lcs_tabledef _backtrack(table, source, target, i, j): &quot;&quot;&quot;Backtracks the Longest Common Subsequence table to reconstruct the LCS. Args: table: Precomputed LCS table. source: List of source tokens. target: List of target tokens. i: Current row index. j: Current column index. Returns: List of tokens corresponding to LCS. &quot;&quot;&quot; if i == 0 or j == 0: return [] if source[i - 1] == target[j - 1]: # Append the aligned token to output. return _backtrack(table, source, target, i - 1, j - 1) + [target[j - 1]] if table[i][j - 1] &gt; table[i - 1][j]: return _backtrack(table, source, target, i, j - 1) else: return _backtrack(table, source, target, i - 1, j)def _compute_lcs(source, target): # s1={1,3,4,5,6,7,7,8},s2={3,5,7,4,8,6,7,8,2} return 35778 table = _lcs_table(source, target) return _backtrack(table, source, target, len(source), len(target)) def _get_added_phrases(source: Text, target: Text) -&gt; Sequence[Text]: &quot;&quot;&quot; Computes the phrases that need to be added to the source to get the target. &quot;&quot;&quot; sep = '' source_tokens = utils.get_token_list(source.lower()) target_tokens = utils.get_token_list(target.lower()) #compute Longest Common Subsequence kept_tokens = _compute_lcs(source_tokens, target_tokens) added_phrases = [] kept_idx = 0 phrase = [] for token in target_tokens: if kept_idx &lt; len(kept_tokens) and token == kept_tokens[kept_idx]: kept_idx += 1 if phrase: added_phrases.append(sep.join(phrase)) phrase = [] else: phrase.append(token) if phrase: added_phrases.append(sep.join(phrase)) return added_phrases 词表位于文件label_map.txt.log，本人基于自己的数据集，内容如下所示 12345Idx Frequency Coverage (%) Phrase1 19 94.22 址2 15 95.27 单位3 8 95.76 地4 6 96.17 执勤 4.1.2 tag集本人基于自己的数据集，得到的候选tag如下： 12345678910KEEPDELETEKEEP|址DELETE|址KEEP|单位DELETE|单位KEEP|地DELETE|地KEEP|执勤DELETE|执勤 4.2 Converting Training Targets into Tagspaper上的伪代码： 采用贪心策略，核心思想就是遍历$t$，先和$s$匹配，匹配上就$keep$，然后$i_t+j$，得到潜在的$add \\ phrase \\ p=t(i_t:i_t+j-1) $，然后判断$t(i_t+j)==s(i_s)\\ and \\ p\\in V $ 源码： 和伪代码有一点不同，差异在于#####之间。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def _compute_single_tag( self, source_token, target_token_idx, target_tokens): &quot;&quot;&quot;Computes a single tag. The tag may match multiple target tokens (via tag.added_phrase) so we return the next unmatched target token. Args: source_token: The token to be tagged. target_token_idx: Index of the current target tag. target_tokens: List of all target tokens. Returns: A tuple with (1) the computed tag and (2) the next target_token_idx. &quot;&quot;&quot; source_token = source_token.lower() target_token = target_tokens[target_token_idx].lower() if source_token == target_token: return tagging.Tag('KEEP'), target_token_idx + 1 # source_token!=target_token added_phrase = '' for num_added_tokens in range(1, self._max_added_phrase_length + 1): if target_token not in self._token_vocabulary: break added_phrase += (' ' if added_phrase else '') + target_token next_target_token_idx = target_token_idx + num_added_tokens if next_target_token_idx &gt;= len(target_tokens): break target_token = target_tokens[next_target_token_idx].lower() if (source_token == target_token and added_phrase in self._phrase_vocabulary): return tagging.Tag('KEEP|' + added_phrase), next_target_token_idx + 1 return tagging.Tag('DELETE'), target_token_idxdef _compute_tags_fixed_order(self, source_tokens, target_tokens): &quot;&quot;&quot;Computes tags when the order of sources is fixed. Args: source_tokens: List of source tokens. target_tokens: List of tokens to be obtained via edit operations. Returns: List of tagging.Tag objects. If the source couldn't be converted into the target via tagging, returns an empty list. &quot;&quot;&quot; tags = [tagging.Tag('DELETE') for _ in source_tokens] # Indices of the tokens currently being processed. source_token_idx = 0 target_token_idx = 0 while target_token_idx &lt; len(target_tokens): tags[source_token_idx], target_token_idx = self._compute_single_tag( source_tokens[source_token_idx], target_token_idx, target_tokens) #################################################################################### # If we're adding a phrase and the previous source token(s) were deleted, # we could add the phrase before a previously deleted token and still get # the same realized output. For example: # [DELETE, DELETE, KEEP|&quot;what is&quot;] # and # [DELETE|&quot;what is&quot;, DELETE, KEEP] # Would yield the same realized output. Experimentally, we noticed that # the model works better / the learning task becomes easier when phrases # are always added before the first deleted token. Also note that in the # current implementation, this way of moving the added phrase backward is # the only way a DELETE tag can have an added phrase, so sequences like # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created. if tags[source_token_idx].added_phrase: # # the learning task becomes easier when phrases are always added before the first deleted token first_deletion_idx = self._find_first_deletion_idx( source_token_idx, tags) if first_deletion_idx != source_token_idx: tags[first_deletion_idx].added_phrase = ( tags[source_token_idx].added_phrase) tags[source_token_idx].added_phrase = '' ######################################################################################## source_token_idx += 1 if source_token_idx &gt;= len(tags): break # If all target tokens have been consumed, we have found a conversion and # can return the tags. Note that if there are remaining source tokens, they # are already marked deleted when initializing the tag list. if target_token_idx &gt;= len(target_tokens): # all target tokens have been consumed return tags return [] # TODO 缺陷： 对于一些情况，无法还原，举个例子： ​ source：证件有效期截止日期 target：证件日期格式 ​ 得不到tag结果 可以补充策略来修复bug 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def _compute_tags_fixed_order(self, source_tokens, target_tokens): &quot;&quot;&quot;Computes tags when the order of sources is fixed. Args: source_tokens: List of source tokens. target_tokens: List of tokens to be obtained via edit operations. Returns: List of tagging.Tag objects. If the source couldn't be converted into the target via tagging, returns an empty list. &quot;&quot;&quot; tags = [tagging.Tag('DELETE') for _ in source_tokens] # Indices of the tokens currently being processed. source_token_idx = 0 target_token_idx = 0 while target_token_idx &lt; len(target_tokens): tags[source_token_idx], target_token_idx = self._compute_single_tag( source_tokens[source_token_idx], target_token_idx, target_tokens) ######################################################################################### # If we're adding a phrase and the previous source token(s) were deleted, # we could add the phrase before a previously deleted token and still get # the same realized output. For example: # [DELETE, DELETE, KEEP|&quot;what is&quot;] # and # [DELETE|&quot;what is&quot;, DELETE, KEEP] # Would yield the same realized output. Experimentally, we noticed that # the model works better / the learning task becomes easier when phrases # are always added before the first deleted token. Also note that in the # current implementation, this way of moving the added phrase backward is # the only way a DELETE tag can have an added phrase, so sequences like # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created. if tags[source_token_idx].added_phrase: # # the learning task becomes easier when phrases are always added before the first deleted token first_deletion_idx = self._find_first_deletion_idx( source_token_idx, tags) if first_deletion_idx != source_token_idx: tags[first_deletion_idx].added_phrase = ( tags[source_token_idx].added_phrase) tags[source_token_idx].added_phrase = '' ####################################################################################### source_token_idx += 1 if source_token_idx &gt;= len(tags): break # If all target tokens have been consumed, we have found a conversion and # can return the tags. Note that if there are remaining source tokens, they # are already marked deleted when initializing the tag list. if target_token_idx &gt;= len(target_tokens): # all target tokens have been consumed return tags ####fix bug by lavine ###strategy1 added_phrase = &quot;&quot;.join(target_tokens[target_token_idx:]) if added_phrase in self._phrase_vocabulary: tags[-1] = tagging.Tag('DELETE|' + added_phrase) print(''.join(source_tokens)) print(''.join(target_tokens)) print(str([str(tag) for tag in tags] if tags != None else None)) return tags ###strategy2 return [] # TODO 4.3 模型结构 模型主要包含两个部分：1.encoder:generates activation vectors for each element in the input sequence 2.decoder：converts encoder activations into tag labels 4.3.1 encoder由于$BERT$在sentence encoding tasks上做到state-of-the-art，所以使用$BERT$ 作为encoder部分。作者选择了$BERT_{base}$,包含12个self-attention层 4.3.2 decoder在$BERT$原文中，对于标注任务采取了非常简单的decoder结构，即采用一层feed-forward作为decoder，把这种组合叫做$LASERTAGGER_{FF}$，这种结构的缺点在于预测的标注词相互独立，没有考虑标注词的关联性。 为了考虑标注词的关联性，decode使用了Transformer decoder，单向连接，记作$LASERTAGGER_{AR}$，这种encoder和decoder的组合的有点像BERT结合GPT的感觉decoder 和encoder在以下方面交流：(i) through a full attention over the sequence of encoder activations (ii) by directly consuming the encoder activation at the current step 五.realize对于基本的tag，比如$KEEP$，$DELETE$，$ADD$，$realize$就是根据输入和tag直接转换就行；对于特殊的tag，需要一些特定操作，看情况维护规则。 六 loss假设句子长度为n，tag数量为m, loss为n个m分类任务的和 七.评价指标评价指标，不同任务不同评价指标 1 Sentence Fusion Exact score ：percentage of exactly correctly predicted fusions（类似accuracy） SARI ：average F1 scores of the added, kept, and deleted n-grams 2 Split and Rephrase SARI 3 Abstractive Summarization ROUGE-L 4 Grammatical Error Correction (GEC) precision and recall, F0:5 八.实验结果baseline： based on Transformer where both the encoder and decoder replicate the $BERT_{base}$ architecture 速度：1.$LASERTAGGER_{AR} $is already 10x faster than comparable-in-accuracy $SEQ2SEQ_{BERT}$ baseline. This difference is due to the former model using a 1-layer decoder (instead of 12 layers) and no encoder-decoder cross attention. 2.$LASERTAGGER_{FF}$ is more than 100x faster 其余结果参考paper 参考https://arxiv.org/pdf/1909.01187.pdf https://github.com/google-research/lasertagger https://zhuanlan.zhihu.com/p/348109034","link":"/2021/07/27/lasertagger/"},{"title":"常见题目","text":"1 topk快排 O(n) 每排一次，就知道基准的位置，就可以得出TopK是在基准左边部分还是右边部分，因此不需要全部排序 堆排 O(k+nlog(k)) 21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 给定一个字符串s和一个字符串p，请问最少去掉s中的多少个字符，才能使得p是s的子串呢？# 解答要求时间限制：1000ms, 内存限制：100MB# 输入# 两行，第一行为字符串s，第二行为字符串p。(s和p只包含小写英文字母,s的长度不超过2000,p的长度不超过10,且保证有解）# 输出# 最少去掉的字符个数。# 样例# 输入样例 1# axb# ab# 输出# 1# 输入样例 2# axabc# abc# 输出# 0# 输入样例 3# axbacxbc# abc# 输出# 2#dp[i][j] 表示 s[:i]最少需要删几个p[:j]是它的子串 O(mn) O(mn)###def minedit(str1,str2): m=len(str1) n=len(str2) # max_num=9999 dp=[[None for i in range(n+1)]for j in range(m+1)] dp[0][0]=0 for i in range(m): dp[i][0]=0 for i in range(1,m+1): for j in range(1,n+1): if str1[i-1]==str2[j-1]: dp[i][j]=dp[i-1][j-1] else: ## if dp[i-1][j]!=None: dp[i][j]=dp[i-1][j]+1 result=None for i in range(n,m+1): if result==None: result=dp[i][n] else: result=min(result,dp[i][n]) return result print(minedit(&quot;axbacxbc&quot;,&quot;abc&quot;))","link":"/2022/05/19/leet-normal/"},{"title":"lightgbm","text":"Light gradient boosting machine 用于排序12import lightgbm as lgblgb.LGBMRanker 原理 LGBMRanker模型和这个LambdaMART的原理很像 https://blog.csdn.net/wuzhongqiang/article/details/110521519 参考https://zhuanlan.zhihu.com/p/99069186 https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w https://www.jianshu.com/p/765efe2b951a https://blog.csdn.net/wuzhongqiang/article/details/110521519","link":"/2022/06/09/lightgbm/"},{"title":"leetcode常见套路","text":"一.常见算法分治策略，动态规划，回溯，分支限界，贪心策略 二.巧用数据结构普通栈、单调栈 队列 堆 字典树 三.技巧双指针/滑窗，二分查找，排序，快慢指针，取余，位运算，倍增（29. 两数相除）、递归 时空转化（hashtable） 四 套路选择https://zhuanlan.zhihu.com/p/358653377 https://zhuanlan.zhihu.com/p/341176507 参考https://zhuanlan.zhihu.com/p/358653377 https://zhuanlan.zhihu.com/p/341176507","link":"/2021/08/10/leetcode_algorith-tech/"},{"title":"常见操作","text":"1 软连接https://www.cnblogs.com/sueyyyy/p/10985443.html# 当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在其它的 目录下用ln命令链接（link）就可以，不必重复的占用磁盘空间。 ln -s source_path target_path 例子：ln -s /home/atguigu/bin/ ~/bin 2 SSH无密登录配置http://www.yaowenming.com/A/gAJG0mvg5Z/ （1）hadoop102上生成公钥和私钥： [atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） （2）将hadoop102公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104 （3）hadoop103上生成公钥和私钥： [atguigu@hadoop103 .ssh]$ ssh-keygen -t rsa 然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥） （4）将hadoop103公钥拷贝到要免密登录的目标机器上 [atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop102 [atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop103 [atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop104 3 将text内容加入到file文件的第1行之后sed -i ‘1 a text’ file 例子：sed -i ‘1 a kinit -kt /etc/security/keytab/hive.keytab hive’ hdfs_to_ods_log.sh 4 修改脚本执行权限chmod chmod u+x lg .sh https://blog.csdn.net/u013197629/article/details/73608613 5 集群分发脚本基于rsync命令 1rsync -av /opt/module root@hadoop103:/opt/ 1.编写脚本 12345678910111213141516171819202122232425262728#!/bin/bash#1. 判断参数个数if [ $# -lt 1 ]then echo Not Enough Arguement! exit;fi#2. 遍历集群所有机器for host in hadoop102 hadoop103 hadoop104do echo ==================== $host ==================== #3. 遍历所有目录，挨个发送 for file in $@ do #4 判断文件是否存在 if [ -e $file ] then #5. 获取父目录 pdir=$(cd -P $(dirname $file); pwd) #6. 获取当前文件的名称 fname=$(basename $file) ssh $host &quot;mkdir -p $pdir&quot; rsync -av $pdir/$fname $host:$pdir else echo $file does not exists! fi donedone 2.修改脚本xsync具有执行权限 1chmod +x xsync 6 环境变量https://blog.csdn.net/white_idiot/article/details/78253004 7 linux根目录下各个文件夹的作用https://blog.51cto.com/u_14233078/2443062 8 如何让你的脚本可以在任意地方都可执行？https://www.cnblogs.com/yychuyu/p/12918957.html 9 Linux服务器jps报process information unavailablehttps://blog.csdn.net/weixin_44803002/article/details/103332889 cd /tmp rm -rf /tmp/hsperfdata_* 10 sudo命令 使用hive用户启动hiveserver2 1[root@hadoop102 ~]# sudo -i -u hive hiveserver2 11 jpshttps://blog.csdn.net/wangzhongshun/article/details/112546027 不过jps有个缺点是只能显示当前用户的进程id，要显示其他用户的还只能用linux的ps命令 12 scp scp -r root@hosts : addr ./ 13 tarhttps://www.cnblogs.com/w54255787/p/10175202.html tar -cvf models.tar models 14 kill -9 sparksubmit 无效https://www.codetd.com/article/4229439 重启 15 创建用户和用户组https://www.jianshu.com/p/1e3fcfc8e3ef","link":"/2022/02/07/linun-common-command/"},{"title":"linux","text":"root用户也是用户，就是权限高于普通用户 cd ~ ：回到当前用户目录 cd /：回到根目录","link":"/2022/01/27/linux-comm/"},{"title":"listwise","text":"博客 https://zhuanlan.zhihu.com/p/66514492 Listwise Approach to Learning to Rank - Theory and Algorithm（ListMLE） http://icml2008.cs.helsinki.fi/papers/167.pdf Learning to Rank: From Pairwise Approach to Listwise Approach（ListNet） https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf","link":"/2021/12/20/listwise/"},{"title":"Learning to Rank From Pairwise Approach to Listwise Approach(listnet)","text":"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf https://blog.csdn.net/Mr_tyting/article/details/80554849 核心思想为： Given two lists of scores（模型和人） we can first calculate two permutation probability distributions from them（简化到用top1） and then calculate the distance between the two distributions as the listwise loss function.（交叉熵） 4. Probability Models4.1. Permutation Probability $\\pi=(2,3,1) $指的是对象2排在第一位 上面是topn的形式 因为总共有n！次排序组合 4.2. Top One Probabilitytopk： P_s(\\pi)=\\prod \\limits_{j=1}^K\\frac{\\phi(S_{\\pi(j)})}{\\sum_{k=j}^n\\phi(S_{\\pi(k)})}总共有N ! / ( N − k ) ! 种不同排列，大大减少了计算复杂度 top1： 此时有n种不同排列情况 概率分布的含义：对于每个j，分别都处于第一的概率是多少 5.Learning Method: ListNetWe employ a new learning method for optimizing the listwise loss function based on top one probability, with Neural Network as model and Gradient Descent as optimization algorithm. We refer to the method as ListNet. $y^{(i)}$是真实的score list，有个疑问就是$y^{(i)}$怎么得到？关于这个，应该是先有真实的score list（人打），然后基于score list得到排序，参考 https://zhuanlan.zhihu.com/p/66514492 核心步骤1.打标得到真实的score list，模型得到预测的score list 2.然后用softmax得到真实的和预测的score list的概率分布 3.然后用交叉熵计算两种概率分布的差距","link":"/2021/12/21/listnet/"},{"title":"损失函数","text":"损失函数一般都要用可导函数，因为常用的优化算法，比如梯度下降，牛顿法，都需要导数。 1.回归损失1.1 Mean Squared Error L=\\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y}_i)^21.2 Mean Absolute Error L=\\frac{1}{N}\\sum_{i=1}^{N}|y_i-\\hat{y}_i|1.3 Huber Loss ( Smooth Mean Absolute Error Loss ) L=\\frac{1}{N}\\sum_{i=1}^{N}[\\mathbb{1}_{|y_i-\\hat{y_i}|\\le \\delta}\\frac{(y_i-\\hat{y_i})^2}{2}+\\mathbb{1}_{|y_i-\\hat{y_i}|> \\delta} \\ (\\delta \\cdot |y_i-\\hat{y_i}|-\\frac{1}{2}\\delta^{2})]2.分类损失2.1 Cross-entropy losshttps://zhuanlan.zhihu.com/p/100921909 K是种类数量 y是标签 p是神经网络的输出，也就是指类别是i的概率 2.2 Hinge loss L=max(0,1-y\\cdot \\hat{y})SVM模型的损失函数本质上就是 Hinge Loss + L2 正则化 参考https://blog.csdn.net/m0_38007695/article/details/114983574","link":"/2021/09/07/loss-func/"},{"title":"逻辑回归","text":"逻辑回归（lr）= 线性回归+sigmoid","link":"/2022/08/20/logistic-reg/"},{"title":"超长文本处理","text":"bert最大长度固定，默认为512 数据层面： 1 直接截断：太粗暴，可能把重要的丢了 2 抽取重要部分 3 分段+拼接 ​ 问题很多，怎么训练？？怎么预测？？？ 模型层面： transformer-xl based的ptm，比如xlnet 传统rnn based的seq2seq 参考https://www.zhihu.com/question/395903256","link":"/2022/05/31/long-text/"},{"title":"调节学习率","text":"当学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢，合理的学习率才能让模型收敛到最小点而非局部最优点或鞍点 经验值： 0.01 ~ 0.001 学习率衰减原因：起初距离目标偏离大，可以设置较大，为了快速收敛，后续逐渐靠近目标，需要精细化一点，所以希望值小一点 分类 1.轮数衰减 每经过k个epochs后学习率减半 2.指数衰减 \\alpha_t=decay\\_rate^{epoch}*\\alpha_{t-1}3.分数衰减 \\alpha_t=\\frac{\\alpha_{t-1}}{1+decay\\_rate*epoch}参考https://blog.csdn.net/LiuPeiP_VIPL/article/details/119581343","link":"/2021/12/18/lr/"},{"title":"消息传递","text":"1.使用内置的消息传递api比如GraphConv 2.实现自己的消息传递策略Write your own GNN module https://docs.dgl.ai/tutorials/blitz/3_message_passing.html Message Passing APIs https://docs.dgl.ai/guide/message-api.html#guide-message-passing-api https://docs.dgl.ai/guide/message-heterograph.html Apply Message Passing On Part Of The Graph https://docs.dgl.ai/guide/message-part.html message function,Reduce function https://docs.dgl.ai/api/python/dgl.function.html","link":"/2021/12/31/make-own-gnn-module/"},{"title":"Spark vs MapReduce","text":"对比https://www.educba.com/mapreduce-vs-spark/ MapReduce Spark Product’s Category From the introduction, we understood that MapReduce enables the processing of data and hence is majorly a data processing engine. Spark, on the other hand, is a framework that drives complete analytical solutions or applications and hence making it an obvious choice for data scientists to use this as a data analytics engine. Framework’s Performance and Data Processing In the case of MapReduce, reading and writing operations are performed from and to a disk thus leading to slowness in the processing speed. In Spark, the number of read/write cycles is minimized along with storing data in memory allowing it to be 10 times faster. But spark may suffer a major degradation if data doesn’t fit in memory. Latency As a result of lesser performance than Spark, MapReduce has a higher latency in computing. Since Spark is faster, it enables developers with low latency computing. Manageability of framework MapReduce being only a batch engine, other components must be handled separately yet synchronously thus making it difficult to manage. Spark is a complete data analytics engine, has the capability to perform batch, interactive streaming, and similar component all under the same cluster umbrella and thus easier to manage! Real-time Analysis MapReduce was built mainly for batch processing and hence fails when used for real-time analytics use cases. Data coming from real-time live streams like Facebook, Twitter, etc. can be efficiently managed and processed in Spark. Interactive Mode MapReduce doesn’t provide the gamut of having interactive mode. In spark it is possible to process the data interactively Security MapReduce has accessibility to all features of Hadoop security and as a result of this, it is can be easily integrated with other projects of Hadoop Security. MapReduce also supports ASLs. In Spark, the security is by default set to OFF which might lead to a major security fallback. In the case of authentication, only the shared secret password method is possible in Spark. Tolerance to Failure In case of crash of MapReduce process, the process is capable of starting from the place where it was left off earlier as it relies on Hard Drives rather than RAMs In case of crash of Spark process, the processing should start from the beginning and hence becomes less fault-tolerant than MapReduce as it relies of RAM usage. spark为什么比MapReduce快https://blog.csdn.net/JENREY/article/details/84873874 1 spark基于内存 ，mapreduce基于磁盘 指的是中间结果 MapReduce：通常需要将计算的中间结果写入磁盘，然后还要读取磁盘，从而导致了频繁的磁盘IO Spark：不需要每次将计算的中间结果写入磁盘 2 spark粗粒度资源申请，MapReduce细粒度资源申请 spark 执行task不需要自己申请资源，提交任务的时候统一申请了 MapReduce 执行task任务的时候，task自己申请 3 spark基于多线程，mapreduce基于多进程","link":"/2022/02/25/mapreduce-spark/"},{"title":"maven","text":"1 简介Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。 2 安装配置https://blog.csdn.net/qq_19734597/article/details/120996418 https://blog.csdn.net/weixin_34234829/article/details/89686175 https://blog.csdn.net/idomyway/article/details/81974677 1 更换镜像 conf/settings.xml https://www.cnblogs.com/digdeep/p/5026066.html https://blog.csdn.net/qq_42931492/article/details/107283590 2 网络配置 conf/settings.xml https://blog.csdn.net/zongf0504/article/details/88797831 3 问题 (org.apache.maven.wagon.providers.http.httpclient.NoHttpResponseException) caught when processing request to {}-&gt;http://XXXXXX-&gt;http://maven.aliyun.com:80: The target server failed to respond 3 管理包https://blog.51cto.com/u_15119353/3303815 1 设置setting.xml 2 编写pom 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.44&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-csv&lt;/artifactId&gt; &lt;version&gt;1.10.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3 生成本地包的repository 默认位置在/user/.m2/repository，在setting可以修改 4 环境使用本地repository https://blog.csdn.net/weixin_42476601/article/details/87884514 4 问题1.com/atguigu/gmall/hive/udtf/ExplodeJSONArray has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 解决：两边jdk版本对齐就好 打包时候注意要：maven clean,maven compile class file versions对应jdk版本： 1234567891049 = Java 550 = Java 651 = Java 752 = Java 853 = Java 954 = Java 1055 = Java 1156 = Java 1257 = Java 1358 = Java 14 2.Unknown host maven.aliyun.com","link":"/2022/03/06/maven/"},{"title":"BERT在美团搜索核心排序的探索和实践","text":"很有启发，抱着学习态度，mark一下 模型层面整体结构如下 1 BERT预训练 2 多任务学习 ​ 场景层：根据业务场景进行划分，每个业务场景单独设计网络结构 3 联合训练 两个任务分别为： ​ 1 相关性任务：相关性+NER（多任务增强相关性） ​ 2 排序任务 怎么联合没看出来 之前是两阶段finetune： 1. 先相关性任务 2 然后排序任务 参考https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html","link":"/2021/08/21/meituan/"},{"title":"元数据管理","text":"元数据https://www.ruanyifeng.com/blog/2007/03/metadata.html# https://dataedo.com/kb/data-glossary/what-is-metadata https://www.cnblogs.com/alight/p/3982086.html Metadata is simply data about data. It means it is a description and context of the data. It helps to organize, find and understand data. Here are a few real world examples of metadata: Those are some typical metadata elements: Title and description Tags and categories Who created and when Who last modified and when Who can access or update 元数据管理https://zhuanlan.zhihu.com/p/336504407 元数据管理工具atlas hive","link":"/2022/02/07/meta-data-management/"},{"title":"细粒度NLP任务","text":"AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization （ByteDance AI Lab） https://arxiv.org/pdf/2008.11869.pdf 参考","link":"/2022/06/30/micro-Grained/"},{"title":"迭代分析","text":"","link":"/2022/07/26/ml-iteration-analyze/"},{"title":"深度学习模型部署","text":"0 OONXOpen Neural Network Exchange 跨框架的模型中间表达，模型的统一存储形式,ONNX 模型一般用于中间部署阶段 1 pytorchlibtorch 2 TensorFlowTensorflow Serving TensorFlow Lite 3 TensorRT加速用的 NVIDIA® TensorRT™ is an SDK for optimizing trained deep learning models to enable high-performance inference. https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html 4 NCNNncnn is a high-performance neural network inference computing framework optimized for mobile platforms. 5 TVM加速 ApacheTVM是一个面向CPU、GPU和机器学习加速器的开源机器学习编译器框架。它旨在使机器学习工程师能够在任何后端设备高效地优化并运行计算 https://chinese.tvm.wiki/tutorial/introduction.html#an-overview-of-tvm-and-model-optimization 6 onnxruntime 加速用的 7 组合使用 1 TensorFlow -&gt; oonx-&gt;onnxruntime https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html 参考https://zhuanlan.zhihu.com/p/346511883 https://zhuanlan.zhihu.com/p/423551635 https://hub.fastgit.org/onnx/tutorials https://blog.csdn.net/zxgmlcj/article/details/103279869 https://hub.fastgit.org/microsoft/onnxruntime https://hub.fastgit.org/onnx/onnx https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html https://chinese.tvm.","link":"/2022/05/23/model-deploy/"},{"title":"集群监控","text":"常用运维监控工具详细对比 https://www.leoheng.com/2021/08/10/%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94/ Zabbix+Grafana https://www.cnblogs.com/wushuaishuai/p/10852355.html","link":"/2022/02/05/monitor-pcs/"},{"title":"单调栈","text":"分类： 单调栈也分为单调递增栈和单调递减栈 单调递增栈：单调递增栈就是从栈底到栈顶数据是从大到小 单调递减栈：单调递减栈就是从栈底到栈顶数据是从小到大 举例：单调递增栈 现在有一组数10，3，7，4，12。从左到右依次入栈，则如果栈为空或入栈元素值小于栈顶元素值，则入栈；否则，如果入栈则会破坏栈的单调性，则需要把比入栈元素小的元素全部出栈。 10入栈时，栈为空，直接入栈，栈内元素为10。 3入栈时，栈顶元素10比3大，则入栈，栈内元素为10，3。 7入栈时，栈顶元素3比7小，则栈顶元素出栈，此时栈顶元素为10，比7大，则7入栈，栈内元素为10，7。 4入栈时，栈顶元素7比4大，则入栈，栈内元素为10，7，4。 12入栈时，栈顶元素4比12小，4出栈，此时栈顶元素为7，仍比12小，栈顶元素7继续出栈，此时栈顶元素为10，仍比12小，10出栈，此时栈为空，12入栈，栈内元素为12。 伪代码 123456789101112131415161718stack&lt;int&gt; st;//单调递增栈for (遍历这个数组){ if (栈空 || 栈顶元素大于等于当前比较元素) { 入栈; } else { while (栈不为空 &amp;&amp; 栈顶元素小于当前元素) { 栈顶元素出栈; 更新结果; } 当前数据入栈; }} 应用： 主要用于$O(n)$ 解决NGE问题（Next Greater Element） 比当前元素更大的下一个元素 比当前元素更大的前一个元素 比当前元素更小的下一个元素 比当前元素更小的前一个元素 参考https://blog.csdn.net/lucky52529/article/details/89155694","link":"/2021/09/24/monotonous-stack/"},{"title":"多标签","text":"1 分类标签 方法 1 转成多个2分类 2 直接多标签分类 loss： 有个疑问，直接softmax+交叉熵不行吗？？？ https://zhuanlan.zhihu.com/p/385475273 https://zhuanlan.zhihu.com/p/138117543 1 原来的交叉熵 m为样本总和，q为类别数量 2 “softmax+交叉熵”推广 评价指标 https://zhuanlan.zhihu.com/p/385475273 2 文本翻译一句英文输入，有多个版本的中文翻译，这种一对多怎么训练？？？？ 参考https://zhuanlan.zhihu.com/p/138117543 https://www.jianshu.com/p/ac3bec3dde3e","link":"/2022/06/08/multi-label/"},{"title":"多流转换","text":"简单划分的话，多流转换可以分为“分流”和“合流”两大类。在 Flink中，分流操作可以通过处理函数的侧输出流（ side output）很容易地实现而合流则提供不同层级的各种 API 任务： stream数量 ，每个stream可以有多个子任务（并行度） keyby只能算分组，不算分流 资源： task manager数量，slot数量 1 分流所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，得到完全平等的多个子 DataStream，如图 8-1所示。一般来说，我们会定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。 在Flink 1.13版本中，已经弃用了 .split()方法，取而代之的是直接用处理函数（ process function）的侧输出流 （side output）。 2 合流既然一条流可以分开，自然多条流就可以合并。在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以 Flink中合流的操作会更加普遍，对应的API也更加丰富。 2.1 基本合流1 联合（ Union） 可以多条（大于2）合并，数据类型必须一致 2 连接（ Connect） 必须两条，数据类型可以不同 2.2 双流联结 join对于两条流的合并，很多情况我们并不是简单地将所有数据放在一起，而是希望根据某个字段的值将它们联结起来，“配对”去做处理。 1 窗口联结（ Window Join 2 间隔联结（ Interval Join 3 窗口同组联结（ Window CoGroup","link":"/2022/03/25/multi-stream-flink/"},{"title":"多任务学习","text":"好处1 减少模型数量，相似任务可以共享 2 可以提升单一任务的效果 微软的MT-DNN[33]已经证明基于预训练模型的多任务Fine-tuning可以提升各项子任务效果 结构 loss设计loss是重点 除了各个子任务的损失直接相加，还有别的方式吗 辅助任务怎么设计 参考https://zhuanlan.zhihu.com/p/397196665 https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html","link":"/2022/06/11/multi-task-learning/"},{"title":"mysql","text":"开机自启配置Binlogflinkcdc需要用 12345678910111213141 sudo vim /etc/my.cnf2 写入server id = 1 log-bin=mysql-bin binlog_format=row binlog-do-db=gmall2021binlog-do-db=gmall2021_realtime注意：binlog-do-db 根据自己的情况进行修 改，指定具体要同步的数据库，多个就写多行3 重启mysqlsudo systemctl restart mysqld4 查看有没有生效 cd /var/lib sudo ls -l mysql | grep bin 修改mysql库里数据，观察&quot;mysql-bin.最新&quot;文件大小的变化 问题1 Job for mysqld.service failed because the control process exited with error code https://blog.csdn.net/qq_41179691/article/details/104598293 2 navicat连不上服务器的mysql https://blog.csdn.net/u014264373/article/details/85564524 https://blog.csdn.net/MTbaby/article/details/56836986","link":"/2022/05/21/mysql/"},{"title":"分类算法之朴素贝叶斯","text":"https://www.cnblogs.com/pinard/p/6069267.html","link":"/2021/11/03/naive-bayse/"},{"title":"ip和端口","text":"https://blog.csdn.net/weixin_33950757/article/details/90441617 IP地址（家庭地址）： 例如：218.18.170.149；理解为：广东省.深圳市.龙岗区.电信（桥头东路二道巷149号）； 端口后（门牌号）： 例如：218.18.170.149:1011，端口为（1011）号；意思就是我家有很多房间，其中的一个房间为1011号； 禁止端口：禁止任何人来打开我的1011号房间； 端口登陆：1011号房间的门是加密的防盗门，你必须输入用户名和密码你才能进入1011号房间","link":"/2022/03/01/netport/"},{"title":"nginx","text":"0 概念正向代理：代理的是客户端 反向代理：代理的是服务器端 概念 1 作用https://zhuanlan.zhihu.com/p/54793789 1、静态HTTP服务器 2、反向代理服务器 3、负载均衡 负载均衡通常是指将请求”均匀”分摊到集群中多个服务器节点上执行 4、虚拟主机 5、FastCGI 2 问题1 nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) 12lsof -i :80 |grep nginx |grep -v grep|awk '{print $2}'kill -9 XXX XXX https://blog.csdn.net/fmwind/article/details/120786375","link":"/2022/05/15/nginxx/"},{"title":"nlpcda-NLP中文数据增强工具，强推","text":"下载：pip install nlpcda 工具支持 1.随机实体替换 2.近义词 3.近义近音字替换 4.随机字删除（内部细节：数字时间日期片段，内容不会删） 5.NER类 BIO 数据增强 6.随机置换邻近的字：研表究明，汉字序顺并不定一影响文字的阅读理解&lt;&lt;是乱序的 7.中文等价字替换（1 一 壹 ①，2 二 贰 ②） 8.翻译互转实现的增强 9.使用simbert做生成式相似句生成 10.Cluster2Cluster生成更多样化的新数据","link":"/2021/07/27/nlp-data-augment/"},{"title":"nlp教材","text":"《embeddings in natural language processing》 http://josecamachocollados.com/book_embNLP_draft.pdf 《Speech and Language Processing》 https://web.stanford.edu/~jurafsky/slp3/","link":"/2022/04/06/nlp-reference/"},{"title":"常见算法总结","text":"类别归类：蛮力，分治，动态规划，贪心，回溯，分支限界。 实现方式有两种，一般为基于迭代和基于递归。 优化原则：无非时间换空间或者空间换时间 一.蛮力Brute-force没有什么好说，就是遍历或者枚举。 二.分治分而治之，将大问题拆解成相互独立且相似的子问题。 步骤：1、先分 2 求解 3 .合并 实现方式：一般递归实现 三.动态规划和暴力相比，dp利用了子问题间的依赖关系，减少了一些计算 适用条件：1.满足最优子结构性质，即最优解所包含的子问题的解也是最优的 构造递推式： 0.确定维度，一般一维或者二维 1.注意含义 2.选择分隔点 ​ a. 一般和前一步或者两步有关,比如最大子序和，$dp[i]=max(s[i],s[i]+dp[i-1])$ ​ b. 但有时候需要遍历全部分割点，比如单词拆分，$dp[i]=dp[j] \\ \\&amp; \\&amp; \\ check(s[j..i−1])$， ​ c. 有时候动态选择分隔点，比如丑数，$ dp[i]=min(dp[p2]2,dp[p3]3,dp[p5]*5) $,其中$p2,p3,p5$动态变化 3.注意+， - +，-决定计算次序 一定是用已有的递推没有的 举例比如62. 不同路径，$dp[i][j] = dp[i-1][j] + dp[i][j-1]$； 123456789class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: dp = [[1]*n] + [[1]+[0] * (n-1) for _ in range(m-1)] #print(dp) for i in range(1, m): for j in range(1, n): dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[-1][-1] 比如5. 最长回文子串，$dp[i][j]=dp[i+1][j-1] \\ and \\ s[i]==s[j]$ s = “babad” n=5 （i，j） (0,1) (1,2) (2,3) (3,4) (0,2) (1,3) (2,4) (0,3) 12345678910111213141516171819202122232425262728293031323334353637class Solution: def longestPalindrome(self, s: str) -&gt; str: n = len(s) if n &lt; 2: return s max_len = 1 begin = 0 # dp[i][j] 表示 s[i..j] 是否是回文串 dp = [[False] * n for _ in range(n)] for i in range(n): dp[i][i] = True # 递推开始 # 先枚举子串长度 for L in range(2, n + 1): # 枚举左边界，左边界的上限设置可以宽松一些 for i in range(n): # 由 L 和 i 可以确定右边界，即 j - i + 1 = L 得 j = L + i - 1 # 如果右边界越界，就可以退出当前循环 if j &gt;= n: break if s[i] != s[j]: dp[i][j] = False else: if j - i &lt; 3: dp[i][j] = True else: dp[i][j] = dp[i + 1][j - 1] # 只要 dp[i][L] == true 成立，就表示子串 s[i..L] 是回文，此时记录回文长度和起始位置 if dp[i][j] and j - i + 1 &gt; max_len: max_len = j - i + 1 begin = i return s[begin:begin + max_len] 实现方式 递归实现：效率不高的原因在于子问题重复计算了，比起暴力哪个快？应该还是这个 迭代+备忘录：不一定全部都要存储，需要的存着了就可以了，每次子问题计算一次（空间换时间） 一般采用迭代+备忘录 解的追踪：有时候需要借助备忘录搜索解，时间复杂度不超过计算备忘录 状态压缩动态规划 https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b https://segmentfault.com/a/1190000038223084 https://blog.csdn.net/wxgaws/article/details/114693162 Usually, the state of DP can use limited variables to represent such a dp[i] , dp[i] [j] ,dp[i] [j] [k]. However, sometimes, the states of a DP problem may contain multiple statuses. In this case, we can think about using the state compression approaches to represent the DP state. 说白了就是在状态很多的时候对状态降维 目的：可以优化空间复杂度，不知道可以不可以优化时间复杂度 四.贪心短视（少考虑很多，计算量就下去了），局部最优，需要证明 五.回溯回溯算法是暴力求解的一种，它能系统地搜索一个问题的所有解，区别在于回溯会回头（减枝），以此减少计算量 三步一回头，一般用于可以用树型结构建模的问题。 实现方式：利用DFS搜索解空间，一般递归实现 https://zhuanlan.zhihu.com/p/93530380 https://zhuanlan.zhihu.com/p/112926891 优化策略： 剪枝 https://blog.csdn.net/arabic1666/article/details/80147606 六.分支限界代价函数 极大值问题，以当前结点为根的子树所有可行解的值的上界，每个结点都有自己的 界 极大值问题，当前得到的可行解的值的最大值，就1个 代价函数和界可以用于减枝，对于极大值问题，界大于某结点代价函数，该结点就不需要继续搜索了 类似回溯，区别如下 参考文献https://blog.csdn.net/zm1_1zm/article/details/69224626 https://blog.csdn.net/wxgaws/article/details/114693162 https://segmentfault.com/a/1190000038223084 https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b","link":"/2021/07/24/normal-algrithm/"},{"title":"nvidia-smi 查看GPU使用率很高但是看不到进程","text":"https://blog.csdn.net/gostman/article/details/107456597","link":"/2022/01/19/nvidia/"},{"title":"OLAP和OLTP的区别","text":"https://www.cnblogs.com/schoolbag/p/9759214.html","link":"/2022/02/09/olap-oltp/"},{"title":"代价函数，损失函数，目标函数区别","text":"https://blog.csdn.net/lyl771857509/article/details/79428475","link":"/2021/11/04/object-func/"},{"title":"oov怎么解决","text":"oov：Out-Of-Vocabulary 中文：采用字粒度 英文：subword","link":"/2022/06/07/oov/"},{"title":"open set recognization","text":"1 A Survey on Open Set Recognition https://arxiv.org/abs/2109.00893 2 Open-Set Recognition: A Good Closed-Set Classifier is All You Need https://arxiv.org/abs/2110.06207 3 Recent Advances in Open Set Recognition: A Survey https://arxiv.org/abs/1811.08581","link":"/2022/03/29/open-set-recognization/"},{"title":"优化算法","text":"mark https://www.cnblogs.com/zingp/p/11352012.html#_label8 https://www.jianshu.com/p/71f39c2ea512","link":"/2021/10/14/optimize-algorithm/"},{"title":"过拟合，欠拟合以及解决办法","text":"1.偏差和方差 \\overline{f}(\\textbf{x})=\\mathbb{E}_D[f(\\textbf{x};D)]a.偏差 期望输出与真实标记的差别称为偏差（bias），即 bias^{2}(\\textbf{x})=(\\overline{f}(\\textbf{x})-y)^{2}b.方差 var(\\textbf{x})=\\mathbb{E}_D[(f(\\textbf{x};D)-\\overline{f}(x))^2]c.噪声 \\xi^{2}=\\mathbb{E}_D[(y_D-y)^2]d.泛化误差（error） \\begin{align*} error&=\\mathbb{E}_D[(f(\\textbf{x};D)-y_D)^2] \\\\&=... \\\\&=(\\overline{f}(\\textbf{x})-y)^{2}+\\mathbb{E}_D[(f(\\textbf{x};D)-\\overline{f}(x))^2]+\\mathbb{E}_D[(y_D-y)^2] \\\\&=bias^{2}(\\textbf{x})+var(\\textbf{x})+\\xi^{2} \\end{align*}2.过拟合、欠拟合与偏差、方差的关系 欠拟合：模型不能适配训练样本，有一个很大的偏差。 过拟合：模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。 3.如何解决过拟合和欠拟合a.模型能力（一个模型参数数量不同，不同模型） b.正则化 正则化参数出现的目的其实是防止过拟合情形的出现；如果我们的模型已经出现了欠拟合的情形，就可以通过减少正则化参数来消除欠拟合 c.特征数量 欠拟合：增加特征项 过拟合：减少特征项 d、训练的数据量 欠拟合：减少数据量 过拟合：增加数据量 参考https://zhuanlan.zhihu.com/p/38853908 https://blog.csdn.net/hurry0808/article/details/78148756 https://blog.csdn.net/cltcj/article/details/119155683","link":"/2021/09/04/overfit-underfit/"},{"title":"pairwise","text":"pairwise learning to rank 的方法可以分为两大类。 第一类是基于打分函数，它们通过一些特殊的设计让模型依靠“样本对”的信息来学习得到每个样本的score。所以得到这类方法最后的全局排序结果很简单，就是用所有样本的score来排序即可。 另一类方法是基于优先函数的方法。这类方法的整个过程分为两个阶段，第一阶段是用机器学习模型来学习两个样本之间的优先关系，例如f(x1, x2)=1表示样本x1优先于x2（x1应该排在x2前面），f(x1, x2)=-1表示样本x2优先于x1（x1应该排在x2后面）。从题主的问题来看，可能问的是“当我们已经训练出了优先函数f之后，如何对所有样本进行排序，并且使该排序在最大程度上与f的结果一致”。这个问题在学界被称为Rank Aggregation（排列聚合）。 具体参考 https://www.zhihu.com/question/389068269 别的相关参考： https://www.jianshu.com/p/235756fbf6b6 https://zhuanlan.zhihu.com/p/318300682 https://zhuanlan.zhihu.com/p/65224450 https://zhuanlan.zhihu.com/p/65756030 https://www.zhihu.com/question/389068269/answer/1180120736","link":"/2021/12/20/pairwise/"},{"title":"感知机","text":"","link":"/2022/08/20/perceptron/"},{"title":"持久化","text":"https://cloud.tencent.com/developer/article/1760389 https://blog.csdn.net/dudadudadd/article/details/114102341 https://yiqingqing.blog.csdn.net/article/details/121772325 https://blog.csdn.net/feizuiku0116/article/details/122839247 https://blog.csdn.net/CyAurora/article/details/119654676 https://www.cnblogs.com/Transkai/p/11347224.html https://blog.csdn.net/CyAurora/article/details/119654676 https://blog.csdn.net/dudadudadd/article/details/114102341 1 缓存懒执行 空间换时间 rdd3如果不消失，那么绿色链路就不用执行两次 持久化的目标就是将rdd3保存到内存或者磁盘 但是有丢失风险，比如硬盘损坏，内存被清理等，所以为了规避风险，会保留rdd的血缘（依赖）关系 如何保存： 1 persist 2 cachehttps://blog.csdn.net/donger__chen/article/details/86366339 底层调用persist，persist的特殊情况，persist(MEMORY_ONLY) 2 checkpoint特殊的持久化 仅支持硬盘 设计上认为安全没有风险，所以不需要保留血缘关系 如何保存： 3 对比","link":"/2022/03/04/persist/"},{"title":"物理分区","text":"https://www.cnblogs.com/wdh01/p/16038278.html 首先和逻辑分区区别开，逻辑分区包括keyBy等算子 逻辑分区只不过将数据按照key分组，哪个key分到哪个task，系统自动控制，万一分配不均，会发生数据倾斜 物理分区就是按一定逻辑将数据分配到不同Task，可以缓解数据倾斜 source（1）-》不同物理分区方式（3）-》slot 分类 1 随机分区 random 2 轮询分区round-robin 3 重缩放分区 rescale 4 分局分区 global 5 自定义 custom 6 广播 不完全算物理分区方式","link":"/2022/05/09/phisical-partition/"},{"title":"phoenix","text":"在hbase上构建SQL层，使得hbase 能够使用标准SQL管理数据，Phoenix中的sql语句还是有些不同的 安装配置https://www.hangge.com/blog/cache/detail_2980.html 启动使用python2，python2 sqlline.py 问题1 KeeperErrorCode = NoNode for /hbase 或者 Retrieve cluster id failed 参考https://www.hangge.com/blog/cache/detail_2980.html","link":"/2022/05/22/phoenix/"},{"title":"pointwise vs pairwise","text":"pairwise算法聚焦于精确的预测每个文档之间的相关度，pairwise算法主要关心两个文档之间的顺序，相比pointwise的算法更加接近于排序的概念。","link":"/2022/01/17/pointwise-pairwise/"},{"title":"pom","text":"POM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。 执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。","link":"/2022/03/14/pom/"},{"title":"pointwise","text":"https://zhuanlan.zhihu.com/p/113302654#","link":"/2021/12/20/pointwise/"},{"title":"postman","text":"作用后端服务启动后，需要前端请求才会有反馈，真实场景是需要前端点击触发，为了简化，利用postman模拟前端的请求 真实前端/postman 《—》后端 使用服务器 1 后端启应用 postman 2 填写后端请求地址 3 发送数据 body raw json {。。。。} 问题1 setting 设置 ssl certificate verification 为off","link":"/2022/06/08/postman/"},{"title":"预训练任务","text":"分类 TLM : Translation Language Modeling DAE: Denoising Autoencoder CTL: Contrastive Learning RTD： Replaced Token Detection SOP：Sentence Order Prediction DIM：Deep InfoMAx 参考https://arxiv.org/pdf/2003.08271v4.pdf https://zhuanlan.zhihu.com/p/360892229","link":"/2022/05/29/pretrain-task/"},{"title":"粗排","text":"","link":"/2022/07/15/pre-ranking/"},{"title":"pretrain","text":"https://huggingface.co/docs/transformers/task_summary Language Modeling https://huggingface.co/blog/how-to-train","link":"/2022/01/24/pretrain/"},{"title":"Prompt-learning小帮手-openprompt","text":"清华NLP实验室推出OpenPrompt开源工具包 1 结构 2 教程可以参考官方https://hub.fastgit.xyz/thunlp/OpenPrompt 有详细的步骤和case 参考https://hub.fastgit.xyz/thunlp/OpenPrompt https://zhuanlan.zhihu.com/p/420335724 https://github.com/thunlp/OpenPrompt","link":"/2022/01/17/prompt-assitaant/"},{"title":"先验概率与后验概率","text":"$P(X=玩 lol)=0.6；P(X=不玩lol)=0.4$，这个概率是统计得到的,或者你自身依据经验给出的一个概率值，我们称其为先验概率(prior probability)； $P(X=玩lol|Y=男性)$称之为$X$的后验概率，即它获得是在观察到事件$Y=男性$发生后得到的 参考https://zhuanlan.zhihu.com/p/26464206","link":"/2021/08/29/prior-Posterior/"},{"title":"prompt trick","text":"目的通过模板使得预测任务与预训练模型的训练任务相统一，拉近预训练任务目标与下游微调目标的差距 和finetune差异finetune：PTM向下兼容specific task prompt：specific task向上兼容PTM 应用场景由于其当前预测任务与预训练模型的训练任务相统一，所以我们可以在训练数据较少，甚至没有的情况下去完成当前任务，总结一下，其比较适合的应用场景： zero-shot few-shot 冷启动 参考https://zhuanlan.zhihu.com/p/424888379 https://zhuanlan.zhihu.com/p/440169921","link":"/2022/06/27/prompt-trick/"},{"title":"ptm之间的联系","text":"","link":"/2022/03/28/ptm-relation/"},{"title":"pyspark依赖","text":"PYSPARK_PYTHON= PYSPARK_DRIVER_PYTHON= JAVA_HOME = /usr/local/jdk1.8.0_11 HADOOP_CONF_DIR=/cloud/dahua/spark-2.4.4-binhadoop2.7/conf SPARK_HOME=/usr/local/spark-2.4.4-bin-hadoop2.7 SCALA_HOME=/usr/local/scala-2.11.8","link":"/2022/03/07/pyspark-dependency/"},{"title":"Pre-trained Models for Natural Language Processing A Survey","text":"原文内容很丰富，慢慢学习更新。 摘要这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。 1.Introduction随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，GNN以及attention。 尽管NLP任务的取得很大成功，但是和CV比较，性能提高可能不是非常明显。这主要是因为NLP任务的数据集都非常小（除了机器翻译），然而深度网络参数非常多，没有足够的数据支撑网络训练会导致过拟合问题。 最近，大量工作表明，预先训练的模型（PTMs），在大型语料库上可以学习通用语言表示，这有利于下游NLP任务可以避免从零开始训练新模型。随着算力的发展，深度模型（例如，transformer）的出现和训练技巧的不断调高，PTM的结构从浅层发展成深层。第一代PTM被用于Non-contextual word Embedding。由于下游任务不需要这些模型本身，只需要训练好的词向量矩阵，因此对于现在的算力，这些模型非常浅层，比如Skip-Gram和GloVe。虽然这些预训练词向量可以捕获词语的语义，但它们不受上下文限制，无法捕获上下文中的高级含义，某些任务会失效，例如多义词，句法结构，语义角色、回指。第二代PTM关注Contextual word embeddings，比如BERT，GPT等。这些编码器任然需要通过下游任务在上下文中表示词语。 2.Background2.1 Language Representation LearningThe core idea of distributed representation is to describe the meaning of a piece of text by low-dimensional real-valued vectors. And each dimension of the vector has no corresponding sense, while the whole represents a concrete concept. Non-contextual Embeddings 这一步主要是将分割的字符，比如图中的$x$，变成向量表达$e_x \\in \\mathbb{R}^{D_e}$，$D_e$是词向量维度。向量化过程就是基于一个离线训练的词向量矩阵$E\\in \\mathbb{R}^{D_e\\times |\\mathcal{V}|} $做查找，$\\mathcal{V}$是词汇表。 这个过程主要有两个问题。第一个是这个词向量是静态的，没有考虑上下文含义，无法处理多义词。第二个是oov问题，许多算法可以缓解这个问题，比如基于character level，比如基于subword，subword算法有BPE，CharCNN等。 Contextual Embeddings To address the issue of polysemous and the context-dependent nature of words, we need distinguish the semantics of words in different contexts： [\\textbf{h}_1,\\textbf{h}_2,...,\\textbf{h}_T]=f_{enc}(x_1,x_2,...,x_T)其中$f_{enc}(\\cdot)$为深度编码器。$\\textbf{h}_t$就是contextual embedding或者dynamical embedding。 2.2 Neural Contextual Encoders 可以分成两类，sequence models and non-sequence models。 2.2.1 sequence modelssequence models 分为两类，Convolutional Models和Recurrent Models，见上图。 Convolutional Convolutional models take the embeddings of words in the input sentence and capture the meaning of a word by aggregating the local information from its neighbors by convolution operations Recurrent Recurrent models capture the contextual representations of words with short memory, such as LSTMs and GRUs . In practice, bi-directional LSTMs or GRUs are used to collect information from both sides of a word, but its performance is often affected by the long-term dependency problem. 2.2.2 non-sequence modelstransformer： model the relation of every two words 2.2.3 AnalysisSequence models： 1.Sequence models learn the contextual representation of the word with locality bias and are hard to capture the long-range interactions between words. 2.Nevertheless, sequence models are usually easy to train and get good results for various NLP tasks. fully-connected self-attention model： 1.can directly model the dependency between every two words in a sequence, which is more powerful and suitable to model long range dependency of language 2.However, due to its heavy structure and less model bias, the Transformer usually requires a large training corpus and is easy to overfit on small or modestly-sized datasets 结论：the Transformer has become the mainstream architecture of PTMs due to its powerful capacity. 2.3 Why Pre-training? Pre-training on the huge text corpus can learn universal language representations and help with the downstream tasks. Pre-training provides a better model initialization,which usually leads to a better generalization performance and speeds up convergence on the target task. Pre-training can be regarded as a kind of regularization to avoid overfitting on small data 3 Overview of PTMs3.1 Pre-training Tasks预训练任务对于学习通用语言表示至关重要。通常，这些预训练任务应具有挑战性，并拥有大量训练数据。在本节中，我们将预训练任务分成三个类别：Supervised learning、Unsupervised learning和Self-Supervised learning。 Self-Supervised learning： is a blend of supervised learning and unsupervised learning. The learning paradigm of SSL is entirely the same as supervised learning, but the labels of training data are generated automatically. The key idea of SSL is to predict any part of the input from other parts in some form. For example, the masked language model (MLM) is a self-supervised task that attempts to predict the masked words in a sentence given the rest words. 接下来基于介绍常用的基于Self-Supervised learning的预训练任务。 3.1.1 Language Modeling (LM)3.1.2 Masked Language Modeling (MLM)3.1.3 Permuted Language Modeling (PLM)3.1.4 Denoising Autoencoder (DAE)3.1.5 Contrastive Learning (CTL)nsp也属于CTL https://zhuanlan.zhihu.com/p/360892229 3.1.6 Others3.2 Taxonomy of PTMs 作者从以下四个角度，即Representation Type，Architectures，Pre-Training Task Types，Extensions，对现有的PTM分类，分类结果如上。图和这里有一点不统一，是作者没注意？图里有5个类别，多了Tuning Strategies，而且Representation Type在图中为Contextual?。 3.3 Model Analysis4 Extensions of PTMs4.1 Knowledge-Enriched PTMs4.2 Multilingual and Language-Specific PTMs4.3 Multi-Modal PTMs4.4 Domain-Specific and Task-Specific PTMs4.5 Model Compression5 Adapting PTMs to Downstream Tasks虽然PTM学习了很多通用知识，但是如何将这些知识有效应用到下游任务是个挑战。 5.1 Transfer LearningTransfer learning is to adapt the knowledge from a source task (or domain) to a target task (or domain).如下图。 5.2 How to Transfer?5.2.1 Choosing appropriate pre-training task, model architecture and corpus5.2.2 Choosing appropriate layers使用哪些层参与下游任务 选择的层model1+下游任务model2 对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。 用$\\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\\cdot)$为特定的任务模型。有以下几种方法选择representation: a) Embedding Only choose only the pre-trained static embeddings，即$g(\\textbf{H}^{1})$ b) Top Layer 选择顶层的representation，然后接入特定的任务模型，即$g(\\textbf{H}^{L})$ c) All Layers 输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下 g(\\textbf{r}_t)=g(\\gamma \\sum_{l=1}^{L}\\alpha_l\\textbf{H}^{(l)})其中$\\alpha$ is the softmax-normalized weight for layer $l$ and $\\gamma$ is a scalar to scale the vectors output by pre-trained model 5.2.3 To tune or not to tune?总共有两种常用的模型迁移方式：feature extraction (where the pre-trained parameters are frozen), and fine-tuning (where the pre-trained parameters are unfrozen and fine-tuned). 选择的层model1参数是否固定，model2一定要训练 bert 只有top layer finetune？？？？ 5.3 Fine-Tuning StrategiesTwo-stage fine-tuning 第一阶段为中间任务，第二阶段为目标任务 Multi-task fine-tuning multi-task learning and pre-training are complementary technologies. Fine-tuning with extra adaptation modules The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed. Others self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing 参考https://arxiv.org/pdf/2003.08271v4.pdf","link":"/2021/08/10/ptm-survey/"},{"title":"pyspark","text":"PySpark宗旨是在不破坏Spark已有的运行时架构，在Spark架构外层包装一层Python API，借助Py4j实现Python和Java的交互，进而实现通过Python编写Spark应用程序，其运行时架构如下图所示。","link":"/2022/03/02/pyspark/"},{"title":"python bif(内置函数)","text":"1 ord（） 如何获取ascii码，使用内置函数ord（） 1234ord(&quot;A&quot;)65ord(&quot;a&quot;)97 804. 唯一摩尔斯密码词2 sorted() https://www.runoob.com/python3/python3-func-sorted.html 1234sorted([-3,4,2],key=abs)Out[5]: [2, -3, 4]sorted([-3,4,2])Out[6]: [-3, 2, 4]","link":"/2022/04/10/python-bif/"},{"title":"运算","text":"1. 逻辑运算 operation result x or y if x is false，then y ,else x x and y if x is false，then x ,else y not x if x is false，then True , else False","link":"/2022/09/12/python-cal-symbol/"},{"title":"python不可变对象和可变对象","text":"https://zhuanlan.zhihu.com/p/34395671 可变对象：list dict set 不可变对象：tuple string int float bool","link":"/2022/09/12/python-changed_obj/"},{"title":"collections","text":"1 Counter 1234from collections import CounterCounter([-3,4,2,2])Out[8]: Counter({-3: 1, 4: 1, 2: 2})","link":"/2022/04/01/python-counter/"},{"title":"python环境","text":"1.环境管理工具python版本主要为2,3两个大版本 anaconda管理python和包的版本 2.包2.1 下载包下载源有官方源，阿里源，豆瓣源，清华源等 1 离线下载2 在线下载下载工具有pip，conda 更换pip源 修改文件 1vim ~/.pip/pip.conf # 没有就创建一个，在 ~/.pip/下 增加内容 12345[global]index-url=https://pypi.tuna.tsinghua.edu.cn/simple# index-url=http://pypi.douban.com/simple/[install]trusted-host=pypi.tuna.tsinghua.edu.cn 查看路径 which python, which pip ,which conda （~用户主目录， /根目录） 2.2 使用包import 绝对路径：从工程的最外层开始 相对路径：利用.（同级）和..（上级） 怎么添加包的搜路径 https://blog.csdn.net/weixin_40449300/article/details/79327201 3 ubuntu修改python环境变量1.vim ~/.bashrc 2.添加如下内容 export PYTHON_HOME=/usr/local/anaconda3/bin export PATH=$PYTHON_HOME/bin:$PATH export PATH=/home/user_name/anaconda3/bin:$PATH # 指定python路径 4.ubuntu修改pyton默认版本https://blog.csdn.net/White_Idiot/article/details/78240298","link":"/2021/09/10/python-env/"},{"title":"可迭代对象、迭代器与生成器","text":"可迭代对象对象+ iter () 123class IsIterable: def __iter__(self): pass 迭代器对象+ iter ()+next() 12345678910111213141516171819202122232425262728&gt;&gt;&gt; class MyIterator: &quot;&quot;&quot; 迭代器类 Author：可乐python说 &quot;&quot;&quot; def __init__(self): self.num = 0 def __iter__(self): return self def __next__(self): return_num = self.num # 只要值大于等于6，就停止迭代 if return_num &gt;= 6: raise StopIteration self.num += 2 return return_num&gt;&gt;&gt; my_iterator = MyIterator()&gt;&gt;&gt; next(my_iterator)0&gt;&gt;&gt; next(my_iterator)2&gt;&gt;&gt; next(my_iterator)4&gt;&gt;&gt; next(my_iterator)Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;StopIteration 生成器生成器函数定义与常规函数相同，区别在于，它使用 yield 语句 而不是 return 语句 12345678910111213141516171819202122&gt;&gt;&gt; def my_generator(): my_num = 0 while my_num &lt; 5: yield my_num my_num += 1&gt;&gt;&gt;generator_ = my_generator()&gt;&gt;&gt; next(generator_)0&gt;&gt;&gt; next(generator_)1&gt;&gt;&gt; next(generator_)2&gt;&gt;&gt; next(generator_)3&gt;&gt;&gt; next(generator_)4&gt;&gt;&gt; next(generator_)# 终止迭代则会抛出 StopIteration 异常Traceback (most recent call last): File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;StopIteration 生成器表达式使用小括号 - () 包裹，而不是中括号 1234567891011import sysmy_list = [i for i in range(1000000)]print(&quot;列表消耗的内存：{}&quot;.format(sys.getsizeof(my_list)))my_generator = (i for i in range(1000000))print(&quot;生成器消耗的内存：{}&quot;.format(sys.getsizeof(my_generator)))列表消耗的内存：8448728生成器消耗的内存：112 参考https://www.runoob.com/python3/python3-iterator-generator.html https://kelepython.readthedocs.io/zh/latest/c01/c01_11.html","link":"/2022/09/12/python-iterator/"},{"title":"函数参数","text":"1 默认参数https://blog.csdn.net/weixin_41972881/article/details/81562731 https://blog.csdn.net/weixin_45775963/article/details/103696945 123456def fun(va1,va2=[]): print(va2) va2.append(va1) return va2te1=fun(10)te1=fun(20) va2如果没有传参，采用默认的，默认的会变化，不是一直是[] va2如果是外部的传参，以传参为主，会覆盖","link":"/2021/12/15/python-paramerter/"},{"title":"正则","text":"1 re.match1matchObj/None = re.match(pattern, string, flags=0) 2 re.search1matchObj/None = re.search(pattern, string, flags=0)","link":"/2022/09/12/python-pattern/"},{"title":"python在Ubuntu系统下的调试工具pdb","text":"https://blog.csdn.net/lemonaha/article/details/71305344 两种方式： 1.侵入式 不改代码 python -m pdb XX.py 2.非倾入式 加代码 import pdb pdb.set_trace()","link":"/2022/03/18/python-pdb/"},{"title":"编程规范","text":"1 命名规范1.小写+_lower_with_fun 包，模块，函数，方法，函数参数，变量 2.首字母大写CapWords 异常处理 3.全大写+_CAPS_WITH_UNDER 常量 4._开头类私有成员，外部仍可访问 a._ _fun b.__ __fun 会被编译器自动改名，使用新名字外部也可以访问 2.比较Noneis is not bool不要==，is 直接 if XX 3.字典取值用a.get(key)，而不是a[key] 4.类型判断用isinstance(variable,type) 不用type（variable）","link":"/2022/09/12/python-program-std/"},{"title":"ide","text":"pycharm1 pycharm连接远程，本地无法查看源码，但是可以正常运行1 原因 由于远程的包没有同步到本地，导致无法在本地查看，但是代码运行在远程，服务器上有包，所以可以运行 2.解决 将包从远程同步到本地，一般情况重启pycharm会自动同步，或者重新安装","link":"/2021/07/25/python_ide/"},{"title":"字符串","text":"1 格式化https://blog.csdn.net/zjbyough/article/details/96466658 f-stringhttps://blog.csdn.net/sunxb10/article/details/81036693 %format","link":"/2022/09/12/python-string/"},{"title":"魔法方法","text":"getitemhttps://zhuanlan.zhihu.com/p/27661382 all","link":"/2022/01/13/python_magic/"},{"title":"继承","text":"1 super子类把父类的 __init__()放到自己的 __init__() 当中 , 这样子类就有了父类的 __init__() 的那些东西 12345678910111213141516171819202122232425262728293031323334class test1: def __init__(self): self.a=1class test2(test1): def __init__(self): super(test2, self).__init__() self.b=2tt=test2()# print(tt.a)print(tt.b)print(tt.a)21############################class test1: def __init__(self): self.a=1class test2(test1): def __init__(self): # super(test2, self).__init__() self.b=2tt=test2()# print(tt.a)print(tt.b)print(tt.a)2AttributeError: 'test2' object has no attribute 'a' 12345class pointwise_hybird_contrasive(hybird): def __init__(self,config_roberta, path,num): super(pointwise_hybird_contrasive, self).__init__(config_roberta, path,num)super(pointwise_hybird_contrasive, self).\\__init\\__(config_roberta, path,num)就是对父类hybird的属性进行初始化 参考https://blog.csdn.net/zyh19980527/article/details/107206483","link":"/2021/12/09/python_succed/"},{"title":"pytorch常见问题","text":"1 Gather function not implemented for CPU tensors多卡训练时候，net的forward里面存在Tensor变成其它类型的操作，比如变成numpy，list 解决：改成Tensor操作 2 RuntimeError: element 0 of tensors does not require grad and does not have a grad_fnhttps://blog.csdn.net/weixin_41990278/article/details/90311313 https://blog.csdn.net/wu_xin1/article/details/116502378","link":"/2022/04/12/pytorch-problem/"},{"title":"query理解","text":"https://zhuanlan.zhihu.com/p/112719984 https://zhuanlan.zhihu.com/p/383733052 https://zhuanlan.zhihu.com/p/344631739","link":"/2021/10/08/query-understanding/"},{"title":"Ranger","text":"架构 Ranager的核心是Web应用程序，也称为RangerAdmin模块，此模块由管理策略，审计日志和报告等三部分组成。 管理员角色的用户可以通过RangerAdmin提供的web界面或REST APIS来定制安全策略。这些策略会由Ranger提供的轻量级的针对不同Hadoop体系中组件的插件来执行。插件会在Hadoop的不同组件的核心进程启动后，启动对应的插件进程来进行安全管理！ Ranger对Hive进行权限管理https://www.jianshu.com/p/d9941b8687b7","link":"/2022/02/07/ranger/"},{"title":"A Deep Look into Neural Ranking Models for Information Retrieval","text":"https://par.nsf.gov/servlets/purl/10277191 3 A Unified Model Formulation So a generalized LTR problem is to find the optimal ranking function f ∗ by minimizing the loss function over some labeled dataset f 是ranking function，s是query，t是候选集，y is the label set where labels represent grades Without loss of generality, the ranking function f could be further abstracted by the following unified formulation ψ, ϕare representation functions which extract features from s and t respectively η is the interaction function which extracts features from (s, t) pair, and g is the evaluation function which computes the relevance score based on the feature representations. 4. Model Architecture4.1. Symmetric vs. Asymmetric ArchitecturesSymmetric Architecture: The inputs s and t are assumed to be homogeneous, so that symmetric network structure could be applied over the inputs Asymmetric Architecture: The inputs s and t are assumed to be heterogeneous, so that asymmetric network structures should be applied over the inputs 4.2. Representation-focused vs. Interaction-focused Architectures Representation-focused Architecture: The underlying assumption of this type of architecture is that relevance depends on compositional meaning of the input texts. Therefore, models in this category usually define complex representation functions ϕ and ψ (i.e., deep neural networks), but no interaction function η Interaction-focused Architecture: The underlying assumption of this type of architecture is that relevance is in essence about the relation between the input texts, so it would be more effective to directly learn from interactions rather than from individual representations. Models in this category thus define the interaction function η rather than the representation functions ϕ and ψ Hybrid Architecture: In order to take advantage of both representation focused and interaction-focused architectures, a natural way is to adopt a hybrid architecture for feature learning. We find that there are two major hybrid strategies to integrate the two architectures, namely combined strategy and coupled strategy. 4.3. Single-granularity vs. Multi-granularity ArchitectureSingle-granularity Architecture: The underlying assumption of the single granularity architecture is that relevance can be evaluated based on the high level features extracted by ϕ, ψ and η from the single-form text inputs. Multi-granularity Architecture: The underlying assumption of the multigranularity architecture is that relevance estimation requires multiple granularities of features, either from different-level feature abstraction or based on different types of language units of the inputs 5. Model Learning5.1. Learning objectiveSimilar to other LTR algorithms, the learning objective of neural ranking models can be broadly categorized into three groups: pointwise, pairwise, and listwise. 5.1.1. Pointwise Ranking Objective1 loss The idea of pointwise ranking objectives is to simplify a ranking problem to a set of classification or regression problems a. Cross Entropy For example, one of the most popular pointwise loss functions used in neural ranking models is Cross Entropy: b. Mean Squared Error There are other pointwise loss functions such as Mean Squared Error for numerical labels, but they are more commonly used in recommendation tasks. 2 优缺点 a.advantages First, it simple and easy to scale. Second, the outputs have real meanings and value in practice. For instance, in sponsored search, a model learned with cross entropy loss and clickthrough rates can directly predict the probability of user clicks on search ads, which is more important than creating a good result list in some application scenarios. b.disadvantages less effective ，Because pointwise loss functions consider no document preference or order information, they do not guarantee to produce the best ranking list when the model loss reaches the global minimum. 5.1.2. Pairwise Ranking Objective1 loss Pairwise ranking objectives focus on optimizing the relative preferences between documents rather than their labels. a.Hinge loss b.cross entropy ​ RankNet 2 优缺点 a.advantages effective in many tasks b.disadvantages pairwise methods does not always lead to the improvement of final ranking metrics due to two reasons: (1) it is impossible to develop a ranking model that can correctly predict document preferences in all cases; and (2) in the computation of most existing ranking metrics, not all document pairs are equally important. 5.1.3. Listwise Ranking Objective1 loss listwise loss functions compute ranking loss with each query and their candidate document list together a. ListMLE https://blog.csdn.net/qq_36478718/article/details/122598406 b.Attention Rank function https://arxiv.org/abs/1804.05936 c. softmax-based listwise https://arxiv.org/pdf/1811.04415.pdf 2 优缺点 a.advantages While listwise ranking objectives are generally more effective than pairwise ranking objectives b.disadvantages their high computational cost often limits their applications. They are suitable for the re-ranking phase over a small set of candidate documents 5.1.4. Multi-task Learning Objective the optimization of neural ranking models may include the learning of multiple ranking or non-ranking objectives at the same time. 5.2. Training Strategies1 Supervised learning 2 Weakly supervised learning 3 Semi-supervised learning 6. Model Comparison比较了常见模型在不同应用的效果 1 Ad-hoc Retrieval https://blog.csdn.net/qq_44092699/article/details/106335971 Ad-hoc information retrieval refers to the task of returning information resources related to a user query formulated in natural language. 2 QA","link":"/2022/03/31/ranking-survey/"},{"title":"ranknet对比listnet","text":"The ListNet method grows on the bases of RankNet, they both employ the Cross Entropy function as a loss function and Gradient Descendant as algorithm to train a Neural Network Model. While the ListNet uses document list as instances, RankNet uses document pairs. We investigated why the listwise method ListNet can outperform the pairwise methods of RankNet, Ranking SVM, and RankBoost. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf 1.for the pairwise approach the number of document pairs varies largely from query to query 2.The pairwise approach actually employs a ‘pairwise’ loss function, which might be too loose as an approximation of the performance measures","link":"/2022/01/04/ranknet-listnet/"},{"title":"实时数仓案例（电商）","text":"0 架构 1 ods1 日志数据 前端（jar，产生日志数据）-》Nginx（集群间负载均衡）-》日志服务器（springboot，采集数据，jar）-》log，ods(kafka) 本地测试，本地起应用 -》 单机部署，单服务器起应用 -》 集群部署，集群起应用 2 业务数据 前端，jar，产生业务数据-》mysql,配置什么同步-》flinkcdc-》ods(kafka) 2 dim、dwd1 用户行为日志 ods(Kafka)-&gt; flink -&gt; dwd(kafka) 1 识别新老用户 业务需要 2 日志数据拆分 这3类日志，结构不同，写回Kafka不同主题 2 业务数据 ods(kafka) -&gt; flink -&gt; 1 维度数据，dim（HBASE） 2 事实数据 dwd(kafka) 1 ETL 过滤控制 2 动态分流 维度数据到hbase 事实数据到kafka 怎么分流？ ods的表里面哪些是维度表，哪些是事实表，需要提前知道表的分类信息，后面才可以分流。业务库的表会变化，表的分类信息实时更新，需要动态同步。这里将表的分类信息存在mysql，利用广播流发送。 3 dwmdmd（kafka）-&gt; flink -&gt; dwm（kafka） 1 访问uv计算 UV，unique visitor 2 跳出明细计算 跳出率=跳出次数 / 访问次数 3 订单主题表 4 支付主题表 4 dwsdwm（kafka）-&gt; flink -&gt; dws（clickhouse） 1 访客主题宽表 2 商品主题宽表 3 地区主题表 4 关键词主题表 5 ads","link":"/2022/04/14/real-datawarehouse-case/"},{"title":"分层结构","text":"DWMhttps://blog.csdn.net/jianghuaijie/article/details/122009653 作用 DWM层的定位是什么，DWM层主要服务DWS，因为部分需求直接从DWD层到DWS层中间会有一定的计算量，而且这部分计算的结果很有可能被多个DWS层主题复用 构建 分主题 dwt实时数仓没有dwt，因为dwt是累计统计，实时系统不适用 dws作用 轻度聚合，生成一系列的中间表，提升公共指标的复用性，减少重复加工 分主题，便于管理 构建 分主题 宽表 轻度聚合","link":"/2022/04/10/realware-multi-layer/"},{"title":"推荐之召回","text":"总结 https://blog.csdn.net/luanfenlian0992/article/details/107416438 https://zhuanlan.zhihu.com/p/364053939 好用的工具 https://hub.fastgit.org/shenweichen/DeepMatch","link":"/2021/10/25/recall-survey/"},{"title":"推荐系统评价指标","text":"https://zhuanlan.zhihu.com/p/67287992 http://sofasofa.io/forum_main_post.php?postid=1000292","link":"/2021/10/21/recommend-metrice/"},{"title":"推荐系统","text":"一般推荐系统的结构拆分为：召回-》粗排-》精排-》重排 大佬总结的干货： https://xieyangyi.blog.csdn.net/article/details/123095982 0 召回缩小规模，减小候选集，不需要十分准确，但不可遗漏 必须轻量快速低延迟 1 粗排兼顾精准性和低延迟 一般模型也不能过于复杂 2 精排要求准 多特征，复杂模型 3 重排业务相关 规则比较多 参考https://xieyangyi.blog.csdn.net/article/details/123095982 https://www.cnblogs.com/gczr/p/12564617.html","link":"/2021/08/30/recommmend-sys/"},{"title":"递归,迭代","text":"首先，迭代（有递推过程）区别于非递归 关系 迭代可以转换为递归，但递归不一定能转换为迭代 递归一定可以非递归表示 递归，迭代对比 空间复杂度：递归需要额外开销 时间复杂度： 参考https://www.jianshu.com/p/32bcc45efd32","link":"/2022/05/23/recusive-iteration/"},{"title":"递归","text":"自己调用自己 注意出口 123def function(): 1 最小子问题，出口，特解 2 通释","link":"/2022/05/31/recusive/"},{"title":"Use reduceByKey instead of groupByKey","text":"groupByKey creates a lot of shuffling which hampers the performance, while reduceByKey does not shuffle the data as much https://blog.csdn.net/qq_17685725/article/details/123033552","link":"/2022/03/04/reduceByKey/"},{"title":"正则化","text":"是机器学习中对原始损失函数引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。 1.L1正则（Lasso回归）L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。 \\begin{align*} L_{L1}(w)&=L(w)+\\lambda\\Vert w \\Vert_1=L(w)+\\lambda\\sum_{i=1}^{N}|w_i| \\\\ \\frac{\\partial L_{L1}}{\\partial w_i}&=\\frac{\\partial L}{\\partial w_i}+\\lambda \\ sgn(w_i) \\\\w_i &\\rightarrow w_i-\\eta(\\frac{\\partial L}{\\partial w_i}+\\lambda \\ sgn(w_i)) \\rightarrow w_i-\\eta\\lambda \\ sgn(w_i)-\\eta\\frac{\\partial L}{\\partial w_i} \\end{align*}L1是每次减去一个常数的收敛，所以L1更容易收敛到0。 2.L2正则（Ridge回归）L2正则化使得参数平滑。 \\begin{align*} L_{L2}(w)&=L(w)+\\lambda\\Vert w \\Vert_2^2=L(w)+\\lambda\\sum_{i=1}^{N}w_i^2 \\\\ \\frac{\\partial L_{L2}}{\\partial w_i}&=\\frac{\\partial L}{\\partial w_i}+2\\lambda w_i \\\\w_i& \\rightarrow w_i-\\eta(\\frac{\\partial L}{\\partial w_i}+2\\lambda w_i) \\rightarrow(1-2\\eta\\lambda)w_i-\\eta \\frac{\\partial L}{\\partial w_i} \\end{align*}L2是每次乘上一个小于1的倍数进行收敛，所以L2使得参数平滑。 3.dropout 使用：在训练时，每个神经单元以概率$p$被保留(Dropout丢弃率为$1−p$)；在预测阶段，每个神经单元都是存在的。 原理：神经网络通过Dropout层以一定比例随即的丢弃神经元，使得每次训练的网络模型都不相同，多个Epoch下来相当于训练了多个模型，同时每一个模型都参与了对最终结果的投票，从而提高了模型的泛化能力，类似bagging。 参考https://www.cnblogs.com/zingp/p/11631913.html https://blog.csdn.net/b876144622/article/details/81276818 https://www.zhihu.com/question/37096933/answer/70494622","link":"/2021/09/07/regulize/"},{"title":"实体关系抽取","text":"参考https://zhuanlan.zhihu.com/p/77868938","link":"/2022/06/11/relation-extract/"},{"title":"RNN总结","text":"1.单元1.1 普通RNN单元1.2 LSTM1.3 GRU2.结构1.1 输入、输出 1.2 是否双向 1.3 是否堆叠 参考https://blog.csdn.net/gaohanjie123/article/details/88699664 https://www.cnblogs.com/Luv-GEM/p/10788849.html","link":"/2021/09/15/rnn/"},{"title":"RoBERTa A Robustly Optimized BERT Pretraining Approach","text":"1.和BERT比较在结构上和原版BERT没有差异，主要的改动在于： 2.改动分析2.1 Static vs. Dynamic Maskingstatic masking: 原本的BERT采用的是static mask的方式，就是在create pretraining data中，先对数据进行提前的mask dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。 结果对比： 结论：动态占优 2.2 Model Input Format and Next Sentence Prediction做了结果对比试验，结果如下： 结论： Model Input Format: ​ 1.find that using individual sentences hurts performance on downstream tasks Next Sentence Prediction: ​ 1.removing the NSP loss matches or slightly improves downstream task performance 2.3 Training with large batches 2.4 Text Encoding采用BBPE而不是wordpiece 3 常见问题1 roberta tokenizer 没有token_type_ids？roberta 取消了NSP，所以不需要segment embedding 也就不需要token_type_ids，但是使用的时候发现中文是有token_type_ids的，英文没有token_type_ids的。没有token_type_ids，两句话怎么区别，分隔符sep还是有的，只是没有segment embedding 2 使用避坑 https://blog.csdn.net/zwqjoy/article/details/107533184 https://hub.fastgit.org/ymcui/Chinese-BERT-wwm 参考https://zhuanlan.zhihu.com/p/103205929 https://zhuanlan.zhihu.com/p/143064748 https://blog.csdn.net/zwqjoy/article/details/107533184 https://hub.fastgit.org/ymcui/Chinese-BERT-wwm","link":"/2021/09/15/roberta/"},{"title":"搜索系统","text":"综述 https://zhuanlan.zhihu.com/p/112719984 https://zhuanlan.zhihu.com/p/382001982 https://www.cnblogs.com/davidwang456/articles/10251599.html DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval https://arxiv.org/pdf/1710.05649.pdf","link":"/2021/08/06/search-rank-init/"},{"title":"IaaS、PaaS和SaaS","text":"IaaS： Infrastructure-as-a-Service（基础设施即服务） PaaS： Platform-as-a-Service（平台即服务） SaaS： Software-as-a-Service（软件即服务） https://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html","link":"/2022/03/01/saas/"},{"title":"Sentence-BERT Sentence Embeddings using Siamese BERT-Networks","text":"paper: https://arxiv.org/abs/1908.10084 giit: https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications 1.贡献基于bert利用孪生结构或者三胞胎结构训练，使得产生在低维空间可用的句子Embedding。对于文本匹配任务，可以离线计算句子Embedding，然后基于句子Embedding在线匹配，可实现快速高精度的匹配。 2.结构 文章提出三种结构和目标函数，三胞胎结构作者没有画图 1.Classification Objective Function loss=cross-entropy(softmax(W_t(u,v,|u-v|)),y_{true})2.Regression Objective Function loss=MSE(cosine-sim(u, v),y_{true})3.Triplet Objective Function loss=max(||s_a-s_p||-||s_a-s_n||+\\sigma,0)$||.||$计算向量距离，$s_a$为样本本身，$s_p$为正样本，$s_n$为负样本，$\\sigma$使得正样本至少比负样本距离样本近$\\sigma$。 对于pooling，文章提出三种策略 1.Using the output of the CLS-token2.computing the mean of all output vectors (MEAN_strategy)3.computing a max-over-time of the output vectors (MAX_strategy). The default configuration is MEAN. 3.实验结果3.1 Unsupervised STS 3.2 Supervised STS 3.3 Argument Facet Similarity 3.4 Wikipedia Sections DistinctionWe use the Triplet Objective 4.代码123456789101112131415161718192021222324252627282930313233from sentence_bert.sentence_transformers import SentenceTransformer, util###load modelmodel = SentenceTransformer(model_path)# Single list of sentencessentences = ['The cat sits outside', 'A man is playing guitar', 'I love pasta', 'The new movie is awesome', 'The cat plays in the garden', 'A woman watches TV', 'The new movie is so great', 'Do you like pizza?']#Compute embeddingsembeddings = model.encode(sentences, convert_to_tensor=True)#Compute cosine-similarities for each sentence with each other sentencecosine_scores = util.pytorch_cos_sim(embeddings, embeddings)#Find the pairs with the highest cosine similarity scorespairs = []for i in range(len(cosine_scores)-1): for j in range(i+1, len(cosine_scores)): pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})#Sort scores in decreasing orderpairs = sorted(pairs, key=lambda x: x['score'], reverse=True)for pair in pairs[0:10]: i, j = pair['index'] print(&quot;{} \\t\\t {} \\t\\t Score: {:.4f}&quot;.format(sentences[i], sentences[j], pair['score'])) 1234567891011The new movie is awesome The new movie is so great Score: 0.9283The cat sits outside The cat plays in the garden Score: 0.6855I love pasta Do you like pizza? Score: 0.5420I love pasta The new movie is awesome Score: 0.2629I love pasta The new movie is so great Score: 0.2268The new movie is awesome Do you like pizza? Score: 0.1885A man is playing guitar A woman watches TV Score: 0.1759The new movie is so great Do you like pizza? Score: 0.1615The cat plays in the garden A woman watches TV Score: 0.1521The cat sits outside The new movie is awesome Score: 0.1475","link":"/2021/07/27/sentence-bert/"},{"title":"attention seq2seq","text":"1.结构 左边为encoder，对输入文本编码，右边为decoder，解码并应用。 整个流程的图解可以参考https://blog.csdn.net/weixin_44388679/article/details/102575223 中的“四、图解Attention Seq2Seq”，非常详细。 2.Teacher Forcing在训练阶段，如果使用Teacher Forcing策略，那么目标句子单词的word embedding使用真值，不适用Teacher Forcing则为使用预测结果；至于预测阶段不能使用Teacher Forcing。 3.beam searchbeam search本质为介于蛮力与贪心之间的策略。对于贪心，每一级的输出只选择top1的结果作为下一级输入，然后top1的结果只是局部最优，不一定是全局最优，精度可能较低。对于蛮力，每级将全部结果输入下级，假设$L$为词表大小，那么最后一级的数据量为$L^{m}$，$m$为decoder 的cell数量，计算效率太低。对于beam search，每级选择top k作为下级输入，综合了效率和精度。 4 常见问题0 为什么rnn based seq2seq不需要额外添加位置信息？ 天然有位置信息（迭代顺序） 1 为什么rnn based seq2seq输入输出长度可变？ 因为rnn based seq2seq是迭代进行的，所以长度可变 2 训练的时候要padding吗？ 不用padding 参考https://zhuanlan.zhihu.com/p/47929039 https://www.cnblogs.com/liuxiaochong/p/14399416.html https://blog.csdn.net/thriving_fcl/article/details/74853556","link":"/2021/07/24/seq2seq/"},{"title":"序列标注","text":"序列标注（Sequence Tagging）是NLP中最基础的任务，应用十分广泛，如分词、词性标注（POS tagging）、命名实体识别（Named Entity Recognition，NER）、关键词抽取、语义角色标注（Semantic Role Labeling）、槽位抽取（Slot Filling）等实质上都属于序列标注的范畴。 标注方式https://zhuanlan.zhihu.com/p/147537898# 参考https://zhuanlan.zhihu.com/p/268579769 https://zhuanlan.zhihu.com/p/147537898#","link":"/2022/06/09/sequence-annotation/"},{"title":"Neural Graph Matching Networks for Chinese Short Text Matching","text":"https://aclanthology.org/2020.acl-main.547.pdf 1.摘要对于中文短文本匹配，通常基于词粒度而不是字粒度。但是分词结果可能是错误的、模糊的或不一致的，从而损害最终的匹配性能。比如下图：字符序列“南京市长江大桥”经过不同的分词可能表达为不同的意思。 为了解决这个问题，作者提出了一种基于图神经网络的中文短文本匹配方法。不是将句子分割成一个单词序列，而是保留所有可能的分割路径，形成一个Lattice（segment1，segment2，segment3），如上图所示。 2.问题定义将两个待匹配中文短文本分别定义为$S_a=\\left \\{ C_1^a,C_2^a,…,C_{t_a}^a \\right \\}$，$S_b=\\left \\{ C_1^b,C_2^b,…,C_{t_b}^b \\right \\}$，其中$C_i^a$表示句子$a$第$i$个字，$C_j^b$表示句子$b$第$j$个字，$t_a$，$t_b$分别表示两个句子的长度。$f(S_a,S_b)$是目标函数，输出为两个文本的匹配度。词格图用$G=(\\nu,\\xi)$表示，其中$\\nu$是节点集，包括所有字符序列。$\\xi$表示边集，如果$\\nu$中两个顶点$v_i$和$v_j$相邻，那么就存在一个边为$e_{ij}$。$N_{fw}(v_i)$表示节点$v_i$ 正向的所有可达节点的集合,$N_{bw}(v_i)$表示节点$v_i$ 反向的所有可达节点的集合。句子$a$的词格图为$G^a(\\nu_a,\\xi_a)$，句子$b$的词格图为$G^b(\\nu_b,\\xi_b)$。 3.模型结构 模型分成3个部分，1.语言节点表示 2.图神经匹配 3.相关性分类器 3.1 语言节点表示这一部分基于BERT的结构。BERT的token表示基于字粒度，可以得到$\\left \\{ [CLS],C_1^a,C_2^a,…,C_{ta}^a,[SEP],C_1^b,C_2^b,…,C_{t_b}^b,[SEP] \\right \\}$,如上图所示。BERT的输出为各个字的Embedding，$ \\left \\{\\textbf{C}^{CLS},\\textbf{C}_1^a,\\textbf{C}_2^a,…,\\textbf{C}_{t_a}^a,\\textbf{C}^{SEP},\\textbf{C}_1^b,\\textbf{C}_2^b,…,\\textbf{C}_{t_b},\\textbf{C}^{SEP} \\right \\}$。 3.2 图神经匹配初始化：假设节点$v_i$包含$n_i$个连续字符，起始字符位置为$s_i$，即$ \\left \\{C_{s_i},C_{s_{i+1}},…,C_{s_{i}+n_i-1} \\right \\}$，这里$v_i$表示句子$a$或者$b$的结点。$V_i=\\sum_{k=0}^{n_i-1}\\textbf{U}_{s_i+k}\\odot\\textbf{C}_{s_i+k}$，其中$\\odot$表示两个向量对应各个元素相乘。特征识别分数向量$\\textbf{U}_{s_i+k}=softmax(FFN(\\textbf{C}_{s_i+k}))$，$FFN$为两层。$h$为结点的向量表示，将$h_i^0$等于$V_i$ Message Propagation : 对于第$l$次迭代，$G_a$中某个结点$v_i$由如下四个部分组成 m_i^{fw}=\\sum_{v_j \\in N_{fw}(v_i)}\\alpha_{ij}(W^{fw}h_j^{l-1}), \\\\m_i^{bw}=\\sum_{v_k \\in N_{bw}(v_i)}\\alpha_{ik}(W^{bw}h_k^{l-1}), \\\\m_i^{b1}=\\sum_{v_m \\in V^b}\\alpha_{im}(W^{fw}h_m^{l-1}), \\\\m_i^{b2}=\\sum_{v_q \\in V^b}\\alpha_{iq}(W^{bw}h_q^{l-1})，其中$\\alpha_{ij},\\alpha_{ik},\\alpha_{im},\\alpha_{iq}$是注意力系数，$W^{fw},W^{bw}$是注意力系数参数 然后定义两种信息为$m_i^{self}\\triangleq[m_i^{fw},m_i^{bw}]，m_i^{cross}\\triangleq[m_i^{b1},m_i^{b2}]$ Representation Updating：得到两种信息后，需要更新结点$ v_i$的向量表示 d_k=cosine(w_k^{cos}\\odot m_i^{self},w_k^{cos}\\odot m_i^{cross})其中$w_k^{cos}$为参数，$d_k$为multi-perspective cosine distance，可以衡量两种信息的距离，$k \\in \\left \\{ 1,2,3,…P\\right\\}$，$P$是视角的数量。 h_i^l=FFN([m_i^{self},\\textbf{d}_i])其中$\\textbf{d}_i\\triangleq[d_1,d_2,…,d_P]$,$FFN$两层。 句子的图级别表示： 总共经历了$L$次迭代（layer），得到$h_i^L$为结点$v_i$最终的向量表示（$h_i^L$includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph) 最终，两个句子的图级别表示分别为 g^a=attentive pooling(\\left \\{ h_{1a}^L,h_{2a}^L,...,h_{node-num_a a}^L \\right \\}), \\\\g^b=attentive pooling(\\left \\{ h_{1b}^L,h_{2b}^L,...,h_{node-num_b b}^L \\right \\})3.3 分类器得到$g^a,g^b$后，两句子的相似度可以用分类器衡量： P=FFN([g^a,g^b,g^a \\odot g^b,|g^a-g^b|])其中$P \\in [0,1]$。 4.实验结果 lattice和JIEBA+PKU的区别？ JIEBA+PKU is a small lattice graph generated by merging two word segmentation results lattice：overall lattice，应该是全部的组合 两者效果差不多是因为Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence). 参考https://blog.csdn.net/qq_43390809/article/details/114077216","link":"/2021/08/04/short-chinese-text-match/"},{"title":"shell命令执行hive脚本","text":"https://blog.csdn.net/longshenlmj/article/details/50542683","link":"/2022/02/15/shell-hive/"},{"title":"SimCSE Simple Contrastive Learning of Sentence Embeddings","text":"https://arxiv.org/pdf/2104.08821.pdf 1.背景 1 target 对于$D=\\{(x_i,x_i^{+})\\}_{i=1}^{m}$,where $x_i$ and $x_i^{+}$ are semantically related. xi,xj+ are not semantically related x-&gt;h Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors N is mini-batch size，分子是正样本，分母为负样本（有一个正样本,感觉是可以忽略） 分母会包含分子的项吗？从代码看，会的 loss https://www.jianshu.com/p/d73e499ec859 12345678910111213141516171819202122232425def loss(self,y_pred,y_true,lamda=0.05): ''' exist a query q1 and ranked condidat list [d1,d2,d3,...,dn] loss= -log( exp^sim(q1,d1)/t / sum(exp^sim(q1,di)/t) i=2,...,n) [q1,q2] [[d11,d12,d13],[d21,d22,d23]] similarities=[[sim(q1d11),sim(q1d12),sim(q1d13)],[sim(q2d21),sim(q2d22),sim(q2d23)] ] y_true=[y1 ,y2 ] loss = F.cross_entropy(similarities, y_true) ref ： https://www.jianshu.com/p/d73e499ec859 ''' # idxs = torch.arange(0, y_pred.shape[0]) # y_true = idxs + 1 - idxs % 2 * 2 y_pred = y_pred.reshape(-1, y_true.shape[1]) # y_true=[0]*y_pred.sha pe[0] # similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2) # similarities = similarities - torch.eye(y_pred.shape[0]) * 1e12 y_pred = y_pred / lamda y_true = torch.argmax(y_true, dim=1) loss = F.cross_entropy(y_pred, y_true) return loss 2 representations评价指标 Alignment： calculates expected distance between embeddings of the paired instances（paired instances就是正例） uniformity： measures how well the embeddings are uniformly distributed 2.结构 2.1 Unsupervised$x_i-&gt;h_i^{z_i},x_i-&gt;h_i^{z_i^{‘}}$ z is a random mask for dropout，loss为 2.2 Supervised引入非目标任务的有标签数据集，比如NLI任务，$(x_i,x_i^{+},x_i^{-})$,where $x_i$ is the premise, $x_i^{+}$and $x_i^{-}$are entailment and contradiction hypotheses. $(h_i,h_j^{+})$为normal negatives，$(h_i,h_j^{-})$为hard negatives","link":"/2021/10/20/simcse/"},{"title":"Semi-supervised Learning和Self-Supervised Learning","text":"https://blog.csdn.net/qq_44015059/article/details/106448533","link":"/2021/11/22/simi-self-learning/"},{"title":"Solr","text":"https://zhuanlan.zhihu.com/p/71629409?fileGuid=It0Qkg2AiecFMx62 Solr是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化。","link":"/2022/02/07/solr/"},{"title":"sota","text":"https://www.jiqizhixin.com/sota","link":"/2021/11/22/sota/"},{"title":"Spark 数据倾斜","text":"https://blog.csdn.net/kaede1209/article/details/81145560 https://tech.meituan.com/2016/05/12/spark-tuning-pro.html 发生在两个过程： 数据源数据不均匀 shuffle过程中key的分布不均 单个rdd中进行聚合的时候key分布不均 多个rdd进行join过程中key的不均匀","link":"/2022/01/20/spark-data-im/"},{"title":"spark容错机制","text":"https://blog.csdn.net/JasonDing1354/article/details/46882585 https://blog.csdn.net/dengxing1234/article/details/73613484 容错指的是一个系统在部分模块出现故障时还能否持续的对外提供服务 1 Lineage机制2 Checkpoint机制","link":"/2022/04/04/spark-error-telerant/"},{"title":"spark常见错误","text":"Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions核心思路：分别指定driver和excutor的python版本，使其统一 方法一：修改环境变量 1./在环境变量文件 /etc/profile 中添加指定的pyspark，python的版本 12export PYSPARK_PYTHON=指定的python路径export PYSPARK_DRIVER_PYTHON=指定的python路径 保存后source一下 /etc/profile ,使之生效 2.代码内指定 12os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;]=&quot;&quot; ##driver os.environ[&quot;PYSPARK_PYTHON&quot;]=&quot;&quot; ### worker ,excutor 方法二：spark-submit工具指定 在spark-submit时增加参数 --conf spark.pyspark.python和 --conf spark.pyspark.driver.python 123spark-submit \\--driver-memory 5g --num-executors 5 --executor-cores 1 --executor-memory 1G --conf spark.pyspark.python=./.../bin/python --conf spark.pyspark.driver.python=./.../bin/python xx.py spark.sql 不能查询到hive的数据库，只查询到default数据库说明spark没有连接到hive https://www.cnblogs.com/yjt1993/p/13963144.html","link":"/2022/03/09/spark-error/"},{"title":"spark交互工具","text":"https://blog.csdn.net/u010886217/article/details/82916401 spark-shell、spark-sql，thriftserver","link":"/2022/03/19/spark-intercation-tool/"},{"title":"数据划分,rdd分区","text":"1 application，job，stage，task 0 application 任务 1 Job 一个action 一个job 2 Stage 一个job包含一个或者多个stage，根据rdd的依赖关系构建dag，根据dag划分stage 3 Task 1个stage包含1个或者多个task Task的类型分为2种：ShuffleMapTask和ResultTask ShuffleMapTask要进行Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask。如果要按照map端和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为Spark中的Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色。 2 rdd分区 record就是记录 2 为什么分区？ 分区的主要作用是用来实现并行计算，提高效率 3 分区方式 Spark包含两种数据分区方式：HashPartitioner（哈希分区）和RangePartitioner（范围分区）。 4 分区数设置 https://justdodt.github.io/2018/04/23/Spark-RDD-%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F%E7%A1%AE%E5%AE%9A/ 3 关系 task数量和Partition数量一样 参考https://www.jianshu.com/p/3aa52ee3a802 https://blog.csdn.net/hjw199089/article/details/77938688 https://blog.csdn.net/mys_35088/article/details/80864092 https://blog.csdn.net/dmy1115143060/article/details/82620715 https://blog.csdn.net/xxd1992/article/details/85254666 https://blog.csdn.net/m0_46657040/article/details/108737350 https://blog.csdn.net/heiren_a/article/details/111954523 https://blog.csdn.net/u011564172/article/details/53611109 https://blog.csdn.net/qq_22473611/article/details/107822168 https://www.jianshu.com/p/3aa52ee3a802","link":"/2022/02/26/spark-job_partition/"},{"title":"spark模块","text":"整个Spark 框架模块包含：Spark Core、Spark SQL、Spark Streaming、Spark GraphX、Spark MLlib，而后四项的能力都是建立在核心引擎之上 Spark Core：Spark的核心，Spark核心功能均由Spark Core模块提供，是Spark运行的基础。Spark Core以RDD为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。SparkSQL：基于SparkCore之上，提供结构化数据的处理模块。SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了StructuredStreaming模块，可以以SparkSQL为基础，进行数据的流式计算。 数据抽象:dataset(Java、Scala) dataframe(Java、Scala、Python、R)SparkStreaming：以SparkCore为基础，提供数据的流式计算功能。MLlib：以SparkCore为基础，进行机器学习计算，内置了大量的机器学习库和API算法等。方便用户以分布式计算的模式进行机器学习计算。GraphX：以SparkCore为基础，进行图计算，提供了大量的图计算API，方便用于以分布式计算模式进行图计算。","link":"/2022/02/26/spark-module/"},{"title":"spark oom(out of memory)问题","text":"https://blog.csdn.net/yhb315279058/article/details/51035631 https://www.cnblogs.com/yanshw/p/11988347.html 1 driver内存不够增加 Driver 内存 1--driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). 1 读入数据太大 解决思路是增加 Driver 内存 12345from pyspark import SparkContextsc = SparkContext(master='yarn')rdd = sc.parallelize(range(300000000))# spark-submit --master yarn-client --driver-memory 512M driver_oom.py 内存溢出# spark-submit --master yarn-client --driver-memory 3G driver_oom.py 可以执行 2 数据回传太大，也就是聚合到driver的数据太大 解决思路是分区输出，具体做法是 foreach 12345rdd = sc.parallelize(range(100))rdd.flatMap(lambda x: ['%d'%x*50 for _ in range(100000)]).collect() # 内存溢出def func(x): print(x)rdd.flatMap(lambda x: ['%d'%x*50 for _ in range(100000)]).foreach(func) # 分区输出 2 excutor内存不够通用的解决办法就是增加 Executor 内存 但这并不一定是最好的办法 1 map 过程产生大量对象 解决思路是减少每个 task 的大小，从而减少每个 task 的输出 具体做法是在 会产生大量对象的 map 操作前 添加 repartition(重新分区) 方法，分区成更小的块传入 map 1234rdd.flatMap(lambda x: ['%d'%x*50 for _ in range(100000000)]).count() # 100 * 100000000 个对象，内存溢出rdd.flatMap(lambda x: len(['%d'%x*50 for _ in range(100000000)])).sum() # 内存溢出rdd.repartition(1000000).flatMap(lambda x: ['%d'%x*50 for _ in range(100000000)]).count() 2 shuffle导致 shuffle内存溢出的情况可以说都是shuffle后发生数据倾斜，单个文件过大导致 参考数据倾斜解决方案","link":"/2022/04/05/spark-oom/"},{"title":"spark优化","text":"8 Performance Optimization Techniques Using Spark https://www.syntelli.com/eight-performance-optimization-techniques-using-spark# Spark性能优化指南（美团） https://tech.meituan.com/2016/04/29/spark-tuning-basic.html https://tech.meituan.com/2016/05/12/spark-tuning-pro.html","link":"/2022/01/05/spark-optimaization/"},{"title":"spark优化手段","text":"1.多个map合并（存疑）1rdd1.map().map() -&gt; rdd1.map() 2.减少job数量也就是减少action 说白了就是多个action操作，transformation逻辑可以写一起，最后action 3 广播大变量首先广播是操作，大变量是对象","link":"/2022/04/19/spark-optimazation-operation/"},{"title":"数据抽象","text":"1.RDD，DataFrame，Datasethttps://blog.knoldus.com/spark-rdd-vs-dataframes/ https://blog.csdn.net/hellozhxy/article/details/82660610 https://www.cnblogs.com/lestatzhang/p/10611320.html#Spark_16 https://www.jianshu.com/p/77811ae29fdd https://zhuanlan.zhihu.com/p/379578271 https://spark.apache.org/docs/3.2.0/sql-programming-guide.html#content 1 DataFrame 和 RDDs 应该如何选择？DataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据 如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。 如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs， 2 为什么出现Dataset？1.相比DataFrame，Dataset提供了编译时类型检查，对于分布式程序来讲，提交一次作业太费劲了（要编译、打包、上传、运行），到提交到集群运行时才发现错误，实在是想骂人，这也是引入Dataset的一个重要原因。 2.RDD转换DataFrame后不可逆，但RDD转换Dataset是可逆的（这也是Dataset产生的原因） 注意： The Dataset API is available in Scala and Java. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally row.columnName). 2.RDD算子就是分布式集合对象的api rdd算子分为两类：1.transformation 2.action https://blog.csdn.net/weixin_45271668/article/details/106441457 3.dataframeDataFrame支持两种风格进行编程，分别是： 1 DSL风格DSL称之为：领域特定语言。其实就是指DataFrame的特有APIDSL风格意思就是以调用API的方式来处理Data比如：df.where().limit() 2 SQL风格SQL语法风格SQL风格就是使用SQL语句处理DataFrame的数据比如：spark.sql(“SELECT * FROM xxx)","link":"/2022/01/25/spark-rdd/"},{"title":"shuffle","text":"https://www.cnblogs.com/arachis/p/Spark_Shuffle.html https://zhuanlan.zhihu.com/p/70331869 https://www.educba.com/spark-shuffle/ https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502 https://blog.csdn.net/zp17834994071/article/details/107887292 https://zhuanlan.zhihu.com/p/431015932 0 shuffle是什么，什么时候shuffle what：多个partition的数据流向一个partition when：宽依赖会有shuffle shuffle分为两个阶段：shuffle read ， shuffle write map端-》shuffle read-》 shuffle write-》reduce端 1 mapreduce shuffle 2 spark shuffle1 简介 2 分类https://www.51cto.com/article/703950.html# 过去hash shuffle ，现在sort shuffle 1.Hash Shuffle 2.Sort Shuffle1 普通机制的SortShuffleManager 2 bypass 此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash，然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。 3 总结 bypass与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同;第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 3 对比https://www.zhihu.com/question/27643595 4 优化因此在我们的开发过程中，能避免则尽可能避免使用会进行shuffle的算子，尽量使用非shuffle算子 1 shuffle算子： https://blog.csdn.net/py_tamir/article/details/95457813 reduceByKey、join、distinct、repartition 2 非shuffle算子 map，flatMap","link":"/2022/01/19/spark-shuffle/"},{"title":"Spark支持的存储介质","text":"Spark 支持多种的存储介质，在存储层 Spark 支持从 HDFS、HBase、Hive、ES、MongoDB、MySQL、PostgreSQL、AWS、Ali Cloud 等不同的存储系统、大数据库、关系型数据库中读入和写出数据，在实时流计算中可以从 Flume、Kafka 等多种数据源获取数据并执行流式计算。 https://cloud.tencent.com/developer/article/1942980","link":"/2022/02/21/spark-store/"},{"title":"Spark架构","text":"https://zhuanlan.zhihu.com/p/70424613 https://zhuanlan.zhihu.com/p/348024116 https://spark.apache.org/docs/latest/cluster-overview.html https://www.zhihu.com/question/437293024 There are several useful things to note about this architecture: Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system. Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN/Kubernetes). The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes. Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes. 上图的4个框应该不是指四台机器，仅仅用来示意spark的架构组成，在机器上的分布取决于部署方式 客户端/客户机指的是提交任务的机器 主从架构（master/slave）：集群由一个主服务器和若干个从服务器（机器）组成 一个worker可以有多个excutor，默认情况下，只会启动一个Executor https://www.cnblogs.com/ExMan/p/14358363.html 一个executor可以执行多个task，取决于cpu的核数 https://blog.csdn.net/mzqadl/article/details/104217828","link":"/2022/01/05/spark-struct/"},{"title":"提交Spark任务","text":"1.spark-submithttps://spark.apache.org/docs/latest/submitting-applications.html The spark-submit script in Spark’s bin directory is used to launch applications on a cluster. It can use all of Spark’s supported cluster managers through a uniform interface so you don’t have to configure your application especially for each one. 12345678./bin/spark-submit \\ --class &lt;main-class&gt; \\ --master &lt;master-url&gt; \\ --deploy-mode &lt;deploy-mode&gt; \\ --conf &lt;key&gt;=&lt;value&gt; \\ ... # other options &lt;application-jar&gt; \\ [application-arguments] --class: The entry point for your application (e.g. org.apache.spark.examples.SparkPi) --master: The master URL for the cluster (e.g. spark://23.195.26.187:7077) --deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client) † --conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. --conf = --conf =) application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes. application-arguments: Arguments passed to the main method of your main class, if any 当前为客户端，driver在哪取决于deploy mode 2.run file.py当前为客户机，而且这种方式默认driver在客户机，也就是client模式 此时若是代码指定cluster会报错 1config(&quot;spark.submit.deployMode&quot;, &quot;cluster&quot;) Exception in thread “main” org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.","link":"/2022/02/17/spark-task/"},{"title":"RDD依赖关系","text":"为什么要提出宽窄依赖根据rdd的依赖关系构建dag，根据dag划分stage 如何区别宽窄依赖 父rdd -&gt; 子rdd， 蓝色框是rdd分区 union：2个父rdd -&gt; 1个子rdd 区分宽窄依赖主要就是看父RDD数据流向，要是流向一个的话就是窄依赖，流向多个的话就是宽依赖。 划分stagehttps://blog.csdn.net/weixin_40271036/article/details/79996516 整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中 参考https://www.jianshu.com/p/5c2301dfa360 https://zhuanlan.zhihu.com/p/67068559 https://blog.csdn.net/m0_49834705/article/details/113111596 https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502","link":"/2022/01/19/spark-wide-narrow-dependancy/"},{"title":"spark资源参数调优","text":"概念并发：调度器切换CPU给不同进程使用，实际上CPU在同一时刻只在运行一个进程 并行： 1、一台物理机的物理CPU的个数 2、一个CPU上的核数 3、一个核上面支持的线程数（逻辑cpu） 123456789101112131415# 查看CPU信息（型号）cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz # 查看物理CPU个数 cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l2 # 查看每个物理CPU中core的个数(即核数) cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniqcpu cores : 6 # 查看逻辑CPU的个数cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l24 进程（要拿cpu为单位）：单CPU中进程只能是并发，多CPU计算机中进程可以并行。 线程（核作为单位）：单CPU单核中线程只能并发，单CPU多核中线程可以并行 参数task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。 worker: 几个执行节点 ，一个worker 对应几个 executor executor：对应进程 num-executors 参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。 参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-cores 参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。 参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。 task:线程，应该等于partition数量 job数量取决于action数量，stage数量取决于rdd依赖关系的划分 spark.default.parallelism 参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。 参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 注意：executor-cores为可用的计算资源，task为拆分的任务数量 内存： driver-memory 参数说明：该参数用于设置Driver进程的内存。 参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 executor-memory 参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。 参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。 因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。 spark.storage.memoryFraction 参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。 参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 spark.shuffle.memoryFraction 参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。 参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 资源参数参考示例以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节： 123456789./bin/spark-submit \\ --master yarn-cluster \\ --num-executors 100 \\ --executor-memory 6G \\ --executor-cores 4 \\ --driver-memory 1G \\ --conf spark.default.parallelism=1000 \\ --conf spark.storage.memoryFraction=0.5 \\ --conf spark.shuffle.memoryFraction=0.3 \\ 参考https://www.cnblogs.com/gtscool/p/13072051.html https://blog.csdn.net/l1394049664/article/details/81811642 https://tech.meituan.com/2016/04/29/spark-tuning-basic.html","link":"/2022/03/16/spark_para_optimization-resoucrece/"},{"title":"sparkcore","text":"1 共享变量https://blog.csdn.net/Android_xue/article/details/79780463 https://chowdera.com/2022/02/202202091419262471.html 两种共享变量：广播变量（broadcast variable）与累加器（accumulator） 广播变量解决了什么问题?分布式集合RDD和本地集合进行关联使用的时候, 降低内存占用以及减少网络IO传输, 提高性能. 累加器解决了什么问题?分布式代码执行中, 进行全局累加 1.广播变量 2.累加器 123456789101112131415161718192021222324sc = spark.sparkContextrdd1=sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)count=sc.accumulator(0)def map_func(data):global countcount+=data# count = sc.accumulator(0)# rdd3=rdd2.map(lambda x:x)# rdd3.collect()# print(count)start_time = time.time()result=rdd1.reduce(lambda a, b: a + b)end_time = time.time()print(result)print(end_time - start_time)# print(count)start_time = time.time()rdd2 = rdd1.map(map_func)rdd2.collect()end_time = time.time()print(count)print(end_time - start_time) 1234551.092909574508667550.09459614753723145 累加器和reduce都可以得到聚合结果，效率？？？谁先执行 谁短，怎么衡量 2 *ByKey 操作https://blog.csdn.net/weixin_43810802/article/details/120772452 https://blog.csdn.net/zhuzuwei/article/details/104446388 https://blog.csdn.net/weixin_40161254/article/details/103472056 Use reduceByKey instead of groupByKey groupByKey creates a lot of shuffling which hampers the performance, while reduceByKey does not shuffle the data as much 3 reduce1234567in：table=pd.DataFrame({&quot;num&quot;:[1,1,2]})table = spark.createDataFrame(table)from pyspark.sql import Rowtable.rdd.reduce(lambda a,b:Row(num=a[0]+b[0]))out：Row(num=4) 聚合的作用 注意点就是 a,b 操作后的数据类型和a，b保持一致，举个例子 a+b 和a ，b类型一致，否则（a+b）+c会报错 4 collect、 take、top、first，foreach1 collect collect() 返回包含此RDD中的所有元素的列表 注意：因为所有数据都会加载到driver，所有只适用于数据量不大的情况 2 first first() 返回RDD中的第一个元素 3 take take(num) 取RDD的前num个元素 4 top top(num, key=None) 排序 Get the top N elements from an RDD 5 foreach foreach(f) Applies a function to all elements of this RDD 分区输出 1234567891011121314151617def f(x): print(x) sc.parallelize([1,2,3,4,5]).foreach(f)output：43215会变35241","link":"/2022/02/27/sparkcore/"},{"title":"Sparkcore运行流程","text":"运行流程 1 注册并申请资源，分配资源 2 job-&gt;stage-&gt;task 3 4 5 driver executor 交互执行任务 executor 向driver申请任务， 还是driver给executor 分配任务？ 例子","link":"/2022/03/02/sparkcore-run/"},{"title":"Spark on Hive &amp; Hive on Spark","text":"https://cloud.tencent.com/developer/article/1624245 https://blog.csdn.net/weixin_41290471/article/details/106203419 https://www.cnblogs.com/qingyunzong/p/8992664.html 1 Hive on Spark就是将spark作为hive的计算引擎 Hive既作为存储元数据又负责SQL的解析优化，语法是HQL语法，执行引擎变成了Spark，Spark负责采用RDD执行。 2 Spark on Hive就是因为Spark自身没有元数据管理功能，所以使用Hive的Metastore服务作为元数据管理服务。计算由Spark执行。 Hive只作为存储元数据，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。 SparkSQL 的元数据的状态有两种： 1、in_memory,用完了元数据也就丢了 2、hive , 通过hive去保存的，也就是说，hive的元数据存在哪儿，它的元数据也就存在哪儿。换句话说，SparkSQL的数据仓库在建立在Hive之上实现的。我们要用SparkSQL去构建数据仓库的时候，必须依赖于Hive。","link":"/2022/02/01/sparkonhive/"},{"title":"sparksql对比hive sql","text":"Hive和Spark 均是“分布式SQL计算引擎”，mysql等不是，mysql跑单机上 均是构建大规模结构化数据计算的绝佳利器，同时SparkSQL拥有更好的性能。目前，企业中使用Hive仍旧居多，但SparkSQL将会在很近的未来替代Hive成为分布式SQL计算市场的顶级","link":"/2022/02/27/sparksql-vs-hql/"},{"title":"Sparksql运行流程","text":"转成rdd来做 https://blog.csdn.net/qq_25002995/article/details/104748504","link":"/2022/03/02/sparksql-run/"},{"title":"sparksql","text":"1 用户自定义函数 步骤： https://blog.csdn.net/qq_43665254/article/details/112379113 https://blog.csdn.net/sunflower_sara/article/details/104044412 1、定义函数 2、注册函数 3、使用函数 2 withColumn12345678910from pyspark.sql.functions import col, lit###df.withColumn('age2', df.age + 2).collect()### 结合udfdef fun(A,B): XXXX return XXfun1 = udf(fun, StringType())df.withColumn('age2', fun1(col_name1,col_name2))###","link":"/2022/02/27/sparksql/"},{"title":"特征稀疏","text":"What are sparse features?Features with sparse data are features that have mostly zero values. This is different from features with missing data. Why is machine learning difficult with sparse features?Common problems with sparse features include: If the model has many sparse features, it will increase the space and time complexity of models. Linear regression models will fit more coefficients, and tree-based models will have greater depth to account for all features. Model algorithms and diagnostic measures might behave in unknown ways if the features have sparse data. Kuss [2002] shows that goodness-of-fit tests are flawed when the data is sparse. If there are too many features, models fit the noise in the training data. This is called overfitting. When models overfit, they are unable to generalize to newer data when they are put in production. This negatively impacts the predictive power of models. Some models may underestimate the importance of sparse features and given preference to denser features even though the sparse features may have predictive power. Tree-based models are notorious for behaving like this. For example, random forests overpredict the importance of features that have more categories than those features that have fewer categories. Methods for dealing with sparse features Removing features from the model Make the features dense Using models that are robust to sparse features 参考https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html#:~:text=%20Methods%20for%20dealing%20with%20sparse%20features%20,that%20are%20robust%20to%20sparse%20features%20More%20","link":"/2021/09/30/sparsity-feature/"},{"title":"springboot","text":"java框架，对spring的改进","link":"/2022/04/23/springboot/"},{"title":"建表","text":"1234567CREATE TABLE IF NOT EXISTS `runoob_tbl`( `runoob_id` INT UNSIGNED AUTO_INCREMENT, `runoob_title` VARCHAR(100) NOT NULL, `runoob_author` VARCHAR(40) NOT NULL, `submission_date` DATE, PRIMARY KEY ( `runoob_id` ))ENGINE=InnoDB DEFAULT CHARSET=utf8; 字段数据类型https://www.w3school.com.cn/sql/sql_datatypes.asp array https://www.educba.com/array-in-sql/ 约束（Constraints） NOT NULL - 指示某列不能存储 NULL 值。 UNIQUE - 保证某列的每行必须有唯一的值。 PRIMARY KEY - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。 FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性。 CHECK - 保证列中的值符合指定的条件。 DEFAULT - 规定没有给列赋值时的默认值。 自增字段 AUTO INCREMENT 123456789CREATE TABLE Persons(ID int NOT NULL AUTO_INCREMENT,LastName varchar(255) NOT NULL,FirstName varchar(255),Address varchar(255),City varchar(255),PRIMARY KEY (ID)) 开始值是 1，每条新记录递增 1","link":"/2022/05/05/sql-buildtable/"},{"title":"sql常见操作","text":"1 拼接1 union ，union all https://www.w3school.com.cn/sql/sql_union.asp 123select 'mobile' as platform unionselect 'desktop' as platform unionselect 'both' as platform 2 join left join 、right join、 inner join，FULL OUTER JOIN，默认join 为 inner join https://www.cnblogs.com/ingstyle/p/4368064.html 12345678#多个left joinselect a.*,b.*,c.* from a left join b on a.id=b.id left join c on b.id=c.idselect *from Trips T left join Users U1 on T.client_id =U1.users_id left join Users U2 on T.driver_id =U2.users_id 12345from Trips T1left join Users U1 on (T1.client_id =U1.users_id and U1.banned =&quot;Yes&quot; ) 不可from Trips T1join Users U1 on (T1.client_id =U1.users_id and U1.banned =&quot;Yes&quot; ) 可 3.from student A,student B,student C 将三个 student 表相互连接成一个 https://blog.csdn.net/zhangyj_315/article/details/2249209 2 分组1 group by 分组, 分完每组就一行，取每组第一行数据 group by columns1,columns2 #有时候不能取第一行，怎么解决？1070. 产品销售分析 III a 可以先排序，把目标行变成第一行 可以参考https://blog.csdn.net/shiyong1949/article/details/78482737 好像不行 b 使用开窗函数解决，可以 #分组+条件判断 Having having 子句的作用是筛选满足条件的组，不是在组内选行 在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。 #4 子查询嵌套 http://c.biancheng.net/sql/sub-query.html 1234select a.Score as Score, (select count(DISTINCT b.Score) from Scores b where b.Score &gt;= a.Score) as 'Rank'from Scores aorder by a.Score DESC 5 别名as后加别名，也可不要as 6 匹配分为完整匹配和模糊匹配 https://www.cnblogs.com/Leophen/p/11397621.html 关键字like 1select * from table where name like &quot;%三%&quot; 通配符：%，_，[charlist] ，[!charlist] 7 条件1.IF 表达式：IF( expr1 , expr2 , expr3 ) expr1条件，条件为true，则值是expr2 ，false，值就是expr3 2 case input几行output几行 一行一行来 https://zhuanlan.zhihu.com/p/240717732 两种形式： 1234567891011121314--type1CASE &lt;表达式&gt; WHEN &lt;值1&gt; THEN &lt;操作&gt; WHEN &lt;值2&gt; THEN &lt;操作&gt; ... ELSE &lt;操作&gt;END--type2CASE WHEN &lt;条件1&gt; THEN &lt;命令&gt; WHEN &lt;条件2&gt; THEN &lt;命令&gt; ... ELSE commandsEND then后面多个值 then 1 可 then (1,1) 不可 where xx in （1,1）可 3 where 1126. 查询活跃业务12345678# Write your MySQL query statement belowselect t.business_id from (select *, avg(E1.occurences ) over(partition by E1.event_type ) as ave_occufrom Events E1 ) twhere t.occurences &gt; t.ave_occugroup by t.business_idhaving count(t.business_id)&gt;=2 4 条件判断 (E1.id , E1.month) in ((1,8)) BETWEEN ‘2019-06-28’ AND ‘2019-07-27’ EXISTS ：用于判断查询子句是否有记录，如果有一条或多条记录存在返回 True，否则返回 False。 1234SELECT column_name(s)FROM table_nameWHERE EXISTS(SELECT column_name FROM table_name WHERE condition); 8 NULL1.空字符和null的区别 https://blog.csdn.net/weixin_42214393/article/details/80463912 2.判断NULL is not NULL !=NULL 有问题 ifnull(sum(quantity), 0) 3 in、not in https://blog.csdn.net/qq_22592457/article/details/108024521 使用in时，in后面的括号中忽略null值 使用not in时，如果 not in后面的括号中没有null，正常判断，会查询条件列中符合要求的数据 使用not in时，如果 not in后面的括号中有null，直接返回false，查询结果为空。 9 日期1 大小判断 available_from &lt; ‘2019-05-23’ datediff(date1,date2) 2 格式转化 DATE_FORMAT() https://www.w3school.com.cn/sql/func_date_format.asp 10 去重1 distinct https://blog.csdn.net/zhangzehai2234/article/details/88361586 1select distinct expression[,expression...] from tables [where conditions]; 在使用distinct的过程中主要注意一下几点： 在对字段进行去重的时候，要保证distinct在所有字段的最前面 如果distinct关键字后面有多个字段时，则会对多个字段进行组合去重，只有多个字段组合起来的值是相等的才会被去重 distinct , count 一起用 12345678910111213141516171819##建表Create table If Not Exists Spending (user_id int, spend_date date, platform ENUM('desktop', 'mobile'), amount int);Truncate table Spending;insert into Spending (user_id, spend_date, platform, amount) values ('1', '2019-07-01', 'mobile', '100');insert into Spending (user_id, spend_date, platform, amount) values ('1', '2019-07-01', 'desktop', '100');insert into Spending (user_id, spend_date, platform, amount) values ('2', '2019-07-01', 'mobile', '100');insert into Spending (user_id, spend_date, platform, amount) values ('2', '2019-07-02', 'mobile', '100');insert into Spending (user_id, spend_date, platform, amount) values ('3', '2019-07-01', 'desktop', '100');insert into Spending (user_id, spend_date, platform, amount) values ('3', '2019-07-02', 'desktop', '100');###查询select distinct spend_date ,count(user_id ) from Spending##result##返回结果就一行，distinct后多行，count一行，多行一行还是一行；count结果还是distinct前的数量spend_date count(user_id )2019-07-01 6 11 show statushttps://blog.csdn.net/qq_29168493/article/details/79149132 查看当前数据库状态，可以统计当前数据库不同语句的执行频次 12 explain获取sql执行计划，结果明细参考 https://cloud.tencent.com/developer/article/1093229 14 事务http://m.biancheng.net/sql/transaction.html 15 递归查询https://zhuanlan.zhihu.com/p/372330656 https://medium.com/swlh/recursion-in-sql-explained-graphically-679f6a0f143b https://www.sqlservertutorial.net/sql-server-basics/sql-server-recursive-cte/ 123456789101112WITH expression_name (column_list)AS( -- Anchor member initial_query UNION ALL -- Recursive member that references expression_name. recursive_query )-- references expression nameSELECT *FROM expression_name 分析 https://zhuanlan.zhihu.com/p/372330656 R0 ： 123SELECT UserID,ManagerID,Name,Name AS ManagerName FROM dbo.Employee WHERE ManagerID=-1 userid managerid name managername 1 -1 boss boss R1： 123SELECT c.UserID,c.ManagerID,c.Name,p.Name AS ManagerNameFROM CTE PINNER JOIN dbo.Employee c ON p.UserID=c.ManagerID 此时Employee为完整的，cte为 userid managerid name managername 1 -1 boss boss 得到结果为 c.userid c.managerid c.name c.managername p.userid p.managerid p.name p.managername 11 1 A1 A1 1 -1 boss boss 12 1 A2 A2 1 -1 boss boss 13 1 A3 A3 1 -1 boss boss 1SELECT c.UserID,c.ManagerID,c.Name,p.Name AS ManagerName userid managerid name name 11 1 A1 boss 12 1 A2 boss 13 1 A3 boss R2,R3… 最后union all R0，R1，R2，。。。 16 select 常数12345678910111213SELECT 1,-1,N'Boss'UNION ALLSELECT 11,1,N'A1'UNION ALLSELECT 12,1,N'A2'UNION ALLSELECT 13,1,N'A3'UNION ALLSELECT 111,11,N'B1'UNION ALLSELECT 112,11,N'B2'UNION ALLSELECT 121,12,N'C1' 1 -1 Boss —字段 1 -1 Boss11 1 A112 1 A213 1 A3111 11 B1112 11 B2121 12 C1 17 CTECommon Table Expression with …as… 18 触发器就是做了某些操作，自动触发的行为 触发器是自动执行的，当用户对表中数据作了某些操作之后立即被触发。 https://blog.csdn.net/weixin_48033173/article/details/111713117","link":"/2022/02/27/sql-common-operation/"},{"title":"sql增删改查","text":"https://blog.csdn.net/Zhangxichao100/article/details/55099118 1 增1 insert insert into table (姓名,性别,出生日期) values (‘王伟华’,’男’,’1983/6/15’) insert into table (‘姓名’,’地址’,’电子邮件’)select name,address,email from Strdents 2 SELECT INTO 123SELECT column_nameINTO newtableFROM table1; 2 删1 delete 删除数据某些数据 delete from table where name=’王伟华’ 2 truncate 删除整个表的数据 truncate table addressList 3 改1 update update table set 年龄=18 where 姓名=’王伟华’ 4 查 1 select","link":"/2022/04/02/sql-crud/"},{"title":"sql函数","text":"1 单行函数和多行函数单行函数：单入单出 多行函数：多入单出，最常见的就是聚合函数 应该不存在单入多出，多入多出可以简化为单入单出的多次 https://blog.csdn.net/lailai186/article/details/12570899 注意： 多行，单行结果组合返回一行 例子：select column1 ,count(column2) ，只返回一行 问题来了，多多不一样呢，比如3和2，应该不存在 2 用户自定义函数https://blog.csdn.net/qq_23833037/article/details/53170789 3 聚合函数顾名思义，将数据聚集返回单一的值 https://blog.csdn.net/qq_40456829/article/details/83657396 4 开窗函数（Window Function）https://segmentfault.com/a/1190000040088969 https://www.51cto.com/article/639541.html https://blog.csdn.net/weixin_43412569/article/details/107992998 作用 行数保持不变 输入多行（一个窗口）、返回一个值 计算过程 当前行-》分区-》排序-》范围-》计算-》结果填入当前行 语法 1234window_function ([expression]) OVER ( [ PARTITION BY part_list ] [ ORDER BY order_list ] [ { ROWS | RANGE } BETWEEN frame_start AND frame_end ] ) expression PARTITION BY 表示将数据按 part_list 进行分区， 不加partition by 默认用全部（一个分区） partition by columns1,columns2 ORDER BY 表示将各个分区内的数据按 order_list进行排序 ROWS / RANGE 决定数据范围 https://blog.csdn.net/qq_42374697/article/details/115109386 分类 https://www.cnblogs.com/52xf/p/4209211.html 可以分为以下 3 类： 聚合（Aggregate）：AVG(), COUNT(), MIN(), MAX(), SUM()… 1234567891011sum(frequency) over(partiton by num ) --分组累加sum(frequency) over() --total_frequency 全部累加sum(frequency) over(order by num desc) --desc_frequency, 逆序累加sum(frequency) over(order by num asc) --asc_frequency,正序累加SUM(Salary) OVER (PARTITION BY Id ORDER BY Month asc range 2 PRECEDING) --range 2 PRECEDING 当前以及前面2行avg(frequency) over() --total_frequency ，全部平均avg(frequency) over(order by num desc) --desc_frequency, 逆序平均avg(frequency) over(order by num asc) --asc_frequency, 正序平均 ​ https://blog.csdn.net/qq_54494937/article/details/119537881 ​ https://leetcode-cn.com/problems/find-median-given-frequency-of-numbers/ ​ https://blog.csdn.net/qq_54494937/article/details/119537881 取值（Value）：FIRST_VALUE(), LAST_VALUE(), LEAD(), LAG()… 排序（Ranking）：RANK(), DENSE_RANK(), ROW_NUMBER(), NTILE()… 1rank() OVER (PARTITION BY Id ORDER BY Month DESC) https://leetcode-cn.com/problems/find-cumulative-salary-of-an-employee/ 5 数学运算函数https://blog.csdn.net/a_lllll/article/details/87880389 abs1234select distinct C1.seat_id as seat_idfrom Cinema C1 join Cinema C2on C1.free=1 and C2.free=1 and abs(C1.seat_id-C2.seat_id)=1order by seat_id power1234# Write your MySQL query statement belowselect round(min(Power(Power(P1.x-P2.x,2)+Power(P1.y-P2.y,2),0.5)),2) as shortest from Point2D P1 join Point2D P2 on P1.x!=P2.x or P1.y!=P2.y","link":"/2022/02/27/sql-func/"},{"title":"sql优化","text":"1 汇总https://blog.csdn.net/hguisu/article/details/5731629 https://www.analyticsvidhya.com/blog/2021/10/a-detailed-guide-on-sql-query-optimization/ https://blog.devart.com/how-to-optimize-sql-query.html#sql-query-optimization-basics 2 减少全表扫描https://www.cnblogs.com/feiling/p/3393356.html https://www.jianshu.com/p/03968ac9d8ad 3 创建索引https://blog.csdn.net/happyheng/article/details/53143345 https://www.runoob.com/mysql/mysql-index.html https://blog.csdn.net/wangfeijiu/article/details/113409719 0 作用可以提高查询效率 和主键的区别，主键是特殊的索引，索引不一定是主键 https://blog.csdn.net/krismile__qh/article/details/98477484 https://blog.csdn.net/weixin_33375360/article/details/113371197 既然有主键为啥还要索引，关键在于这是两个东西，一个是为了唯一表示，一个是为了提高查询效率，底层也不同 https://cache.one/read/17347789 1 索引分类聚集索引与非聚集索引 2 常见操作1、创建索引 创建表时指定索引 123456789drop TABLE if EXISTS s1;create table s1( id int , age int, email varchar(30),index(id) ); 创建表后 CREATE INDEX 索引名 ON 表名(列的列表);/ALTER TABLE 表名 ADD INDEX 索引名 (列名1，列名2,…); 2、删除索引 DROP INDEX index_name ON talbe_nameALTER TABLE table_name DROP INDEX index_name 3、查看索引 SHOW INDEX FROM table_name; 4、使用索引 建立了索引，底层查询效率变高了 查询语句编写上和原来一样，没有区别 https://blog.csdn.net/lenux2017/article/details/80086265 3 索引失效https://www.cnblogs.com/technologykai/articles/14172224.html 4 索引设计http://c.biancheng.net/view/7366.html 4 视图https://blog.csdn.net/talentluke/article/details/6420197 http://m.biancheng.net/sql/create-view.html 视图为虚拟的表，包含的不是数据而是sql查询 视图和表的主要区别在于： 表占用物理存储空间，也包含真正的数据； 视图不需要物理存储空间（除非您为视图添加索引），也不包含真正的数据，它只是从表中引用数据。 作用 简化数据访问，让复杂的 SQL 语句简单化。用户只需要对视图写简单的代码就能返回需要的数据，一些复杂的逻辑放在视图中完成。 防止敏感的字段被选中，同时仍然提供对其它重要数据的访问。 可以对视图添加一些额外的索引，来提高查询的效率。 使用视图的时候跟表一样 和cte的区别 https://blog.csdn.net/happyboy53/article/details/2731420 子查询包含的是数据，将数据存在内存，而视图包含的不是数据而是sql查询 5 存储过程 SQL 语言层面的代码封装与重用 https://www.runoob.com/w3cnote/mysql-stored-procedure.html","link":"/2022/02/27/sql-optimization/"},{"title":"sql","text":"1.SQL语句类别SQL 语句主要可以划分为以下 3 个类别。 DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。 DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查） DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。 2.sql语句执行顺序https://www.cnblogs.com/Qian123/p/5669259.html https://www.cnblogs.com/Qian123/p/5669259.html https://cloud.tencent.com/developer/article/1600323 123456781、from子句组装来自不同数据源的数据；2、where子句基于指定的条件对记录行进行筛选； 3、group by子句将数据划分为多个分组； 4、聚合函数进行计算； 5、having子句筛选分组； 6、计算所有的表达式； 7、select字段； 8、order by对结果集进行排序。 感觉好像先select后having","link":"/2022/02/07/sql/"},{"title":"sqoop","text":"https://www.yangshuaibin.com/detail/388673 作用关系型数据库 &lt;&lt;——&gt;&gt; hdfs ，双向，用的mapreduce sqoop的作用就是将关系型数据库中的某张表数据抽取到Hadoop的hdfs文件系统当中，底层运用的还是Map Reduce 。 它利用MapReduce加快数据传输速度，批处理方式进行数据传输。 也可以将HDFS上的文件数据导出到关系型数据库中的某张表。 脚本https://blog.csdn.net/qq_44665283/article/details/120709047","link":"/2022/01/31/sqoop/"},{"title":"sugar","text":"百度的BI可视化工具","link":"/2022/04/23/sugar/"},{"title":"superset","text":"作用Apache Superset是一个开源的、现代的、轻量级BI（Business Intelligence）分析工具，能够对接多种数据源、拥有丰富的图表展示形式、支持自定义仪表盘，且拥有友好的用户界面，十分易用。 由于Superset能够对接常用的大数据分析工具，如Hive、Kylin、Druid等，且支持自定义仪表盘，故可作为数仓的可视化工具。","link":"/2022/02/05/superset/"},{"title":"SVM","text":"1.回顾线性回归和LR线性回归是解决回归任务的线性模型 LR是二分类模型，在线性模型的基础上加入激活函数sigmoid，适用在线性可分的二分类任务 2.介绍svm简单总结：1.对于完全线性可分，硬间隔 2.不能够完全线性可分，引入松弛变量 ，软间隔 3.线性不可分，引入核函数 原理可参考： https://zhuanlan.zhihu.com/p/77750026 或者 https://blog.csdn.net/qq_37321378/article/details/108807595 核函数 https://blog.csdn.net/mengjizhiyou/article/details/103437423 与LR区别： https://www.jianshu.com/p/1b4d9de7000c","link":"/2021/10/09/svm/"},{"title":"各种表","text":"1 分区表https://www.jianshu.com/p/1cdd3e3c5b3c https://www.jianshu.com/p/163f8375c0d6 1 mysql分区 RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区。 LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择。 HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。 KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL 服务器提供其自身的哈希函数。必须有一列或多列包含整数值。 2 Hive中分区表 分两类：静态分区、动态分区； Hive中没有复杂的分区类型（List,Range,Hash） 2 内部表&amp;外部表https://www.cnblogs.com/qiaoyihang/p/6225151.html https://blog.csdn.net/qq_36743482/article/details/78393678 1 未被external修饰的是内部表（managed table），被external修饰的为外部表（external table）； 2 内部表数据由Hive自身管理，外部表数据由HDFS管理； 3 内部表数据存储的位置是hive.metastore.warehouse.dir（默认：/user/hive/warehouse），外部表数据的存储位置由自己制定（如果没有LOCATION，Hive将在HDFS上的/user/hive/warehouse文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）；4 删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除；5 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;） 3 临时表https://www.cnblogs.com/duanxz/p/3724120.html 4 字典表https://www.cnblogs.com/jpfss/p/10418873.html 5 全量表，增量表、快照表、流水表全量表：所有数据的最新状态 增量表：新增数据 快照表： 流水表： 对于表的每一个修改都会记录，可以用于反映实际记录的变更。 6 切片表切片表：切片表根据基础表，往往只反映某一个维度的相应数据。其表结构与基础表结构相同，但数据往往只有某一维度，或者某一个事实条件的数据 7 拉链表1 定义 2 应用场景 3 分区 4 数据加载 1）首日装载 拉链表首日装载，需要进行初始化操作，具体工作为将截止到初始化当日的全部历史用户导入一次性导入到拉链表中。目前的ods_user_info 表的第一个分区，即2020-06-14 分区中就是全部的历史用户，故将该分区数据进行一定处理后导入拉链表的9999-99-99 分区即可。 2）每日装载","link":"/2022/01/30/table/"},{"title":"Embedding based Product Retrieval in Taobao Search","text":"https://arxiv.org/pdf/2106.09297.pdf http://xtf615.com/2021/10/07/taobao-ebr/ 1.INTRODUCTION 框架是搜索系统主流的结构，即匹配/检索，粗排，精排，重排。 2.RELATED WORK2.1 Deep Matching in Searchfall into two categories: representation-based learning and interaction-based learning. Other than semantic and relevance matching, more complex factors/trade-offs, e.g., user personalization [2, 3, 10] and retrieval efficiency [5], need to be considered when applying deep models to a large-scale online retrieval system. 2.2 Deep Retrieval in Industry SearchRepresentation-based models with an ANN (approximate near neighbor) algorithm have become the mainstream trend to efficiently deploy neural retrieval models in industry. 3 MODEL整体结构入下： 3.1 Problem Formulation$\\mathcal{U}=\\{u_1,…,u_u,…u_N\\}$表示$N$个用户，$\\mathcal{Q}=\\{q_1,…,q_u,…q_N\\}$表示与用户对应的$N$个query，$\\mathcal{I}=\\{i_1,…,i_u,…i_M\\}$表示$M$个商品。将用户$u$的历史行为根据时间分成3个部分：1.real-time，beforethe current time step，$\\mathcal{R}^u=\\{i_1^u,…,i_t^u,…i_T^u\\}$ 2.short-term, before $\\mathcal{R}$ and within ten days,$\\mathcal{S}^u=\\{i_1^u,…,i_t^u,…i_T^u\\}$ 3.long-term sequences,before $\\mathcal{S}$ and within one month,$\\mathcal{L}^u=\\{i_1^u,…,i_t^u,…i_T^u\\}$ ，$T$为时间长度。任务可以定义为： z=\\mathcal{F}(\\phi(q_u,\\mathcal{R}^u,\\mathcal{S}^u,\\mathcal{L}^u),\\varphi(i))其中$\\mathcal{F}(\\cdot),\\phi(\\cdot),\\varphi(\\cdot)$分别表示scoring function, query and behaviors encoder, and item encoder 3.2 User Tower3.2.1 Multi-Granular Semantic Unit挖掘query的语义，原始输入包含当前query和历史query 没有说明为什么这么设计，感觉就是工程试验的结论。有个疑问，直接用BERT等深度语言模型来挖掘query的语义不好吗？ query表示为$q_u=\\{w_1^u,…,w_n^u\\}$,例如{红色，连衣裙}，$w_u=\\{c_1^u,…,c_m^u\\}$,例如{红，色}，history query表示为$q_{his}=\\{q_1^u,…,q_k^u\\} $,例如{绿色，半身裙，黄色，长裙}，其中$w_n \\in \\mathbb{R}^{1\\times d},c_m \\in \\mathbb{R}^{1\\times d},q_k \\in \\mathbb{R}^{1\\times d}$ q_{1\\_gram}=mean\\_pooling(c_1,...,c_m) \\\\q_{2\\_gram}=mean\\_pooling(c_1c_2,...,c_{m-1}c_m) \\\\q_{seq}=mean\\_pooling(w_1,...,w_n) \\\\q_{seq\\_seq}=mean\\_pooling(T_{rm}(w_1,...,w_n)) \\\\q_{his\\_seq}=softmax(q_{seg}\\cdot(q_{his})^{T})q_{his} \\\\q_{mix}=q_{1\\_gram}+q_{2\\_gram}+q_{seq}+q_{seq\\_seq}+q_{his\\_seq} \\\\Q_{mgs}=concat(q_{1\\_gram},q_{2\\_gram},q_{seq},q_{seq\\_seq},q_{his\\_seq},q_{mix})其中𝑇𝑟𝑚,𝑚𝑒𝑎𝑛_𝑝𝑜𝑜𝑙𝑖𝑛𝑔, and 𝑐𝑜𝑛𝑐𝑎𝑡 denote the Transformer ,average, and vertical concatenation operation, respectively 3.2.2 User Behaviors Attention e_i^f=W_f\\cdot x_i^{f} \\in \\mathbb{R}^{1\\times d_f} \\tag{9} \\\\i_t^u=concat(\\{e_i^f\\ | \\ f \\in \\mathcal{F} \\})其中$W_f$是embedding matrix，$x_i^{f}$是one-hot vector, $\\mathcal{F}$是side information (e.g., leaf category, first-level category, brand and,shop) real-time sequences User’s click_item \\mathcal{R}_{lstm}^u=LSTM(\\mathcal{R}^u)=\\{h_1^{u},...,h_t^{u},...,h_T^{u} \\} \\\\\\mathcal{R}_{self\\_att}^u=multihead\\_selfattention(\\mathcal{R}_{lstm}^u)=\\{h_1^{u},...,h_t^{u},...,h_T^{u} \\} \\\\\\mathcal{R}_{zero\\_att}^u=\\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \\} \\ \\# add \\ a \\ zero \\ vector \\ at \\ the \\ first \\ position \\ of \\ \\mathcal{R}_{self\\_att}^u \\\\H_{real}=softmax(Q_{mgs}\\cdot\\mathcal{R}_{zero\\_att}^T)\\cdot\\mathcal{R}_{zero\\_att}^Tshort-term sequences User’s click_item \\mathcal{S}_{self\\_att}^u=multihead\\_selfattention(\\mathcal{S}^u)=\\{h_1^{u},...,h_t^{u},...,h_T^{u} \\} \\\\\\mathcal{S}_{zero\\_att}^u=\\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \\} \\\\H_{short}=softmax(Q_{mgs}\\cdot\\mathcal{S}_{zero\\_att}^T)\\cdot\\mathcal{S}_{zero\\_att}^Tlong-term sequence $\\mathcal{L}^u$由四个部分构成，分别为$\\mathcal{L}^{u}_{item},\\mathcal{L}^{u}_{shop},\\mathcal{L}^{u}_{leaf},\\mathcal{L}^{u}_{brand}$,每个部分包含3个动作，分别为click，buy，collect。 \\\\ \\mathcal{L}_{click\\_item},\\mathcal{L}_{buy\\_item},\\mathcal{L}_{collect\\_item} \\rightarrow L^{T}_{item} \\\\H_{a\\_item}=softmax(Q_{mgs}\\cdot L^{T}_{item})\\cdot L^{T}_{item} \\\\H_{long}=H_{a\\_item}+H_{a\\_shop}+H_{a\\_leaf}+H_{a\\_brand}3.2.3 Fusion of Semantics and Personalization H_{qu}=Self\\_Att^{first}([[cls],Q_{mgs},H_{real},H_{short},H_{long}]) \\in \\mathbb{R}^{1\\times d}3.3 Item TowerFor the item tower, we experimentally use item ID and title to obtain the item representation 𝐻𝑖𝑡𝑒𝑚.Given the representation of item 𝑖’s ID, $e_i \\in \\mathbb{R}^{1\\times d}$ , and its title segmentation result $T_i=\\{w_1^{i},…,w_N^{i}\\}$ H_{item}=e+tanh(W_t\\cdot\\frac{\\sum_{i=1}^Nw_i}{N})where $W_t$ is the transformation matrix. We empirically find that applying LSTM [12] or Transformer [27] to capture the context of the title is not as effective as simple mean-pooling since the title is stacked by keywords and lacks grammatical structure. 3.4 Loss Functionadapt the softmax cross-entropy loss as the training objective \\hat{y}(i^+|q_u)=\\frac{exp(\\mathcal{F}(q_u,i^{+}))}{\\sum_{i^{'}\\in I }exp(\\mathcal{F}(q_u,i^{'}))} \\\\L(\\nabla )=-\\sum_{i\\in I}y_ilog(\\hat{y_i})where $\\mathcal{F},I,i^+,q_u$denote the inner product, the full item pool, the item tower’s representation $H_{item}$, and the user tower’s representation $H_{qu}$, respectively. 3.4.1 Smoothing Noisy Training Datathe softmax function with the temperature parameter $\\tau$ is defined as follows \\hat{y}(i^+|q_u)=\\frac{exp(\\mathcal{F}(q_u,i^{+})/\\tau)}{\\sum_{i^{'}\\in I }exp(\\mathcal{F}(q_u,i^{'})/\\tau)}If 𝜏-&gt;0, the fitted distribution is close to one hot distribution,If 𝜏-&gt;∞, the fitted distribution is close to a uniform distribution 3.4.2 Generating Relevance-improving Hard Negative SamplesWe first select the negative items of $i^-$ that have the top-𝑁 inner product scores with $q_u $ to form the hard sample set $I_{hard}$ I_{mix}=\\alpha i^++(1-\\alpha)I_{hard}其中$\\alpha\\in \\mathbb{R}^{N\\times1}$is sampled from the uniform distribution 𝑈 (𝑎, 𝑏) (0 ≤ 𝑎 &lt; 𝑏 ≤ 1). \\hat{y}(i^+|q_u)=\\frac{exp(\\mathcal{F}(q_u,i^{+})/\\tau)}{\\sum_{i^{'}\\in (I\\cup I_{mix}) }exp(\\mathcal{F}(q_u,i^{'})/\\tau)}","link":"/2021/09/28/tao-emb-search/"},{"title":"中文文本分类工具","text":"感谢大佬开源 https://github.com/649453932/Chinese-Text-Classification-Pytorch","link":"/2021/11/30/text-classifying/"},{"title":"TextCNN TextRNN TextRCNN","text":"1.TextCNN (Convolutional Neural Networks for Sentence Classification)原文 https://arxiv.org/abs/1408.5882 调参论文 https://arxiv.org/abs/1510.03820 模型的整体结构如上所示。Feature Map是输入图像经过神经网络卷积产生的结果，filter是卷积核。 输入表示： 假设输入文本的长度为$n$，对于长度不够的需要做padding，任意一个单词可以用一个$k$维的向量表示，即$X_i \\in \\mathbb{R}^{k}$，那么一个句子可以表示为 X_{1:n}=X_1 \\oplus X_2\\oplus...\\oplus X_n其中$\\oplus$是向量拼接操作，$X_{1:n} \\in \\mathbb{R}^{nk\\times 1}$。 卷积： 对于某个滑窗$X_{i,i+h-1}=\\{X_i,X_{i+1},…,X_{i+h-1}\\}$经过某个卷积核$W_j$可得 c_{i,j}=f(W_j\\cdot X_{i,i+h-1}+b)其中$f=tanh(\\cdot)$，$W_j\\in \\mathbb{R}^{ 1\\times hk}，c_{i,j} $是标量 假设卷积通道数为$m$，在NLP中，卷积滑动步伐$k=1$，那么经过卷积层后得到的完整的特征矩阵为 C=[[c_{1,1},c_{2,1},...,c_{n-h+1,1}]^T,[c_{1,2},c_{2,2},...,c_{n-h+1,2}]^T,...,[c_{1,m},c_{2,m},...,c_{n-h+1,m}]^T]其中$C \\in \\mathbb{R}^{(n-h+1)\\times m}$ maxpooling： \\hat{C}=max\\{C\\} , \\hat{C}\\in \\mathbb{R}^{m}全连接： 然后将$\\hat{C}$接个全连接，就可以做分类或者回归任务了。 2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)原文 https://www.ijcai.org/Proceedings/16/Papers/408.pdf 该文的场景为Recurrent Neural Network for Text Classification with Multi-Task Learning，就是论文的题目。文中给出了三种结构，如上图所示，图中的RNN单元为LSTM。 Model-I: Uniform-Layer Architecture 对于任务$m$，输入$\\hat X_t$包含两个部分 \\hat{X}_t^{(m)}=X_{t}^{(m)}\\oplus X_{t}^{(s)}其中$X_{t}^{(m)}$表示特定任务的词向量，$X_{t}^{(s)}$表示共享的词向量，$\\oplus$表示向量拼接的操作。 Model-II: Coupled-Layer Architecture \\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \\ \\#原来 \\\\\\downarrow \\\\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+\\sum_{i\\in\\{m,n\\}}g^{(i\\longrightarrow m)}U_c^{(i\\longrightarrow m)}h_{t-1}^{(i)}) \\ \\#现在 \\\\g^{(i\\longrightarrow m)}=\\sigma(W_{g}^{(m)}x_t+U_g^{(i)}h_{t-1}^{(i)})Model-III: Shared-Layer Architecture \\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \\ \\#原来 \\\\\\downarrow \\\\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+g^{(m)}U_c^{(m)}h_{t-1}^{(m)}+g^{(s\\longrightarrow m)}U_c^{(s)}h_{t}^{(s)} \\ \\#现在 \\\\g^{( m)}=\\sigma(W_{g}^{(m)}x_t+U_g^{(m)}h_{t-1}^{(m)}), g^{( s\\longrightarrow m)}=\\sigma(W_{g}^{(m)}x_t+U_g^{(s\\longrightarrow m)}h_{t}^{(s)}), h_t^{(s)}=\\overrightarrow{h_t^{(s)}}\\oplus\\overleftarrow{h_t^{(s)}}3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)原文 https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf 整体结构如上图所示，解释一下为啥叫RCNN，一般的 CNN 网络，都是卷积层 + 池化层，这里是将卷积层换成了双向 RNN，所以结果是，双向 RNN + 池化层。作者原话为：From the perspective of convolutional neural networks, the recurrent structure we previously mentioned is the convolutional layer. 词语表示 对于一个词语$w_i$，可以用一个三元组表示为 x_i=[c_l(w_i);e(w_i);c_r(w_i)]其中$e(w_i)$表示$w_i$的词向量，$c_l(w_i)$表示$w_i$句子左边的内容的向量表示，$c_r(w_i)$表示$w_i$句子右边的内容的向量表示，用式子表示如下 c_l(w_i)=f(W^{l}c_l(w_{i-1})+W^{(sl)}e(w_{i-1})) \\\\c_r(w_i)=f(W^{r}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))然后将$x_i$经过全连接得到$y_i^{(2)}$，$y_i^{(2)}$is a latent semantic vector y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})语句表示 获取众多的词语表示后，通过max-pooling得到句子表示 y^{(3)}=\\mathop{\\max}_{i=1}^{n}y_i^{(2)}然后接全连接和softmax y^{(4)}=W^{(4)}y^{(3)}+b^{(4)} \\\\p=softmax(y^{(4)})参考https://www.cnblogs.com/wangduo/p/6773601.html","link":"/2021/08/23/text-cnn/"},{"title":"text edit","text":"https://blog.csdn.net/qq_27590277/article/details/118534238 https://thinkwee.top/2021/05/11/text-edit-generation/ https://zhuanlan.zhihu.com/p/144995580#:","link":"/2021/11/08/text-edit/"},{"title":"文本生成评价指标","text":"1.BLEUBLEU,全称为bilingual evaluation understudy，一般用于机器翻译和文本生成的评价，比较候选译文和参考译文里的重合程度，重合程度越高就认为译文质量越高，取值范围为[0,1]。 优点 它的易于计算且速度快，特别是与人工翻译模型的输出对比； 它应用范围广泛，这可以让你很轻松将模型与相同任务的基准作对比。 缺点 它不考虑语义，句子结构 不能很好地处理形态丰富的语句（BLEU原文建议大家配备4条翻译参考译文） BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强） 1.1 完整式子BLEU完整式子为： BLEU=BP*e^{\\sum_{n=1}^{N}W_nlogP_{n}}1.2 $BP$目的：$n-gram$匹配度可能会随着句子长度的变短而变好，比如，只翻译了一个词且对了，那么匹配度很高,为了避免这种评分的偏向性,引入长度惩罚因子 Brevity Penalty为长度惩罚因子，其中$l_c$表示机器翻译的译文长度，$l_s$表示参考答案的有效长度 BP=\\begin{equation} \\left\\{ \\begin{aligned} 1 & & if \\ \\ l_c>l_s \\\\e^{1-\\frac{l_s}{l_c}} & & l_c","link":"/2021/07/27/text-generate-evaluate/"},{"title":"Evaluation of Text Generation A Survey","text":"从3个维度将评价指标分类 1 Human-Centric Evaluation Methods ​ gold standard expensive to execute 2 Untrained Automatic Evaluation Metrics widely used 汇总 property： 应该是说这个方法的关注点 3 Untrained Automatic Evaluation Metrics over fitting and `gaming of the metric.’ 参考https://arxiv.org/abs/2006.14799","link":"/2022/05/18/text-generate-metric-survey/"},{"title":"文本匹配","text":"1.无监督1.1 编辑距离定义编辑距离，英文名字为Levenshtein distance，通过描述一个字符串A需要多少次基本操作可以变成字符串B，来衡量两个字符串的相似度。 基本操作包括：增、删、改 增：字符串A为“AS”，字符串B为“ ASD“，字符串A-&gt;字符串B需要增加一个字符“D” 删：字符串A为“ASD”，字符串B为“ AS“，字符串A-&gt;字符串B需要删除一个字符“D” 改：字符串A为“ASX”，字符串B为“ ASD“，字符串A-&gt;字符串B需要将字符“X”变成字符“D” 代码实现过程使用动态规划，递推公式为 lev_{a,b}(i,j)= \\begin{equation} f(x)=\\left\\{ \\begin{aligned} max(i,j) & & if\\ \\min(i,j)=0 \\\\ min\\left\\{ \\begin{aligned} lev_{a,b}(i-1,j)+1 \\\\ lev_{a,b}(i,j-1)+1 \\\\ lev_{a,b}(i-1,j-1)+1_{(a_i\\neq b_j)} \\end{aligned} \\right. \\end{aligned} \\right. \\end{equation}$i$和$j$分别表示字符串$a$和字符串$b$的下标，$lev_{a,b}(i,j)$表示子串$a[:i]$到子串$b[:j]$的编辑距离。 123456789101112131415161718192021222324def lev(str_a,str_b): &quot;&quot;&quot; ED距离，用来衡量单词之间的相似度 :param str_a: :param str_b: :return: &quot;&quot;&quot; str_a=str_a.lower() str_b=str_b.lower() matrix_ed=np.zeros((len(str_a)+1,len(str_b)+1),dtype=np.int) matrix_ed[0]=np.arange(len(str_b)+1) matrix_ed[:,0] = np.arange(len(str_a) + 1) for i in range(1,len(str_a)+1): for j in range(1,len(str_b)+1): # 表示删除a_i dist_1 = matrix_ed[i - 1, j] + 1 # 表示插入b_i dist_2 = matrix_ed[i, j - 1] + 1 # 表示替换b_i dist_3 = matrix_ed[i - 1, j - 1] + (1 if str_a[i - 1] != str_b[j - 1] else 0) #取最小距离 matrix_ed[i,j]=np.min([dist_1, dist_2, dist_3]) print(matrix_ed) return matrix_ed[-1,-1] 1.2 TF-IDF原理（1）TF 针对某个文本 $TF_{word}=\\frac{word在文本中出现的次数}{文本中所有词的总数}$ （2）IDF 针对语料库 $IDF_{word}=log(\\frac{语料库的文本总数}{包含该word的文本数+1})$ （3）TF-IDF $TF-IDF_{word}=TF_{word}*IDF_{word}$ （4）TF-IDF VEC 现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”] $VEC_{A}=[TF-IDF_{今天},TF-IDF_{天气}，TF-IDF_{真好},0,0]$ （5）计算两句话的文本相似度 假设词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”],现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],句子B:”天气不错呀”，分词后[“天气”,”不错呀”] 利用（3）得到句子A的TF-IDF VEC $VEC_{A}$，句子B的TF-IDF VEC $VEC_B$，利用余弦相似度计算文本相似度 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import pandas as pdimport jiebaimport numpy as npfrom sklearn.externals import joblibfrom scipy.linalg import normclass TF_IDF_Model(object): def __init__(self, corpus_list): self.documents_list = corpus_list self.documents_number = len(corpus_list) self.get_idf() def get_idf(self): df = {} self.idf = {} tf = [] for document in self.documents_list: temp = {} for word in document: temp[word] = temp.get(word, 0) + 1 / len(document) tf.append(temp) for key in temp.keys(): df[key] = df.get(key, 0) + 1 for key, value in df.items(): self.idf[key] = np.log10(self.documents_number / (value + 1)) def get_tf(self, document): document = list(jieba.cut(document)) # tf = [] temp = {} for word in document: temp[word] = temp.get(word, 0) + 1 / len(document) # tf.append(temp) return temp def tf_idf_vec(self, text): tf = self.get_tf(text) word = list(self.idf.keys()) vec = [0] * len(self.idf) text = list(jieba.cut(text)) for ele in text: if ele in word: vec[word.index(ele)] = tf[ele] * self.idf[ele] return vec def cal_similarty(self, sentence1, sentence2): vec1 = self.tf_idf_vec(sentence1) vec2 = self.tf_idf_vec(sentence2) similarty = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2)) return similartydef train_model(): #####bulid corpus corpus = pd.read_csv(corpus_path) corpus_list = corpus[&quot;name&quot;].get_values().tolist() # corpus_list = corpus1[&quot;name&quot;].get_values().tolist() corpus_list = [list(jieba.cut(str(doc))) for doc in corpus_list] tf_idf_model = TF_IDF_Model(corpus_list) joblib.dump(tf_idf_model, model_path)def load_model(path): tf_idf_model = joblib.load(path) return tf_idf_modelif __name__ == '__main__': from supercat.data_qualifier.tf_idf import TF_IDF_Model #### train_model() ###### tf_idf_model = load_model(model_path) sentence1=&quot;XXXX&quot; sentence2=&quot;XXXX&quot; print(tf_idf_model.get_tf(sentence1)) print(tf_idf_model.idf) print(tf_idf_model.tf_idf_vec(sentence1)) print(tf_idf_model.cal_similarty(sentence1,sentence2)) 2.有监督基于表示的匹配方法：使用深度学习模型分别表征Query和Doc，通过计算向量相似度来作为语义匹配分数。微软的DSSM[26]及其扩展模型属于基于表示的语义匹配方法，美团搜索借鉴DSSM的双塔结构思想，左边塔输入Query信息，右边塔输入POI、品类信息，生成Query和Doc的高阶文本相关性、高阶品类相关性特征，应用于排序模型中取得了很好的效果。此外，比较有代表性的表示匹配模型还有百度提出 SimNet[27]，中科院提出的多视角循环神经网络匹配模型（MV-LSTM）[28]等。 基于交互的匹配方法：这种方法不直接学习Query和Doc的语义表示向量，而是在神经网络底层就让Query和Doc提前交互，从而获得更好的文本向量表示，最后通过一个MLP网络获得语义匹配分数。代表性模型有华为提出的基于卷积神经网络的匹配模型ARC-II[29]，中科院提出的基于矩阵匹配的的层次化匹配模型MatchPyramid[30]。 基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量，缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。 匹配不同于排序，匹配是1对1的，排序是1对多 2.1基于表示https://zhuanlan.zhihu.com/p/138864580 https://blog.csdn.net/qq_27590277/article/details/121391770 2.2.基于交互https://blog.csdn.net/guofei_fly/article/details/107501276","link":"/2021/10/26/text-matching/"},{"title":"文本改写和term分析","text":"1.文本改写改写主要步骤query纠错、query对齐、query扩展 1.1query纠错在搜索过程中由于对先验知识的掌握不足或者在使用输入法的时候误输入导致的，本质为去噪过程。 常用的query纠错方法有数字、拼音、漏字、重复字、谐音/形近字等方式。 1.2query对齐对于输入query并无错误，但表达上与搜索引擎索引内容不相符而作的一种改写操作。例如“星爷是哪一年生的”，通过实体对齐，可改写为“周星驰的出生时间”。 方法:1.对齐规则 2.文本改写模型 1.3 query扩展是将与用户输入的query的相似扩展query进行展示，使得用户可以选择更多的搜索内容，帮助用户挖掘潜在需求。 2.term分析一段文本分词后，对于不同的词语，在相同文本中的重要性应该是不同的。 baseline的无监督方法可以是：tf-idf。 参考https://zhuanlan.zhihu.com/p/344631739","link":"/2021/10/12/text-mod/"},{"title":"TextGCN Graph Convolutional Networks for Text Classification","text":"https://arxiv.org/abs/1809.05679 1.build a single text graph for a corpus based on word co-occurrence and document word relations, 2.then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents.","link":"/2022/01/19/textgcn/"},{"title":"tensorflow模型导出总结","text":"https://zhuanlan.zhihu.com/p/113734249","link":"/2021/12/29/tf-export/"},{"title":"tf ranking","text":"https://zhuanlan.zhihu.com/p/52447211# https://github.com/tensorflow/ranking","link":"/2021/11/16/tf-ranking/"},{"title":"Tensorflow中的Seq2Seq全家桶","text":"https://zhuanlan.zhihu.com/p/47929039","link":"/2021/11/16/tf-seq2seq/"},{"title":"PyTorch VS TensorFlow","text":"https://zhuanlan.zhihu.com/p/37102973 随着TF2.0出现，TF也有动态图了","link":"/2022/01/18/tf-torch/"},{"title":"tensorflow2.x 和tensorflow1.x对比","text":"tf 1.x : 1. session run 2. 官方推荐 tf.data.Dataset + tf.estimator.Estimator tf 2.x: 官方推荐的是 tf.data.Dataset + tf.keras 参考https://blog.csdn.net/qq_38978225/article/details/108942427 https://blog.csdn.net/keeppractice/article/details/105934521 https://blog.csdn.net/sxlsxl119/article/details/104835420 https://www.zhihu.com/question/267809209","link":"/2021/11/16/tf/"},{"title":"天池新闻推荐","text":"目标: 为不同用户（测试为5万）分别推荐top5的新闻文章（总数36万） 标签：不同用户在不同时间的点击新闻 特征： 整体框架也是：多路召回+排序 召回123456# 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中user_multi_recall_dict = {'itemcf_sim_itemcf_recall': {}, 'embedding_sim_item_recall': {}, 'youtubednn_recall': {}, 'youtubednn_usercf_recall': {}, 'cold_start_recall': {}} 基于itemcf计算的item之间的相似度sim进行的召回 基于embedding搜索得到的item之间的相似度进行的召回 YoutubeDNN召回 YoutubeDNN得到的user之间的相似度进行的召回 基于冷启动策略的召回 排序排序 LGB的排序模型 LGB的分类模型 深度学习的分类模型DIN 模型集成 输出结果加权融合 Staking 参考https://tianchi.aliyun.com/notebook-ai/detail","link":"/2022/06/09/tianchi-recommend/"},{"title":"时间复杂度计算","text":"1 普通https://blog.csdn.net/firefly_2002/article/details/8008987 2 递归https://www.jianshu.com/p/d6b94dac001d# 1 代换法 2 递归树方法 3 主定理 3 回溯https://blog.csdn.net/u013009552/article/details/107064859 https://zhuanlan.zhihu.com/p/62335966","link":"/2021/11/12/time-complex/"},{"title":"时间序列预测总结","text":"https://zhuanlan.zhihu.com/p/67832773 https://cloud.tencent.com/developer/article/1800614 https://blog.csdn.net/weixin_39653948/article/details/105422337 transformer based https://arxiv.org/pdf/2001.08317.pdf https://arxiv.org/pdf/2012.07436.pdf https://arxiv.org/abs/1907.00235 https://arxiv.org/pdf/1912.09363.pdf","link":"/2021/11/22/time-series-survey/"},{"title":"token embedding","text":"https://www.cnblogs.com/d0main/p/10447853.html start=>start: 开始 io=>inputoutput: 输入文本 cond=>condition: 条件 sub=>subroutine: 子流程 end=>end: 结束 op1=>operation: 输入文本 op2=>operation: tokenize op3=>operation: 词向量矩阵（预训练的或者随机初始化） op4=>operation: token embbedding op1->op2->op3->op4{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);","link":"/2021/11/22/token-emb/"},{"title":"Tokenization","text":"对于中文和英文而言，由于语言差异导致算法也有差异。对于中文，存在字粒度和词粒度。对于英文，存在三个级别的粒度，character level，subword level，word level。下面主要阐述中文的词粒度和英文的subword level。 一、中文1.1 原理https://zhuanlan.zhihu.com/p/146792308 1.2 常见中文分词工具12345678910111213141516171819202122232425# #####stanfordcorenlpfrom timeit import default_timer as timerfrom stanfordcorenlp import StanfordCoreNLPtic = timer()path=&quot;XXXXX&quot;nlp_zh = StanfordCoreNLP(path,lang='zh')#模型文件路径sentence = &quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;tic = timer()print ('Tokenize:', nlp_zh.word_tokenize(sentence))toc = timer()print(toc - tic) # 输出的时间，秒为单位#########thulacimport thulacthu1 = thulac.thulac(seg_only=True) #默认模式tic = timer()text = thu1.cut(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;, text=True).split(&quot; &quot;) #进行一句话分词toc = timer()print(text)print(toc - tic)####jiebaimport jiebatic = timer()print(jieba.lcut(str(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;)))toc = timer()print(toc - tic) 123456Tokenize: ['搜索', '引擎', '会', '通过', '日志', '文件', '把', '用户', '每次', '检索', '使用', '的', '所有', '检索', '串都', '记录', '下来']运行时间：22.68650701455772['搜索引擎会', '通过', '日志', '文件', '把', '用户', '每', '次', '检索', '使用', '的', '所有', '检索', '串', '都', '记录', '下', '来']运行时间：0.0016864966601133347['搜索引擎', '会', '通过', '日志', '文件', '把', '用户', '每次', '检索', '使用', '的', '所有', '检索', '串', '都', '记录下来']运行时间：0.9094752036035061 观察结果，可以看出thulac分词效率最高，jieba分词的精度和效率比较平衡，stanfordcorenlp分词粒度很细，但是速度慢 二、英文SubWord算法如今已成为一个重要的NLP模型的提升算法。其主要优势如下： 1.word level存在OOV问题，一旦碰到就是back off to a dictionary，无法很好地处理未知和罕见词汇2.Character level可以解决OOV，但是相比于 word-level , Character-level 的输入句子变长，使得数据变得稀疏，而且对于远距离的依赖难以学到，训练速度降低。常见的SubWord算法有：BPE，WordPiece，Unigram Language Model等 2.1 BPE全称为Byte Pair Encoding，算法来自paper《Neural Machine Translation of Rare Words with Subword Units》。 2.1.1 构建BPE subword词表原理 准备足够大的训练语料 确定期望的subword词表大小 将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”并且统计单词频率。停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。具体来说，不加”&lt;/w&gt;”可以出现在词首，加了”&lt;/w&gt;”只能位于词尾。例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5 统计每一个连续字节对的出现频率，选择最高频者合并成新的subword 重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1 注意，每次合并后词表可能出现3种变化： +1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词都不是完全随着另一个字词的出现而紧跟着出现） +0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（只有一个字词完全随着另一个字词的出现而紧跟着出现） -1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现） 例子： 训练语料为： 1{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w e s t &lt;/w&gt;': 6, 'w i d e s t &lt;/w&gt;': 3} Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”，输出： 1{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w es t &lt;/w&gt;': 6, 'w i d es t &lt;/w&gt;': 3} Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”，输出： 1{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est &lt;/w&gt;': 6, 'w i d est &lt;/w&gt;': 3} Iter 3, 以此类推，最高频连续字节对为”est”和”&lt;/w&gt;” ，合并后输出： 1{'l o w &lt;/w&gt;': 5, 'l o w e r &lt;/w&gt;': 2, 'n e w est&lt;/w&gt;': 6, 'w i d est&lt;/w&gt;': 3} …… Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。 代码 12345678910111213141516171819202122232425import re, collectionsdef get_stats(vocab): pairs = collections.defaultdict(int) for word, freq in vocab.items(): symbols = word.split() for i in range(len(symbols)-1): pairs[symbols[i],symbols[i+1]] += freq return pairsdef merge_vocab(pair, v_in): v_out = {} bigram = re.escape(' '.join(pair)) p = re.compile(r'(?&lt;!\\S)' + bigram + r'(?!\\S)') for word in v_in: w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_outvocab = {'l o w &lt;/w&gt;' : 5, 'l o w e r &lt;/w&gt;' : 2,'n e w e s t &lt;/w&gt;':6, 'w i d e s t &lt;/w&gt;':3}num_merges = 10for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) 12345678910('e', 's')('es', 't')('est', '&lt;/w&gt;')('l', 'o')('lo', 'w')('n', 'e')('ne', 'w')('new', 'est&lt;/w&gt;')('low', '&lt;/w&gt;')('w', 'i') 2.1.2 编解码编码 1.将subword词表按照子词长度由大到小排序。 2.对于每个单词，遍历排好序的subword词表，寻找是否有token是当前单词的子字符串。最终，我们将迭代所有token，并将所有子字符串替换为token。 3.如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子字符串替换为特殊token，如。 例子： 12345678910# 给定单词序列[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]# 假设已有排好序的subword词表[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]# 迭代结果&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;] 编码的计算量很大。对于已知数据，我们可以pre-tokenize所有单词，并在词典中保存单词和tokenize的结果。如果存在字典中不存在的未知单词，可以应用上述编码方法对单词进行tokenize，然后将新单词以及tokenize的结果添加到字典中备用。 解码 将所有的subword拼在一起。 例子： 12345# 编码序列[[“the&lt;/w&gt;”], [“high”, “est&lt;/w&gt;”], [“moun”, “tain&lt;/w&gt;”]]# 解码序列[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”] 2.1.3 和embedding结合1.构建词表，假设有subword词表：[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”] 2.编码，词语”highest”编码成[“high”, “est&lt;/w&gt;”] 3.向量表示，$[E_{high},\\ E_{est(/w)}]$] https://www.cnblogs.com/d0main/p/10447853.html 2.2 WordPiece算法来自于《Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》 WordPiece算法可以看作是BPE的变种。不同点在于，WordPiece基于概率生成新的subword而不是下一最高频字节对。 2.2.1 原理 准备足够大的训练语料 确定期望的subword词表大小 将单词拆分成字符序列 基于第3步数据训练语言模型 从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元 重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值 2.3 ULM2.4 char n-gramhttps://arxiv.org/abs/1607.04606 2.5 Byte-Level BPE《Neural Machine Translation with Byte-Level Subwords》 3.总结我们在进行中文NLP任务的时候，目前基本都是字粒度；英文的话大多数是使用subword的wordpiece。 参考https://zhuanlan.zhihu.com/p/112444056 https://arxiv.org/pdf/1508.07909.pdf https://zhuanlan.zhihu.com/p/38130825 https://zhuanlan.zhihu.com/p/86965595 https://blog.csdn.net/zhangxiaolinxin/article/details/107052054?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&amp;spm=1001.2101.3001.4242 https://www.cnblogs.com/mj-selina/p/13687291.html","link":"/2021/07/20/tokenization/"},{"title":"TensorFlow，pytorch，cuda，cudnn，显卡驱动之间的区别以及对应关系","text":"一.概念理解显卡驱动连接操作系统与底层硬件。 CUDA和NVIDIA的显卡驱动程序完全是两个不同的概念。CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。CUDA的本质是一个工具包（ToolKit）。 cuDNN是一个SDK，是一个专门用于神经网络的加速包，注意，它跟我们的CUDA没有一一对应的关系，即每一个版本的CUDA可能有好几个版本的cuDNN与之对应，但一般有一个最新版本的cuDNN版本与CUDA对应更好。 TensorFlow为谷歌推出的深度学习框架，pytorch是Facebook 推出的深度学习框架。 二.版本对应关系深度学习框架基于GPU运算效率远高于CPU，但是需要满足框架的版本和cuda，cudnn以及显卡驱动版本匹配才可以正常工作。 tensorflow pytorch cuDNN与CUDA CUDA和NVIDIA显卡驱动关系 三.常用命令查看GPU型号 1lspci | grep -i nvidia 查看NVIDIA驱动版本 1cat /proc/driver/nvidia/version Python 查看pytorch版本、判断CUDA是否可用 123import torch print(torch.__version__) print(torch.cuda.is_available()) 查看cuda版本 12cat /usr/local/cuda/version.txtconda list | grep cuda Tensorflow中查看GPU是否可用 12import tensorflow as tftf.test.is_gpu_available() 四.参考文献https://blog.csdn.net/caiguanhong/article/details/112184290","link":"/2021/07/20/torch-cuda/"},{"title":"pytorch GPU训练","text":"模型，数据需要指定设备 单机单卡单机多卡https://blog.csdn.net/weixin_38208912/article/details/105122668 https://blog.csdn.net/leviopku/article/details/109318226 https://blog.csdn.net/weixin_43750248/article/details/115698356 https://zhuanlan.zhihu.com/p/74792767 多机多卡","link":"/2022/01/17/torch-multi-GPU/"},{"title":"pytorch常见操作","text":"1 pytorch中对tensor操作https://blog.csdn.net/HailinPan/article/details/109818774 2 模型加载1 model.load_state_dict(torch.load(path)) 2 model=BertModel.from_pretrained 后者的底层为前者 用法不同，前者model为一个对象，然后用load_state_dict加载权重；后者BertModel为一个类，然后用from_pretrained创建对象并加载权重","link":"/2021/12/09/torch-normal/"},{"title":"torch可以把string变Tensor吗？","text":"NO. there is no string tensor so you cannot directly convert string to tensor","link":"/2022/01/13/torch-notice/"},{"title":"pytorch搭建神经网络","text":"0.准备数据，处理数据1.搭建网络结构https://www.cnblogs.com/tian777/p/15341522.html 12345678910111213141516171819202122232425262728293031323334353637class pointwise_hybird_contrasive(hybird): def __init__(self,config_roberta, path,num): super(pointwise_hybird_contrasive, self).__init__(config_roberta, path,num) # self.softmax=torch.nn.Softmax() # self.CrossEntropyLoss=torch.nn.CrossEntropyLoss() # self.FFN2= # self.softmax = nn.Softmax(dim=1) return def forward(self, input_ids, input_mask, segment_ids, all_en_query, all_en_ans): ch_match_embedding = self.ch_matching_model(input_ids, input_mask, segment_ids) en_match_embedding = self.en_matching_model(all_en_query, all_en_ans) hybird_represent = torch.cat([ch_match_embedding, en_match_embedding], dim=1) output = self.FFN2(self.relu(self.FFN1(self.dropout(hybird_represent)))) # y_pred_prob, y_pred = torch.max(self.softmax(output.data), 1) return output def loss(self,predict,target): # predict=predict.reshape(-1,target.shape[1]) # predict = torch.squeeze(predict, dim=1) # predict=torch.unsqueeze(predict, dim=0) # target=torch.argmax(target,dim=1) # target= torch.unsqueeze(target, dim=0) # self.loss(predict,target) CrossEntropyLoss=torch.nn.CrossEntropyLoss() return CrossEntropyLoss(predict,target) def predict(self, output): softmax = nn.Softmax(dim=1) y_pred_prob, y_pred = torch.max(softmax(output.data), 1) # y_pred = y_pred.cpu().numpy() y_pred_prob = y_pred_prob.cpu().numpy() for i in range(len(y_pred_prob)): if not y_pred[i]: y_pred_prob[i] = 1 - y_pred_prob[i] return y_pred_prob nn.Module https://www.cnblogs.com/tian777/p/15341522.html 1 init2 forward3 losspytorch各种交叉熵函数的汇总具体使用 https://blog.csdn.net/comway_Li/article/details/121490170 L2和L1正则化 https://blog.csdn.net/guyuealian/article/details/88426648 优化器固定实现L2正则化,源码注释：weight_decay (:obj:float, optional, defaults to 0):Weight decay (L2 penalty) 123456param_optimizer = list(model.named_parameters())no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']optimizer_grouped_parameters = [ {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01}, {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}] 4 predict2.构建训练框架a.迭代器Dataset/TensorDataset -》 Sampler -》 Dataloader https://zhuanlan.zhihu.com/p/337850513# https://blog.csdn.net/ljp1919/article/details/116484330 https://blog.csdn.net/qq_39507748/article/details/105385709 b.优化器https://pytorch.org/docs/stable/optim.html 12345optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)optimizer=optim.SGD([ {'params': model.base.parameters()}, {'params': model.classifier.parameters(), 'lr': 1e-3} ], lr=1e-2, momentum=0.9) c 训练optimizer.zero_grad() 梯度归零, loss.backward() 反向传播 , optimizer.step() 参数更新 https://blog.csdn.net/PanYHHH/article/details/107361827 d. 验证with torch.no_grad() 验证，测试时候用：可显著减少显存占用 https://wstchhwp.blog.csdn.net/article/details/108405102 https://blog.csdn.net/weixin_44134757/article/details/105775027 e. 评价指标f. 模型保存https://blog.csdn.net/m0_37605642/article/details/120325062 https://blog.csdn.net/weixin_41278720/article/details/80759933 g. 可视化https://blog.csdn.net/Wenyuanbo/article/details/118937790 3.预测加载模型，输入数据，调用网络结构 参考https://blog.csdn.net/qq_45847624/article/details/114885655","link":"/2021/11/10/torch_buildnet/"},{"title":"模型联合训练","text":"https://www4.comp.polyu.edu.hk/~csxmwu/papers/KDD-2021-Taobao-Search.pdf user tower（model1），item tower（model2） 利用softmax cross entropy as the trainning objective","link":"/2022/05/11/train-union/"},{"title":"训练,验证同步进行","text":"参数没变，只是选择了训练中验证最好的模型，类似early stop 不同时行不行？不同时可能无法得到最优的step，有可能过拟合","link":"/2022/06/01/train-with-valid/"},{"title":"transformer综述","text":"Transformer-XL https://arxiv.org/abs/1901.02860v3 RoFormer https://arxiv.org/pdf/2104.09864.pdf google2020出品的transformer的综述 https://arxiv.org/pdf/2009.06732.pdf","link":"/2021/10/26/transformer-survey/"},{"title":"Transformer时间序列预测","text":"1.基本的Transformer https://zhuanlan.zhihu.com/p/360829130 2.改进的Transformer Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting https://arxiv.org/pdf/1907.00235.pdf https://zhuanlan.zhihu.com/p/391337035","link":"/2021/10/29/transformer-time-seties/"},{"title":"Transformer-XL  Attentive Language Models Beyond a Fixed-Length Context","text":"https://arxiv.org/abs/1901.02860 Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling ( memory and computation受限，长度不可能很大 ). propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. 3 Model3.1 Vanilla Transformer Language Models 问题：In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.通常做法为vanilla model。 vanilla model就是说把长文本分隔成固定长度的seg来处理，如上图。 During training，There are two critical limitations of using a fixed length context. First, the largest possible dependency length is upper bounded by the segment length. Second. simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem During evaluation, As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. 3.2 Segment-Level Recurrence with State Reuse introduce a recurrence mechanism to the Transformer architecture. 定义变量 转换过程 SG(） stands for stop-gradient，$\\circ$ 表示矩阵拼接 具体过程如下图 During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a. during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. 3.3 Relative Positional Encodingshow can we keep the positional information coherent when we reuse the states? 如果保留原来的位置编码形式，可以得到如下 这种方式存在问题： 为了解决这个问题提出了relative positional information。 standard Transformer we propose 3.4 完整算法流程","link":"/2021/12/06/transformer-xl/"},{"title":"transformer(attention is all your need)","text":"1.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. 2.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. 1 Positional Encodingin order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies: PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\\\ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})详细可参考 https://wmathor.com/index.php/archives/1453/ 2 Attention 其中不同颜色表示不同head，颜色深浅表示词的关联程度。 不同head表示不同应用场景 ，单一head表示某个场景下，各个字之间的关联程度 1 Scaled Dot-Product Attention Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_{k}}})V$d_{k}$ ： keys of dimension 为什么scale？We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. Mask 可以分为两类：Attention Mask和Padding Mask，接下来具体讲解。 1.Attention Mask ensures that the predictions for position i can depend only on the known outputs at positions less than i. sotfmax前要mask，上三角mask掉 2.Padding Mask Padding位置上的信息是无效的，所以需要丢弃。 过程如下图示： 2 Multi-Head Attention Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. MultiHead(Q,K,V)=Concat(head1,...,head_n)W^O \\\\ where \\ head_{i}=Attetion(QW_{i}^{Q},kW_{i}^{K},VW_{i}^{V})3 Applications of Attention in our Model1.encoder-decoder attention layers 结构：queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder. 目的：This allows every position in the decoder to attend over all positions in the input sequence. 2.encoder contains self-attention layers 结构：keys, values and queries come from the same place 目的：Each position in the encoder can attend to all positions in the previous layer of the encoder. 3.self-attention layers in the decoder 结构：keys, values and queries come from the same place 目的：allow each position in the decoder to attend to all positions in the decoder up to and including that position 3 Encoder and Decoder Stacks1 encoder1).Input Embedding与Positional Encoding X = \\text{Input Embedding}+ \\text{Positional Encoding}\\\\2). multi-head attention Q = \\text{Linear}_q(X) = XW_{Q}\\\\ K = \\text{Linear}_k(X) = XW_{K}\\\\ V = \\text{Linear}_v(X) = XW_{V}\\\\ X_{attention} = \\text{Attention}(Q,K,V)3). 残差连接与 Layer Normalization X_{attention} = X + X_{attention}\\\\ X_{attention} = \\text{LayerNorm}(X_{attention})4). FeedForward X_{hidden} = \\text{Linear}(\\text{ReLU}(\\text{Linear}(X_{attention})))5). 残差连接与 Layer Normalization X_{hidden} = X_{attention} + X_{hidden}\\\\ X_{hidden} = \\text{LayerNorm}(X_{hidden})其中$ X_{hidden} \\in \\mathbb{R}^{batch_size \\ \\ seq_len \\ \\ embed_dim} $ 2 decoder我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是： Masked Multi-Head Self-Attention Multi-Head Encoder-Decoder Attention FeedForward Network 4 常见问题1 并行化 训练encoder，decoder都并行，测试encoder并行，decoder不是并行 https://zhuanlan.zhihu.com/p/368592551 2 self-attention和普通attention的区别 取决于query和key是否在一个地方 3 Why Self-Attention Motivating our use of self-attention we consider three desiderata. 1.One is the total computational complexity per layer. 2.Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. 3.The third is the path length between long-range dependencies in the network 参考https://arxiv.org/abs/1706.03762 大佬详解： https://jalammar.github.io/illustrated-transformer/","link":"/2021/07/18/transformer/"},{"title":"字典树","text":"一.核心思想Trie tree，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较。Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。字典树的查询时间复杂度是O (L)，L是待查字符串的长度。如果是普通的线性表结构，那么查询效率为O（NL），N为待查数据集的大小。 假设有b，abc，abd，bcd，abcd，efg，hii 这6个单词,那我们创建字典树如下： 二.应用目的：利用汉语拼音缩写还原中文汉字 准备：数据集（包含中文汉字以及对应汉语缩写） 思想：1.基于汉语拼音缩写检索出数据集中对应的所有中文汉字 2.基于中文汉字出现频次排序，将top1作为汉语拼音的还原结果 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import pandas as pdimport pickleimport osimport joblibclass TrieNode(object): def __init__(self): &quot;&quot;&quot; Initialize your data structure here. &quot;&quot;&quot; self.data = {}###字母字符 self.data1={}###中文 self.is_word = False###标识是否汉字class Trie(object): def __init__(self): self.root = TrieNode() def insert(self, word,word1): &quot;&quot;&quot; Inserts a word into the trie. :type word: str :rtype: void &quot;&quot;&quot; node = self.root for letter in word: child = node.data.get(letter) if not child: node.data[letter] = TrieNode() node = node.data[letter] node.is_word = True if word1 not in node.data1: node.data1[word1]=1 else: node.data1[word1]+=1 def search(self, word): &quot;&quot;&quot; Returns if the word is in the trie. :type word: str :rtype: bool &quot;&quot;&quot; node = self.root for letter in word: node = node.data.get(letter) if not node: return False return node.is_word def starts_with(self, prefix): &quot;&quot;&quot; Returns if there is any word in the trie that starts with the given prefix. :type prefix: str :rtype: bool &quot;&quot;&quot; node = self.root for letter in prefix: node = node.data.get(letter) if not node: return False return True def get_start(self, prefix): &quot;&quot;&quot; Returns words started with prefix :param prefix: :return: words (list) &quot;&quot;&quot; def _get_key(pre, pre_node): words_list = [] if pre_node.is_word: words_list.append([pre,pre_node.data1]) for x in pre_node.data.keys(): words_list.extend(_get_key(pre + str(x), pre_node.data.get(x))) return words_list words = [] if not self.starts_with(prefix): return words # if self.search(prefix): # words.append(prefix) # return words node = self.root for letter in prefix: node = node.data.get(letter) return _get_key(prefix, node) def find_result(self,string): result =self.get_start(string) result = sort_by_value(result[0][1]) result.reverse() return result[0] def sort_by_value(d): return sorted(d.items(), key=lambda k: k[1]) # k[1] 取到字典的值。def build_tree(data,save_path): trie = Trie() for element in data.values: trie.insert(element[0], element[1]) joblib.dump(trie, save_path) returndef load_tree(path): trie = joblib.load(path) return trieif __name__ == '__main__': ### build_tree(data,save_path) ### tree=load_tree(save_path) print(tree.find_result(&quot;XXXXXXXX&quot;)) 参考https://zhuanlan.zhihu.com/p/28891541","link":"/2021/08/02/trie-tree/"},{"title":"双指针","text":"指针$i$，指针$j$，序列长度为$n$ 1 $O(n^2)$ $i$总共遍历$n$，$j$总共遍历$n^2$ 1 $O(n )$ $i$总共遍历$n$，$j$总共遍历$n$","link":"/2022/06/16/two-pointer/"},{"title":"text Span抽取","text":"基于问题在段落中寻找答案 1231 问题：苏轼是哪里人？2 描述：苏轼是北宋著名的文学家与政治家，眉州眉山人。3 标签：眉州眉山人 bert中的SQuAD问答任务 标签引入start 和 end 标签 结构 损失123456789sequence_output = all_encoder_outputs[-1] #[src_len, batch_size, hidden_size]logits = self.qa_outputs(sequence_output) # [src_len, batch_size,2] start_logits, end_logits = logits.split(1, dim=-1)start_logits = start_logits.squeeze(-1).transpose(0, 1) # [batch_size,src_len]end_logits = end_logits.squeeze(-1).transpose(0, 1) # [batch_size,src_len]loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)start_loss = loss_fct(start_logits, start_positions)end_loss = loss_fct(end_logits, end_positions)final_loss=(start_loss + end_loss) / 2 模型输出为： [src_len, batch_size,2] 两个（start 和 end ）src_len分类的平均 预测假设候选文本长度为n，输出n个2分类结果，选出最大的start概率和end概率最为start和end label 参考https://zhuanlan.zhihu.com/p/77868938 https://blog.csdn.net/guangyacyb/article/details/105526482 https://zhuanlan.zhihu.com/p/473157694","link":"/2022/06/11/txt-span/"},{"title":"处理无界和有界数据","text":"https://flink.apache.org/zh/flink-architecture.html 任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。 数据可以被作为无界或者有界流来处理。 无界流 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。 有界流 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理 Apache Flink 擅长处理无界和有界数据集精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。","link":"/2022/03/20/unboundstream-boundstream/"},{"title":"可视化报表","text":"https://zhuanlan.zhihu.com/p/410170345","link":"/2022/02/09/visrion-report/"},{"title":"各组件web ui的地址","text":"https://blog.csdn.net/qq_41851454/article/details/79938811 node ip+port yarnresource maneger +8088 hdfsnamenode +50070/9870/9871 spark4040: 是一个运行的Application在运行的过程中临时绑定的端口,用以查看当前任务的状态.4040被占用会顺延到4041.4042等.4040是一个临时端口,当前程序运行完成后, 4040就会被注销哦。当使用spark交互工具，如spark-sql,spark-shell 8080: 默认是StandAlone下, Master角色(进程)的WEB端口,用以查看当前Master(集群)的状态 18080: 默认是历史服务器的端口, 由于每个程序运行完成后,4040端口就被注销了. 在以后想回看某个程序的运行状态就可以通过历史服务器查看,历史服务器长期稳定运行,可供随时查看被记录的程序的运行过程. 配置历史服务器 https://blog.csdn.net/Heitao5200/article/details/79674684 https://blog.csdn.net/yu0_zhang0/article/details/80396080 注意端口号和hadoop一致，9000-&gt;8020 flinkApache Flink runs the dashboard on port 8081. Since this is a common port there might be conflict with some other services running on the same machines port和端口可以在flink/conf/flink-conf.yaml 中查看 hive metastore端口9083 hbase16010","link":"/2022/03/01/web-ui/"},{"title":"权重初始化","text":"参数初始权重为什么不全0或者任意相同值 某一层任意一个神经元 \\\\ z=W_{1\\times m}X_{m\\times 1}+b_{1\\times1}如果我们将神经网络中的权重集初始化为零或者相同，那么同一层的所有神经元将在反向传播期间开始产生相同的输出和相同的梯度。导致同一层每个神经元完全一样，等价于只有一个 常用的三种权值初始化方法 随机初始化、Xavier initialization、He initialization 参考https://mdnice.com/writing/6fe7dfe1954945d180d6b36562658af8 https://m.ofweek.com/ai/2021-06/ART-201700-11000-30502442.html https://blog.csdn.net/qq_15505637/article/details/79362970","link":"/2021/11/26/weiht-init/"},{"title":"nlp中使用预训练的词向量和随机初始化的词向量的区别在哪里？","text":"当你训练数据不充足的时候，可以直接使用别人已经预训练好的词向量，也可以根据自己的训练数据微调(fine-tuning)预训练词向量，也可以把词向量和整个模型一块训练，但是通常预训练的词向量我们不会再在训练的过程中进行更新。 当你的训练数据比较充足的时候，并且想让词向量能更好的捕捉自己的训练数据的语义信息时，应该使用随机初始化的词向量。当然，随机初始化的词向量必须要在训练网络的过程中不断进行更新，就和神经网络的权重参数一样进行训练。 例子： 1.直观展示 123456789101112import torchfrom torch import nnfrom torch.autograd import Variable###randomembeds = nn.Embedding(2, 5) print(embeds.weight)embeds = nn.Embedding(2, 5) print(embeds.weight)###from pretrainweight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])embedding = nn.Embedding.from_pretrained(weight)print(embedding.weight) 123456789Parameter containing:tensor([[-0.1754, 1.6604, -1.5025, -1.0980, -0.4718], [-1.1276, 0.1408, -1.0746, -1.2768, -0.6789]], requires_grad=True)Parameter containing:tensor([[-0.7366, 0.0607, 0.6151, 0.2282, 0.3878], [-1.1365, 0.1844, -1.1191, -0.8787, -0.5121]], requires_grad=True)Parameter containing:tensor([[1.0000, 2.3000, 3.0000], [4.0000, 5.1000, 6.3000]]) 2.n-gram 123self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed) 参考https://www.zhihu.com/question/337950427","link":"/2021/07/28/word-emb-add/"},{"title":"文本表示","text":"文本表示的表示形式可以是单一数值（基本没人用），可以是向量（目前主流），好奇有没有高纬tensor表示的？下文是基于向量表示的。 1.词语表示1.1 one hot举个例子，有样本如下： ​ Jane wants to go to Shenzhen. ​ Bob wants to go to Shanghai. 基于上述两个文档中出现的单词，构建如下一个词典： Vocabulary= [Jane, wants, to, go, Shenzhen, Bob, Shanghai] 那么wants 可以表示为 1[0,1,0,0,0,0,0] 1.2 word embedding词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度的向量空间当中，使得语意相似的词在向量空间上也会比较相近，举个例子，如 上表为词向量矩阵，其中行表示不同特征，列表示不同词，Man可以表示为 1[-1,0.01,0.03,0.09] 性质：$emb_{Man}-emb_{Women}\\approx emb_{King}-emb_{Queen}$ 常见的词向量矩阵构建方法有，word2vec，GloVe 2.句子表示2.1 词袋模型词袋模型不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。 例句: ​ Jane wants to go to Shenzhen. ​ Bob wants to go to Shanghai. 基于上述两个文档中出现的单词，构建如下一个词典： Vocabulary= [Jane, wants, to, go, Shenzhen, Bob, Shanghai] 那么上面两个例句就可以用以下两个向量表示，其值为该词语出现的次数： 12[1,1,2,1,1,0,0][0,1,2,1,0,1,1] 2.2 Sentence Embedding2.2.1 评价工具SentEval is a popular toolkit to evaluate the quality of sentence embeddings. 2.2.2 常见方法sentence BERT BERT-flow https://zhuanlan.zhihu.com/p/444346578 参考文献https://zhuanlan.zhihu.com/p/353187575 https://www.jianshu.com/p/0587bc01e414 https://www.cnblogs.com/chenyusheng0803/p/10978883.html","link":"/2021/07/19/word-representation/"},{"title":"词语的文本相似度","text":"一.基于词典人为构建，比较主观，不利于维护 1.1 基于词林1.1.1 结构扩展版同义词词林分为5层结构，如图，随着级别的递增，词义刻画越来越细，到了第五层，每个分类里词语数量已经不大，很多只有一个词语，已经不可再分，可以称为原子词群、原子类或原子节点。不同级别的分类结果可以为自然语言处理提供不同的服务，例如第四层的分类和第五层的分类在信息检索、文本分类、自动问答等研究领域得到应用。有研究证明，对词义进行有效扩展，或者对关键词做同义词替换可以明显改善信息检索、文本分类和自动问答系统的性能。 下载后的词典文件如下所示： 12345Aa01A01= 人 士 人物 人士 人氏 人选Aa01A02= 人类 生人 全人类Aa01A03= 人手 人员 人口 人丁 口 食指Aa01A04= 劳力 劳动力 工作者Aa01A05= 匹夫 个人 表中的编码位是按照从左到右的顺序排列。第八位的标记有3 种，分别是“=”、“#”、“@”， “=”代表“相等”、“同义”。末尾的“#”代表“不等”、“同类”，属于相关词语。末尾的“@”代表“自我封闭”、“独立”，它在词典中既没有同义词，也没有相关词。 源码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133class WordSimilarity2010(SimilarBase): ''' 本类根据下面的论文方法： 基于同义词词林的词语相似度计算方法，田久乐, 赵 蔚(东北师范大学 计算机科学与信息技术学院, 长春 130117 ) 计算两个单词所有编码组合的相似度，取最大的一个 ''' def __init__(self): super(WordSimilarity2010, self).__init__() self.a = 0.65 self.b = 0.8 self.c = 0.9 self.d = 0.96 self.e = 0.5 self.f = 0.1 self.degree = 180 self.PI = math.pi def similarity(self, w1, w2): ''' 判断两个词的相似性。 :param w1: [string] :param w2: [string] :return: [float]0~1之间。 ''' code1 = self._data.get(w1, None) code2 = self._data.get(w2, None) if not code1 or not code2: return 0 # 只要有一个不在库里则代表没有相似性。 # 最终返回的最大相似度 sim_max = 0 # 两个词可能对应多个编码 for c1 in code1: for c2 in code2: cur_sim = self.sim_by_code(c1, c2) # print(c1, c2, '的相似度为：', cur_sim) if cur_sim &gt; sim_max: sim_max = cur_sim return sim_max def sim_by_code(self, c1, c2): &quot;&quot;&quot; 根据编码计算相似度 &quot;&quot;&quot; # 先把code的层级信息提取出来 clayer1 = self._parse_code(c1) clayer2 = self._parse_code(c2) common_layer = self.get_common_layer(clayer1,clayer2) length = len(common_layer) # 如果有一个编码以'@'结尾，那么表示自我封闭，这个编码中只有一个词，直接返回f if c1.endswith('@') or c2.endswith('@') or 0 == length: return self.f cur_sim = 0 if 6 &lt;= length: # 如果前面七个字符相同，则第八个字符也相同，要么同为'='，要么同为'#'' if c1.endswith('=') and c2.endswith('='): cur_sim = 1 elif c1.endswith('#') and c2.endswith('#'): cur_sim = self.e else: k = self.get_k(clayer1, clayer2) n = self.get_n(common_layer) if 1 == length: cur_sim = self.sim_formula(self.a, n, k) elif 2 == length: cur_sim = self.sim_formula(self.b, n, k) elif 3 == length: cur_sim = self.sim_formula(self.c, n, k) elif 4 == length: cur_sim = self.sim_formula(self.d, n, k) return cur_sim def sim_formula(self, coeff, n, k): &quot;&quot;&quot; 计算相似度的公式，不同的层系数不同 &quot;&quot;&quot; return coeff * math.cos(n * self.PI / self.degree) * ((n - k + 1) / n) def get_common_layer(self, ca, cb): ''' 返回相应的layer层 :param ca: [list(str)] 分解后的编码。 :param cb: [list(str)] 分解后的编码。 :return: [list(str)]列表代表相应的根编码。 ''' common_layer = [] for i, j in zip(ca, cb): if i == j: common_layer.append(i) else: break return common_layer def get_k(self, c1, c2): &quot;&quot;&quot; 返回两个编码对应分支的距离，相邻距离为1 &quot;&quot;&quot; if c1[0] != c2[0]: return abs(ord(c1[0]) - ord(c2[0])) elif c1[1] != c2[1]: return abs(ord(c1[1]) - ord(c2[1])) elif c1[2] != c2[2]: return abs(int(c1[2]) - int(c2[2])) elif c1[3] != c2[3]: return abs(ord(c1[3]) - ord(c2[3])) else: return abs(int(c1[4]) - int(c2[4])) def get_n(self, common_layer): ''' 返回相应结点下有多少个同级子结点。 :param common_layer: [listr(str)]相同的结点。 :return: int ''' end_node = self._code_tree for t_node_name in common_layer: end_node = end_node[t_node_name] if not isinstance(end_node, dict): return end_node return len(end_node.keys()) 1.1.2 使用环境准备：pip install WordSimilarity 1234567891011121314151617from word_similarity import WordSimilarity2010import timews_tool = WordSimilarity2010()start = time.time()b_a = &quot;联系方式&quot;b_b = &quot;电话&quot;sim_b = ws_tool.similarity(b_a, b_b)print(b_a, b_b, '相似度为', sim_b)end = time.time()print(&quot;运行时间：&quot;+str(end-start))b_a = &quot;手机&quot;b_b = &quot;电话&quot;sim_b = ws_tool.similarity(b_a, b_b)print(b_a, b_b, '相似度为', sim_b)end = time.time()print(&quot;运行时间：&quot;+str(end-start)) 1234联系方式 电话 相似度为 0运行时间：5.793571472167969e-05手机 电话 相似度为 0.30484094213212237运行时间：0.0001442432403564453 1.2 基于知网与词林的词语语义相似度计算1.2.1 原理综合了词林cilin与知网hownet的相似度计算方法，采用混合策略，混合策略具体可以参考源码，如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485from hownet.howNet import How_Similarityfrom cilin.V3.ciLin import CilinSimilarityclass HybridSim(): ''' 混合相似度计算策略。使用了词林与知网词汇量的并集。扩大了词汇覆盖范围。 ''' ci_lin = CilinSimilarity() # 实例化词林相似度计算对象 how_net = How_Similarity() # 实例化知网相似度计算对象 Common = ci_lin.vocab &amp; how_net.vocab A = how_net.vocab - ci_lin.vocab B = ci_lin.vocab - how_net.vocab @classmethod def get_Final_sim(cls, w1, w2): lin = cls.ci_lin.sim2018(w1, w2) if w1 in cls.ci_lin.vocab and w2 in cls.ci_lin.vocab else 0 how = cls.how_net.calc(w1, w2) if w1 in cls.how_net.vocab and w2 in cls.how_net.vocab else 0 if w1 in cls.Common and w2 in cls.Common: # 两个词都被词林和知网共同收录。 # print('两个词都被词林和知网共同收录。', end='\\t') # print(w1, w2, '词林改进版相似度：', lin, end='\\t') # print('知网相似度结果为：', how, end='\\t') return lin * 1 + how * 0 # 可以调节两者的权重，以获取更优结果！！ if w1 in cls.A and w2 in cls.A: # 两个词都只被知网收录。 return how if w1 in cls.B and w2 in cls.B: # 两个词都只被词林收录。 return lin if w1 in cls.A and w2 in cls.B: # 一个只被词林收录，另一个只被知网收录。 print('触发策略三，左词为知网，右词为词林') same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]] if not same_words: return 0.2 all_sims = [cls.how_net.calc(word, w1) for word in same_words] print(same_words, all_sims, end='\\t') return max(all_sims) if w2 in cls.A and w1 in cls.B: print('触发策略三，左词为词林，右词为知网') same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]] if not same_words: return 0.2 all_sims = [cls.how_net.calc(word, w2) for word in same_words] print(w1, '词林同义词有：', same_words, all_sims, end='\\t') return max(all_sims) if w1 in cls.A and w2 in cls.Common: print('策略四（左知网）：知网相似度结果为：', how) same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]] if not same_words: return how all_sims = [cls.how_net.calc(word, w1) for word in same_words] print(w2, '词林同义词有：', same_words, all_sims, end='\\t') return 0.6 * how + 0.4 * max(all_sims) if w2 in cls.A and w1 in cls.Common: print('策略四（右知网）：知网相似度结果为：', how) same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]] if not same_words: return how all_sims = [cls.how_net.calc(word, w2) for word in same_words] print(same_words, all_sims, end='\\t') return 0.6 * how + 0.4 * max(all_sims) if w1 in cls.B and w2 in cls.Common: print(w1, w2, '策略五（左词林）：词林改进版相似度：', lin) same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]] if not same_words: return lin all_sims = [cls.how_net.calc(word, w2) for word in same_words] print(w1, '词林同义词有：', same_words, all_sims, end='\\t') return 0.6 * lin + 0.4 * max(all_sims) if w2 in cls.B and w1 in cls.Common: print(w1, w2, '策略五（右词林）：词林改进版相似度：', lin) same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]] if not same_words: return lin all_sims = [cls.how_net.calc(word, w1) for word in same_words] print(w2, '词林同义词有：', same_words, all_sims, end='\\t') return 0.6 * lin + 0.4 * max(all_sims) print('对不起，词语可能未收录，无法计算相似度！') return -1 1.2.2 使用参考https://github.com/yaleimeng/Final_word_Similarity 12345678910111213141516171819202122232425from Hybrid_Sim import HybridSimfrom Pearson import *import timeif __name__ == '__main__': print('词林词汇量', len(HybridSim.ci_lin.vocab ),'\\t知网词汇量', len(HybridSim.how_net.vocab)) print('两者总词汇量',len(HybridSim.ci_lin.vocab | HybridSim.how_net.vocab),'\\t重叠词汇量', len(HybridSim.Common)) b_a = &quot;联系方式&quot; b_b = &quot;电话&quot; start = time.time() hybrid = HybridSim.get_Final_sim(b_a, b_a) end = time.time() print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid) print(&quot;运行时间：&quot;+str(end-start)) b_a = &quot;手机&quot; b_b = &quot;电话&quot; start = time.time() hybrid = HybridSim.get_Final_sim(b_a, b_a) end = time.time() print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid) print(&quot;运行时间：&quot;+str(end-start)) 1234567词林词汇量 77498 知网词汇量 53336两者总词汇量 85817 重叠词汇量 45017对不起，词语可能未收录，无法计算相似度！联系方式 电话相似度为： -1运行时间：3.504753112792969e-05手机 电话相似度为： 1.0运行时间：0.019332408905029297 二.基于词向量基于样本构建，利于维护 2.1 基于word2vec2.2.1 原理word2vec的原理和词向量获取过程不在此赘述，在本部分主要讲解基于word2vec的词向量如何计算词语相似度。源码如下 1234567891011121314151617def similarity(self, w1, w2): &quot;&quot;&quot;Compute cosine similarity between two keys. Parameters ---------- w1 : str Input key. w2 : str Input key. Returns ------- float Cosine similarity between `w1` and `w2`. &quot;&quot;&quot; return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2])) 2.2.2 使用训练 1234567891011from gensim.models.word2vec import Word2Vecimport pandas as pdfrom gensim import modelsimport jieba###traindata=pd.read_csv(data_path)sentences=data.tolist()model= Word2Vec()model.build_vocab(sentences)model.train(sentences,total_examples = model.corpus_count,epochs = 5)model.save(model_path) 使用 12345678910111213141516171819from gensim import modelsimport timeif __name__ == '__main__': model=models.Word2Vec.load(model_path) start = time.time() b_a = &quot;联系方式&quot; b_b = &quot;电话&quot; sim_b = model.wv.n_similarity(b_a, b_b) end = time.time() start = time.time() print(b_a, b_b, '相似度为', sim_b) print(&quot;运行时间：&quot; + str(end - start)) b_a = &quot;手机&quot; b_b = &quot;电话&quot; sim_b = model.wv.n_similarity(b_a, b_b) end = time.time() print(b_a, b_b, '相似度为', sim_b) print(&quot;运行时间：&quot; + str(end - start)) 1234联系方式 电话 相似度为 -0.014857853运行时间：-4.76837158203125e-07手机 电话 相似度为 0.1771852运行时间：0.0004227161407470703 参考文献https://blog.csdn.net/sinat_33741547/article/details/80016713 https://github.com/yaleimeng/Final_word_Similarity","link":"/2021/07/21/word-similarity/"},{"title":"word2vec","text":"一.原理两种训练模型 如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』 而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』 训练技巧 hierarchical softmax 和 negative sampling 二.代码训练代码 12345678910111213from gensim.models.word2vec import Word2Vecimport pandas as pdfrom gensim import modelsimport jieba###traindata=pd.read_csv(data_path)sentences=data.tolist()model= Word2Vec()model.build_vocab(sentences)model.train(sentences,total_examples = model.corpus_count,epochs = 5)model.save(model_path) 词向量矩阵 12345678from gensim import modelsif __name__ == '__main__': model=models.KeyedVectors.load_word2vec_format(model_path,binary=True) print(model.vectors) ##(779845, 400)) print(&quot;\\n&quot;) print(model.index_to_key) print(&quot;\\n&quot;) print(model[&quot;的&quot;]) 12345array([[-1.3980628e+00, -4.6281612e-01, 5.8368486e-01, ..., 5.3952241e-01, 4.4697687e-01, 1.3505782e+00], [ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, ..., 1.1110669e+00, 2.1992767e-01, 7.0457202e-01], [-8.5650706e-01, 8.2832746e-02, -8.4218192e-01, ..., 2.1654253e+00, 6.4846051e-01, -5.7714492e-01], ..., [ 7.5072781e-03, -1.3543828e-02, 2.3101490e-02, ..., 4.2363801e-03, -5.6749382e-03, 6.3404259e-03], [-2.6244391e-04, -3.0459568e-02, 5.9752418e-03, ..., 1.7844304e-02, -4.7109672e-04, 7.7916058e-03], [ 7.2062697e-04, -6.5988898e-03, 1.1346856e-02, ..., -3.7340564e-03, -1.8825980e-02, 2.7245486e-03]], dtype=float32)['，', '的', '。', '、', '０', '１', '在', '”', '２', '了', '“', '和', '是', '５', ...]array([ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, -3.6405793e-01, 1.0851435e-01, 4.9507666e-02, -7.1219063e-01, -5.4614645e-01, -1.3581418e+00, 3.0274218e-01, 6.1700332e-01, 3.5553512e-01, 1.6602433e+00, 7.5298291e-01, -1.4151905e-01, -2.1077128e-01, -2.6325354e-01, 1.6108564e+00, -4.6750236e-01, -1.6261842e+00, 1.3063166e-01, 8.0702168e-01, 4.0011466e-01, 1.2198541e+00, -6.2879241e-01, ... 2.1928079e-01, 7.1725255e-01, -2.3430648e-01, -1.2066336e+00, 9.7590965e-01, -1.5906478e-01, -3.5802779e-01, -3.8005975e-01, 1.9056025e-01, 1.1110669e+00, 2.1992767e-01, 7.0457202e-01], dtype=float32) 参考https://zhuanlan.zhihu.com/p/26306795 https://arxiv.org/abs/1301.3781v3 https://arxiv.org/abs/1405.4053","link":"/2021/08/04/word2vec/"},{"title":"xgboost","text":"a scalable tree boosting system 是对gbdt的高效实现 基学习器：XGBoost的可以使用cart回归树作为基学习器，也可以使用线性分类器作为基学习器；gbdt的基学习器只能是cart回归树 XGBoost算法原理小结 https://www.cnblogs.com/pinard/p/10979808.html XGBoost类库使用小结 https://www.cnblogs.com/pinard/p/11114748.html paper原文 https://arxiv.org/pdf/1603.02754.pdf","link":"/2021/10/11/xgboost/"},{"title":"XLNet Generalized Autoregressive Pretraining for Language Understanding","text":"1 主要改动relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. (3) , XLNet integrates ideas from Transformer-XL example：[New, York, is, a, city] . select the two tokens [New, York] as the prediction targets and maximize log p （New York | is a city） In this case, BERT and XLNet respectively reduce to the following objectives: 2 现有PTM的问题1 AR language modeling 对于给定的句子$\\textbf{x}=[x_1,…,x_T]$，AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization \\max \\limits_{\\theta} \\quad logp_{\\theta}(\\textbf{x})=\\sum_{t=1}^{T}logp_{\\theta}(x_t|\\textbf{x}_{","link":"/2021/08/27/xlnet/"},{"title":"youtubednn","text":"原文： https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf 几篇优秀博客： https://zhuanlan.zhihu.com/p/52169807 https://zhuanlan.zhihu.com/p/52504407 https://zhuanlan.zhihu.com/p/61827629 https://zhuanlan.zhihu.com/p/46247835 下文为本人总结。 2.SYSTEM OVERVIEW 3.CANDIDATE GENERATION3.1 Recommendation as Classification把推荐问题转换成多分类问题 where $u \\in \\mathbb{R}^{N}$ represents a high-dimensional embedding”of the user, context pair and the $ v_j \\in \\mathbb{R}^{N}$ represent embeddings of each candidate video. train： to efficiently train such a model with millions of classes 1.hierarchical softmax，效果不佳 2.采用candidate sampling，correct for this sampling via importance weighting At serving time 3.2 CANDIDATE GENERATION 3.3 Heterogeneous Signals3.4 Label and Context Selection 3.5 Experiments with Features and Depth 4.RANKING","link":"/2021/10/25/youtubednn/"},{"title":"ZooKeeper","text":"是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。 简单操作1 查看状态 bin/zkServer.sh status 有leader follower shell脚本编写：start，stop，status 问题1 某个节点起不来 https://blog.csdn.net/qq_48268603/article/details/117687875 https://blog.csdn.net/u012453843/article/details/70878117 参考https://geek-docs.com/zookeeper/zookeeper-tutorial/zookeeper-profile.html#","link":"/2022/01/30/zookeeper/"}],"tags":[{"name":"二分类","slug":"二分类","link":"/tags/%E4%BA%8C%E5%88%86%E7%B1%BB/"},{"name":"即席查询","slug":"即席查询","link":"/tags/%E5%8D%B3%E5%B8%AD%E6%9F%A5%E8%AF%A2/"},{"name":"Atlas","slug":"Atlas","link":"/tags/Atlas/"},{"name":"Azkaban","slug":"Azkaban","link":"/tags/Azkaban/"},{"name":"冷启动","slug":"冷启动","link":"/tags/%E5%86%B7%E5%90%AF%E5%8A%A8/"},{"name":"DGL","slug":"DGL","link":"/tags/DGL/"},{"name":"Wide&amp;Deep和DeepFM","slug":"Wide-Deep和DeepFM","link":"/tags/Wide-Deep%E5%92%8CDeepFM/"},{"name":"FM","slug":"FM","link":"/tags/FM/"},{"name":"GCN","slug":"GCN","link":"/tags/GCN/"},{"name":"Grafana","slug":"Grafana","link":"/tags/Grafana/"},{"name":"GNN核心构成","slug":"GNN核心构成","link":"/tags/GNN%E6%A0%B8%E5%BF%83%E6%9E%84%E6%88%90/"},{"name":"Kerberos","slug":"Kerberos","link":"/tags/Kerberos/"},{"name":"Kylin","slug":"Kylin","link":"/tags/Kylin/"},{"name":"L2R","slug":"L2R","link":"/tags/L2R/"},{"name":"极大似然估计","slug":"极大似然估计","link":"/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"},{"name":"多路召回","slug":"多路召回","link":"/tags/%E5%A4%9A%E8%B7%AF%E5%8F%AC%E5%9B%9E/"},{"name":"NER","slug":"NER","link":"/tags/NER/"},{"name":"Negative Sampling 负采样","slug":"Negative-Sampling-负采样","link":"/tags/Negative-Sampling-%E8%B4%9F%E9%87%87%E6%A0%B7/"},{"name":"NLP子任务的评价指标","slug":"NLP子任务的评价指标","link":"/tags/NLP%E5%AD%90%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"name":"归一化","slug":"归一化","link":"/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"},{"name":"Presto","slug":"Presto","link":"/tags/Presto/"},{"name":"Prompt","slug":"Prompt","link":"/tags/Prompt/"},{"name":"PTM","slug":"PTM","link":"/tags/PTM/"},{"name":"Zabbix","slug":"Zabbix","link":"/tags/Zabbix/"},{"name":"对比学习在NLP应用","slug":"对比学习在NLP应用","link":"/tags/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E5%9C%A8NLP%E5%BA%94%E7%94%A8/"},{"name":"激活函数","slug":"激活函数","link":"/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"ad hoc和routing","slug":"ad-hoc和routing","link":"/tags/ad-hoc%E5%92%8Crouting/"},{"name":"loss不下降的解决方法","slug":"loss不下降的解决方法","link":"/tags/loss%E4%B8%8D%E4%B8%8B%E9%99%8D%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/"},{"name":"调参","slug":"调参","link":"/tags/%E8%B0%83%E5%8F%82/"},{"name":"answer select","slug":"answer-select","link":"/tags/answer-select/"},{"name":"it的技术场景","slug":"it的技术场景","link":"/tags/it%E7%9A%84%E6%8A%80%E6%9C%AF%E5%9C%BA%E6%99%AF/"},{"name":"attention","slug":"attention","link":"/tags/attention/"},{"name":"AutoTokenizer","slug":"AutoTokenizer","link":"/tags/AutoTokenizer/"},{"name":"auxiliary loss(辅助损失）","slug":"auxiliary-loss-辅助损失）","link":"/tags/auxiliary-loss-%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%EF%BC%89/"},{"name":"basic algorithm","slug":"basic-algorithm","link":"/tags/basic-algorithm/"},{"name":"Bert文本表示","slug":"Bert文本表示","link":"/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"},{"name":"分类回归模型总结","slug":"分类回归模型总结","link":"/tags/%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"},{"name":"bert_serving","slug":"bert-serving","link":"/tags/bert-serving/"},{"name":"BertGCN","slug":"BertGCN","link":"/tags/BertGCN/"},{"name":"bertviz:attention可视化工具","slug":"bertviz-attention可视化工具","link":"/tags/bertviz-attention%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"},{"name":"大数据组件","slug":"大数据组件","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6/"},{"name":"大数据","slug":"大数据","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"bm25","slug":"bm25","link":"/tags/bm25/"},{"name":"GB, GBDT, XGBoost, LightGBM","slug":"GB-GBDT-XGBoost-LightGBM","link":"/tags/GB-GBDT-XGBoost-LightGBM/"},{"name":"bp算法","slug":"bp算法","link":"/tags/bp%E7%AE%97%E6%B3%95/"},{"name":"离线数仓搭建例子","slug":"离线数仓搭建例子","link":"/tags/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%90%AD%E5%BB%BA%E4%BE%8B%E5%AD%90/"},{"name":"CDC（Change Data Capture）工具对比","slug":"CDC（Change-Data-Capture）工具对比","link":"/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"},{"name":"召回","slug":"召回","link":"/tags/%E5%8F%AC%E5%9B%9E/"},{"name":"chatbot","slug":"chatbot","link":"/tags/chatbot/"},{"name":"checkpoints_iterator","slug":"checkpoints-iterator","link":"/tags/checkpoints-iterator/"},{"name":"ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information","slug":"ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information","link":"/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"},{"name":"分类任务的衡量指标","slug":"分类任务的衡量指标","link":"/tags/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87/"},{"name":"clickhouse","slug":"clickhouse","link":"/tags/clickhouse/"},{"name":"闭包","slug":"闭包","link":"/tags/%E9%97%AD%E5%8C%85/"},{"name":"spark部署方式","slug":"spark部署方式","link":"/tags/spark%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/"},{"name":"Google colab","slug":"Google-colab","link":"/tags/Google-colab/"},{"name":"表字段的数据类型","slug":"表字段的数据类型","link":"/tags/%E8%A1%A8%E5%AD%97%E6%AE%B5%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"},{"name":"文本表示","slug":"文本表示","link":"/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"},{"name":"使用DGL构造图","slug":"使用DGL构造图","link":"/tags/%E4%BD%BF%E7%94%A8DGL%E6%9E%84%E9%80%A0%E5%9B%BE/"},{"name":"SparkSession、SparkContext、HiveContext、SQLContext","slug":"SparkSession、SparkContext、HiveContext、SQLContext","link":"/tags/SparkSession%E3%80%81SparkContext%E3%80%81HiveContext%E3%80%81SQLContext/"},{"name":"convtrans","slug":"convtrans","link":"/tags/convtrans/"},{"name":"CRF和HMM","slug":"CRF和HMM","link":"/tags/CRF%E5%92%8CHMM/"},{"name":"常见问题","slug":"常见问题","link":"/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"},{"name":"DMR","slug":"DMR","link":"/tags/DMR/"},{"name":"DAG","slug":"DAG","link":"/tags/DAG/"},{"name":"数据采集","slug":"数据采集","link":"/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"name":"数据字典","slug":"数据字典","link":"/tags/%E6%95%B0%E6%8D%AE%E5%AD%97%E5%85%B8/"},{"name":"数据集成","slug":"数据集成","link":"/tags/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/"},{"name":"数据不平衡","slug":"数据不平衡","link":"/tags/%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1/"},{"name":"数据质量","slug":"数据质量","link":"/tags/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/"},{"name":"数据安全","slug":"数据安全","link":"/tags/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/"},{"name":"数据集划分","slug":"数据集划分","link":"/tags/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86/"},{"name":"数据结构汇总","slug":"数据结构汇总","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%B1%87%E6%80%BB/"},{"name":"数据同步","slug":"数据同步","link":"/tags/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/"},{"name":"数据仓库书单","slug":"数据仓库书单","link":"/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E4%B9%A6%E5%8D%95/"},{"name":"数据库分类","slug":"数据库分类","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%88%86%E7%B1%BB/"},{"name":"数据库建模","slug":"数据库建模","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BB%BA%E6%A8%A1/"},{"name":"分库分表","slug":"分库分表","link":"/tags/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/"},{"name":"datastream","slug":"datastream","link":"/tags/datastream/"},{"name":"大数据工具","slug":"大数据工具","link":"/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B7%A5%E5%85%B7/"},{"name":"构建数据仓库","slug":"构建数据仓库","link":"/tags/%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"数仓架构","slug":"数仓架构","link":"/tags/%E6%95%B0%E4%BB%93%E6%9E%B6%E6%9E%84/"},{"name":"数仓主题和主题域","slug":"数仓主题和主题域","link":"/tags/%E6%95%B0%E4%BB%93%E4%B8%BB%E9%A2%98%E5%92%8C%E4%B8%BB%E9%A2%98%E5%9F%9F/"},{"name":"滞后","slug":"滞后","link":"/tags/%E6%BB%9E%E5%90%8E/"},{"name":"决策树vs逻辑回归","slug":"决策树vs逻辑回归","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91vs%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"决策树","slug":"决策树","link":"/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"装饰器","slug":"装饰器","link":"/tags/%E8%A3%85%E9%A5%B0%E5%99%A8/"},{"name":"维度退化","slug":"维度退化","link":"/tags/%E7%BB%B4%E5%BA%A6%E9%80%80%E5%8C%96/"},{"name":"dfs，bfs","slug":"dfs，bfs","link":"/tags/dfs%EF%BC%8Cbfs/"},{"name":"DGL notice","slug":"DGL-notice","link":"/tags/DGL-notice/"},{"name":"list，dict，set的时间复杂度","slug":"list，dict，set的时间复杂度","link":"/tags/list%EF%BC%8Cdict%EF%BC%8Cset%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"name":"词表特殊词的含义","slug":"词表特殊词的含义","link":"/tags/%E8%AF%8D%E8%A1%A8%E7%89%B9%E6%AE%8A%E8%AF%8D%E7%9A%84%E5%90%AB%E4%B9%89/"},{"name":"DIEN","slug":"DIEN","link":"/tags/DIEN/"},{"name":"降维","slug":"降维","link":"/tags/%E9%99%8D%E7%BB%B4/"},{"name":"数仓建模","slug":"数仓建模","link":"/tags/%E6%95%B0%E4%BB%93%E5%BB%BA%E6%A8%A1/"},{"name":"DIN","slug":"DIN","link":"/tags/DIN/"},{"name":"距离&#x2F;相似度","slug":"距离-相似度","link":"/tags/%E8%B7%9D%E7%A6%BB-%E7%9B%B8%E4%BC%BC%E5%BA%A6/"},{"name":"超长文本","slug":"超长文本","link":"/tags/%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC/"},{"name":"docker容器与虚拟机有什么区别？","slug":"docker容器与虚拟机有什么区别？","link":"/tags/docker%E5%AE%B9%E5%99%A8%E4%B8%8E%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F/"},{"name":"DSSM","slug":"DSSM","link":"/tags/DSSM/"},{"name":"电商业务","slug":"电商业务","link":"/tags/%E7%94%B5%E5%95%86%E4%B8%9A%E5%8A%A1/"},{"name":"early stop","slug":"early-stop","link":"/tags/early-stop/"},{"name":"ELMo","slug":"ELMo","link":"/tags/ELMo/"},{"name":"Enhanced-RCNN","slug":"Enhanced-RCNN","link":"/tags/Enhanced-RCNN/"},{"name":"特征向量化","slug":"特征向量化","link":"/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E5%8C%96/"},{"name":"entropy","slug":"entropy","link":"/tags/entropy/"},{"name":"集成学习","slug":"集成学习","link":"/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"ETL","slug":"ETL","link":"/tags/ETL/"},{"name":"分类任务的类别数量很大","slug":"分类任务的类别数量很大","link":"/tags/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%B1%BB%E5%88%AB%E6%95%B0%E9%87%8F%E5%BE%88%E5%A4%A7/"},{"name":"facebook推荐系统","slug":"facebook推荐系统","link":"/tags/facebook%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"文本分类","slug":"文本分类","link":"/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"容错机制","slug":"容错机制","link":"/tags/%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"},{"name":"特征工程","slug":"特征工程","link":"/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"特征提取","slug":"特征提取","link":"/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"},{"name":"feature scale","slug":"feature-scale","link":"/tags/feature-scale/"},{"name":"文本改写","slug":"文本改写","link":"/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"},{"name":"小样本","slug":"小样本","link":"/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC/"},{"name":"前馈神经网络","slug":"前馈神经网络","link":"/tags/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"finetune","slug":"finetune","link":"/tags/finetune/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"flink分层api","slug":"flink分层api","link":"/tags/flink%E5%88%86%E5%B1%82api/"},{"name":"流批选择","slug":"流批选择","link":"/tags/%E6%B5%81%E6%89%B9%E9%80%89%E6%8B%A9/"},{"name":"flink cdc","slug":"flink-cdc","link":"/tags/flink-cdc/"},{"name":"flink cep","slug":"flink-cep","link":"/tags/flink-cep/"},{"name":"flink部署","slug":"flink部署","link":"/tags/flink%E9%83%A8%E7%BD%B2/"},{"name":"算子链","slug":"算子链","link":"/tags/%E7%AE%97%E5%AD%90%E9%93%BE/"},{"name":"flink优化","slug":"flink优化","link":"/tags/flink%E4%BC%98%E5%8C%96/"},{"name":"并行度设置","slug":"并行度设置","link":"/tags/%E5%B9%B6%E8%A1%8C%E5%BA%A6%E8%AE%BE%E7%BD%AE/"},{"name":"处理函数(process funtion)","slug":"处理函数-process-funtion","link":"/tags/%E5%A4%84%E7%90%86%E5%87%BD%E6%95%B0-process-funtion/"},{"name":"Flink程序构成部分","slug":"Flink程序构成部分","link":"/tags/Flink%E7%A8%8B%E5%BA%8F%E6%9E%84%E6%88%90%E9%83%A8%E5%88%86/"},{"name":"flink vs spark","slug":"flink-vs-spark","link":"/tags/flink-vs-spark/"},{"name":"Flink架构原理","slug":"Flink架构原理","link":"/tags/Flink%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"name":"状态编程","slug":"状态编程","link":"/tags/%E7%8A%B6%E6%80%81%E7%BC%96%E7%A8%8B/"},{"name":"Table API和SQL","slug":"Table-API和SQL","link":"/tags/Table-API%E5%92%8CSQL/"},{"name":"任务生成和分配","slug":"任务生成和分配","link":"/tags/%E4%BB%BB%E5%8A%A1%E7%94%9F%E6%88%90%E5%92%8C%E5%88%86%E9%85%8D/"},{"name":"任务槽 task slots","slug":"任务槽-task-slots","link":"/tags/%E4%BB%BB%E5%8A%A1%E6%A7%BD-task-slots/"},{"name":"flink提交任务","slug":"flink提交任务","link":"/tags/flink%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1/"},{"name":"flink时间语义","slug":"flink时间语义","link":"/tags/flink%E6%97%B6%E9%97%B4%E8%AF%AD%E4%B9%89/"},{"name":"watermark(水位线)","slug":"watermark-水位线","link":"/tags/watermark-%E6%B0%B4%E4%BD%8D%E7%BA%BF/"},{"name":"flume","slug":"flume","link":"/tags/flume/"},{"name":"forward","slug":"forward","link":"/tags/forward/"},{"name":"GBDT","slug":"GBDT","link":"/tags/GBDT/"},{"name":"PyG,DGL","slug":"PyG-DGL","link":"/tags/PyG-DGL/"},{"name":"gradient_accumulate_steps，调节学习率","slug":"gradient-accumulate-steps，调节学习率","link":"/tags/gradient-accumulate-steps%EF%BC%8C%E8%B0%83%E8%8A%82%E5%AD%A6%E4%B9%A0%E7%8E%87/"},{"name":"梯度爆炸、梯度消失","slug":"梯度爆炸、梯度消失","link":"/tags/%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E3%80%81%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/"},{"name":"gradient boosting","slug":"gradient-boosting","link":"/tags/gradient-boosting/"},{"name":"GNN综述","slug":"GNN综述","link":"/tags/GNN%E7%BB%BC%E8%BF%B0/"},{"name":"Graph和Session","slug":"Graph和Session","link":"/tags/Graph%E5%92%8CSession/"},{"name":"hadoop架构","slug":"hadoop架构","link":"/tags/hadoop%E6%9E%B6%E6%9E%84/"},{"name":"Hadoop部署方式","slug":"Hadoop部署方式","link":"/tags/Hadoop%E9%83%A8%E7%BD%B2%E6%96%B9%E5%BC%8F/"},{"name":"hadoop安全模式","slug":"hadoop安全模式","link":"/tags/hadoop%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F/"},{"name":"Hadoop in Secure Mode","slug":"Hadoop-in-Secure-Mode","link":"/tags/Hadoop-in-Secure-Mode/"},{"name":"hadoop trick","slug":"hadoop-trick","link":"/tags/hadoop-trick/"},{"name":"哈希表","slug":"哈希表","link":"/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"HBase","slug":"HBase","link":"/tags/HBase/"},{"name":"hcan(Hybrid Co-Attention Network)","slug":"hcan-Hybrid-Co-Attention-Network","link":"/tags/hcan-Hybrid-Co-Attention-Network/"},{"name":"堆，优先队列","slug":"堆，优先队列","link":"/tags/%E5%A0%86%EF%BC%8C%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/"},{"name":"hive数据导入导出","slug":"hive数据导入导出","link":"/tags/hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"hive架构","slug":"hive架构","link":"/tags/hive%E6%9E%B6%E6%9E%84/"},{"name":"Hive MetaStore","slug":"Hive-MetaStore","link":"/tags/Hive-MetaStore/"},{"name":"Hive与传统数据库对比","slug":"Hive与传统数据库对比","link":"/tags/Hive%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94/"},{"name":"hive优化","slug":"hive优化","link":"/tags/hive%E4%BC%98%E5%8C%96/"},{"name":"hive","slug":"hive","link":"/tags/hive/"},{"name":"hql建表","slug":"hql建表","link":"/tags/hql%E5%BB%BA%E8%A1%A8/"},{"name":"hql常见操作","slug":"hql常见操作","link":"/tags/hql%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/"},{"name":"hql增删改查","slug":"hql增删改查","link":"/tags/hql%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/"},{"name":"hql函数","slug":"hql函数","link":"/tags/hql%E5%87%BD%E6%95%B0/"},{"name":"sql,hql区别","slug":"sql-hql区别","link":"/tags/sql-hql%E5%8C%BA%E5%88%AB/"},{"name":"hql加载数据","slug":"hql加载数据","link":"/tags/hql%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE/"},{"name":"huggingface","slug":"huggingface","link":"/tags/huggingface/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"克隆虚拟机修改静态IP不成功解决办法","slug":"克隆虚拟机修改静态IP不成功解决办法","link":"/tags/%E5%85%8B%E9%9A%86%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81IP%E4%B8%8D%E6%88%90%E5%8A%9F%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"},{"name":"倒排索引","slug":"倒排索引","link":"/tags/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"},{"name":"信息抽取 Information Extraction","slug":"信息抽取-Information-Extraction","link":"/tags/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-Information-Extraction/"},{"name":"Informer","slug":"Informer","link":"/tags/Informer/"},{"name":"意图识别和槽位填充","slug":"意图识别和槽位填充","link":"/tags/%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB%E5%92%8C%E6%A7%BD%E4%BD%8D%E5%A1%AB%E5%85%85/"},{"name":"意图识别","slug":"意图识别","link":"/tags/%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB/"},{"name":"java应用场景","slug":"java应用场景","link":"/tags/java%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/"},{"name":"java框架","slug":"java框架","link":"/tags/java%E6%A1%86%E6%9E%B6/"},{"name":"多线程编程","slug":"多线程编程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"java常见问题","slug":"java常见问题","link":"/tags/java%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"},{"name":"java学习网站","slug":"java学习网站","link":"/tags/java%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99/"},{"name":"jdbc","slug":"jdbc","link":"/tags/jdbc/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"jupyter notebook","slug":"jupyter-notebook","link":"/tags/jupyter-notebook/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"k-fold","slug":"k-fold","link":"/tags/k-fold/"},{"name":"kafka常见计算","slug":"kafka常见计算","link":"/tags/kafka%E5%B8%B8%E8%A7%81%E8%AE%A1%E7%AE%97/"},{"name":"kafka常见问题","slug":"kafka常见问题","link":"/tags/kafka%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"},{"name":"kafka常见命令","slug":"kafka常见命令","link":"/tags/kafka%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/"},{"name":"kafka与Zookeeper的关系","slug":"kafka与Zookeeper的关系","link":"/tags/kafka%E4%B8%8EZookeeper%E7%9A%84%E5%85%B3%E7%B3%BB/"},{"name":"Kafka原理结构","slug":"Kafka原理结构","link":"/tags/Kafka%E5%8E%9F%E7%90%86%E7%BB%93%E6%9E%84/"},{"name":"主外键","slug":"主外键","link":"/tags/%E4%B8%BB%E5%A4%96%E9%94%AE/"},{"name":"key&#x2F;value","slug":"key-value","link":"/tags/key-value/"},{"name":"KG-BERT","slug":"KG-BERT","link":"/tags/KG-BERT/"},{"name":"KNN","slug":"KNN","link":"/tags/KNN/"},{"name":"知识图谱综述","slug":"知识图谱综述","link":"/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%BB%BC%E8%BF%B0/"},{"name":"LambdaMART","slug":"LambdaMART","link":"/tags/LambdaMART/"},{"name":"lambda","slug":"lambda","link":"/tags/lambda/"},{"name":"刷题指南","slug":"刷题指南","link":"/tags/%E5%88%B7%E9%A2%98%E6%8C%87%E5%8D%97/"},{"name":"常见题目","slug":"常见题目","link":"/tags/%E5%B8%B8%E8%A7%81%E9%A2%98%E7%9B%AE/"},{"name":"lightgbm","slug":"lightgbm","link":"/tags/lightgbm/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"常见操作","slug":"常见操作","link":"/tags/%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"listwise","slug":"listwise","link":"/tags/listwise/"},{"name":"listnet","slug":"listnet","link":"/tags/listnet/"},{"name":"损失函数","slug":"损失函数","link":"/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"},{"name":"逻辑回归","slug":"逻辑回归","link":"/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"},{"name":"超长文本处理","slug":"超长文本处理","link":"/tags/%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"},{"name":"调节学习率","slug":"调节学习率","link":"/tags/%E8%B0%83%E8%8A%82%E5%AD%A6%E4%B9%A0%E7%8E%87/"},{"name":"消息传递","slug":"消息传递","link":"/tags/%E6%B6%88%E6%81%AF%E4%BC%A0%E9%80%92/"},{"name":"Spark vs MapReduce","slug":"Spark-vs-MapReduce","link":"/tags/Spark-vs-MapReduce/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"排序学习","slug":"排序学习","link":"/tags/%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0/"},{"name":"元数据管理","slug":"元数据管理","link":"/tags/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/"},{"name":"细粒度NLP任务","slug":"细粒度NLP任务","link":"/tags/%E7%BB%86%E7%B2%92%E5%BA%A6NLP%E4%BB%BB%E5%8A%A1/"},{"name":"迭代分析","slug":"迭代分析","link":"/tags/%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90/"},{"name":"深度学习模型部署","slug":"深度学习模型部署","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"},{"name":"集群监控","slug":"集群监控","link":"/tags/%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/"},{"name":"单调栈","slug":"单调栈","link":"/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"},{"name":"多标签","slug":"多标签","link":"/tags/%E5%A4%9A%E6%A0%87%E7%AD%BE/"},{"name":"多流转换","slug":"多流转换","link":"/tags/%E5%A4%9A%E6%B5%81%E8%BD%AC%E6%8D%A2/"},{"name":"多任务学习","slug":"多任务学习","link":"/tags/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","link":"/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"},{"name":"ip和端口","slug":"ip和端口","link":"/tags/ip%E5%92%8C%E7%AB%AF%E5%8F%A3/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"中文数据增强","slug":"中文数据增强","link":"/tags/%E4%B8%AD%E6%96%87%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/"},{"name":"nlp教材","slug":"nlp教材","link":"/tags/nlp%E6%95%99%E6%9D%90/"},{"name":"算法导论","slug":"算法导论","link":"/tags/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"},{"name":"nvidia-smi 查看GPU使用率很高但是看不到进程","slug":"nvidia-smi-查看GPU使用率很高但是看不到进程","link":"/tags/nvidia-smi-%E6%9F%A5%E7%9C%8BGPU%E4%BD%BF%E7%94%A8%E7%8E%87%E5%BE%88%E9%AB%98%E4%BD%86%E6%98%AF%E7%9C%8B%E4%B8%8D%E5%88%B0%E8%BF%9B%E7%A8%8B/"},{"name":"OLAP和OLTP的区别","slug":"OLAP和OLTP的区别","link":"/tags/OLAP%E5%92%8COLTP%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"name":"代价函数，损失函数，目标函数区别","slug":"代价函数，损失函数，目标函数区别","link":"/tags/%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%EF%BC%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%8C%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E5%8C%BA%E5%88%AB/"},{"name":"oov怎么解决","slug":"oov怎么解决","link":"/tags/oov%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3/"},{"name":"open set recognization","slug":"open-set-recognization","link":"/tags/open-set-recognization/"},{"name":"优化算法","slug":"优化算法","link":"/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"name":"过拟合、欠拟合","slug":"过拟合、欠拟合","link":"/tags/%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88/"},{"name":"pairwise","slug":"pairwise","link":"/tags/pairwise/"},{"name":"感知机","slug":"感知机","link":"/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"name":"持久化","slug":"持久化","link":"/tags/%E6%8C%81%E4%B9%85%E5%8C%96/"},{"name":"物理分区","slug":"物理分区","link":"/tags/%E7%89%A9%E7%90%86%E5%88%86%E5%8C%BA/"},{"name":"phoenix","slug":"phoenix","link":"/tags/phoenix/"},{"name":"pointwise vs pairwise","slug":"pointwise-vs-pairwise","link":"/tags/pointwise-vs-pairwise/"},{"name":"pom","slug":"pom","link":"/tags/pom/"},{"name":"pointwise","slug":"pointwise","link":"/tags/pointwise/"},{"name":"postman","slug":"postman","link":"/tags/postman/"},{"name":"预训练任务","slug":"预训练任务","link":"/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1/"},{"name":"粗排","slug":"粗排","link":"/tags/%E7%B2%97%E6%8E%92/"},{"name":"pretrain","slug":"pretrain","link":"/tags/pretrain/"},{"name":"Prompt-learning小帮手-openprompt","slug":"Prompt-learning小帮手-openprompt","link":"/tags/Prompt-learning%E5%B0%8F%E5%B8%AE%E6%89%8B-openprompt/"},{"name":"概率统计","slug":"概率统计","link":"/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"},{"name":"prompt trick","slug":"prompt-trick","link":"/tags/prompt-trick/"},{"name":"ptm之间的联系","slug":"ptm之间的联系","link":"/tags/ptm%E4%B9%8B%E9%97%B4%E7%9A%84%E8%81%94%E7%B3%BB/"},{"name":"pyspark依赖","slug":"pyspark依赖","link":"/tags/pyspark%E4%BE%9D%E8%B5%96/"},{"name":"pyspark","slug":"pyspark","link":"/tags/pyspark/"},{"name":"python bif","slug":"python-bif","link":"/tags/python-bif/"},{"name":"运算","slug":"运算","link":"/tags/%E8%BF%90%E7%AE%97/"},{"name":"python不可变对象和可变对象","slug":"python不可变对象和可变对象","link":"/tags/python%E4%B8%8D%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1%E5%92%8C%E5%8F%AF%E5%8F%98%E5%AF%B9%E8%B1%A1/"},{"name":"collections","slug":"collections","link":"/tags/collections/"},{"name":"python环境","slug":"python环境","link":"/tags/python%E7%8E%AF%E5%A2%83/"},{"name":"可迭代对象、迭代器与生成器","slug":"可迭代对象、迭代器与生成器","link":"/tags/%E5%8F%AF%E8%BF%AD%E4%BB%A3%E5%AF%B9%E8%B1%A1%E3%80%81%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/"},{"name":"函数参数","slug":"函数参数","link":"/tags/%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0/"},{"name":"正则","slug":"正则","link":"/tags/%E6%AD%A3%E5%88%99/"},{"name":"python在Ubuntu系统下的调试工具pdb","slug":"python在Ubuntu系统下的调试工具pdb","link":"/tags/python%E5%9C%A8Ubuntu%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7pdb/"},{"name":"编程规范","slug":"编程规范","link":"/tags/%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83/"},{"name":"python ide","slug":"python-ide","link":"/tags/python-ide/"},{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"魔法方法","slug":"魔法方法","link":"/tags/%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95/"},{"name":"继承","slug":"继承","link":"/tags/%E7%BB%A7%E6%89%BF/"},{"name":"pytorch常见问题","slug":"pytorch常见问题","link":"/tags/pytorch%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"},{"name":"query理解","slug":"query理解","link":"/tags/query%E7%90%86%E8%A7%A3/"},{"name":"Ranger","slug":"Ranger","link":"/tags/Ranger/"},{"name":"ranking survey","slug":"ranking-survey","link":"/tags/ranking-survey/"},{"name":"ranknet对比listnet","slug":"ranknet对比listnet","link":"/tags/ranknet%E5%AF%B9%E6%AF%94listnet/"},{"name":"实时数仓案例（电商）","slug":"实时数仓案例（电商）","link":"/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%A1%88%E4%BE%8B%EF%BC%88%E7%94%B5%E5%95%86%EF%BC%89/"},{"name":"实时数仓分层","slug":"实时数仓分层","link":"/tags/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E5%88%86%E5%B1%82/"},{"name":"推荐系统评价指标","slug":"推荐系统评价指标","link":"/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"name":"推荐系统","slug":"推荐系统","link":"/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"递归,迭代","slug":"递归-迭代","link":"/tags/%E9%80%92%E5%BD%92-%E8%BF%AD%E4%BB%A3/"},{"name":"递归","slug":"递归","link":"/tags/%E9%80%92%E5%BD%92/"},{"name":"Use reduceByKey instead of groupByKey","slug":"Use-reduceByKey-instead-of-groupByKey","link":"/tags/Use-reduceByKey-instead-of-groupByKey/"},{"name":"正则化","slug":"正则化","link":"/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"},{"name":"实体关系抽取","slug":"实体关系抽取","link":"/tags/%E5%AE%9E%E4%BD%93%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"搜索系统","slug":"搜索系统","link":"/tags/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/"},{"name":"IaaS、PaaS和SaaS","slug":"IaaS、PaaS和SaaS","link":"/tags/IaaS%E3%80%81PaaS%E5%92%8CSaaS/"},{"name":"seq2seq","slug":"seq2seq","link":"/tags/seq2seq/"},{"name":"序列标注","slug":"序列标注","link":"/tags/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8/"},{"name":"文本匹配","slug":"文本匹配","link":"/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"},{"name":"shell命令执行hive脚本","slug":"shell命令执行hive脚本","link":"/tags/shell%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8Chive%E8%84%9A%E6%9C%AC/"},{"name":"半监督和自监督","slug":"半监督和自监督","link":"/tags/%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%92%8C%E8%87%AA%E7%9B%91%E7%9D%A3/"},{"name":"Solr","slug":"Solr","link":"/tags/Solr/"},{"name":"sota","slug":"sota","link":"/tags/sota/"},{"name":"Spark 数据倾斜","slug":"Spark-数据倾斜","link":"/tags/Spark-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/"},{"name":"spark容错机制","slug":"spark容错机制","link":"/tags/spark%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6/"},{"name":"spark常见错误","slug":"spark常见错误","link":"/tags/spark%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"},{"name":"spark交互工具","slug":"spark交互工具","link":"/tags/spark%E4%BA%A4%E4%BA%92%E5%B7%A5%E5%85%B7/"},{"name":"数据划分,rdd分区","slug":"数据划分-rdd分区","link":"/tags/%E6%95%B0%E6%8D%AE%E5%88%92%E5%88%86-rdd%E5%88%86%E5%8C%BA/"},{"name":"spark模块","slug":"spark模块","link":"/tags/spark%E6%A8%A1%E5%9D%97/"},{"name":"spark oom(out of memory)问题","slug":"spark-oom-out-of-memory-问题","link":"/tags/spark-oom-out-of-memory-%E9%97%AE%E9%A2%98/"},{"name":"spark优化","slug":"spark优化","link":"/tags/spark%E4%BC%98%E5%8C%96/"},{"name":"spark优化手段","slug":"spark优化手段","link":"/tags/spark%E4%BC%98%E5%8C%96%E6%89%8B%E6%AE%B5/"},{"name":"RDD、DataFrame、DataSet","slug":"RDD、DataFrame、DataSet","link":"/tags/RDD%E3%80%81DataFrame%E3%80%81DataSet/"},{"name":"shuffle","slug":"shuffle","link":"/tags/shuffle/"},{"name":"Spark支持的存储介质","slug":"Spark支持的存储介质","link":"/tags/Spark%E6%94%AF%E6%8C%81%E7%9A%84%E5%AD%98%E5%82%A8%E4%BB%8B%E8%B4%A8/"},{"name":"Spark架构","slug":"Spark架构","link":"/tags/Spark%E6%9E%B6%E6%9E%84/"},{"name":"提交Spark任务","slug":"提交Spark任务","link":"/tags/%E6%8F%90%E4%BA%A4Spark%E4%BB%BB%E5%8A%A1/"},{"name":"RDD依赖关系","slug":"RDD依赖关系","link":"/tags/RDD%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/"},{"name":"spark资源参数调优","slug":"spark资源参数调优","link":"/tags/spark%E8%B5%84%E6%BA%90%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98/"},{"name":"sparkcore","slug":"sparkcore","link":"/tags/sparkcore/"},{"name":"Sparkcore运行流程","slug":"Sparkcore运行流程","link":"/tags/Sparkcore%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B/"},{"name":"Spark on Hive &amp; Hive on Spark","slug":"Spark-on-Hive-Hive-on-Spark","link":"/tags/Spark-on-Hive-Hive-on-Spark/"},{"name":"sparksql对比hive sql","slug":"sparksql对比hive-sql","link":"/tags/sparksql%E5%AF%B9%E6%AF%94hive-sql/"},{"name":"Sparksql运行流程","slug":"Sparksql运行流程","link":"/tags/Sparksql%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B/"},{"name":"sparksql","slug":"sparksql","link":"/tags/sparksql/"},{"name":"特征稀疏","slug":"特征稀疏","link":"/tags/%E7%89%B9%E5%BE%81%E7%A8%80%E7%96%8F/"},{"name":"springboot","slug":"springboot","link":"/tags/springboot/"},{"name":"建表","slug":"建表","link":"/tags/%E5%BB%BA%E8%A1%A8/"},{"name":"sql常见操作","slug":"sql常见操作","link":"/tags/sql%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/"},{"name":"sql增删改查","slug":"sql增删改查","link":"/tags/sql%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/"},{"name":"sql函数","slug":"sql函数","link":"/tags/sql%E5%87%BD%E6%95%B0/"},{"name":"sql优化","slug":"sql优化","link":"/tags/sql%E4%BC%98%E5%8C%96/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"sqoop","slug":"sqoop","link":"/tags/sqoop/"},{"name":"sugar","slug":"sugar","link":"/tags/sugar/"},{"name":"superset","slug":"superset","link":"/tags/superset/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"},{"name":"各种表","slug":"各种表","link":"/tags/%E5%90%84%E7%A7%8D%E8%A1%A8/"},{"name":"Embedding based Product Retrieval in Taobao Search","slug":"Embedding-based-Product-Retrieval-in-Taobao-Search","link":"/tags/Embedding-based-Product-Retrieval-in-Taobao-Search/"},{"name":"中文文本分类","slug":"中文文本分类","link":"/tags/%E4%B8%AD%E6%96%87%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"text edit","slug":"text-edit","link":"/tags/text-edit/"},{"name":"文本生成评价指标","slug":"文本生成评价指标","link":"/tags/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"name":"Evaluation of Text Generation A Survey","slug":"Evaluation-of-Text-Generation-A-Survey","link":"/tags/Evaluation-of-Text-Generation-A-Survey/"},{"name":"文本改写和term分析","slug":"文本改写和term分析","link":"/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99%E5%92%8Cterm%E5%88%86%E6%9E%90/"},{"name":"TextGCN","slug":"TextGCN","link":"/tags/TextGCN/"},{"name":"tensorflow export","slug":"tensorflow-export","link":"/tags/tensorflow-export/"},{"name":"tf ranking","slug":"tf-ranking","link":"/tags/tf-ranking/"},{"name":"Tensorflow中的Seq2Seq全家桶","slug":"Tensorflow中的Seq2Seq全家桶","link":"/tags/Tensorflow%E4%B8%AD%E7%9A%84Seq2Seq%E5%85%A8%E5%AE%B6%E6%A1%B6/"},{"name":"PyTorch VS TensorFlow","slug":"PyTorch-VS-TensorFlow","link":"/tags/PyTorch-VS-TensorFlow/"},{"name":"tensorflow2.x 和tensorflow1.x的区别","slug":"tensorflow2-x-和tensorflow1-x的区别","link":"/tags/tensorflow2-x-%E5%92%8Ctensorflow1-x%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"name":"天池新闻推荐","slug":"天池新闻推荐","link":"/tags/%E5%A4%A9%E6%B1%A0%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90/"},{"name":"时间复杂度计算","slug":"时间复杂度计算","link":"/tags/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E8%AE%A1%E7%AE%97/"},{"name":"时间序列预测总结","slug":"时间序列预测总结","link":"/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E6%80%BB%E7%BB%93/"},{"name":"token embedding","slug":"token-embedding","link":"/tags/token-embedding/"},{"name":"Tokenization","slug":"Tokenization","link":"/tags/Tokenization/"},{"name":"框架依赖","slug":"框架依赖","link":"/tags/%E6%A1%86%E6%9E%B6%E4%BE%9D%E8%B5%96/"},{"name":"pytorch GPU训练","slug":"pytorch-GPU训练","link":"/tags/pytorch-GPU%E8%AE%AD%E7%BB%83/"},{"name":"pytorch常见操作","slug":"pytorch常见操作","link":"/tags/pytorch%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C/"},{"name":"torch可以把string变Tensor吗？","slug":"torch可以把string变Tensor吗？","link":"/tags/torch%E5%8F%AF%E4%BB%A5%E6%8A%8Astring%E5%8F%98Tensor%E5%90%97%EF%BC%9F/"},{"name":"pytorch搭建神经网络","slug":"pytorch搭建神经网络","link":"/tags/pytorch%E6%90%AD%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"模型联合训练","slug":"模型联合训练","link":"/tags/%E6%A8%A1%E5%9E%8B%E8%81%94%E5%90%88%E8%AE%AD%E7%BB%83/"},{"name":"训练,验证同步进行","slug":"训练-验证同步进行","link":"/tags/%E8%AE%AD%E7%BB%83-%E9%AA%8C%E8%AF%81%E5%90%8C%E6%AD%A5%E8%BF%9B%E8%A1%8C/"},{"name":"transformer综述","slug":"transformer综述","link":"/tags/transformer%E7%BB%BC%E8%BF%B0/"},{"name":"Transformer时间序列预测","slug":"Transformer时间序列预测","link":"/tags/Transformer%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/"},{"name":"Transformer-XL","slug":"Transformer-XL","link":"/tags/Transformer-XL/"},{"name":"transformer","slug":"transformer","link":"/tags/transformer/"},{"name":"字典树","slug":"字典树","link":"/tags/%E5%AD%97%E5%85%B8%E6%A0%91/"},{"name":"双指针","slug":"双指针","link":"/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"text Span抽取","slug":"text-Span抽取","link":"/tags/text-Span%E6%8A%BD%E5%8F%96/"},{"name":"处理无界和有界数据","slug":"处理无界和有界数据","link":"/tags/%E5%A4%84%E7%90%86%E6%97%A0%E7%95%8C%E5%92%8C%E6%9C%89%E7%95%8C%E6%95%B0%E6%8D%AE/"},{"name":"可视化报表","slug":"可视化报表","link":"/tags/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%A5%E8%A1%A8/"},{"name":"各组件web ui的地址","slug":"各组件web-ui的地址","link":"/tags/%E5%90%84%E7%BB%84%E4%BB%B6web-ui%E7%9A%84%E5%9C%B0%E5%9D%80/"},{"name":"权重初始化","slug":"权重初始化","link":"/tags/%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"name":"xgboost","slug":"xgboost","link":"/tags/xgboost/"},{"name":"XLNet","slug":"XLNet","link":"/tags/XLNet/"},{"name":"youtubednn","slug":"youtubednn","link":"/tags/youtubednn/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/tags/ZooKeeper/"}],"categories":[{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"大数据","slug":"大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"模型结构","slug":"机器学习/模型结构","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/"},{"name":"推荐系统","slug":"推荐系统","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"GNN","slug":"GNN","link":"/categories/GNN/"},{"name":"数据仓库","slug":"大数据/数据仓库","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"},{"name":"广告系统","slug":"广告系统","link":"/categories/%E5%B9%BF%E5%91%8A%E7%B3%BB%E7%BB%9F/"},{"name":"基础组件","slug":"大数据/基础组件","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/"},{"name":"排序","slug":"推荐系统/排序","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/"},{"name":"推荐系统","slug":"推荐系统/推荐系统","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"},{"name":"数学基础","slug":"机器学习/数学基础","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/"},{"name":"召回","slug":"推荐系统/召回","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"小帮手","slug":"GNN/小帮手","link":"/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"},{"name":"数据构造","slug":"机器学习/数据构造","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0/"},{"name":"即席查询","slug":"大数据/数据仓库/即席查询","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E5%8D%B3%E5%B8%AD%E6%9F%A5%E8%AF%A2/"},{"name":"Atlas","slug":"大数据/基础组件/Atlas","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Atlas/"},{"name":"搜索系统","slug":"搜索系统","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/"},{"name":"GCN","slug":"GNN/GCN","link":"/categories/GNN/GCN/"},{"name":"训练技巧","slug":"机器学习/训练技巧","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/"},{"name":"Grafana","slug":"大数据/基础组件/Grafana","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Grafana/"},{"name":"对话系统","slug":"对话系统","link":"/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"name":"java","slug":"java","link":"/categories/java/"},{"name":"Azkaban","slug":"大数据/基础组件/Azkaban","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Azkaban/"},{"name":"GNN","slug":"GNN/GNN","link":"/categories/GNN/GNN/"},{"name":"loss","slug":"机器学习/loss","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/loss/"},{"name":"基础算法","slug":"基础算法","link":"/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/"},{"name":"Kerberos","slug":"大数据/基础组件/Kerberos","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Kerberos/"},{"name":"Kylin","slug":"大数据/基础组件/Kylin","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Kylin/"},{"name":"排序","slug":"推荐系统/排序/排序","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E6%8E%92%E5%BA%8F/"},{"name":"基础组件","slug":"大数据/基础组件/基础组件","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/"},{"name":"大数据","slug":"大数据/大数据","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"集成学习","slug":"机器学习/集成学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习/机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"信息抽取","slug":"NLP/信息抽取","link":"/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/"},{"name":"数仓","slug":"大数据/数据仓库/数仓","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E4%BB%93/"},{"name":"PTM","slug":"NLP/PTM","link":"/categories/NLP/PTM/"},{"name":"深度学习框架","slug":"机器学习/深度学习框架","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"},{"name":"NLP","slug":"NLP/NLP","link":"/categories/NLP/NLP/"},{"name":"评价指标","slug":"机器学习/评价指标","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"},{"name":"数据库","slug":"大数据/数据库","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"spark","slug":"大数据/基础组件/spark","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/spark/"},{"name":"Prompt","slug":"NLP/Prompt","link":"/categories/NLP/Prompt/"},{"name":"文本表示","slug":"NLP/文本表示","link":"/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"},{"name":"时间序列预测","slug":"时间序列预测","link":"/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/"},{"name":"Zabbix","slug":"大数据/基础组件/Zabbix","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Zabbix/"},{"name":"对比学习","slug":"NLP/对比学习","link":"/categories/NLP/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/"},{"name":"数据集成","slug":"大数据/数据仓库/数据集成","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/"},{"name":"数据质量","slug":"大数据/数据仓库/数据质量","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F/"},{"name":"搜索系统","slug":"搜索系统/搜索系统","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/"},{"name":"数据安全","slug":"大数据/数据仓库/数据安全","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8/"},{"name":"flink","slug":"大数据/基础组件/flink","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/flink/"},{"name":"java","slug":"java/java","link":"/categories/java/java/"},{"name":"特征提取器","slug":"NLP/特征提取器","link":"/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/"},{"name":"小帮手","slug":"NLP/小帮手","link":"/categories/NLP/%E5%B0%8F%E5%B8%AE%E6%89%8B/"},{"name":"文本分类","slug":"NLP/文本分类","link":"/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"},{"name":"算法导论","slug":"基础算法/算法导论","link":"/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA/"},{"name":"部署","slug":"部署","link":"/categories/%E9%83%A8%E7%BD%B2/"},{"name":"排序","slug":"搜索系统/排序","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/"},{"name":"业务","slug":"业务","link":"/categories/%E4%B8%9A%E5%8A%A1/"},{"name":"文本匹配","slug":"NLP/文本匹配","link":"/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"},{"name":"特征工程","slug":"机器学习/特征工程","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"name":"ETL","slug":"大数据/数据仓库/ETL","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/ETL/"},{"name":"文本生成","slug":"NLP/文本生成","link":"/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/"},{"name":"小样本","slug":"NLP/小样本","link":"/categories/NLP/%E5%B0%8F%E6%A0%B7%E6%9C%AC/"},{"name":"web前后端","slug":"web前后端","link":"/categories/web%E5%89%8D%E5%90%8E%E7%AB%AF/"},{"name":"boosting","slug":"机器学习/集成学习/boosting","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting/"},{"name":"离线数仓","slug":"大数据/数据仓库/数仓/离线数仓","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E4%BB%93/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93/"},{"name":"flume","slug":"大数据/基础组件/flume","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/flume/"},{"name":"tensorflow","slug":"机器学习/深度学习框架/tensorflow","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/tensorflow/"},{"name":"pytorch","slug":"机器学习/深度学习框架/pytorch","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/pytorch/"},{"name":"关系型","slug":"大数据/数据库/关系型","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%85%B3%E7%B3%BB%E5%9E%8B/"},{"name":"hadoop","slug":"大数据/基础组件/hadoop","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/hadoop/"},{"name":"使用","slug":"大数据/基础组件/spark/使用","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/spark/%E4%BD%BF%E7%94%A8/"},{"name":"数据结构","slug":"基础算法/数据结构","link":"/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"非关系型","slug":"大数据/数据库/非关系型","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B/"},{"name":"hive","slug":"大数据/基础组件/hive","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/hive/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"数据库","slug":"大数据/数据库/数据库","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"sql","slug":"大数据/数据库/sql","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/sql/"},{"name":"ide","slug":"java/ide","link":"/categories/java/ide/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"召回","slug":"搜索系统/召回","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/"},{"name":"原理","slug":"大数据/基础组件/spark/原理","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/spark/%E5%8E%9F%E7%90%86/"},{"name":"query理解","slug":"搜索系统/query理解","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/query%E7%90%86%E8%A7%A3/"},{"name":"框架","slug":"java/框架","link":"/categories/java/%E6%A1%86%E6%9E%B6/"},{"name":"多线程编程","slug":"java/多线程编程","link":"/categories/java/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/"},{"name":"网络编程","slug":"java/网络编程","link":"/categories/java/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"基础语法","slug":"java/基础语法","link":"/categories/java/%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"},{"name":"jvm","slug":"java/jvm","link":"/categories/java/jvm/"},{"name":"验证","slug":"机器学习/验证","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AA%8C%E8%AF%81/"},{"name":"Kafka","slug":"大数据/基础组件/Kafka","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Kafka/"},{"name":"知识图谱","slug":"知识图谱","link":"/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"name":"listwise","slug":"推荐系统/排序/listwise","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/listwise/"},{"name":"leetcode","slug":"基础算法/leetcode","link":"/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/leetcode/"},{"name":"使用","slug":"大数据/基础组件/flink/使用","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/flink/%E4%BD%BF%E7%94%A8/"},{"name":"实时数仓","slug":"大数据/数据仓库/数仓/实时数仓","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E6%95%B0%E4%BB%93/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93/"},{"name":"元数据管理","slug":"大数据/数据仓库/元数据管理","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/"},{"name":"迭代分析","slug":"机器学习/迭代分析","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90/"},{"name":"集群监控","slug":"大数据/数据仓库/集群监控","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/"},{"name":"多标签","slug":"机器学习/多标签","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E6%A0%87%E7%AD%BE/"},{"name":"多任务学习","slug":"机器学习/多任务学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"name":"计算机网络","slug":"计算机网络","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"nginx","slug":"大数据/基础组件/nginx","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/nginx/"},{"name":"深度学习框架","slug":"机器学习/深度学习框架/深度学习框架","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"},{"name":"open set recognization","slug":"机器学习/open-set-recognization","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/open-set-recognization/"},{"name":"docker","slug":"部署/docker","link":"/categories/%E9%83%A8%E7%BD%B2/docker/"},{"name":"pairwise","slug":"推荐系统/排序/pairwise","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/pairwise/"},{"name":"粗排","slug":"搜索系统/排序/粗排","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E7%B2%97%E6%8E%92/"},{"name":"优化","slug":"大数据/基础组件/spark/优化","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/spark/%E4%BC%98%E5%8C%96/"},{"name":"电商","slug":"业务/电商","link":"/categories/%E4%B8%9A%E5%8A%A1/%E7%94%B5%E5%95%86/"},{"name":"phoenix","slug":"大数据/基础组件/phoenix","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/phoenix/"},{"name":"pointwise","slug":"推荐系统/排序/pointwise","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/pointwise/"},{"name":"集成学习","slug":"机器学习/集成学习/集成学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"},{"name":"原理","slug":"大数据/基础组件/flink/原理","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/flink/%E5%8E%9F%E7%90%86/"},{"name":"Ranger","slug":"大数据/基础组件/Ranger","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Ranger/"},{"name":"flask","slug":"web前后端/flask","link":"/categories/web%E5%89%8D%E5%90%8E%E7%AB%AF/flask/"},{"name":"shell","slug":"shell","link":"/categories/shell/"},{"name":"Solr","slug":"大数据/基础组件/Solr","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/Solr/"},{"name":"优化","slug":"大数据/基础组件/flink/优化","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/flink/%E4%BC%98%E5%8C%96/"},{"name":"sqoop","slug":"大数据/基础组件/sqoop","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/sqoop/"},{"name":"可视化报表","slug":"大数据/数据仓库/可视化报表","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8A%A5%E8%A1%A8/"},{"name":"superset","slug":"大数据/基础组件/superset","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/superset/"},{"name":"clickhouse","slug":"大数据/数据库/关系型/clickhouse","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%85%B3%E7%B3%BB%E5%9E%8B/clickhouse/"},{"name":"小帮手","slug":"推荐系统/排序/小帮手","link":"/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E5%B0%8F%E5%B8%AE%E6%89%8B/"},{"name":"Tokenization","slug":"NLP/Tokenization","link":"/categories/NLP/Tokenization/"},{"name":"HBase","slug":"大数据/数据库/非关系型/HBase","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B/HBase/"},{"name":"hql","slug":"大数据/基础组件/hive/hql","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/hive/hql/"},{"name":"hive","slug":"大数据/基础组件/hive/hive","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/hive/hive/"},{"name":"倒排索引","slug":"搜索系统/召回/倒排索引","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/"},{"name":"框架","slug":"java/框架/框架","link":"/categories/java/%E6%A1%86%E6%9E%B6/%E6%A1%86%E6%9E%B6/"},{"name":"jdbc","slug":"java/框架/jdbc","link":"/categories/java/%E6%A1%86%E6%9E%B6/jdbc/"},{"name":"maven","slug":"java/框架/maven","link":"/categories/java/%E6%A1%86%E6%9E%B6/maven/"},{"name":"深度学习","slug":"部署/深度学习","link":"/categories/%E9%83%A8%E7%BD%B2/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"postman","slug":"web前后端/postman","link":"/categories/web%E5%89%8D%E5%90%8E%E7%AB%AF/postman/"},{"name":"springboot","slug":"java/框架/springboot","link":"/categories/java/%E6%A1%86%E6%9E%B6/springboot/"},{"name":"向量召回","slug":"搜索系统/召回/向量召回","link":"/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%90%91%E9%87%8F%E5%8F%AC%E5%9B%9E/"},{"name":"ZooKeeper","slug":"大数据/基础组件/ZooKeeper","link":"/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6/ZooKeeper/"}]}