<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: PTM - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/NLP/">NLP</a></li><li class="is-active"><a href="#" aria-current="page">PTM</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-06-30  <a class="commentCountImg" href="/2022/06/30/micro-Grained/#comment-container"><span class="display-none-class">cb2f67bb88e49e2e6d7a61905ca0e5f3</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="cb2f67bb88e49e2e6d7a61905ca0e5f3">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/30/micro-Grained/">细粒度NLP任务</a></h1><div class="content"><p>AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization （ByteDance AI Lab）</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.11869.pdf">https://arxiv.org/pdf/2008.11869.pdf</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2></div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E7%BB%86%E7%B2%92%E5%BA%A6NLP%E4%BB%BB%E5%8A%A1/">细粒度NLP任务</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-06-17  <a class="commentCountImg" href="/2022/06/17/chinesebert/#comment-container"><span class="display-none-class">bda1c146ae84c6c61553927c0c0f34f6</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="bda1c146ae84c6c61553927c0c0f34f6">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/17/chinesebert/">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</a></h1><div class="content"><p>考虑字形和拼音的中文PTM</p>
<h2 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1 模型结构"></a>1 模型结构</h2><p><img src="/2022/06/17/chinesebert/1.JPG" alt></p>
<p><strong>变动在bert的输入</strong></p>
<p>原来Char embedding+Position embedding+segment embedding-&gt; 现在 Fusion embedding+Position embedding （omit the segment embedding）</p>
<p>Char embedding +Glyph ( 字形 ) embedding +Pinyin （拼音）embedding -》Fusion embedding</p>
<h2 id="2-预训练任务"><a href="#2-预训练任务" class="headerlink" title="2 预训练任务"></a>2 预训练任务</h2><p> Whole Word Masking (WWM) and Char Masking (CM)</p>
<h2 id="3-使用"><a href="#3-使用" class="headerlink" title="3 使用"></a>3 使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from datasets.bert_dataset import BertDataset</span><br><span class="line">&gt;&gt;&gt; from models.modeling_glycebert import GlyceBertModel</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; tokenizer = BertDataset([CHINESEBERT_PATH])</span><br><span class="line">&gt;&gt;&gt; chinese_bert = GlyceBertModel.from_pretrained([CHINESEBERT_PATH])</span><br><span class="line">&gt;&gt;&gt; sentence = &#x27;我喜欢猫&#x27;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)</span><br><span class="line">&gt;&gt;&gt; length = input_ids.shape[0]</span><br><span class="line">&gt;&gt;&gt; input_ids = input_ids.view(1, length)</span><br><span class="line">&gt;&gt;&gt; pinyin_ids = pinyin_ids.view(1, length, 8)</span><br><span class="line">&gt;&gt;&gt; output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[0]</span><br><span class="line">&gt;&gt;&gt; print(output_hidden)</span><br><span class="line">tensor([[[ 0.0287, -0.0126,  0.0389,  ...,  0.0228, -0.0677, -0.1519],</span><br><span class="line">         [ 0.0144, -0.2494, -0.1853,  ...,  0.0673,  0.0424, -0.1074],</span><br><span class="line">         [ 0.0839, -0.2989, -0.2421,  ...,  0.0454, -0.1474, -0.1736],</span><br><span class="line">         [-0.0499, -0.2983, -0.1604,  ..., -0.0550, -0.1863,  0.0226],</span><br><span class="line">         [ 0.1428, -0.0682, -0.1310,  ..., -0.1126,  0.0440, -0.1782],</span><br><span class="line">         [ 0.0287, -0.0126,  0.0389,  ...,  0.0228, -0.0677, -0.1519]]],</span><br><span class="line">       grad_fn=&lt;NativeLayerNormBackward&gt;)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://github.com/ShannonAI/ChineseBert">https://github.com/ShannonAI/ChineseBert</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.16038.pdf">https://arxiv.org/pdf/2106.16038.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-06-11  <a class="commentCountImg" href="/2022/06/11/finetune/#comment-container"><span class="display-none-class">ff2ed5a2fbb088083a33f8c473f3db69</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="ff2ed5a2fbb088083a33f8c473f3db69">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/06/11/finetune/">finetune</a></h1><div class="content"><h2 id="1-使用哪些层参与下游任务"><a href="#1-使用哪些层参与下游任务" class="headerlink" title="1 使用哪些层参与下游任务"></a>1 使用哪些层参与下游任务</h2><p>使用哪些层参与下游任务</p>
<p>选择的层model1+下游任务model2</p>
<p>对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。</p>
<p>用$\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\cdot)$为特定的任务模型。有以下几种方法选择representation:</p>
<p><strong>a) Embedding Only</strong></p>
<p>choose only the pre-trained static embeddings，即$g(\textbf{H}^{1})$</p>
<p><strong>b) Top Layer</strong></p>
<p>选择顶层的representation，然后接入特定的任务模型，即$g(\textbf{H}^{L})$</p>
<p><strong>c) All Layers</strong></p>
<p>输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下</p>
<script type="math/tex; mode=display">
g(\textbf{r}_t)=g(\gamma \sum_{l=1}^{L}\alpha_l\textbf{H}^{(l)})</script><p>其中$\alpha$ is the softmax-normalized weight for layer $l$ and $\gamma$ is a scalar to scale the vectors output by pre-trained model</p>
<h2 id="2-参数是否固定"><a href="#2-参数是否固定" class="headerlink" title="2 参数是否固定"></a>2 参数是否固定</h2><p>总共有两种常用的模型迁移方式：<strong>feature extraction</strong> (where the pre-trained parameters are frozen), and <strong>fine-tuning</strong> (where the pre-trained parameters are unfrozen and fine-tuned).</p>
<p><img src="/2022/06/11/finetune/ptm7.JPG" alt></p>
<h2 id="3-Fine-Tuning-Strategies"><a href="#3-Fine-Tuning-Strategies" class="headerlink" title="3 Fine-Tuning Strategies"></a>3 Fine-Tuning Strategies</h2><p><strong>Two-stage fine-tuning</strong></p>
<p>第一阶段为中间任务，第二阶段为目标任务</p>
<p><strong>Multi-task fine-tuning</strong></p>
<p>multi-task learning and pre-training are complementary technologies.</p>
<p><strong>Fine-tuning with extra adaptation modules</strong></p>
<p>The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed.</p>
<p><strong>Others</strong></p>
<p>self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/finetune/">finetune</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-05-29  <a class="commentCountImg" href="/2022/05/29/pretrain-task/#comment-container"><span class="display-none-class">caa4d2c00fbf24ca0cc72ecc916f97fe</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="caa4d2c00fbf24ca0cc72ecc916f97fe">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/05/29/pretrain-task/">预训练任务</a></h1><div class="content"><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p><img src="/2022/05/29/pretrain-task/2.JPG" alt></p>
<p><img src="/2022/05/29/pretrain-task/1.JPG" alt></p>
<p>TLM : Translation Language Modeling</p>
<p>DAE: Denoising Autoencoder</p>
<p>CTL: Contrastive Learning</p>
<p>RTD： Replaced Token Detection </p>
<p>SOP：Sentence Order Prediction</p>
<p>DIM：Deep InfoMAx</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360892229">https://zhuanlan.zhihu.com/p/360892229</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1/">预训练任务</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-03-28  <a class="commentCountImg" href="/2022/03/28/ptm-relation/#comment-container"><span class="display-none-class">81968be5cd4d92e9426b0d24ca360006</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="81968be5cd4d92e9426b0d24ca360006">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/28/ptm-relation/">ptm之间的联系</a></h1><div class="content"><p><img src="/2022/03/28/ptm-relation/1.jpg" alt></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/ptm%E4%B9%8B%E9%97%B4%E7%9A%84%E8%81%94%E7%B3%BB/">ptm之间的联系</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-04  <a class="commentCountImg" href="/2021/11/04/bert-wwm/#comment-container"><span class="display-none-class">b17e53cb12eb2fba48a006bf6a67add0</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="b17e53cb12eb2fba48a006bf6a67add0">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/04/bert-wwm/">Pre-Training with Whole Word Masking for Chinese BERT</a></h1><div class="content"><p>BERT-wwm-ext</p>
<p>wwm：whole word mask</p>
<p>ext： we also use extended training data  (mark with ext in the model name)</p>
<h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>1 改变mask策略</p>
<p>Whole Word Masking，wwm</p>
<p><img src="/2021/11/04/bert-wwm/1.JPG" alt></p>
<p>cws: Chinese Word Segmentation</p>
<p>对比四种mask策略</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Pre-Training with Whole Word Masking for Chinese BERT</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08101v3">https://arxiv.org/abs/1906.08101v3</a></p>
<p>Revisiting Pre-trained Models for Chinese Natural Language Processing</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.13922">https://arxiv.org/abs/2004.13922</a></p>
<p>github：<a target="_blank" rel="noopener" href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-04  <a class="commentCountImg" href="/2021/11/04/albert/#comment-container"><span class="display-none-class">218eec650c59229902b7dd7e3520a172</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="218eec650c59229902b7dd7e3520a172">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/04/albert/">ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></h1><div class="content"><p>There are three main contributions that ALBERT makes over the design choices of BERT：</p>
<p><strong>1 Factorized embedding parameterization</strong></p>
<p>原来embedding层是一个矩阵$M_{emb[V\times H]} $,现在变为两个$M_{emb1[V\times E]}$和$M_{emb2[E\times H]}$,参数量从VH变为VE+EH（This parameter reduction is significant when H &gt;&gt; E.）</p>
<p><strong>2 Cross-layer parameter sharing</strong></p>
<p>The default decision for ALBERT is to share all parameters across layers（attention，FFN)）</p>
<p><strong>3 Inter-sentence coherence loss</strong></p>
<p>原来的NSP改为现在的sop，正例的构建和NSP是一样的，不过负例则是将两句话反过来。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88099919">https://zhuanlan.zhihu.com/p/88099919</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37947156/article/details/101529943">https://blog.csdn.net/weixin_37947156/article/details/101529943</a></p>
<p><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=H1eA7AEtvS">https://openreview.net/pdf?id=H1eA7AEtvS</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/ch-word-bert/#comment-container"><span class="display-none-class">6fc64fa940d006e20b55dd3fa555894d</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6fc64fa940d006e20b55dd3fa555894d">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/ch-word-bert/">中文词粒度BERT</a></h1><div class="content"><p><strong>1 Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</strong></p>
<p>we find that charbased（字粒度） models consistently <strong>outperform</strong> wordbased （词粒度）models.</p>
<p>We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. </p>
<p><strong>2 腾讯中文词模型</strong></p>
<p>词模型在公开数据集的表现逊于字模型</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.05526.pdf">https://arxiv.org/pdf/1905.05526.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-06-27-17">https://www.jiqizhixin.com/articles/2019-06-27-17</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/T5/#comment-container"><span class="display-none-class">9b456cc8675c74a5066445aae3f9352b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9b456cc8675c74a5066445aae3f9352b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/T5/">T5</a></h1><div class="content"><p><strong>T5</strong></p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">https://arxiv.org/pdf/1910.10683.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88438851">https://zhuanlan.zhihu.com/p/88438851</a></p>
<p><strong>mT5</strong></p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11934.pdf">https://arxiv.org/pdf/2010.11934.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/302380842">https://zhuanlan.zhihu.com/p/302380842</a></p>
<p><strong>Sentence-T5</strong>（文本表示新SOTA）</p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.08877.pdf">https://arxiv.org/pdf/2108.08877.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/403153114">https://zhuanlan.zhihu.com/p/403153114</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-15  <a class="commentCountImg" href="/2021/09/15/roberta/#comment-container"><span class="display-none-class">607add707f93360424dff10eb802fe91</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="607add707f93360424dff10eb802fe91">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/15/roberta/">RoBERTa A Robustly Optimized BERT Pretraining Approach</a></h1><div class="content"><h2 id="1-和BERT比较"><a href="#1-和BERT比较" class="headerlink" title="1.和BERT比较"></a>1.和BERT比较</h2><p>在结构上和原版BERT没有差异，主要的改动在于：</p>
<p><img src="/2021/09/15/roberta/55.JPG" alt></p>
<h2 id="2-改动分析"><a href="#2-改动分析" class="headerlink" title="2.改动分析"></a>2.改动分析</h2><h3 id="2-1-Static-vs-Dynamic-Masking"><a href="#2-1-Static-vs-Dynamic-Masking" class="headerlink" title="2.1 Static vs. Dynamic Masking"></a>2.1 Static vs. Dynamic Masking</h3><p>static masking: 原本的BERT采用的是static mask的方式，就是在create pretraining data中，先对数据进行提前的mask</p>
<p>dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。</p>
<p>结果对比：</p>
<p><img src="/2021/09/15/roberta/22.JPG" alt></p>
<p>结论：动态占优</p>
<h3 id="2-2-Model-Input-Format-and-Next-Sentence-Prediction"><a href="#2-2-Model-Input-Format-and-Next-Sentence-Prediction" class="headerlink" title="2.2 Model Input Format and Next Sentence Prediction"></a>2.2 Model Input Format and Next Sentence Prediction</h3><p>做了结果对比试验，结果如下：</p>
<p><img src="/2021/09/15/roberta/33.JPG" alt></p>
<p>结论：</p>
<p>Model Input Format: </p>
<p>​    1.find that using individual sentences hurts performance on downstream tasks</p>
<p>Next Sentence Prediction: </p>
<p>​    1.removing the NSP loss matches or slightly improves downstream task performance</p>
<h3 id="2-3-Training-with-large-batches"><a href="#2-3-Training-with-large-batches" class="headerlink" title="2.3 Training with large batches"></a>2.3 Training with large batches</h3><p><img src="/2021/09/15/roberta/44.JPG" alt></p>
<h3 id="2-4-Text-Encoding"><a href="#2-4-Text-Encoding" class="headerlink" title="2.4 Text Encoding"></a>2.4 Text Encoding</h3><p>采用BBPE而不是wordpiece</p>
<h2 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3 常见问题"></a>3 常见问题</h2><p><strong>1 roberta tokenizer 没有token_type_ids？</strong><br>roberta 取消了NSP，所以不需要segment embedding 也就不需要token_type_ids，但是使用的时候发现中文是有token_type_ids的，英文没有token_type_ids的。没有token_type_ids，两句话怎么区别，分隔符sep还是有的，只是没有segment embedding </p>
<p><strong>2 使用避坑</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zwqjoy/article/details/107533184">https://blog.csdn.net/zwqjoy/article/details/107533184</a></p>
<p><a target="_blank" rel="noopener" href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/103205929">https://zhuanlan.zhihu.com/p/103205929</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/143064748">https://zhuanlan.zhihu.com/p/143064748</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zwqjoy/article/details/107533184">https://blog.csdn.net/zwqjoy/article/details/107533184</a></p>
<p><a target="_blank" rel="noopener" href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/NLP/PTM/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/NLP/PTM/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/NLP/PTM/">1</a></li><li><a class="pagination-link" href="/categories/NLP/PTM/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">405</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">142</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">386</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:58:45.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/gray-test/">灰度测试</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:53:30.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/abtest/">abtest</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:51:45.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/diff/">diff评测</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:44:10.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/seach-evluation/">搜索系统评价指标</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:29:10.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/nlp_evalution/">NLP评价指标</a></p><p class="categories"><a href="/categories/NLP/">NLP</a> / <a href="/categories/NLP/NLP/">NLP</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>