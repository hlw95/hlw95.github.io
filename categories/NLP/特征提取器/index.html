<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 特征提取器 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/NLP/">NLP</a></li><li class="is-active"><a href="#" aria-current="page">特征提取器</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-06  <a class="commentCountImg" href="/2021/12/06/transformer-xl/#comment-container"><span class="display-none-class">3d8bcf90c1ebc82a648361c567d855a6</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="3d8bcf90c1ebc82a648361c567d855a6">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/06/transformer-xl/">Transformer-XL  Attentive Language Models Beyond a Fixed-Length Context</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a></p>
<p>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling ( memory and computation受限，长度不可能很大 ).  propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence.</p>
<h2 id="3-Model"><a href="#3-Model" class="headerlink" title="3 Model"></a>3 Model</h2><h3 id="3-1-Vanilla-Transformer-Language-Models"><a href="#3-1-Vanilla-Transformer-Language-Models" class="headerlink" title="3.1 Vanilla Transformer Language Models"></a>3.1 Vanilla Transformer Language Models</h3><p><img src="/2021/12/06/transformer-xl/1.JPG" alt></p>
<p>问题：In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.通常做法为vanilla model。 vanilla model就是说把长文本分隔成固定长度的seg来处理，如上图。</p>
<p>During  training，There are two critical limitations of using a fixed length context. First, the largest possible dependency length is upper bounded by the segment length. Second. simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem</p>
<p>During evaluation, As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive.</p>
<h3 id="3-2-Segment-Level-Recurrence-with-State-Reuse"><a href="#3-2-Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="3.2 Segment-Level Recurrence with State Reuse"></a>3.2 Segment-Level Recurrence with State Reuse</h3><p> introduce a <strong>recurrence mechanism</strong> to the Transformer architecture.</p>
<p>定义变量</p>
<p><img src="/2021/12/06/transformer-xl/2.JPG" alt></p>
<p>转换过程</p>
<p><img src="/2021/12/06/transformer-xl/3.JPG" alt></p>
<p>SG(） stands for stop-gradient，$\circ$ 表示矩阵拼接</p>
<p>具体过程如下图</p>
<p><img src="/2021/12/06/transformer-xl/4.JPG" alt></p>
<p>During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a.</p>
<p>during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model.</p>
<h3 id="3-3-Relative-Positional-Encodings"><a href="#3-3-Relative-Positional-Encodings" class="headerlink" title="3.3 Relative Positional Encodings"></a>3.3 Relative Positional Encodings</h3><p>how can we keep the positional information coherent when we reuse the states? 如果保留原来的位置编码形式，可以得到如下</p>
<p><img src="/2021/12/06/transformer-xl/5.JPG" alt></p>
<p>这种方式存在问题：</p>
<p><img src="/2021/12/06/transformer-xl/9.JPG" alt></p>
<p>为了解决这个问题提出了relative positional information。</p>
<p>standard Transformer</p>
<p><img src="/2021/12/06/transformer-xl/6.JPG" alt></p>
<p>we propose</p>
<p><img src="/2021/12/06/transformer-xl/7.JPG" alt></p>
<h3 id="3-4-完整算法流程"><a href="#3-4-完整算法流程" class="headerlink" title="3.4 完整算法流程"></a>3.4 完整算法流程</h3><p><img src="/2021/12/06/transformer-xl/8.JPG" alt></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/Transformer-XL/">Transformer-XL</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/transformer-survey/#comment-container"><span class="display-none-class">9f00e3da6337f991e5da2420e8a4f308</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9f00e3da6337f991e5da2420e8a4f308">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/transformer-survey/">transformer综述</a></h1><div class="content"><p>Transformer-XL</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860v3">https://arxiv.org/abs/1901.02860v3</a></p>
<p>RoFormer</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864.pdf">https://arxiv.org/pdf/2104.09864.pdf</a></p>
<p>google2020出品的transformer的综述</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.06732.pdf">https://arxiv.org/pdf/2009.06732.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/transformer%E7%BB%BC%E8%BF%B0/">transformer综述</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-12  <a class="commentCountImg" href="/2021/10/12/attention/#comment-container"><span class="display-none-class">0d7bf89d0a0926a20bc7682e4c4a5c18</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="0d7bf89d0a0926a20bc7682e4c4a5c18">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/12/attention/">attention总结</a></h1><div class="content"><p>1.Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p>
<p>提出了两种 attention 模式，即 hard attention 和 soft attention</p>
<p>2.Effective Approaches to Attention-based Neural Machine Translation</p>
<p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。</p>
<p>3.Attention Is All You Need</p>
<p>提出self attention</p>
<p>4.Hierarchical Attention Networks for Document Classification</p>
<p>提出了Hierarchical Attention用于文档分类</p>
<p>5.Attention-over-Attention Neural Networks for Reading Comprehension</p>
<p>提出了Attention Over Attention的Attention机制</p>
<p>6.Convolutional Sequence to Sequence Learning</p>
<p>论文中还采用了 Multi-step Attention</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/attention/">attention</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-15  <a class="commentCountImg" href="/2021/09/15/rnn/#comment-container"><span class="display-none-class">0d9efcf05de62def8a88e43181dbe72e</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="0d9efcf05de62def8a88e43181dbe72e">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/15/rnn/">RNN总结</a></h1><div class="content"><h2 id="1-单元"><a href="#1-单元" class="headerlink" title="1.单元"></a>1.单元</h2><h3 id="1-1-普通RNN单元"><a href="#1-1-普通RNN单元" class="headerlink" title="1.1 普通RNN单元"></a>1.1 普通RNN单元</h3><h3 id="1-2-LSTM"><a href="#1-2-LSTM" class="headerlink" title="1.2 LSTM"></a>1.2 LSTM</h3><h3 id="1-3-GRU"><a href="#1-3-GRU" class="headerlink" title="1.3 GRU"></a>1.3 GRU</h3><h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><h3 id="1-1-输入、输出"><a href="#1-1-输入、输出" class="headerlink" title="1.1 输入、输出"></a>1.1 输入、输出</h3><p><img src="/2021/09/15/rnn/11.JPG" alt></p>
<h3 id="1-2-是否双向"><a href="#1-2-是否双向" class="headerlink" title="1.2 是否双向"></a>1.2 是否双向</h3><p><img src="/2021/09/15/rnn/22.png" alt></p>
<h3 id="1-3-是否堆叠"><a href="#1-3-是否堆叠" class="headerlink" title="1.3 是否堆叠"></a>1.3 是否堆叠</h3><p><img src="/2021/09/15/rnn/33.png" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gaohanjie123/article/details/88699664">https://blog.csdn.net/gaohanjie123/article/details/88699664</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Luv-GEM/p/10788849.html">https://www.cnblogs.com/Luv-GEM/p/10788849.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/RNN/">RNN</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-18  <a class="commentCountImg" href="/2021/07/18/transformer/#comment-container"><span class="display-none-class">4b6ab54568a681a24f6047500db0eb87</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4b6ab54568a681a24f6047500db0eb87">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.8 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/18/transformer/">transformer(attention is all your need)</a></h1><div class="content"><p>1.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. </p>
<p>2.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.</p>
<p><img src="/2021/07/18/transformer/transformer.JPG" alt></p>
<h2 id="1-Positional-Encoding"><a href="#1-Positional-Encoding" class="headerlink" title="1 Positional Encoding"></a>1 Positional Encoding</h2><p>in order for the model to <strong>make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>详细可参考 <a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1453/">https://wmathor.com/index.php/archives/1453/</a></p>
<h2 id="2-Attention"><a href="#2-Attention" class="headerlink" title="2 Attention"></a>2 Attention</h2><p><img src="/2021/07/18/transformer/11.jpg" alt></p>
<p>其中不同颜色表示不同head，颜色深浅表示词的关联程度。</p>
<p>不同head表示不同应用场景 ，单一head表示某个场景下，各个字之间的关联程度</p>
<h3 id="1-Scaled-Dot-Product-Attention"><a href="#1-Scaled-Dot-Product-Attention" class="headerlink" title="1 Scaled Dot-Product Attention"></a>1 Scaled Dot-Product Attention</h3><p><img src="/2021/07/18/transformer/self-attention.JPG" alt></p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_{k}}})V</script><p>$d_{k}$  ： keys of dimension</p>
<p>为什么scale？We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</p>
<p><strong>Mask</strong></p>
<p>可以分为两类：Attention Mask和Padding Mask，接下来具体讲解。</p>
<p>1.Attention Mask</p>
<p>ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p>
<p>sotfmax前要mask，上三角mask掉</p>
<p><img src="/2021/07/18/transformer/11.jpeg" alt></p>
<p>2.Padding Mask</p>
<p>Padding位置上的信息是无效的，所以需要丢弃。</p>
<p>过程如下图示：</p>
<p><img src="/2021/07/18/transformer/44.jpg" alt></p>
<p><img src="/2021/07/18/transformer/66.jpg" alt></p>
<h3 id="2-Multi-Head-Attention"><a href="#2-Multi-Head-Attention" class="headerlink" title="2 Multi-Head Attention"></a>2 Multi-Head Attention</h3><p><img src="/2021/07/18/transformer/self-attention2.JPG" alt></p>
<p><strong>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</strong></p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head1,...,head_n)W^O
\\ where  \ head_{i}=Attetion(QW_{i}^{Q},kW_{i}^{K},VW_{i}^{V})</script><h3 id="3-Applications-of-Attention-in-our-Model"><a href="#3-Applications-of-Attention-in-our-Model" class="headerlink" title="3 Applications of Attention in our Model"></a>3 Applications of Attention in our Model</h3><p><strong>1.encoder-decoder attention layers</strong></p>
<p>结构：queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.</p>
<p>目的：This allows every position in the decoder to attend over all positions in the input sequence.</p>
<p><strong>2.encoder contains self-attention layers</strong></p>
<p>结构：keys, values and queries come from the same place</p>
<p>目的：Each position in the encoder can attend to all positions in the previous layer of the encoder.</p>
<p><strong>3.self-attention layers in the decoder</strong></p>
<p>结构：keys, values and queries come from the same place</p>
<p>目的：allow each position in the decoder to attend to all positions in the decoder up to and including that position</p>
<h2 id="3-Encoder-and-Decoder-Stacks"><a href="#3-Encoder-and-Decoder-Stacks" class="headerlink" title="3 Encoder and Decoder Stacks"></a>3 Encoder and Decoder Stacks</h2><h3 id="1-encoder"><a href="#1-encoder" class="headerlink" title="1 encoder"></a>1 encoder</h3><p>1).Input Embedding与Positional Encoding</p>
<script type="math/tex; mode=display">
X = \text{Input Embedding}+ \text{Positional Encoding}\\</script><p>2). multi-head attention</p>
<script type="math/tex; mode=display">
Q = \text{Linear}_q(X) = XW_{Q}\\
K = \text{Linear}_k(X) = XW_{K}\\
V = \text{Linear}_v(X) = XW_{V}\\
X_{attention} = \text{Attention}(Q,K,V)</script><p>3). 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
X_{attention} = X + X_{attention}\\
X_{attention} = \text{LayerNorm}(X_{attention})</script><p>4). FeedForward</p>
<script type="math/tex; mode=display">
X_{hidden} = \text{Linear}(\text{ReLU}(\text{Linear}(X_{attention})))</script><p>5). 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
X_{hidden} = X_{attention} + X_{hidden}\\
X_{hidden} = \text{LayerNorm}(X_{hidden})</script><p>其中$ X_{hidden} \in \mathbb{R}^{batch_size  \ <em> \  seq_len \  </em> \  embed_dim} $</p>
<h3 id="2-decoder"><a href="#2-decoder" class="headerlink" title="2 decoder"></a>2 decoder</h3><p>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：</p>
<ul>
<li>Masked Multi-Head Self-Attention</li>
<li>Multi-Head Encoder-Decoder Attention</li>
<li>FeedForward Network</li>
</ul>
<h2 id="4-常见问题"><a href="#4-常见问题" class="headerlink" title="4 常见问题"></a>4 常见问题</h2><p><strong>1 并行化</strong></p>
<p>训练encoder，decoder都并行，测试encoder并行，decoder不是并行</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/368592551">https://zhuanlan.zhihu.com/p/368592551</a></p>
<p><strong>2 self-attention和普通attention的区别</strong></p>
<p>取决于query和key是否在一个地方</p>
<p><strong>3 Why Self-Attention</strong></p>
<p>Motivating our use of self-attention we consider three desiderata.</p>
<p>1.One is the total <strong>computational complexity</strong> per layer.</p>
<p>2.Another is the amount of computation that can be <strong>parallelized</strong>, as measured by the minimum number of sequential operations required.</p>
<p>3.The third is the path length between <strong>long-range dependencies</strong> in the network</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<p>大佬详解： <a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/transformer/">transformer</a></div><hr></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">425</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">405</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:56:21.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/jar/">jar</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:56:07.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-compile/">java编译</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:29:22.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/gc/">gc</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T05:37:12.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-optimize/">java性能优化</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T05:36:18.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-mem/">java内存泄露</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>