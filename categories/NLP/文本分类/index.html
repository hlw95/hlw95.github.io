<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 文本分类 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/NLP/">NLP</a></li><li class="is-active"><a href="#" aria-current="page">文本分类</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-01-19  <a class="commentCountImg" href="/2022/01/19/textgcn/#comment-container"><span class="display-none-class">dfd456161c6a73a2b627dc04717f15ad</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="dfd456161c6a73a2b627dc04717f15ad">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/19/textgcn/">TextGCN Graph Convolutional Networks for Text Classification</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.05679">https://arxiv.org/abs/1809.05679</a></p>
<p>1.build a single text graph for a <strong>corpus</strong> based on word co-occurrence and document word relations,</p>
<p>2.then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. </p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/GCN/">GCN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/TextGCN/">TextGCN</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-17  <a class="commentCountImg" href="/2021/09/17/doc-bert/#comment-container"><span class="display-none-class">497ee909ce15293540c44e2680c35f52</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="497ee909ce15293540c44e2680c35f52">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/17/doc-bert/">HIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICATION</a></h1><div class="content"><p>原版BERT的最大输入为512，为了使得BERT能解决超长文本的问题，作者在finetune阶段提出了两种策略来弥补这个问题，即利用BERT+LSTM或者BERT+transformer。</p>
<p>核心步骤：</p>
<p>1.split the input sequence into segments of a fixed size with overlap.</p>
<p>2.For each of these segments, we obtain H or P from BERT model.</p>
<p><img src="/2021/09/17/doc-bert/11.JPG" alt></p>
<p>3.We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer.//replacing the LSTM recurrent layer in favor of a small Transformer model</p>
<p>4.Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC/">超长文本</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-23  <a class="commentCountImg" href="/2021/08/23/text-cnn/#comment-container"><span class="display-none-class">4e2b48a121bb0f97a9742bd9e3cf6e2c</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4e2b48a121bb0f97a9742bd9e3cf6e2c">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>6 m  <i class="fas fa-pencil-alt"> </i>1.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/23/text-cnn/">TextCNN TextRNN TextRCNN</a></h1><div class="content"><h2 id="1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="1.TextCNN (Convolutional Neural Networks for Sentence Classification)"></a>1.TextCNN (Convolutional Neural Networks for Sentence Classification)</h2><p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a></p>
<p>调参论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1510.03820">https://arxiv.org/abs/1510.03820</a></p>
<p><img src="/2021/08/23/text-cnn/textcnn1.JPG" alt></p>
<p><img src="/2021/08/23/text-cnn/1111.JPG" alt></p>
<p>模型的整体结构如上所示。Feature Map是输入图像经过神经网络卷积产生的结果，filter是卷积核。</p>
<p><strong>输入表示：</strong></p>
<p>假设输入文本的长度为$n$，对于长度不够的需要做padding，任意一个单词可以用一个$k$维的向量表示，即$X_i \in \mathbb{R}^{k}$，那么一个句子可以表示为</p>
<script type="math/tex; mode=display">
X_{1:n}=X_1 \oplus X_2\oplus...\oplus X_n</script><p>其中$\oplus$是向量拼接操作，$X_{1:n} \in \mathbb{R}^{nk\times 1}$。</p>
<p><strong>卷积</strong>：</p>
<p>对于某个滑窗$X_{i,i+h-1}=\{X_i,X_{i+1},…,X_{i+h-1}\}$经过某个卷积核$W_j$可得</p>
<script type="math/tex; mode=display">
c_{i,j}=f(W_j\cdot X_{i,i+h-1}+b)</script><p>其中$f=tanh(\cdot)$，$W_j\in \mathbb{R}^{ 1\times hk}，c_{i,j} $是标量</p>
<p>假设卷积通道数为$m$，在NLP中，卷积滑动步伐$k=1$，那么经过卷积层后得到的完整的特征矩阵为</p>
<script type="math/tex; mode=display">
C=[[c_{1,1},c_{2,1},...,c_{n-h+1,1}]^T,[c_{1,2},c_{2,2},...,c_{n-h+1,2}]^T,...,[c_{1,m},c_{2,m},...,c_{n-h+1,m}]^T]</script><p>其中$C \in \mathbb{R}^{(n-h+1)\times m}$</p>
<p><strong>maxpooling</strong>：</p>
<script type="math/tex; mode=display">
\hat{C}=max\{C\} , \hat{C}\in \mathbb{R}^{m}</script><p><strong>全连接</strong>：</p>
<p>然后将$\hat{C}$接个全连接，就可以做分类或者回归任务了。</p>
<h2 id="2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning"><a href="#2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning" class="headerlink" title="2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)"></a>2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)</h2><p>原文 <a target="_blank" rel="noopener" href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">https://www.ijcai.org/Proceedings/16/Papers/408.pdf</a></p>
<p><img src="/2021/08/23/text-cnn/textrnn.JPG" alt></p>
<p><img src="/2021/08/23/text-cnn/11.png" alt></p>
<p>该文的场景为Recurrent Neural Network for Text Classification with Multi-Task Learning，就是论文的题目。文中给出了三种结构，如上图所示，图中的RNN单元为LSTM。</p>
<p><strong>Model-I: Uniform-Layer Architecture</strong></p>
<p>对于任务$m$，输入$\hat X_t$包含两个部分</p>
<script type="math/tex; mode=display">
\hat{X}_t^{(m)}=X_{t}^{(m)}\oplus X_{t}^{(s)}</script><p>其中$X_{t}^{(m)}$表示特定任务的词向量，$X_{t}^{(s)}$表示共享的词向量，$\oplus$表示向量拼接的操作。</p>
<p><strong>Model-II: Coupled-Layer Architecture</strong></p>
<script type="math/tex; mode=display">
\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来
\\\downarrow

\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+\sum_{i\in\{m,n\}}g^{(i\longrightarrow m)}U_c^{(i\longrightarrow m)}h_{t-1}^{(i)}) \ \#现在
\\g^{(i\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(i)}h_{t-1}^{(i)})</script><p><strong>Model-III: Shared-Layer Architecture</strong></p>
<script type="math/tex; mode=display">
\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来
\\\downarrow

\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+g^{(m)}U_c^{(m)}h_{t-1}^{(m)}+g^{(s\longrightarrow m)}U_c^{(s)}h_{t}^{(s)} \ \#现在
\\g^{( m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(m)}h_{t-1}^{(m)}),
g^{( s\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(s\longrightarrow m)}h_{t}^{(s)}),
h_t^{(s)}=\overrightarrow{h_t^{(s)}}\oplus\overleftarrow{h_t^{(s)}}</script><h2 id="3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification"><a href="#3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification" class="headerlink" title="3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)"></a>3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)</h2><p>原文 <a target="_blank" rel="noopener" href="https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf">https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf</a></p>
<p><img src="/2021/08/23/text-cnn/textrcnn1.JPG" alt></p>
<p>整体结构如上图所示，解释一下为啥叫RCNN，一般的 CNN 网络，都是卷积层 + 池化层，这里是将卷积层换成了双向 RNN，所以结果是，双向 RNN + 池化层。作者原话为：From the perspective of convolutional neural networks, the recurrent structure we previously mentioned is the convolutional layer.</p>
<p><strong>词语表示</strong></p>
<p>对于一个词语$w_i$，可以用一个三元组表示为</p>
<script type="math/tex; mode=display">
x_i=[c_l(w_i);e(w_i);c_r(w_i)]</script><p>其中$e(w_i)$表示$w_i$的词向量，$c_l(w_i)$表示$w_i$句子左边的内容的向量表示，$c_r(w_i)$表示$w_i$句子右边的内容的向量表示，用式子表示如下</p>
<script type="math/tex; mode=display">
c_l(w_i)=f(W^{l}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))
\\c_r(w_i)=f(W^{r}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))</script><p>然后将$x_i$经过全连接得到$y_i^{(2)}$，$y_i^{(2)}$is a latent semantic vector</p>
<script type="math/tex; mode=display">
y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})</script><p><strong>语句表示</strong></p>
<p>获取众多的词语表示后，通过max-pooling得到句子表示</p>
<script type="math/tex; mode=display">
y^{(3)}=\mathop{\max}_{i=1}^{n}y_i^{(2)}</script><p>然后接全连接和softmax</p>
<script type="math/tex; mode=display">
y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}
\\p=softmax(y^{(4)})</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangduo/p/6773601.html">https://www.cnblogs.com/wangduo/p/6773601.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-19  <a class="commentCountImg" href="/2021/07/19/fasttext/#comment-container"><span class="display-none-class">4aed935a7f1d83aa4dfd2aabd7f6373b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4aed935a7f1d83aa4dfd2aabd7f6373b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>9 m  <i class="fas fa-pencil-alt"> </i>1.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/19/fasttext/">fasttext</a></h1><div class="content"><h2 id="1、文本分类"><a href="#1、文本分类" class="headerlink" title="1、文本分类"></a>1、文本分类</h2><h3 id="1-1-n-gram"><a href="#1-1-n-gram" class="headerlink" title="1.1 n-gram"></a>1.1 n-gram</h3><p>由于Bag of words不考虑词语的顺序，因此引入bag of n-gram。针对英文，词内的是char n-gram，用于词向量；词之间的是word n-gram，用于分类；对于中文，存在词粒度和字粒度。</p>
<p>举个例子，句子A为”今天天气真不错”，这里以词粒度举例，先分词为[“今天”，”天气”，”真“，”不错“]</p>
<p>uni-gram：今天   天气   真   不错</p>
<p>2-gram为：今天/天气    天气/真    真/不错</p>
<p>3-gram为：今天/天气/真      天气/真/不错</p>
<p>由于n-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采用了hashing trick的方式，如下图所示：</p>
<p><img src="/2021/07/19/fasttext/1.png" alt></p>
<p>用哈希的方式既能保证查找时O(1)的效率，又可能把内存消耗控制在O(buckets * dim)范围内。不过这种方法潜在的问题是存在哈希冲突，不同的n-gram可能会共享同一个embedding。如果buckets取的足够大，这种影响会很小。</p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">def build_dataset(config, ues_word):</span><br><span class="line">    if ues_word:</span><br><span class="line">        tokenizer = lambda x: x.split(&#x27; &#x27;)  # word-level</span><br><span class="line">    else:</span><br><span class="line">        tokenizer = lambda x: [y for y in x]  # char-level</span><br><span class="line">    if os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, &#x27;rb&#x27;))</span><br><span class="line">    else:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, &#x27;wb&#x27;))</span><br><span class="line">    print(f&quot;Vocab size: &#123;len(vocab)&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    def biGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        return (t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def triGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        t2 = sequence[t - 2] if t - 2 &gt;= 0 else 0</span><br><span class="line">        return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def load_dataset(path, pad_size=32):</span><br><span class="line">        contents = []</span><br><span class="line">        with open(path, &#x27;r&#x27;, encoding=&#x27;UTF-8&#x27;) as f:</span><br><span class="line">            for line in tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                if not lin:</span><br><span class="line">                    continue</span><br><span class="line">                content, label = lin.split(&#x27;\t&#x27;)</span><br><span class="line">                words_line = []</span><br><span class="line">                token = tokenizer(content)</span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                if pad_size:</span><br><span class="line">                    if len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([PAD] * (pad_size - len(token)))</span><br><span class="line">                    else:</span><br><span class="line">                        token = token[:pad_size]</span><br><span class="line">                        seq_len = pad_size</span><br><span class="line">                # word to id</span><br><span class="line">                for word in token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line"></span><br><span class="line">                # fasttext ngram</span><br><span class="line">                buckets = config.n_gram_vocab</span><br><span class="line">                bigram = []</span><br><span class="line">                trigram = []</span><br><span class="line">                # ------ngram------</span><br><span class="line">                for i in range(pad_size):</span><br><span class="line">                    bigram.append(biGramHash(words_line, i, buckets))</span><br><span class="line">                    trigram.append(triGramHash(words_line, i, buckets))</span><br><span class="line">                # -----------------</span><br><span class="line">                contents.append((words_line, int(label), seq_len, bigram, trigram))</span><br><span class="line">        return contents  # [([...], 0), ([...], 1), ...]</span><br><span class="line"></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size)</span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size)</span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size)</span><br><span class="line">    return vocab, train, dev, test</span><br></pre></td></tr></table></figure>
<h3 id="1-2-网络结构"><a href="#1-2-网络结构" class="headerlink" title="1.2 网络结构"></a>1.2 网络结构</h3><p><img src="/2021/07/19/fasttext/fasttext.JPG" alt="fasttext"></p>
<p>模型结构上word2vec的cbow模型很像</p>
<p>输入层：举个例子，输入文本”今天天气真不错”，词粒度的2-gram为</p>
<script type="math/tex; mode=display">
x_2=\begin{bmatrix}
emb_{今天/天气}，emb_{天气/真}，emb_{ 真/不错} \end{bmatrix},emb为词向量矩阵
\\x_{1},x_{2},...,x_{N}最后输入到中间层的形式为:
mean(\begin{bmatrix}x_1 \\ x_2 \\...\\x_N  \end{bmatrix}),其中mean为对每个x的列求平均</script><p>中间层：线形层+relu作为激活函数</p>
<p>输出层：为简单的线形层</p>
<p>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        if config.embedding_pretrained is not None:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">        else:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)</span><br><span class="line">        self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed * 3, config.hidden_size)</span><br><span class="line">        # self.dropout2 = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line"></span><br><span class="line">        out_word = self.embedding(x[0])</span><br><span class="line">        out_bigram = self.embedding_ngram2(x[2])</span><br><span class="line">        out_trigram = self.embedding_ngram3(x[3])</span><br><span class="line">        out = torch.cat((out_word, out_bigram, out_trigram), -1)</span><br><span class="line"></span><br><span class="line">        out = out.mean(dim=1)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>
<h3 id="1-3-分层softmax"><a href="#1-3-分层softmax" class="headerlink" title="1.3 分层softmax"></a>1.3 分层softmax</h3><p>对于分类问题，神经网络的输出结果需要经过softmax将其转为概率分布后才可以利用交叉熵计算loss</p>
<p>由于普通softmax的计算效率比较低，计算效率为$O(Kd)$使用分层的softmax时间复杂度可以达到$dlogK$，$K$为分类的数量，$d$为向量的维度</p>
<h4 id="1-3-1-普通softmax"><a href="#1-3-1-普通softmax" class="headerlink" title="1.3.1 普通softmax"></a>1.3.1 普通softmax</h4><p>假设输出为$Y_{pred}=[y_1,y_2,…,y_K]$,则$P_{y_i}$为</p>
<script type="math/tex; mode=display">
P_{y_i}=\frac{e_{y_i}}{\sum_{j=0}^Ke^{y_j}}</script><p>其中$y_i$的维度为$d$，从公式可以看出计算效率为$O(Kd)$</p>
<h4 id="1-3-2-分层softmax"><a href="#1-3-2-分层softmax" class="headerlink" title="1.3.2 分层softmax"></a>1.3.2 分层softmax</h4><p>霍夫曼树可以参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/154356949">https://zhuanlan.zhihu.com/p/154356949</a></p>
<p>为什么要霍夫曼，普通的不行？</p>
<p>分层softmax核心思想为利用训练样本构建霍夫曼树，如下</p>
<p><img src="/2021/07/19/fasttext/11.png" alt="fasttext"></p>
<p>树的结构是根据不同类在样本中出现的频次构造的，即频次越大的节点距离根节点越近。$K$个不同的类组成所有的叶子节点，$K-1个$内部节点作为参数。从根节点到某个叶子节点$y_i$经过的节点和边形成一条路径，路径长度表示为 $L_{y_i}$,$n_{(y_i,j)}$表示路径上的节点，那么</p>
<script type="math/tex; mode=display">
P_{y_i}=\prod \limits_{j=1}^{L_{y_i}}P_{(n(y_{i},j),left\ or\ right)}
\\=\prod \limits_{j=0}^{L_{y_i}-1}\sigma(f(n(y_i,j+1)==LC(n(y_i,j))){\theta_{n(y_i,j)}^T} Y)
\\其中LC(n(y_i,j)表示n(y_i,j)的左孩子，\sigma 为SIGMOD函数，f(m)=\begin{equation}\left\{
\begin{aligned}
1 && if \ m==true \\
-1 & & \ else \\
\end{aligned}
\right.
\end{equation}</script><p>从公式可以看出时间复杂度降低至$dlogK$。</p>
<p>以图中$y_2$为例：</p>
<script type="math/tex; mode=display">
P_{y_2}=P_{(n(y_{2},1),left)}\cdot P_{(n(y_{2},2),left)}\cdot P_{(n(y_{2},3),right)}
\\=\sigma({\theta_{n(y_2,1)}^T} Y)\cdot \sigma({\theta_{n(y_2,2)}^T} Y)
\cdot \sigma({-\theta_{n(y_2,3)}^T} Y)</script><p>从根节点走到叶子节点 $y_2$ ，实际上是在做了3次逻辑回归。</p>
<h2 id="2-训练词向量"><a href="#2-训练词向量" class="headerlink" title="2.训练词向量"></a>2.训练词向量</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27009517/article/details/80676022">https://blog.csdn.net/qq_27009517/article/details/80676022</a></p>
<p><a target="_blank" rel="noopener" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">http://alex.smola.org/papers/2009/Weinbergeretal09.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p>
<p>fasttext工具 <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><hr></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">394</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">376</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-11-13T06:51:26.000Z">2022-11-13</time></p><p class="title"><a href="/2022/11/13/clean/">clean</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-05T15:23:33.000Z">2022-10-05</time></p><p class="title"><a href="/2022/10/05/python-multi-version/">python多版本兼容</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-04T15:42:29.000Z">2022-10-04</time></p><p class="title"><a href="/2022/10/04/python-multi-thread/">多线程</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-04T14:31:57.000Z">2022-10-04</time></p><p class="title"><a href="/2022/10/04/python-designmode/">设计模式</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-10-04T12:27:11.000Z">2022-10-04</time></p><p class="title"><a href="/2022/10/04/python-net/">网络</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">68</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/PTM/"><span class="level-start"><span class="level-item">PTM</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Prompt/"><span class="level-start"><span class="level-item">Prompt</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Tokenization/"><span class="level-start"><span class="level-item">Tokenization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/"><span class="level-start"><span class="level-item">信息抽取</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2022/11/"><span class="level-start"><span class="level-item">November 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DMR/"><span class="tag">DMR</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2022 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>