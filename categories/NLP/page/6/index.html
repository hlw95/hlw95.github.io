<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: NLP - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-26  <a class="commentCountImg" href="/2021/08/26/gpt/#comment-container"><span class="display-none-class">3521a71f5e83cbc0585d9ef84abedc3f</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="3521a71f5e83cbc0585d9ef84abedc3f">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>9 m  <i class="fas fa-pencil-alt"> </i>1.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/26/gpt/">gpt</a></h1><div class="content"><p>GPT三部曲宣告NLP的“预训练+微调”时代的崛起和走向辉煌。</p>
<p>原文分别为：</p>
<p>《Improving Language Understanding by Generative Pre-Training》</p>
<p>《Language Models are Unsupervised Multitask Learners》</p>
<p>《Language Models are Few-Shot Learners》</p>
<h2 id="1-GPT1"><a href="#1-GPT1" class="headerlink" title="1.GPT1"></a>1.GPT1</h2><p><img src="/2021/08/26/gpt/aa.jpg" alt></p>
<p><img src="/2021/08/26/gpt/aaa.png" alt="img"></p>
<p>模型的整体结构如上图所示。使用过程过程分为两步：第一步预训练，利用大量语料学习得到high-capacity的语言模型；第二步是fine_tuning，利用标签数据使其拟合到特定任务。</p>
<h3 id="1-1-Unsupervised-pre-training"><a href="#1-1-Unsupervised-pre-training" class="headerlink" title="1.1 Unsupervised pre-training"></a>1.1 Unsupervised pre-training</h3><p>作者将transformer decoder中Encoder-Decoder Attention层去掉后作为基本单元，然后多层堆叠作为语言模型的主体，然后将输出经过一个softmax层，来得到目标词的输出分布：</p>
<script type="math/tex; mode=display">
h_0=UW_e+W_p
\\h_l=transformer\_block(h_{l-1}),\ \forall l \in [1,n]
\\P(u|u_{-k},...,u_{-1})    =softmax(h_nW_e^T)\</script><p>其中$U=\{u_{-k},…,u_{-1}\}$ 是预测词$u $前$k$个token的独热编码序列，$n$是模型的层数，$W_e$是token embedding matrix，$W_p$是position embedding matrix。</p>
<p>给定一个无监督的语料库$\mathcal{U}$，use a standard language modeling objective to maximize the following likelihood</p>
<script type="math/tex; mode=display">
L_1(\mathcal{U})=\sum_ilog P(u_i|u_{i-k},...,u_{i-1})</script><p>其中$k$ 是上下文窗口大小。</p>
<h3 id="1-2-Supervised-fine-tuning"><a href="#1-2-Supervised-fine-tuning" class="headerlink" title="1.2 Supervised fine-tuning"></a>1.2 Supervised fine-tuning</h3><p>对于数据集$\mathcal{C}$，有数据$(x^1,x^2,…,x^m,y)$</p>
<script type="math/tex; mode=display">
P(y|x^1,x^2,...,x^m)=softmax(h_l^mW_y)
\\L_2(\mathcal{C})=\sum_{(x,y)}log P(y|x^1,x^2,...,x^m)</script><p>其中$W_y$为全连接层的参数</p>
<p>作者发现，使用语言模型来辅助监督学习进行微调，有两个好处：</p>
<ol>
<li>提高监督模型的泛化能力；</li>
<li>加速收敛。</li>
</ol>
<p>所以，最终下游使用的监督模型损失函数为：</p>
<script type="math/tex; mode=display">
L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda*L_1(\mathcal{C})</script><h3 id="1-3-Task-specific-input-transformations"><a href="#1-3-Task-specific-input-transformations" class="headerlink" title="1.3 Task-specific input transformations"></a>1.3 Task-specific input transformations</h3><p><img src="/2021/08/26/gpt/gpt1.JPG" alt></p>
<p>所有的输入文本都会加上开始和结合token$(s),(e)$</p>
<p><strong>分类</strong></p>
<p>分类过程可如上1.2，输入表示为$[(s);Context;(e)]$</p>
<p><strong>文本蕴含</strong></p>
<p>将输入拼接成$[(s); premise; ($) ; hypothesis ; (e)]$</p>
<p><strong>相似度</strong></p>
<p>由于文本相似度与两个比较文本的前后顺序没有关系，因此将两种文本顺序都考虑进来，如上图所示</p>
<p><strong>问答与常识推理</strong></p>
<p>假设文档为$z$，问题为$q$，一系列答案为$\{a_k\}$，将其输入表示为$[(s); z; q; ($);  a_k;(e)]$，然后多个回答组合的形式，如上图。</p>
<h2 id="2-GPT2"><a href="#2-GPT2" class="headerlink" title="2.GPT2"></a>2.GPT2</h2><p>总结就是：多任务预训练+超大数据集+超大规模模型。通过一个超大数据集涵盖NLP的大多任务，然后使用一个超大规模模型进行多任务预训练，使其无需任何下游任务的finetune就可以做到多个NLP任务的SOTA。举个例子，拿高考为例，人的智力和脑容量可以理解为参数大小，由于个体差异，可以将不同的学生理解为不同参数量的模型，卷子可以理解为数据集，不同的学科可以理解为不同任务。GPT2有点类似学霸，就是有超高的智力和脑容量，然后刷大量不同学科的题目，因此对高考这个多任务的下游任务就可以取得好成绩。</p>
<p><strong>GPT2相对于GPT1有哪些不同呢？</strong></p>
<ol>
<li><p><strong>GPT2去掉了fine-tuning</strong>：不再针对不同任务分别进行微调建模，模型会自动识别出来需要做什么任务。这就好比一个人博览群书，你问他什么类型的问题，他都可以顺手拈来，GPT2就是这样一个博览群书的模型。</p>
</li>
<li><p><strong>超大数据集</strong>：WebText，该数据集做了一些简单的数据清理，并且实验结果表明目前模型仍然处于一个欠拟合的情况。</p>
</li>
<li><p><strong>增加网络参数</strong>：GPT2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。15亿什么概念呢，Bert的参数量也才只有3亿哦~当然，这样的参数量也不是说谁都能达到的，这也得取决于money的多少啊~</p>
</li>
<li><p><strong>调整transformer</strong>：将layer normalization放到每个sub-block之前，并在最后一个transformer后再增加一个layer normalization，如下图。</p>
<p><img src="/2021/08/26/gpt/11.jpg" alt></p>
</li>
<li><p><strong>输入表示</strong>：GPT2采用了BPE这种subword的结构作为输入</p>
</li>
<li><p><strong>其他</strong>：GPT2将词汇表数量增加到50257个；最大的上下文大小 (context size) 从GPT的512提升到了1024 tokens；batchsize增加到512。</p>
</li>
</ol>
<p><strong>GPT2的输入是完全的文本，什么提示都不加吗？</strong></p>
<p>当然不是，它也会加入提示词，比如：$TL;DR:$，GPT2模型就会知道是做摘要工作了，输入的格式就是 $文本+TL;DR:$，然后就等待输出就行了~</p>
<h2 id="3-GPT3"><a href="#3-GPT3" class="headerlink" title="3.GPT3"></a>3.GPT3</h2><p>GPT3，这是一种具有1750亿个参数的超大规模模型，比GPT2大100倍，感觉真是进入算力时代了。距离个人用户太远了，就不深挖了。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/146719974">https://zhuanlan.zhihu.com/p/146719974</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/125139937">https://zhuanlan.zhihu.com/p/125139937</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yifanrensheng/p/13167796.html#_label1_0">https://www.cnblogs.com/yifanrensheng/p/13167796.html#_label1_0</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/96c5d5d5c468">https://www.jianshu.com/p/96c5d5d5c468</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35128926/article/details/111399679">https://blog.csdn.net/qq_35128926/article/details/111399679</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/96791725">https://zhuanlan.zhihu.com/p/96791725</a></p>
<p><a target="_blank" rel="noopener" href="https://terrifyzhao.github.io/2019/02/18/GPT2.0论文解读.html">https://terrifyzhao.github.io/2019/02/18/GPT2.0%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56865533">https://zhuanlan.zhihu.com/p/56865533</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-23  <a class="commentCountImg" href="/2021/08/23/text-cnn/#comment-container"><span class="display-none-class">4e2b48a121bb0f97a9742bd9e3cf6e2c</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4e2b48a121bb0f97a9742bd9e3cf6e2c">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>6 m  <i class="fas fa-pencil-alt"> </i>1.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/23/text-cnn/">TextCNN TextRNN TextRCNN</a></h1><div class="content"><h2 id="1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="1.TextCNN (Convolutional Neural Networks for Sentence Classification)"></a>1.TextCNN (Convolutional Neural Networks for Sentence Classification)</h2><p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a></p>
<p>调参论文 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1510.03820">https://arxiv.org/abs/1510.03820</a></p>
<p><img src="/2021/08/23/text-cnn/textcnn1.JPG" alt></p>
<p><img src="/2021/08/23/text-cnn/1111.JPG" alt></p>
<p>模型的整体结构如上所示。Feature Map是输入图像经过神经网络卷积产生的结果，filter是卷积核。</p>
<p><strong>输入表示：</strong></p>
<p>假设输入文本的长度为$n$，对于长度不够的需要做padding，任意一个单词可以用一个$k$维的向量表示，即$X_i \in \mathbb{R}^{k}$，那么一个句子可以表示为</p>
<script type="math/tex; mode=display">
X_{1:n}=X_1 \oplus X_2\oplus...\oplus X_n</script><p>其中$\oplus$是向量拼接操作，$X_{1:n} \in \mathbb{R}^{nk\times 1}$。</p>
<p><strong>卷积</strong>：</p>
<p>对于某个滑窗$X_{i,i+h-1}=\{X_i,X_{i+1},…,X_{i+h-1}\}$经过某个卷积核$W_j$可得</p>
<script type="math/tex; mode=display">
c_{i,j}=f(W_j\cdot X_{i,i+h-1}+b)</script><p>其中$f=tanh(\cdot)$，$W_j\in \mathbb{R}^{ 1\times hk}，c_{i,j} $是标量</p>
<p>假设卷积通道数为$m$，在NLP中，卷积滑动步伐$k=1$，那么经过卷积层后得到的完整的特征矩阵为</p>
<script type="math/tex; mode=display">
C=[[c_{1,1},c_{2,1},...,c_{n-h+1,1}]^T,[c_{1,2},c_{2,2},...,c_{n-h+1,2}]^T,...,[c_{1,m},c_{2,m},...,c_{n-h+1,m}]^T]</script><p>其中$C \in \mathbb{R}^{(n-h+1)\times m}$</p>
<p><strong>maxpooling</strong>：</p>
<script type="math/tex; mode=display">
\hat{C}=max\{C\} , \hat{C}\in \mathbb{R}^{m}</script><p><strong>全连接</strong>：</p>
<p>然后将$\hat{C}$接个全连接，就可以做分类或者回归任务了。</p>
<h2 id="2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning"><a href="#2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning" class="headerlink" title="2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)"></a>2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)</h2><p>原文 <a target="_blank" rel="noopener" href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">https://www.ijcai.org/Proceedings/16/Papers/408.pdf</a></p>
<p><img src="/2021/08/23/text-cnn/textrnn.JPG" alt></p>
<p><img src="/2021/08/23/text-cnn/11.png" alt></p>
<p>该文的场景为Recurrent Neural Network for Text Classification with Multi-Task Learning，就是论文的题目。文中给出了三种结构，如上图所示，图中的RNN单元为LSTM。</p>
<p><strong>Model-I: Uniform-Layer Architecture</strong></p>
<p>对于任务$m$，输入$\hat X_t$包含两个部分</p>
<script type="math/tex; mode=display">
\hat{X}_t^{(m)}=X_{t}^{(m)}\oplus X_{t}^{(s)}</script><p>其中$X_{t}^{(m)}$表示特定任务的词向量，$X_{t}^{(s)}$表示共享的词向量，$\oplus$表示向量拼接的操作。</p>
<p><strong>Model-II: Coupled-Layer Architecture</strong></p>
<script type="math/tex; mode=display">
\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来
\\\downarrow

\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+\sum_{i\in\{m,n\}}g^{(i\longrightarrow m)}U_c^{(i\longrightarrow m)}h_{t-1}^{(i)}) \ \#现在
\\g^{(i\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(i)}h_{t-1}^{(i)})</script><p><strong>Model-III: Shared-Layer Architecture</strong></p>
<script type="math/tex; mode=display">
\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来
\\\downarrow

\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+g^{(m)}U_c^{(m)}h_{t-1}^{(m)}+g^{(s\longrightarrow m)}U_c^{(s)}h_{t}^{(s)} \ \#现在
\\g^{( m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(m)}h_{t-1}^{(m)}),
g^{( s\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(s\longrightarrow m)}h_{t}^{(s)}),
h_t^{(s)}=\overrightarrow{h_t^{(s)}}\oplus\overleftarrow{h_t^{(s)}}</script><h2 id="3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification"><a href="#3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification" class="headerlink" title="3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)"></a>3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)</h2><p>原文 <a target="_blank" rel="noopener" href="https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf">https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf</a></p>
<p><img src="/2021/08/23/text-cnn/textrcnn1.JPG" alt></p>
<p>整体结构如上图所示，解释一下为啥叫RCNN，一般的 CNN 网络，都是卷积层 + 池化层，这里是将卷积层换成了双向 RNN，所以结果是，双向 RNN + 池化层。作者原话为：From the perspective of convolutional neural networks, the recurrent structure we previously mentioned is the convolutional layer.</p>
<p><strong>词语表示</strong></p>
<p>对于一个词语$w_i$，可以用一个三元组表示为</p>
<script type="math/tex; mode=display">
x_i=[c_l(w_i);e(w_i);c_r(w_i)]</script><p>其中$e(w_i)$表示$w_i$的词向量，$c_l(w_i)$表示$w_i$句子左边的内容的向量表示，$c_r(w_i)$表示$w_i$句子右边的内容的向量表示，用式子表示如下</p>
<script type="math/tex; mode=display">
c_l(w_i)=f(W^{l}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))
\\c_r(w_i)=f(W^{r}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))</script><p>然后将$x_i$经过全连接得到$y_i^{(2)}$，$y_i^{(2)}$is a latent semantic vector</p>
<script type="math/tex; mode=display">
y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})</script><p><strong>语句表示</strong></p>
<p>获取众多的词语表示后，通过max-pooling得到句子表示</p>
<script type="math/tex; mode=display">
y^{(3)}=\mathop{\max}_{i=1}^{n}y_i^{(2)}</script><p>然后接全连接和softmax</p>
<script type="math/tex; mode=display">
y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}
\\p=softmax(y^{(4)})</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wangduo/p/6773601.html">https://www.cnblogs.com/wangduo/p/6773601.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-19  <a class="commentCountImg" href="/2021/08/19/elmo/#comment-container"><span class="display-none-class">0c9b0086f0310ef439a36d4a03dea104</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="0c9b0086f0310ef439a36d4a03dea104">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.8 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/19/elmo/">ELMo(Deep contextualized word representations)</a></h1><div class="content"><p>引入了新的深度考虑上下文的词语表示，模型考虑了两个方面：（1）词语的复杂特性，包括语法和语义，（2）在语境中的不同含义。模型使用了深度双向语言模型，并且在大预料库上做了预训练。这个模型可以很方便地和现有的模型结合，并且在NLP的6个任务上取得了SOTA。作者还揭露了预训练网络的深层构件是关键，这使得下游模型能够混合不同类型的半监督信号。</p>
<h2 id="3-ELMo-Embeddings-from-Language-Models"><a href="#3-ELMo-Embeddings-from-Language-Models" class="headerlink" title="3 ELMo: Embeddings from Language Models"></a>3 ELMo: Embeddings from Language Models</h2><p><img src="/2021/08/19/elmo/elmo1.JPG" alt></p>
<p>模型的整体机构如上所示，由左右两个单向的多层LSTM网络构成，左边为正向，右边为反向。</p>
<h3 id="3-1-Bidirectional-language-models（预训练）"><a href="#3-1-Bidirectional-language-models（预训练）" class="headerlink" title="3.1 Bidirectional language models（预训练）"></a>3.1 Bidirectional language models（预训练）</h3><p>假定一个句子有$N$个token，分别为$(t_1,t_2,…,t_N)$，正向的语言模型的句子概率为：</p>
<script type="math/tex; mode=display">
p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_1,t_2,...,t_{k-1})</script><p>反向的语言模型的句子概率为：</p>
<script type="math/tex; mode=display">
p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},...,t_{N})</script><p>得到正向和反向的语言后，将其结合可以得到双向的语言模型，这里取对数表示为：</p>
<script type="math/tex; mode=display">
\sum_{k=1}^N(log\ p(t_k|t_1,t_2,...,t_{k-1};\Theta_x,\overrightarrow{\Theta}_{LSTM} ,\Theta_s )+log \ p(t_k|t_{k+1},t_{k+2},...,t_{N};\Theta_x,\overleftarrow{\Theta}_{LSTM} ,\Theta_s) )\\</script><p>其中$\Theta_x$为token表示的参数，$\Theta_s$为softmax层的参数，$\overrightarrow{\Theta}_{LSTM}$表示前向语言模型的参数，$\overleftarrow{\Theta}_{LSTM}$表示反向语言模型的参数。</p>
<h3 id="3-2-ELMo（如何表示词向量）"><a href="#3-2-ELMo（如何表示词向量）" class="headerlink" title="3.2 ELMo（如何表示词向量）"></a>3.2 ELMo（如何表示词向量）</h3><p>得到$L$层的预训练双向深度语言模型后，对于token $t_k$，一共包含了$2L+1$个相关的表示，集合如下</p>
<script type="math/tex; mode=display">
R_k=\{x_{k}^{LM},\overrightarrow{h^{LM}_{k,j}},\overleftarrow{h^{LM}_{k,j}}|j=1,2,...,L \}\\=\{h_{k,j}^{LM} | j=0,...,L\}</script><p>注意$h_{k,0}^{LM}=x_{k}^{LM}，h_{k,j}^{LM}=[\overrightarrow{h^{LM}_{k,j}};\overleftarrow{h^{LM}_{k,j}}]$,其中$x_{k}^{LM}$为token表示，$\overrightarrow{h^{LM}_{k,j}},\overleftarrow{h^{LM}_{k,j}}$分别为正反向语言模型的表示</p>
<p>对于下游任务，需要将$2L+1$个表示压缩到一个向量$ELmo_k^{task}$，最简单的做法是只取顶层的表示，即</p>
<script type="math/tex; mode=display">
ELmo_k^{task}=E(R_k)=h_{k,L}^{LM}</script><p>更加通用的做法为线形组合输出，如下图，公式表达为</p>
<script type="math/tex; mode=display">
ELmo_k^{task}=E(R_k,\Theta^{task})=\gamma^{task}\sum_{j=0}^{L}s_{j}^{task}h_{k,j}^{LM}</script><p>其中$\gamma^{task}$用于缩放向量，$s_{j}^{task}$表示权重，通过下游任务学习。</p>
<p><img src="/2021/08/19/elmo/11.jpg" alt></p>
<h3 id="3-3-Using-biLMs-for-supervised-NLP-tasks（fine-tune）"><a href="#3-3-Using-biLMs-for-supervised-NLP-tasks（fine-tune）" class="headerlink" title="3.3 Using biLMs for supervised NLP tasks（fine tune）"></a>3.3 Using biLMs for supervised NLP tasks（fine tune）</h3><p>对于下游任务模型，可以得到不考虑上下文的静态词向量$x_k$和考虑上下文的向量表示$h_k$</p>
<p>对于一部分任务，将$x_k$和$ ELMo_k^{task}$ 拼接作为下游任务的特征：$[x_k;ELMo_k^{task}]$</p>
<p>对于一部分任务，将 $h_k$和 $ ELMo_k^{task}$ 拼接可提升效果：$[h_k;ELMo_k^{task}]$</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/linchuhai/article/details/97170541">https://blog.csdn.net/linchuhai/article/details/97170541</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63115885">https://zhuanlan.zhihu.com/p/63115885</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88993965">https://zhuanlan.zhihu.com/p/88993965</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/ELMo/">ELMo</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-10  <a class="commentCountImg" href="/2021/08/10/ptm-survey/#comment-container"><span class="display-none-class">fa4f8744578ff1a660a64a4ac67da7c2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="fa4f8744578ff1a660a64a4ac67da7c2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>13 m  <i class="fas fa-pencil-alt"> </i>2.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/10/ptm-survey/">Pre-trained Models for Natural Language Processing A Survey</a></h1><div class="content"><p>原文内容很丰富，慢慢学习更新。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，GNN以及attention。</p>
<p>尽管NLP任务的取得很大成功，但是和CV比较，性能提高可能不是非常明显。这主要是因为NLP任务的数据集都非常小（除了机器翻译），然而深度网络参数非常多，没有足够的数据支撑网络训练会导致过拟合问题。</p>
<p>最近，大量工作表明，预先训练的模型（PTMs），在大型语料库上可以学习通用语言表示，这有利于下游NLP任务可以避免从零开始训练新模型。随着算力的发展，深度模型（例如，transformer）的出现和训练技巧的不断调高，PTM的结构从浅层发展成深层。<strong>第一代PTM</strong>被用于Non-contextual  word Embedding。由于下游任务不需要这些模型本身，只需要训练好的词向量矩阵，因此对于现在的算力，这些模型非常浅层，比如Skip-Gram和GloVe。虽然这些预训练词向量可以捕获词语的语义，但它们不受上下文限制，无法捕获上下文中的高级含义，某些任务会失效，例如多义词，句法结构，语义角色、回指。<strong>第二代PTM</strong>关注Contextual word embeddings，比如BERT，GPT等。这些编码器任然需要通过下游任务在上下文中表示词语。</p>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2.Background"></a>2.Background</h2><h3 id="2-1-Language-Representation-Learning"><a href="#2-1-Language-Representation-Learning" class="headerlink" title="2.1 Language Representation Learning"></a>2.1 Language Representation Learning</h3><p>The core idea of distributed representation is to describe the meaning of a piece of text by low-dimensional real-valued vectors. And each dimension of the vector has no corresponding sense, while the whole represents a concrete concept.</p>
<p><img src="/2021/08/10/ptm-survey/ptm1.JPG" alt></p>
<p><strong>Non-contextual Embeddings</strong></p>
<p>这一步主要是将分割的字符，比如图中的$x$，变成向量表达$e_x \in \mathbb{R}^{D_e}$，$D_e$是词向量维度。向量化过程就是基于一个离线训练的词向量矩阵$E\in \mathbb{R}^{D_e\times |\mathcal{V}|} $做查找，$\mathcal{V}$是词汇表。</p>
<p>这个过程主要有两个问题。第一个是这个词向量是静态的，没有考虑上下文含义，无法处理多义词。第二个是oov问题，许多算法可以缓解这个问题，比如基于character level，比如基于subword，subword算法有BPE，CharCNN等。</p>
<p><strong>Contextual Embeddings</strong></p>
<p>To address the issue of polysemous and the context-dependent nature of words, we need distinguish the semantics of words in different contexts：</p>
<script type="math/tex; mode=display">
[\textbf{h}_1,\textbf{h}_2,...,\textbf{h}_T]=f_{enc}(x_1,x_2,...,x_T)</script><p>其中$f_{enc}(\cdot)$为深度编码器。$\textbf{h}_t$就是contextual embedding或者dynamical embedding。</p>
<h3 id="2-2-Neural-Contextual-Encoders"><a href="#2-2-Neural-Contextual-Encoders" class="headerlink" title="2.2 Neural Contextual Encoders"></a>2.2 Neural Contextual Encoders</h3><p><img src="/2021/08/10/ptm-survey/ptm3.JPG" alt></p>
<p>可以分成两类，sequence models and non-sequence models。</p>
<h4 id="2-2-1-sequence-models"><a href="#2-2-1-sequence-models" class="headerlink" title="2.2.1 sequence models"></a>2.2.1 sequence models</h4><p>sequence models 分为两类，Convolutional Models和Recurrent Models，见上图。</p>
<p><strong>Convolutional</strong> </p>
<p>Convolutional models take the embeddings of words in the input sentence and capture the meaning of a word by aggregating the <strong>local information</strong> from its neighbors by convolution operations</p>
<p><strong>Recurrent</strong> </p>
<p>Recurrent models capture the contextual representations of words with short memory, such as LSTMs and GRUs . In practice, bi-directional LSTMs or GRUs are used to collect information from both sides of a word, but its performance is often affected by the <strong>long-term dependency problem</strong>.</p>
<h4 id="2-2-2-non-sequence-models"><a href="#2-2-2-non-sequence-models" class="headerlink" title="2.2.2 non-sequence models"></a>2.2.2 non-sequence models</h4><p>transformer： model the relation of every two words</p>
<h4 id="2-2-3-Analysis"><a href="#2-2-3-Analysis" class="headerlink" title="2.2.3 Analysis"></a>2.2.3 Analysis</h4><p><strong>Sequence models：</strong></p>
<p>1.Sequence models learn the contextual representation of the word with locality bias and are hard to capture the long-range interactions between words. </p>
<p>2.Nevertheless, sequence models are usually easy to train and get good results for various NLP tasks.</p>
<p><strong>fully-connected self-attention model：</strong></p>
<p>1.can directly model the dependency between every two words in a sequence, which is more powerful and suitable to model long range dependency of language</p>
<p>2.However, due to its heavy structure and less model bias, the Transformer usually requires a large training corpus and is easy to overfit on small or modestly-sized datasets</p>
<p><strong>结论</strong>：the Transformer has become the mainstream architecture of PTMs due to its powerful capacity.</p>
<h3 id="2-3-Why-Pre-training"><a href="#2-3-Why-Pre-training" class="headerlink" title="2.3 Why Pre-training?"></a>2.3 Why Pre-training?</h3><ol>
<li>Pre-training on the huge text corpus can <strong>learn universal language representation</strong>s and help with the downstream tasks.</li>
<li>Pre-training provides a <strong>better model initialization</strong>,which usually leads to a better generalization performance and speeds up convergence on the target task.</li>
<li>Pre-training can be <strong>regarded as a kind of regularization</strong> to avoid overfitting on small data</li>
</ol>
<h2 id="3-Overview-of-PTMs"><a href="#3-Overview-of-PTMs" class="headerlink" title="3 Overview of PTMs"></a>3 Overview of PTMs</h2><h3 id="3-1-Pre-training-Tasks"><a href="#3-1-Pre-training-Tasks" class="headerlink" title="3.1 Pre-training Tasks"></a>3.1 Pre-training Tasks</h3><p>预训练任务对于学习通用语言表示至关重要。通常，这些预训练任务应具有挑战性，并拥有大量训练数据。在本节中，我们将预训练任务分成三个类别：Supervised learning、Unsupervised learning和Self-Supervised learning。</p>
<p><strong>Self-Supervised learning</strong>： is a blend of supervised learning and unsupervised learning. The learning paradigm of SSL is entirely the same as supervised learning, but the labels of training data are generated automatically. The key idea of SSL is to predict any part of the input from other parts in some form. For example, the masked language model (MLM) is a self-supervised task that attempts to predict the masked words in a sentence given the rest words.</p>
<p>接下来基于介绍常用的基于Self-Supervised learning的预训练任务。</p>
<h4 id="3-1-1-Language-Modeling-LM"><a href="#3-1-1-Language-Modeling-LM" class="headerlink" title="3.1.1 Language Modeling (LM)"></a>3.1.1 Language Modeling (LM)</h4><h4 id="3-1-2-Masked-Language-Modeling-MLM"><a href="#3-1-2-Masked-Language-Modeling-MLM" class="headerlink" title="3.1.2 Masked Language Modeling (MLM)"></a>3.1.2 Masked Language Modeling (MLM)</h4><h4 id="3-1-3-Permuted-Language-Modeling-PLM"><a href="#3-1-3-Permuted-Language-Modeling-PLM" class="headerlink" title="3.1.3 Permuted Language Modeling (PLM)"></a>3.1.3 Permuted Language Modeling (PLM)</h4><h4 id="3-1-4-Denoising-Autoencoder-DAE"><a href="#3-1-4-Denoising-Autoencoder-DAE" class="headerlink" title="3.1.4 Denoising Autoencoder (DAE)"></a>3.1.4 Denoising Autoencoder (DAE)</h4><h4 id="3-1-5-Contrastive-Learning-CTL"><a href="#3-1-5-Contrastive-Learning-CTL" class="headerlink" title="3.1.5 Contrastive Learning (CTL)"></a>3.1.5 Contrastive Learning (CTL)</h4><p>nsp也属于CTL</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360892229">https://zhuanlan.zhihu.com/p/360892229</a></p>
<h4 id="3-1-6-Others"><a href="#3-1-6-Others" class="headerlink" title="3.1.6 Others"></a>3.1.6 Others</h4><h3 id="3-2-Taxonomy-of-PTMs"><a href="#3-2-Taxonomy-of-PTMs" class="headerlink" title="3.2 Taxonomy of PTMs"></a>3.2 Taxonomy of PTMs</h3><p><img src="/2021/08/10/ptm-survey/ptm5.JPG" alt></p>
<p>作者从以下四个角度，即Representation Type，Architectures，Pre-Training Task Types，Extensions，对现有的PTM分类，分类结果如上。图和这里有一点不统一，是作者没注意？图里有5个类别，多了Tuning Strategies，而且Representation Type在图中为Contextual?。</p>
<h3 id="3-3-Model-Analysis"><a href="#3-3-Model-Analysis" class="headerlink" title="3.3 Model Analysis"></a>3.3 Model Analysis</h3><h2 id="4-Extensions-of-PTMs"><a href="#4-Extensions-of-PTMs" class="headerlink" title="4 Extensions of PTMs"></a>4 Extensions of PTMs</h2><h3 id="4-1-Knowledge-Enriched-PTMs"><a href="#4-1-Knowledge-Enriched-PTMs" class="headerlink" title="4.1 Knowledge-Enriched PTMs"></a>4.1 Knowledge-Enriched PTMs</h3><h3 id="4-2-Multilingual-and-Language-Specific-PTMs"><a href="#4-2-Multilingual-and-Language-Specific-PTMs" class="headerlink" title="4.2 Multilingual and Language-Specific PTMs"></a>4.2 Multilingual and Language-Specific PTMs</h3><h3 id="4-3-Multi-Modal-PTMs"><a href="#4-3-Multi-Modal-PTMs" class="headerlink" title="4.3 Multi-Modal PTMs"></a>4.3 Multi-Modal PTMs</h3><h3 id="4-4-Domain-Specific-and-Task-Specific-PTMs"><a href="#4-4-Domain-Specific-and-Task-Specific-PTMs" class="headerlink" title="4.4 Domain-Specific and Task-Specific PTMs"></a>4.4 Domain-Specific and Task-Specific PTMs</h3><h3 id="4-5-Model-Compression"><a href="#4-5-Model-Compression" class="headerlink" title="4.5 Model Compression"></a>4.5 Model Compression</h3><h2 id="5-Adapting-PTMs-to-Downstream-Tasks"><a href="#5-Adapting-PTMs-to-Downstream-Tasks" class="headerlink" title="5 Adapting PTMs to Downstream Tasks"></a>5 Adapting PTMs to Downstream Tasks</h2><p>虽然PTM学习了很多通用知识，但是如何将这些知识有效应用到下游任务是个挑战。</p>
<h3 id="5-1-Transfer-Learning"><a href="#5-1-Transfer-Learning" class="headerlink" title="5.1 Transfer Learning"></a>5.1 Transfer Learning</h3><p>Transfer learning is to adapt the knowledge from a source task (or domain) to a target task (or domain).如下图。</p>
<p><img src="/2021/08/10/ptm-survey/ptm6.JPG" alt></p>
<h3 id="5-2-How-to-Transfer"><a href="#5-2-How-to-Transfer" class="headerlink" title="5.2 How to Transfer?"></a>5.2 How to Transfer?</h3><h4 id="5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus"><a href="#5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus" class="headerlink" title="5.2.1 Choosing appropriate pre-training task, model architecture and corpus"></a>5.2.1 Choosing appropriate pre-training task, model architecture and corpus</h4><h4 id="5-2-2-Choosing-appropriate-layers"><a href="#5-2-2-Choosing-appropriate-layers" class="headerlink" title="5.2.2 Choosing appropriate layers"></a>5.2.2 Choosing appropriate layers</h4><p>使用哪些层参与下游任务</p>
<p>选择的层model1+下游任务model2</p>
<p>对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。</p>
<p>用$\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\cdot)$为特定的任务模型。有以下几种方法选择representation:</p>
<p><strong>a) Embedding Only</strong></p>
<p>choose only the pre-trained static embeddings，即$g(\textbf{H}^{1})$</p>
<p><strong>b) Top Layer</strong></p>
<p>选择顶层的representation，然后接入特定的任务模型，即$g(\textbf{H}^{L})$</p>
<p><strong>c) All Layers</strong></p>
<p>输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下</p>
<script type="math/tex; mode=display">
g(\textbf{r}_t)=g(\gamma \sum_{l=1}^{L}\alpha_l\textbf{H}^{(l)})</script><p>其中$\alpha$ is the softmax-normalized weight for layer $l$ and $\gamma$ is a scalar to scale the vectors output by pre-trained model</p>
<h4 id="5-2-3-To-tune-or-not-to-tune"><a href="#5-2-3-To-tune-or-not-to-tune" class="headerlink" title="5.2.3 To tune or not to tune?"></a>5.2.3 To tune or not to tune?</h4><p>总共有两种常用的模型迁移方式：<strong>feature extraction</strong> (where the pre-trained parameters are frozen), and <strong>fine-tuning</strong> (where the pre-trained parameters are unfrozen and fine-tuned).</p>
<p><img src="/2021/08/10/ptm-survey/ptm7.JPG" alt></p>
<p>选择的层model1参数是否固定，model2一定要训练</p>
<p>bert 只有top  layer finetune？？？？</p>
<h3 id="5-3-Fine-Tuning-Strategies"><a href="#5-3-Fine-Tuning-Strategies" class="headerlink" title="5.3 Fine-Tuning Strategies"></a>5.3 Fine-Tuning Strategies</h3><p><strong>Two-stage fine-tuning</strong></p>
<p>第一阶段为中间任务，第二阶段为目标任务</p>
<p><strong>Multi-task fine-tuning</strong></p>
<p>multi-task learning and pre-training are complementary technologies.</p>
<p><strong>Fine-tuning with extra adaptation modules</strong></p>
<p>The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed.</p>
<p><strong>Others</strong></p>
<p>self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/word2vec/#comment-container"><span class="display-none-class">5557345d5f80a07d7c4cb69fb82373ba</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="5557345d5f80a07d7c4cb69fb82373ba">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/word2vec/">word2vec</a></h1><div class="content"><h2 id="一-原理"><a href="#一-原理" class="headerlink" title="一.原理"></a>一.原理</h2><p><strong>两种训练模型</strong></p>
<ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<p><strong>训练技巧</strong></p>
<p>hierarchical softmax 和 negative sampling</p>
<h2 id="二-代码"><a href="#二-代码" class="headerlink" title="二.代码"></a>二.代码</h2><p><strong>训练代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure>
<p><strong>词向量矩阵</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.KeyedVectors.load_word2vec_format(model_path,binary=True)</span><br><span class="line">    print(model.vectors)   ##(779845, 400))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model.index_to_key)</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model[&quot;的&quot;])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[-1.3980628e+00, -4.6281612e-01,  5.8368486e-01, ...,         5.3952241e-01,  4.4697687e-01,  1.3505782e+00],       [ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, ...,         1.1110669e+00,  2.1992767e-01,  7.0457202e-01],       [-8.5650706e-01,  8.2832746e-02, -8.4218192e-01, ...,         2.1654253e+00,  6.4846051e-01, -5.7714492e-01],       ...,       [ 7.5072781e-03, -1.3543828e-02,  2.3101490e-02, ...,         4.2363801e-03, -5.6749382e-03,  6.3404259e-03],       [-2.6244391e-04, -3.0459568e-02,  5.9752418e-03, ...,         1.7844304e-02, -4.7109672e-04,  7.7916058e-03],       [ 7.2062697e-04, -6.5988898e-03,  1.1346856e-02, ...,        -3.7340564e-03, -1.8825980e-02,  2.7245486e-03]], dtype=float32)</span><br><span class="line"></span><br><span class="line">[&#x27;，&#x27;, &#x27;的&#x27;, &#x27;。&#x27;, &#x27;、&#x27;, &#x27;０&#x27;, &#x27;１&#x27;, &#x27;在&#x27;, &#x27;”&#x27;, &#x27;２&#x27;, &#x27;了&#x27;, &#x27;“&#x27;, &#x27;和&#x27;, &#x27;是&#x27;, &#x27;５&#x27;, ...]</span><br><span class="line"></span><br><span class="line">array([ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, -3.6405793e-01,        1.0851435e-01,  4.9507666e-02, -7.1219063e-01, -5.4614645e-01,       -1.3581418e+00,  3.0274218e-01,  6.1700332e-01,  3.5553512e-01,        1.6602433e+00,  7.5298291e-01, -1.4151905e-01, -2.1077128e-01,       -2.6325354e-01,  1.6108564e+00, -4.6750236e-01, -1.6261842e+00,        1.3063166e-01,  8.0702168e-01,  4.0011466e-01,  1.2198541e+00,       -6.2879241e-01,  ... 2.1928079e-01,  7.1725255e-01, -2.3430648e-01, -1.2066336e+00,        9.7590965e-01, -1.5906478e-01, -3.5802779e-01, -3.8005975e-01,        1.9056025e-01,  1.1110669e+00,  2.1992767e-01,  7.0457202e-01],      dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781v3">https://arxiv.org/abs/1301.3781v3</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1405.4053">https://arxiv.org/abs/1405.4053</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/short-chinese-text-match/#comment-container"><span class="display-none-class">e42e73069e4b8f9b20abc430afce93b1</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="e42e73069e4b8f9b20abc430afce93b1">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>8 m  <i class="fas fa-pencil-alt"> </i>1.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/short-chinese-text-match/">Neural Graph Matching Networks for Chinese Short Text Matching</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.547.pdf">https://aclanthology.org/2020.acl-main.547.pdf</a></p>
<h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>对于中文短文本匹配，通常基于词粒度而不是字粒度。但是分词结果可能是错误的、模糊的或不一致的，从而损害最终的匹配性能。比如下图：字符序列“南京市长江大桥”经过不同的分词可能表达为不同的意思。</p>
<p><img src="/2021/08/04/short-chinese-text-match/lattice.JPG" alt></p>
<p>为了解决这个问题，作者提出了一种基于图神经网络的中文短文本匹配方法。不是将句子分割成一个单词序列，而是保留所有可能的分割路径，形成一个Lattice（segment1，segment2，segment3），如上图所示。</p>
<h2 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2.问题定义"></a>2.问题定义</h2><p>将两个待匹配中文短文本分别定义为$S_a=\left \{ C_1^a,C_2^a,…,C_{t_a}^a \right \}$，$S_b=\left \{ C_1^b,C_2^b,…,C_{t_b}^b \right \}$，其中$C_i^a$表示句子$a$第$i$个字，$C_j^b$表示句子$b$第$j$个字，$t_a$，$t_b$分别表示两个句子的长度。$f(S_a,S_b)$是目标函数，输出为两个文本的匹配度。词格图用$G=(\nu,\xi)$表示，其中$\nu$是节点集，包括所有字符序列。$\xi$表示边集，如果$\nu$中两个顶点$v_i$和$v_j$相邻，那么就存在一个边为$e_{ij}$。$N_{fw}(v_i)$表示节点$v_i$ 正向的所有可达节点的集合,$N_{bw}(v_i)$表示节点$v_i$ 反向的所有可达节点的集合。句子$a$的词格图为$G^a(\nu_a,\xi_a)$，句子$b$的词格图为$G^b(\nu_b,\xi_b)$。</p>
<h2 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h2><p><img src="/2021/08/04/short-chinese-text-match/entire1.JPG" alt></p>
<p>模型分成3个部分，1.语言节点表示 2.图神经匹配 3.相关性分类器</p>
<h3 id="3-1-语言节点表示"><a href="#3-1-语言节点表示" class="headerlink" title="3.1 语言节点表示"></a>3.1 语言节点表示</h3><p>这一部分基于BERT的结构。BERT的token表示基于字粒度，可以得到$\left \{ [CLS],C_1^a,C_2^a,…,C_{ta}^a,[SEP],C_1^b,C_2^b,…,C_{t_b}^b,[SEP] \right \}$,如上图所示。BERT的输出为各个字的Embedding，$ \left \{\textbf{C}^{CLS},\textbf{C}_1^a,\textbf{C}_2^a,…,\textbf{C}_{t_a}^a,\textbf{C}^{SEP},\textbf{C}_1^b,\textbf{C}_2^b,…,\textbf{C}_{t_b},\textbf{C}^{SEP} \right \}$。</p>
<h3 id="3-2-图神经匹配"><a href="#3-2-图神经匹配" class="headerlink" title="3.2 图神经匹配"></a>3.2 图神经匹配</h3><p><strong>初始化</strong>：假设节点$v_i$包含$n_i$个连续字符，起始字符位置为$s_i$，即$ \left \{C_{s_i},C_{s_{i+1}},…,C_{s_{i}+n_i-1} \right \}$，这里$v_i$表示句子$a$或者$b$的结点。$V_i=\sum_{k=0}^{n_i-1}\textbf{U}_{s_i+k}\odot\textbf{C}_{s_i+k}$，其中$\odot$表示两个向量对应各个元素相乘。特征识别分数向量$\textbf{U}_{s_i+k}=softmax(FFN(\textbf{C}_{s_i+k}))$，$FFN$为两层。$h$为结点的向量表示，将$h_i^0$等于$V_i$</p>
<p><strong>Message Propagation</strong> : 对于第$l$次迭代，$G_a$中某个结点$v_i$由如下四个部分组成</p>
<script type="math/tex; mode=display">
m_i^{fw}=\sum_{v_j \in N_{fw}(v_i)}\alpha_{ij}(W^{fw}h_j^{l-1}),
\\m_i^{bw}=\sum_{v_k \in N_{bw}(v_i)}\alpha_{ik}(W^{bw}h_k^{l-1}),
\\m_i^{b1}=\sum_{v_m \in V^b}\alpha_{im}(W^{fw}h_m^{l-1}),
\\m_i^{b2}=\sum_{v_q \in V^b}\alpha_{iq}(W^{bw}h_q^{l-1})，</script><p>其中$\alpha_{ij},\alpha_{ik},\alpha_{im},\alpha_{iq}$是注意力系数，$W^{fw},W^{bw}$是注意力系数参数</p>
<p>然后定义两种信息为$m_i^{self}\triangleq[m_i^{fw},m_i^{bw}]，m_i^{cross}\triangleq[m_i^{b1},m_i^{b2}]$</p>
<p><strong>Representation Updating</strong>：得到两种信息后，需要更新结点$ v_i$的向量表示</p>
<script type="math/tex; mode=display">
d_k=cosine(w_k^{cos}\odot m_i^{self},w_k^{cos}\odot m_i^{cross})</script><p>其中$w_k^{cos}$为参数，$d_k$为multi-perspective cosine distance，可以衡量两种信息的距离，$k \in \left \{ 1,2,3,…P\right\}$，$P$是视角的数量。</p>
<script type="math/tex; mode=display">
h_i^l=FFN([m_i^{self},\textbf{d}_i])</script><p>其中$\textbf{d}_i\triangleq[d_1,d_2,…,d_P]$,$FFN$两层。</p>
<p><strong>句子的图级别表示</strong>：</p>
<p>总共经历了$L$次迭代（layer），得到$h_i^L$为结点$v_i$最终的向量表示（$h_i^L$includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph)</p>
<p>最终，两个句子的图级别表示分别为</p>
<script type="math/tex; mode=display">
g^a=attentive pooling(\left \{ h_{1a}^L,h_{2a}^L,...,h_{node-num_a a}^L \right \}),
\\g^b=attentive pooling(\left \{ h_{1b}^L,h_{2b}^L,...,h_{node-num_b b}^L \right \})</script><h3 id="3-3-分类器"><a href="#3-3-分类器" class="headerlink" title="3.3 分类器"></a>3.3 分类器</h3><p>得到$g^a,g^b$后，两句子的相似度可以用分类器衡量：</p>
<script type="math/tex; mode=display">
P=FFN([g^a,g^b,g^a \odot g^b,|g^a-g^b|])</script><p>其中$P \in [0,1]$。</p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4.实验结果"></a>4.实验结果</h2><p><img src="/2021/08/04/short-chinese-text-match/22.GIF" alt></p>
<p><img src="/2021/08/04/short-chinese-text-match/33.GIF" alt></p>
<p>lattice和JIEBA+PKU的区别？</p>
<p>JIEBA+PKU is a small lattice graph generated by merging two word segmentation results</p>
<p>lattice：overall lattice，应该是全部的组合</p>
<p>两者效果差不多是因为Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence).</p>
<p><img src="/2021/08/04/short-chinese-text-match/11.GIF" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43390809/article/details/114077216">https://blog.csdn.net/qq_43390809/article/details/114077216</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-28  <a class="commentCountImg" href="/2021/07/28/word-emb-add/#comment-container"><span class="display-none-class">27d8132878abc1f5fae21650cfc4df15</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="27d8132878abc1f5fae21650cfc4df15">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/28/word-emb-add/">nlp中使用预训练的词向量和随机初始化的词向量的区别在哪里？</a></h1><div class="content"><p>当你训练数据<strong>不充足</strong>的时候，可以直接使用别人已经预训练好的词向量，也可以根据自己的训练数据微调(fine-tuning)预训练词向量，也可以把词向量和整个模型一块训练，但是通常预训练的词向量我们不会再在训练的过程中进行更新。</p>
<p>当你的训练数据<strong>比较充足</strong>的时候，并且想让词向量能更好的捕捉自己的训练数据的语义信息时，应该使用随机初始化的词向量。当然，随机初始化的词向量必须要在训练网络的过程中不断进行更新，就和神经网络的权重参数一样进行训练。</p>
<p>例子：</p>
<p>1.直观展示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">###random</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">###from pretrain</span><br><span class="line">weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])</span><br><span class="line">embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">print(embedding.weight)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.1754,  1.6604, -1.5025, -1.0980, -0.4718],</span><br><span class="line">        [-1.1276,  0.1408, -1.0746, -1.2768, -0.6789]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.7366,  0.0607,  0.6151,  0.2282,  0.3878],</span><br><span class="line">        [-1.1365,  0.1844, -1.1191, -0.8787, -0.5121]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[1.0000, 2.3000, 3.0000],</span><br><span class="line">        [4.0000, 5.1000, 6.3000]])</span><br></pre></td></tr></table></figure>
<p>2.n-gram</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/337950427">https://www.zhihu.com/question/337950427</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/lasertagger/#comment-container"><span class="display-none-class">cb752746ba01f81f9394f0e9d77bdc74</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="cb752746ba01f81f9394f0e9d77bdc74">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>18 m  <i class="fas fa-pencil-alt"> </i>2.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/lasertagger/">LASERTAGGER</a></h1><div class="content"><h2 id="一-摘要"><a href="#一-摘要" class="headerlink" title="一. 摘要"></a>一. 摘要</h2><p>对于某一些文本生成任务，输入和输出的文本有很多的重叠部分，如果还是采用encoder-decoder的文本生成模型去从零开始生成，其实是很浪费和没必要的，并且会导致两个问题：1：生成模型的幻觉问题(就是模型胡说八道) ；2：出现叠词(部分片段一致)。</p>
<p>基于上面的考虑，作者提出了lasertagger模型，通过几个常用的操作：keep token、delete token、 add token，给输入序列的每个token打上标签，使得文本生成任务转化为了序列标注任务。</p>
<p>通过这种方式，相较于encoder-decoder模型的优势有如下：1、推理的速度更快 2、在较小的数据集上性能优于seq2seq baseline，在大数据集上和baseline持平（因为输入和输出的文本有很多的重叠部分，对于这种情况，lasertagger的候选词库比较小，因为对于重叠部分的词，词库只需要添加keep，而传统encoder-decoder的候选词库依然很大，因为对于重叠部分的词，词库需要添加对应的词）</p>
<h2 id="二-主要贡献"><a href="#二-主要贡献" class="headerlink" title="二.主要贡献"></a>二.主要贡献</h2><p>1、通过输入和输出文本，自动去提取需要add的token</p>
<p>2、通过输入文本，输出文本和tag集，给训练的输入序列打上标签</p>
<p>3、提出了两个版本，$LASERTAGGER_{AR}$( bert+transformer decoder )和$LASERTAGGER_{FF}$( bert+desen+softmax )</p>
<h2 id="三-整体流程"><a href="#三-整体流程" class="headerlink" title="三. 整体流程"></a>三. 整体流程</h2><p><img src="/2021/07/27/lasertagger/entire.JPG" alt></p>
<p>其实就是两个过程，一.将输入文本变编码成特殊标注，二.将标注解码成文本</p>
<h2 id="四-文本标注"><a href="#四-文本标注" class="headerlink" title="四. 文本标注"></a>四. 文本标注</h2><h3 id="4-1-Tag集构建（也就是label集构建）"><a href="#4-1-Tag集构建（也就是label集构建）" class="headerlink" title="4.1 Tag集构建（也就是label集构建）"></a>4.1 Tag集构建（也就是label集构建）</h3><p>一般情况，tag分为两个大类： base tag $B$和 add tag $P$。对于base tag，就是$KEEP$或者$DELETE$当前token；对于add tag，就是要添加一个词到token前面，添加的词来源于词表$V$。实际在工程中，将$B$和$P$结合来表示，即$^{P}B$，总的tag数量大约等于$B$的数量乘以$P$的数量，即$2|V|$。对于某些任务可以引入特定的tag，比如对于句子融合，可以引入$SWAP$,如下图。</p>
<p><img src="/2021/07/27/lasertagger/case.JPG" alt></p>
<h4 id="4-1-1-词表V的构建"><a href="#4-1-1-词表V的构建" class="headerlink" title="4.1.1 词表V的构建"></a>4.1.1 词表V的构建</h4><p><strong>构建目标：</strong></p>
<ol>
<li>最小化词汇表规模；</li>
<li>最大化目标词语的比例</li>
</ol>
<p>限制词汇表的词组数量可以减少相应输出的决策量；最大化目标词语的比例可以防止模型添加无效词。</p>
<p><strong>构建过程：</strong></p>
<p>通过$LCS$算法（longest common sequence，最长公共子序列，注意和最长公共子串不是一回事），找出输入和输出序列的最长公共子序列，输出剩下的序列，就是需要$add$的token，添加到词表$V$，词表中的词基于词频排序,然后选择$l$个常用的。</p>
<p>举个例子：soruce为“12345678”，target为”1264591”</p>
<p>​                    最长公共子序列为[‘1’, ‘2’, ‘4’, ‘5’]</p>
<p>​                    需要$add$的token为 [‘6’, ‘91’]</p>
<p><strong>源码</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def _lcs_table(source, target):</span><br><span class="line">  &quot;&quot;&quot;Returns the Longest Common Subsequence dynamic programming table.&quot;&quot;&quot;</span><br><span class="line">  rows = len(source)</span><br><span class="line">  cols = len(target)</span><br><span class="line">  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]</span><br><span class="line">  for i in range(1, rows + 1):</span><br><span class="line">    for j in range(1, cols + 1):</span><br><span class="line">      if source[i - 1] == target[j - 1]:</span><br><span class="line">        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1</span><br><span class="line">      else:</span><br><span class="line">        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])</span><br><span class="line">  return lcs_table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _backtrack(table, source, target, i, j):</span><br><span class="line">  &quot;&quot;&quot;Backtracks the Longest Common Subsequence table to reconstruct the LCS.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    table: Precomputed LCS table.</span><br><span class="line">    source: List of source tokens.</span><br><span class="line">    target: List of target tokens.</span><br><span class="line">    i: Current row index.</span><br><span class="line">    j: Current column index.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    List of tokens corresponding to LCS.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if i == 0 or j == 0:</span><br><span class="line">    return []</span><br><span class="line">  if source[i - 1] == target[j - 1]:</span><br><span class="line">    # Append the aligned token to output.</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j - 1) + [target[j - 1]]</span><br><span class="line">  if table[i][j - 1] &gt; table[i - 1][j]:</span><br><span class="line">    return _backtrack(table, source, target, i, j - 1)</span><br><span class="line">  else:</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j)</span><br><span class="line"></span><br><span class="line">def _compute_lcs(source, target):</span><br><span class="line">  # s1=&#123;1,3,4,5,6,7,7,8&#125;,s2=&#123;3,5,7,4,8,6,7,8,2&#125; return 35778</span><br><span class="line">  table = _lcs_table(source, target)</span><br><span class="line">  return _backtrack(table, source, target, len(source), len(target))</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def _get_added_phrases(source: Text, target: Text) -&gt; Sequence[Text]:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the phrases that need to be added to the source to get the target.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sep = &#x27;&#x27;</span><br><span class="line">    source_tokens = utils.get_token_list(source.lower())</span><br><span class="line">    target_tokens = utils.get_token_list(target.lower())</span><br><span class="line">    #compute Longest Common Subsequence</span><br><span class="line">    kept_tokens = _compute_lcs(source_tokens, target_tokens)</span><br><span class="line">    added_phrases = []</span><br><span class="line">    kept_idx = 0</span><br><span class="line">    phrase = []</span><br><span class="line">    for token in target_tokens:</span><br><span class="line">        if kept_idx &lt; len(kept_tokens) and token == kept_tokens[kept_idx]:</span><br><span class="line">            kept_idx += 1</span><br><span class="line">            if phrase:</span><br><span class="line">                added_phrases.append(sep.join(phrase))</span><br><span class="line">                phrase = []</span><br><span class="line">        else:</span><br><span class="line">            phrase.append(token)</span><br><span class="line">    if phrase:</span><br><span class="line">        added_phrases.append(sep.join(phrase))</span><br><span class="line">    return added_phrases</span><br></pre></td></tr></table></figure>
<p>词表位于文件label_map.txt.log，本人基于自己的数据集，内容如下所示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Idx Frequency  Coverage (%)   Phrase</span><br><span class="line">1  19 94.22  址</span><br><span class="line">2  15 95.27  单位</span><br><span class="line">3  8  95.76  地</span><br><span class="line">4  6  96.17  执勤</span><br></pre></td></tr></table></figure>
<h4 id="4-1-2-tag集"><a href="#4-1-2-tag集" class="headerlink" title="4.1.2 tag集"></a>4.1.2 tag集</h4><p>本人基于自己的数据集，得到的候选tag如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEEP</span><br><span class="line">DELETE</span><br><span class="line">KEEP|址</span><br><span class="line">DELETE|址</span><br><span class="line">KEEP|单位</span><br><span class="line">DELETE|单位</span><br><span class="line">KEEP|地</span><br><span class="line">DELETE|地</span><br><span class="line">KEEP|执勤</span><br><span class="line">DELETE|执勤</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Converting-Training-Targets-into-Tags"><a href="#4-2-Converting-Training-Targets-into-Tags" class="headerlink" title="4.2 Converting Training Targets into Tags"></a>4.2 Converting Training Targets into Tags</h3><p><strong>paper上的伪代码：</strong></p>
<p><img src="/2021/07/27/lasertagger/al1.JPG" alt></p>
<p>采用贪心策略，核心思想就是遍历$t$，先和$s$匹配，匹配上就$keep$，然后$i_t+j$，得到潜在的$add \ phrase \ p=t(i_t:i_t+j-1) $，然后判断$t(i_t+j)==s(i_s)\ and \ p\in V $</p>
<p><strong>源码</strong>：</p>
<p>和伪代码有一点不同，差异在于#####之间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">def _compute_single_tag(</span><br><span class="line">        self, source_token, target_token_idx,</span><br><span class="line">        target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes a single tag.</span><br><span class="line"></span><br><span class="line">    The tag may match multiple target tokens (via tag.added_phrase) so we return</span><br><span class="line">    the next unmatched target token.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_token: The token to be tagged.</span><br><span class="line">      target_token_idx: Index of the current target tag.</span><br><span class="line">      target_tokens: List of all target tokens.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A tuple with (1) the computed tag and (2) the next target_token_idx.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    source_token = source_token.lower()</span><br><span class="line">    target_token = target_tokens[target_token_idx].lower()</span><br><span class="line">    if source_token == target_token:</span><br><span class="line">        return tagging.Tag(&#x27;KEEP&#x27;), target_token_idx + 1</span><br><span class="line">    # source_token!=target_token</span><br><span class="line">    added_phrase = &#x27;&#x27;</span><br><span class="line">    for num_added_tokens in range(1, self._max_added_phrase_length + 1):</span><br><span class="line">        if target_token not in self._token_vocabulary:</span><br><span class="line">            break</span><br><span class="line">        added_phrase += (&#x27; &#x27; if added_phrase else &#x27;&#x27;) + target_token</span><br><span class="line">        next_target_token_idx = target_token_idx + num_added_tokens</span><br><span class="line">        if next_target_token_idx &gt;= len(target_tokens):</span><br><span class="line">            break</span><br><span class="line">        target_token = target_tokens[next_target_token_idx].lower()</span><br><span class="line">        if (source_token == target_token and</span><br><span class="line">                added_phrase in self._phrase_vocabulary):</span><br><span class="line">            return tagging.Tag(&#x27;KEEP|&#x27; + added_phrase), next_target_token_idx + 1</span><br><span class="line">    return tagging.Tag(&#x27;DELETE&#x27;), target_token_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        ####################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        ########################################################################################</span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<p><strong>缺陷</strong>：</p>
<p>对于一些情况，无法还原，举个例子：</p>
<p>​        source：证件有效期截止日期  target：证件日期格式</p>
<p>​        得不到tag结果</p>
<p>可以补充策略来修复bug</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        #########################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        #######################################################################################</span><br><span class="line"></span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    ####fix bug by lavine</span><br><span class="line"></span><br><span class="line">    ###strategy1</span><br><span class="line">    added_phrase = &quot;&quot;.join(target_tokens[target_token_idx:])</span><br><span class="line">    if added_phrase in self._phrase_vocabulary:</span><br><span class="line">        tags[-1] = tagging.Tag(&#x27;DELETE|&#x27; + added_phrase)</span><br><span class="line">        print(&#x27;&#x27;.join(source_tokens))</span><br><span class="line">        print(&#x27;&#x27;.join(target_tokens))</span><br><span class="line">        print(str([str(tag) for tag in tags] if tags != None else None))</span><br><span class="line">        return tags</span><br><span class="line">    ###strategy2</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<h3 id="4-3-模型结构"><a href="#4-3-模型结构" class="headerlink" title="4.3 模型结构"></a>4.3 模型结构</h3><p><img src="/2021/07/27/lasertagger/fr.JPG" alt></p>
<p>模型主要包含两个部分：1.encoder:generates activation vectors for each element in the input sequence 2.decoder：converts encoder activations into tag labels</p>
<h4 id="4-3-1-encoder"><a href="#4-3-1-encoder" class="headerlink" title="4.3.1 encoder"></a>4.3.1 encoder</h4><p>由于$BERT$在sentence encoding tasks上做到state-of-the-art，所以使用$BERT$ 作为encoder部分。作者选择了$BERT_{base}$,包含12个self-attention层</p>
<h4 id="4-3-2-decoder"><a href="#4-3-2-decoder" class="headerlink" title="4.3.2 decoder"></a>4.3.2 decoder</h4><p>在$BERT$原文中，对于标注任务采取了非常简单的decoder结构，即采用一层feed-forward作为decoder，把这种组合叫做$LASERTAGGER_{FF}$，这种结构的缺点在于预测的标注词相互独立，没有考虑标注词的关联性。</p>
<p>为了考虑标注词的关联性，decode使用了Transformer decoder，单向连接，记作$LASERTAGGER_{AR}$，这种encoder和decoder的组合的有点像BERT结合GPT的感觉decoder 和encoder在以下方面交流：(i) through a full attention over the sequence of encoder activations (ii) by directly consuming the encoder activation at the current step</p>
<h2 id="五-realize"><a href="#五-realize" class="headerlink" title="五.realize"></a>五.realize</h2><p>对于基本的tag，比如$KEEP$，$DELETE$，$ADD$，$realize$就是根据输入和tag直接转换就行；对于特殊的tag，需要一些特定操作，看情况维护规则。</p>
<h2 id="六-loss"><a href="#六-loss" class="headerlink" title="六 loss"></a>六 loss</h2><p>假设句子长度为n，tag数量为m, loss为n个m分类任务的和</p>
<h2 id="七-评价指标"><a href="#七-评价指标" class="headerlink" title="七.评价指标"></a>七.评价指标</h2><p>评价指标，不同任务不同评价指标</p>
<p>1 Sentence Fusion</p>
<p>Exact score ：percentage of exactly correctly predicted fusions（类似accuracy）</p>
<p>SARI ：average F1 scores of the added, kept, and deleted n-grams</p>
<p>2 Split and Rephrase</p>
<p>SARI</p>
<p>3 Abstractive Summarization</p>
<p>ROUGE-L</p>
<p>4 Grammatical Error Correction (GEC)</p>
<p>precision and recall, F0:5</p>
<h2 id="八-实验结果"><a href="#八-实验结果" class="headerlink" title="八.实验结果"></a>八.实验结果</h2><p><strong>baseline</strong>： based on Transformer where both the encoder and decoder replicate the $BERT_{base}$ architecture</p>
<p><strong>速度</strong>：1.$LASERTAGGER_{AR} $is already 10x faster than comparable-in-accuracy $SEQ2SEQ_{BERT}$ baseline. This difference is due to the former model using a 1-layer decoder (instead of 12 layers) and no encoder-decoder cross attention. 2.$LASERTAGGER_{FF}$ is more than 100x faster</p>
<p>其余结果参考paper</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01187.pdf">https://arxiv.org/pdf/1909.01187.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/lasertagger">https://github.com/google-research/lasertagger</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348109034">https://zhuanlan.zhihu.com/p/348109034</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/">文本改写</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/sentence-bert/#comment-container"><span class="display-none-class">6886a6840e722c992035060d31a77303</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6886a6840e722c992035060d31a77303">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.6 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/sentence-bert/">Sentence-BERT Sentence Embeddings using Siamese BERT-Networks</a></h1><div class="content"><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p>
<p>giit: <a target="_blank" rel="noopener" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications">https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications</a></p>
<h2 id="1-贡献"><a href="#1-贡献" class="headerlink" title="1.贡献"></a>1.贡献</h2><p>基于bert利用孪生结构或者三胞胎结构训练，使得产生在低维空间可用的句子Embedding。对于文本匹配任务，可以离线计算句子Embedding，然后基于句子Embedding在线匹配，可实现快速高精度的匹配。</p>
<h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/07/27/sentence-bert/s-bert1.JPG" alt></p>
<p>文章提出三种结构和目标函数，三胞胎结构作者没有画图</p>
<p>1.Classification Objective Function</p>
<script type="math/tex; mode=display">
loss=cross-entropy(softmax(W_t(u,v,|u-v|)),y_{true})</script><p>2.Regression Objective Function</p>
<script type="math/tex; mode=display">
loss=MSE(cosine-sim(u, v),y_{true})</script><p>3.Triplet Objective Function</p>
<script type="math/tex; mode=display">
loss=max(||s_a-s_p||-||s_a-s_n||+\sigma,0)</script><p>$||.||$计算向量距离，$s_a$为样本本身，$s_p$为正样本，$s_n$为负样本，$\sigma$使得正样本至少比负样本距离样本近$\sigma$。</p>
<p>对于pooling，文章提出三种策略</p>
<p>1.Using the output of the CLS-token<br>2.computing the mean of all output vectors (MEAN_strategy)<br>3.computing a max-over-time of the output vectors (MAX_strategy). The default configuration is MEAN.</p>
<h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h2><h3 id="3-1-Unsupervised-STS"><a href="#3-1-Unsupervised-STS" class="headerlink" title="3.1 Unsupervised STS"></a>3.1 Unsupervised STS</h3><p><img src="/2021/07/27/sentence-bert/11.JPG" alt></p>
<h3 id="3-2-Supervised-STS"><a href="#3-2-Supervised-STS" class="headerlink" title="3.2 Supervised STS"></a>3.2 Supervised STS</h3><p><img src="/2021/07/27/sentence-bert/22.JPG" alt></p>
<h3 id="3-3-Argument-Facet-Similarity"><a href="#3-3-Argument-Facet-Similarity" class="headerlink" title="3.3 Argument Facet Similarity"></a>3.3 Argument Facet Similarity</h3><p><img src="/2021/07/27/sentence-bert/33.JPG" alt></p>
<h3 id="3-4-Wikipedia-Sections-Distinction"><a href="#3-4-Wikipedia-Sections-Distinction" class="headerlink" title="3.4 Wikipedia Sections Distinction"></a>3.4 Wikipedia Sections Distinction</h3><p>We use the Triplet Objective</p>
<p><img src="/2021/07/27/sentence-bert/44.JPG" alt></p>
<h2 id="4-代码"><a href="#4-代码" class="headerlink" title="4.代码"></a>4.代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from sentence_bert.sentence_transformers import SentenceTransformer, util</span><br><span class="line"></span><br><span class="line">###load model</span><br><span class="line">model = SentenceTransformer(model_path)</span><br><span class="line"></span><br><span class="line"># Single list of sentences</span><br><span class="line">sentences = [&#x27;The cat sits outside&#x27;,</span><br><span class="line">             &#x27;A man is playing guitar&#x27;,</span><br><span class="line">             &#x27;I love pasta&#x27;,</span><br><span class="line">             &#x27;The new movie is awesome&#x27;,</span><br><span class="line">             &#x27;The cat plays in the garden&#x27;,</span><br><span class="line">             &#x27;A woman watches TV&#x27;,</span><br><span class="line">             &#x27;The new movie is so great&#x27;,</span><br><span class="line">             &#x27;Do you like pizza?&#x27;]</span><br><span class="line"></span><br><span class="line">#Compute embeddings</span><br><span class="line">embeddings = model.encode(sentences, convert_to_tensor=True)</span><br><span class="line"></span><br><span class="line">#Compute cosine-similarities for each sentence with each other sentence</span><br><span class="line">cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)</span><br><span class="line"></span><br><span class="line">#Find the pairs with the highest cosine similarity scores</span><br><span class="line">pairs = []</span><br><span class="line">for i in range(len(cosine_scores)-1):</span><br><span class="line">    for j in range(i+1, len(cosine_scores)):</span><br><span class="line">        pairs.append(&#123;&#x27;index&#x27;: [i, j], &#x27;score&#x27;: cosine_scores[i][j]&#125;)</span><br><span class="line"></span><br><span class="line">#Sort scores in decreasing order</span><br><span class="line">pairs = sorted(pairs, key=lambda x: x[&#x27;score&#x27;], reverse=True)</span><br><span class="line"></span><br><span class="line">for pair in pairs[0:10]:</span><br><span class="line">    i, j = pair[&#x27;index&#x27;]</span><br><span class="line">    print(&quot;&#123;&#125; \t\t &#123;&#125; \t\t Score: &#123;:.4f&#125;&quot;.format(sentences[i], sentences[j], pair[&#x27;score&#x27;]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">The new movie is awesome 		 The new movie is so great 		 Score: 0.9283</span><br><span class="line">The cat sits outside 		 The cat plays in the garden 		 Score: 0.6855</span><br><span class="line">I love pasta 		 Do you like pizza? 		 Score: 0.5420</span><br><span class="line">I love pasta 		 The new movie is awesome 		 Score: 0.2629</span><br><span class="line">I love pasta 		 The new movie is so great 		 Score: 0.2268</span><br><span class="line">The new movie is awesome 		 Do you like pizza? 		 Score: 0.1885</span><br><span class="line">A man is playing guitar 		 A woman watches TV 		 Score: 0.1759</span><br><span class="line">The new movie is so great 		 Do you like pizza? 		 Score: 0.1615</span><br><span class="line">The cat plays in the garden 		 A woman watches TV 		 Score: 0.1521</span><br><span class="line">The cat sits outside 		 The new movie is awesome 		 Score: 0.1475</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2></div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/nlp-data-augment/#comment-container"><span class="display-none-class">1898b4493fdd44773c33898a3f1bb63e</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="1898b4493fdd44773c33898a3f1bb63e">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/nlp-data-augment/">nlpcda-NLP中文数据增强工具，强推</a></h1><div class="content"><p>下载：pip install nlpcda</p>
<p>工具支持</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#1随机等价实体替换">1.随机实体替换</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#2随机同义词替换">2.近义词</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#3随机近义字替换">3.近义近音字替换</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#4随机字删除">4.随机字删除（内部细节：数字时间日期片段，内容不会删）</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#5ner命名实体-数据增强">5.NER类 <code>BIO</code> 数据增强</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#6随机置换邻近的字">6.随机置换邻近的字：<strong>研表究明，汉字序顺并不定一影响文字的阅读理解</strong>&lt;&lt;是乱序的</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#7等价字替换">7.中文等价字替换（1 一 壹 ①，2 二 贰 ②）</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#8翻译互转实现的增强">8.翻译互转实现的增强</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#9simbert">9.使用<code>simbert</code>做生成式相似句生成</a></li>
<li><a target="_blank" rel="noopener" href="https://pypi.org/project/nlpcda/#10Cluster2Cluster">10.Cluster2Cluster生成更多样化的新数据</a></li>
</ul>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E5%B0%8F%E5%B8%AE%E6%89%8B/">小帮手</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E4%B8%AD%E6%96%87%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA/">中文数据增强</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/NLP/page/5/">Previous</a></div><div class="pagination-next"><a href="/categories/NLP/page/7/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/NLP/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/categories/NLP/page/5/">5</a></li><li><a class="pagination-link is-current" href="/categories/NLP/page/6/">6</a></li><li><a class="pagination-link" href="/categories/NLP/page/7/">7</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">375</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">138</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">358</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-26T15:40:11.000Z">2022-07-26</time></p><p class="title"><a href="/2022/07/26/ml-iteration-analyze/">迭代分析</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90/">迭代分析</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-15T15:01:42.000Z">2022-07-15</time></p><p class="title"><a href="/2022/07/15/indexing/">倒排索引</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/">倒排索引</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-15T14:54:57.000Z">2022-07-15</time></p><p class="title"><a href="/2022/07/15/pre-ranking/">粗排</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E7%B2%97%E6%8E%92/">粗排</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-01T15:10:52.000Z">2022-07-01</time></p><p class="title"><a href="/2022/07/01/bert-serving/">bert_serving</a></p><p class="categories"><a href="/categories/NLP/">NLP</a> / <a href="/categories/NLP/%E5%B0%8F%E5%B8%AE%E6%89%8B/">小帮手</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-30T11:45:19.000Z">2022-06-30</time></p><p class="title"><a href="/2022/06/30/micro-Grained/">细粒度NLP任务</a></p><p class="categories"><a href="/categories/NLP/">NLP</a> / <a href="/categories/NLP/PTM/">PTM</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/CTR/"><span class="level-start"><span class="level-item">CTR</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">68</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/PTM/"><span class="level-start"><span class="level-item">PTM</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Prompt/"><span class="level-start"><span class="level-item">Prompt</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Tokenization/"><span class="level-start"><span class="level-item">Tokenization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">29</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">49</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CTR%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"><span class="tag">CTR一些问题</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2022 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>