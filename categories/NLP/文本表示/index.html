<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 文本表示 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/NLP/">NLP</a></li><li class="is-active"><a href="#" aria-current="page">文本表示</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-21  <a class="commentCountImg" href="/2021/12/21/bert-emb/#comment-container"><span class="display-none-class">271c7cdc6f50e6497130d81a42dcbf61</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="271c7cdc6f50e6497130d81a42dcbf61">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/21/bert-emb/">Bert系列之句向量生成</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/444346578">https://zhuanlan.zhihu.com/p/444346578</a></p>
<h2 id="1-sentence-bert"><a href="#1-sentence-bert" class="headerlink" title="1 sentence-bert"></a>1 sentence-bert</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据，nli有标签</p>
<p>无监督,有监督loss一样，文中有3种loss，区别在于数据集</p>
<p>无监督:nli有标签;有监督:sts有标签数据</p>
<h2 id="2-simcse"><a href="#2-simcse" class="headerlink" title="2 simcse"></a>2 simcse</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据</p>
<p>无监督，有监督区别在于：样本构造不同</p>
<p>无监督样本正负来源于sts无标签数据数据增强，有监督样本正负来源于sts有标签数据</p>
<h2 id="3-consert"><a href="#3-consert" class="headerlink" title="3 consert"></a>3 consert</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据，还有nli数据集（有标签）</p>
<p><strong>相同</strong></p>
<p>和simcse相同之处：都是在finetune引入对比</p>
<p><strong>不同</strong></p>
<p>1 无监督</p>
<p>和simces loss一样为NT-Xent，不同在于sts无标签数据数据增强方式不同</p>
<p>2 有监督</p>
<p>区别在于loss和数据源</p>
<p>simcse loss为NT-Xent，数据源为sts有标签数据</p>
<p>consert  loss为  NT-Xent  +  别的有监督loss（比如cross entropy），数据源为sts无标签数据和nli数据集（有标签），+表示融合 ，论文有3种融合方式</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">Bert文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-22  <a class="commentCountImg" href="/2021/11/22/token-emb/#comment-container"><span class="display-none-class">6e056f7e582fae7d38d3f1d58fb3f420</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6e056f7e582fae7d38d3f1d58fb3f420">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/22/token-emb/">token embedding</a></h1><div class="content"><div id="flowchart-0" class="flow-chart"></div>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p>
<p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">start=>start: 开始
io=>inputoutput: 输入文本

cond=>condition: 条件
sub=>subroutine: 子流程
end=>end: 结束
op1=>operation: 输入文本
op2=>operation: tokenize
op3=>operation: 词向量矩阵（预训练的或者随机初始化） 
op4=>operation: token embbedding
op1->op2->op3->op4</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/token-embedding/">token embedding</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-20  <a class="commentCountImg" href="/2021/10/20/simcse/#comment-container"><span class="display-none-class">2a05431bc7f7001f01cf0651967ad934</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="2a05431bc7f7001f01cf0651967ad934">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/20/simcse/">SimCSE Simple Contrastive Learning of Sentence Embeddings</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08821.pdf">https://arxiv.org/pdf/2104.08821.pdf</a></p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h2><p> <strong>1 target</strong></p>
<p>对于$D=\{(x_i,x_i^{+})\}_{i=1}^{m}$,where $x_i$ and $x_i^{+}$ are semantically related. xi,xj+ are not semantically related</p>
<p>x-&gt;h</p>
<p>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors</p>
<p><img src="/2021/10/20/simcse/1.GIF" alt></p>
<p>N is mini-batch size，分子是正样本，分母为负样本（有一个正样本,感觉是可以忽略）</p>
<p>分母会包含分子的项吗？从代码看，会的</p>
<p>loss</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d73e499ec859">https://www.jianshu.com/p/d73e499ec859</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def loss(self,y_pred,y_true,lamda=0.05):</span><br><span class="line"></span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    exist a query q1 and  ranked condidat list  [d1,d2,d3,...,dn]</span><br><span class="line">     loss=  -log( exp^sim(q1,d1)/t  /   sum(exp^sim(q1,di)/t) i=2,...,n)</span><br><span class="line"></span><br><span class="line">    [q1,q2]    [[d11,d12,d13],[d21,d22,d23]]</span><br><span class="line">     similarities=[[sim(q1d11),sim(q1d12),sim(q1d13)],[sim(q2d21),sim(q2d22),sim(q2d23)] ] y_true=[y1 ,y2 ]</span><br><span class="line"></span><br><span class="line">        loss = F.cross_entropy(similarities, y_true)</span><br><span class="line">    ref ： https://www.jianshu.com/p/d73e499ec859</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    # idxs = torch.arange(0, y_pred.shape[0])</span><br><span class="line">    # y_true = idxs + 1 - idxs % 2 * 2</span><br><span class="line">    y_pred = y_pred.reshape(-1, y_true.shape[1])</span><br><span class="line"></span><br><span class="line">    # y_true=[0]*y_pred.sha pe[0]</span><br><span class="line">    # similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2)</span><br><span class="line">    # similarities = similarities - torch.eye(y_pred.shape[0]) * 1e12</span><br><span class="line">    y_pred = y_pred / lamda</span><br><span class="line">    y_true = torch.argmax(y_true, dim=1)</span><br><span class="line">    loss = F.cross_entropy(y_pred, y_true)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>
<p><strong>2 representations评价指标</strong></p>
<p>Alignment： calculates expected distance between embeddings of the paired instances（paired instances就是正例）</p>
<p><img src="/2021/10/20/simcse/2.GIF" alt></p>
<p>uniformity： measures how well the embeddings are uniformly distributed</p>
<p><img src="/2021/10/20/simcse/3.GIF" alt></p>
<h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/10/20/simcse/11.GIF" alt></p>
<h3 id="2-1-Unsupervised"><a href="#2-1-Unsupervised" class="headerlink" title="2.1 Unsupervised"></a>2.1 Unsupervised</h3><p>$x_i-&gt;h_i^{z_i},x_i-&gt;h_i^{z_i^{‘}}$</p>
<p>z is a random mask for dropout，loss为</p>
<p><img src="/2021/10/20/simcse/4.GIF" alt></p>
<h3 id="2-2-Supervised"><a href="#2-2-Supervised" class="headerlink" title="2.2 Supervised"></a>2.2 Supervised</h3><p>引入非目标任务的有标签数据集，比如NLI任务，$(x_i,x_i^{+},x_i^{-})$,where $x_i$ is the premise, $x_i^{+}$and $x_i^{-}$are entailment and contradiction hypotheses.</p>
<p><img src="/2021/10/20/simcse/5.GIF" alt></p>
<p>$(h_i,h_j^{+})$为normal negatives，$(h_i,h_j^{-})$为hard negatives</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-27  <a class="commentCountImg" href="/2021/08/27/consert/#comment-container"><span class="display-none-class">e6753e8b5df65d65ffd41f2474584eb9</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="e6753e8b5df65d65ffd41f2474584eb9">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>8 m  <i class="fas fa-pencil-alt"> </i>1.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/27/consert/">ConSERT A Contrastive Framework for Self-Supervised Sentence Representation Transfer</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a></p>
<p><a target="_blank" rel="noopener" href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html">https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html</a></p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h2><p>首先，BERT其自身导出的句向量（不经过Fine-tune，对所有词向量求平均）会出现“坍缩（Collapse）”现象，即所有的句子都倾向于编码到一个较小的空间区域内，如图。为了解决这个问题，将对比学习结合到finetune过程，借助无标签数据来提升模型的能力。</p>
<p><img src="/2021/08/27/consert/11.JPG" alt></p>
<h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2.原理"></a>2.原理</h2><p>给定一个类似BERT的预训练语言模型$\textbf{M}$，以及从目标领域数据分布中收集的无标签文本语料库$\mathcal{D}$，我们希望通过构建自监督任务在$\mathcal{D}$上对$\textbf{M}$进行Fine-tune，使得Fine-tune后的模型能够在目标任务（文本语义匹配）上表现最好。</p>
<h3 id="2-1-整体框架"><a href="#2-1-整体框架" class="headerlink" title="2.1 整体框架"></a>2.1 整体框架</h3><p><img src="/2021/08/27/consert/22.JPG" alt></p>
<p>模型整体结构如上图所示，主要由三个部分组成</p>
<p>A <strong>data augmentation module</strong> that generates different views for input samples at the token embedding layer.</p>
<p>A <strong>shared BERT encoder</strong> that computes sentence representations for each input text. During training, we use the average pooling of the token embeddings at the last layer to obtain sentence representations.</p>
<p>A <strong>contrastive loss layer</strong> on top of the BERT encoder. It maximizes the agreement between one representation and its corresponding version that is augmented from the same sentence while keeping it distant from other sentence representations in the same batch.</p>
<p>对于任意一个句子输入$x$，得到其对应的两个增强向量$e_i=T_1(x),e_j=T_2(x),e_i,e_j\in \mathbb{R}^{L\times d}$，然后经过shared BERT encoder编码为$r_i,r_j$,其中$T_1,T_2$为不同的数据增强方式，$L$为句子$x$的长度，$d$为隐藏单元的数量。对于每个train step，从$\mathcal{D}$随机选取$N$个样本作为mini-batch，然后得到$2N$个增强样本，使用NT-Xent构造loss为</p>
<script type="math/tex; mode=display">
\mathcal{L}_{i,j}=-log\frac{exp(sim(r_i,r_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k\neq i]}exp(sim(r_i,r_k)/\tau)}
\\\mathcal{L}_{con}=\frac{1}{2N}\sum_{(i,j)}\mathcal{L}_{i,j}</script><p>其中$sim(.)$为余弦相似度计算，$\tau$表示temperature，是一个超参数，实验中取0.1,$\mathbb{1}$是指示器，当$k=i$时，值为0。上式分子为正样本，分母为全部（但是基本为负样本，所以可以看成负样本），所以loss变小就是让分子变大，分母变小，也就是让正样本相似度变大，负样本相似度变小</p>
<h3 id="2-2-数据增强策略"><a href="#2-2-数据增强策略" class="headerlink" title="2.2 数据增强策略"></a>2.2 数据增强策略</h3><p><strong>显式生成增强样本</strong>的方法包括：回译、同义词替换、意译等，然而这些方法一方面不一定能保证语义一致。所以考虑了在Embedding层<strong>隐式生成增强样本</strong>的方法。</p>
<p><img src="/2021/08/27/consert/44.JPG" alt></p>
<ul>
<li><p><strong>对抗攻击（Adversarial Attack）</strong>：这一方法通过梯度反传生成对抗扰动，将该扰动加到原本的Embedding矩阵上，就能得到增强后的样本。由于生成对抗扰动需要梯度反传，因此这一数据增强方法仅适用于有监督训练的场景。</p>
</li>
<li><p><strong>打乱词序（Token Shuffling）</strong>：这一方法扰乱输入样本的词序。由于Transformer结构没有“位置”的概念，模型对Token位置的感知全靠Embedding中的Position Ids得到。因此在实现上，我们只需要将Position Ids进行Shuffle即可。</p>
</li>
<li><p><strong>裁剪（Cutoff）</strong></p>
<p>：又可以进一步分为两种：</p>
<ul>
<li>Token Cutoff：随机选取Token，将对应Token的Embedding整行置为零。</li>
<li>Feature Cutoff：随机选取Embedding的Feature，将选取的Feature维度整列置为零。</li>
</ul>
</li>
<li><p><strong>Dropout</strong>：Embedding中的每一个元素都以一定概率置为零，与Cutoff不同的是，该方法并没有按行或者按列的约束。</p>
</li>
</ul>
<h3 id="2-3-融合监督信号"><a href="#2-3-融合监督信号" class="headerlink" title="2.3 融合监督信号"></a>2.3 融合监督信号</h3><p>除了无监督训练以外，作者给出3种进一步融合监督信号的策略，以NLI任务为例：</p>
<script type="math/tex; mode=display">
f=Concat(r_1,r_2,|r_1-r_2|)
\\\mathcal{L}_{ce}=CrossEntropy(Wf+b,y)</script><p><strong>Joint training (joint)</strong>:</p>
<script type="math/tex; mode=display">
\mathcal{L}_{joint}=\mathcal{L}_{ce}+\alpha\mathcal{L}_{con}\ \# on\ NLI
\ dataset</script><p><strong>Supervised training then unsupervised transfer (sup-unsup)</strong>:</p>
<p>first train the model with $\mathcal{L}_{ce}$on NLI dataset, then use $\mathcal{L}_{con}$to finetune it on the target dataset.</p>
<p><strong>Joint training then unsupervised transfer (joint-unsup)</strong>:</p>
<p>first train the model with the $\mathcal{L}_{joint}$on NLI dataset, then use $\mathcal{L}_{con }$to fine-tune it on the target dataset.</p>
<h2 id="3-定性分析"><a href="#3-定性分析" class="headerlink" title="3.定性分析"></a>3.定性分析</h2><p>后又发现BERT句向量表示的坍缩和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，坍缩现象可以在一定程度上得到缓解（如图2蓝色曲线所示）。</p>
<p><img src="/2021/08/27/consert/33.JPG" alt></p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h2><h3 id="4-1-Unsupervised-Results"><a href="#4-1-Unsupervised-Results" class="headerlink" title="4.1 Unsupervised Results"></a>4.1 Unsupervised Results</h3><p><img src="/2021/08/27/consert/1.JPG" alt></p>
<h3 id="4-2-Supervised-Results"><a href="#4-2-Supervised-Results" class="headerlink" title="4.2 Supervised Results"></a>4.2 Supervised Results</h3><p><img src="/2021/08/27/consert/2.JPG" alt></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/word2vec/#comment-container"><span class="display-none-class">5557345d5f80a07d7c4cb69fb82373ba</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="5557345d5f80a07d7c4cb69fb82373ba">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/word2vec/">word2vec</a></h1><div class="content"><h2 id="一-原理"><a href="#一-原理" class="headerlink" title="一.原理"></a>一.原理</h2><p><strong>两种训练模型</strong></p>
<ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<p><strong>训练技巧</strong></p>
<p>hierarchical softmax 和 negative sampling</p>
<h2 id="二-代码"><a href="#二-代码" class="headerlink" title="二.代码"></a>二.代码</h2><p><strong>训练代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure>
<p><strong>词向量矩阵</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.KeyedVectors.load_word2vec_format(model_path,binary=True)</span><br><span class="line">    print(model.vectors)   ##(779845, 400))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model.index_to_key)</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model[&quot;的&quot;])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[-1.3980628e+00, -4.6281612e-01,  5.8368486e-01, ...,         5.3952241e-01,  4.4697687e-01,  1.3505782e+00],       [ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, ...,         1.1110669e+00,  2.1992767e-01,  7.0457202e-01],       [-8.5650706e-01,  8.2832746e-02, -8.4218192e-01, ...,         2.1654253e+00,  6.4846051e-01, -5.7714492e-01],       ...,       [ 7.5072781e-03, -1.3543828e-02,  2.3101490e-02, ...,         4.2363801e-03, -5.6749382e-03,  6.3404259e-03],       [-2.6244391e-04, -3.0459568e-02,  5.9752418e-03, ...,         1.7844304e-02, -4.7109672e-04,  7.7916058e-03],       [ 7.2062697e-04, -6.5988898e-03,  1.1346856e-02, ...,        -3.7340564e-03, -1.8825980e-02,  2.7245486e-03]], dtype=float32)</span><br><span class="line"></span><br><span class="line">[&#x27;，&#x27;, &#x27;的&#x27;, &#x27;。&#x27;, &#x27;、&#x27;, &#x27;０&#x27;, &#x27;１&#x27;, &#x27;在&#x27;, &#x27;”&#x27;, &#x27;２&#x27;, &#x27;了&#x27;, &#x27;“&#x27;, &#x27;和&#x27;, &#x27;是&#x27;, &#x27;５&#x27;, ...]</span><br><span class="line"></span><br><span class="line">array([ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, -3.6405793e-01,        1.0851435e-01,  4.9507666e-02, -7.1219063e-01, -5.4614645e-01,       -1.3581418e+00,  3.0274218e-01,  6.1700332e-01,  3.5553512e-01,        1.6602433e+00,  7.5298291e-01, -1.4151905e-01, -2.1077128e-01,       -2.6325354e-01,  1.6108564e+00, -4.6750236e-01, -1.6261842e+00,        1.3063166e-01,  8.0702168e-01,  4.0011466e-01,  1.2198541e+00,       -6.2879241e-01,  ... 2.1928079e-01,  7.1725255e-01, -2.3430648e-01, -1.2066336e+00,        9.7590965e-01, -1.5906478e-01, -3.5802779e-01, -3.8005975e-01,        1.9056025e-01,  1.1110669e+00,  2.1992767e-01,  7.0457202e-01],      dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781v3">https://arxiv.org/abs/1301.3781v3</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1405.4053">https://arxiv.org/abs/1405.4053</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-28  <a class="commentCountImg" href="/2021/07/28/word-emb-add/#comment-container"><span class="display-none-class">27d8132878abc1f5fae21650cfc4df15</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="27d8132878abc1f5fae21650cfc4df15">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/28/word-emb-add/">nlp中使用预训练的词向量和随机初始化的词向量的区别在哪里？</a></h1><div class="content"><p>当你训练数据<strong>不充足</strong>的时候，可以直接使用别人已经预训练好的词向量，也可以根据自己的训练数据微调(fine-tuning)预训练词向量，也可以把词向量和整个模型一块训练，但是通常预训练的词向量我们不会再在训练的过程中进行更新。</p>
<p>当你的训练数据<strong>比较充足</strong>的时候，并且想让词向量能更好的捕捉自己的训练数据的语义信息时，应该使用随机初始化的词向量。当然，随机初始化的词向量必须要在训练网络的过程中不断进行更新，就和神经网络的权重参数一样进行训练。</p>
<p>例子：</p>
<p>1.直观展示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">###random</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">###from pretrain</span><br><span class="line">weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])</span><br><span class="line">embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">print(embedding.weight)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.1754,  1.6604, -1.5025, -1.0980, -0.4718],</span><br><span class="line">        [-1.1276,  0.1408, -1.0746, -1.2768, -0.6789]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.7366,  0.0607,  0.6151,  0.2282,  0.3878],</span><br><span class="line">        [-1.1365,  0.1844, -1.1191, -0.8787, -0.5121]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[1.0000, 2.3000, 3.0000],</span><br><span class="line">        [4.0000, 5.1000, 6.3000]])</span><br></pre></td></tr></table></figure>
<p>2.n-gram</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/337950427">https://www.zhihu.com/question/337950427</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/sentence-bert/#comment-container"><span class="display-none-class">6886a6840e722c992035060d31a77303</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6886a6840e722c992035060d31a77303">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.6 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/sentence-bert/">Sentence-BERT Sentence Embeddings using Siamese BERT-Networks</a></h1><div class="content"><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p>
<p>giit: <a target="_blank" rel="noopener" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications">https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications</a></p>
<h2 id="1-贡献"><a href="#1-贡献" class="headerlink" title="1.贡献"></a>1.贡献</h2><p>基于bert利用孪生结构或者三胞胎结构训练，使得产生在低维空间可用的句子Embedding。对于文本匹配任务，可以离线计算句子Embedding，然后基于句子Embedding在线匹配，可实现快速高精度的匹配。</p>
<h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/07/27/sentence-bert/s-bert1.JPG" alt></p>
<p>文章提出三种结构和目标函数，三胞胎结构作者没有画图</p>
<p>1.Classification Objective Function</p>
<script type="math/tex; mode=display">
loss=cross-entropy(softmax(W_t(u,v,|u-v|)),y_{true})</script><p>2.Regression Objective Function</p>
<script type="math/tex; mode=display">
loss=MSE(cosine-sim(u, v),y_{true})</script><p>3.Triplet Objective Function</p>
<script type="math/tex; mode=display">
loss=max(||s_a-s_p||-||s_a-s_n||+\sigma,0)</script><p>$||.||$计算向量距离，$s_a$为样本本身，$s_p$为正样本，$s_n$为负样本，$\sigma$使得正样本至少比负样本距离样本近$\sigma$。</p>
<p>对于pooling，文章提出三种策略</p>
<p>1.Using the output of the CLS-token<br>2.computing the mean of all output vectors (MEAN_strategy)<br>3.computing a max-over-time of the output vectors (MAX_strategy). The default configuration is MEAN.</p>
<h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h2><h3 id="3-1-Unsupervised-STS"><a href="#3-1-Unsupervised-STS" class="headerlink" title="3.1 Unsupervised STS"></a>3.1 Unsupervised STS</h3><p><img src="/2021/07/27/sentence-bert/11.JPG" alt></p>
<h3 id="3-2-Supervised-STS"><a href="#3-2-Supervised-STS" class="headerlink" title="3.2 Supervised STS"></a>3.2 Supervised STS</h3><p><img src="/2021/07/27/sentence-bert/22.JPG" alt></p>
<h3 id="3-3-Argument-Facet-Similarity"><a href="#3-3-Argument-Facet-Similarity" class="headerlink" title="3.3 Argument Facet Similarity"></a>3.3 Argument Facet Similarity</h3><p><img src="/2021/07/27/sentence-bert/33.JPG" alt></p>
<h3 id="3-4-Wikipedia-Sections-Distinction"><a href="#3-4-Wikipedia-Sections-Distinction" class="headerlink" title="3.4 Wikipedia Sections Distinction"></a>3.4 Wikipedia Sections Distinction</h3><p>We use the Triplet Objective</p>
<p><img src="/2021/07/27/sentence-bert/44.JPG" alt></p>
<h2 id="4-代码"><a href="#4-代码" class="headerlink" title="4.代码"></a>4.代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from sentence_bert.sentence_transformers import SentenceTransformer, util</span><br><span class="line"></span><br><span class="line">###load model</span><br><span class="line">model = SentenceTransformer(model_path)</span><br><span class="line"></span><br><span class="line"># Single list of sentences</span><br><span class="line">sentences = [&#x27;The cat sits outside&#x27;,</span><br><span class="line">             &#x27;A man is playing guitar&#x27;,</span><br><span class="line">             &#x27;I love pasta&#x27;,</span><br><span class="line">             &#x27;The new movie is awesome&#x27;,</span><br><span class="line">             &#x27;The cat plays in the garden&#x27;,</span><br><span class="line">             &#x27;A woman watches TV&#x27;,</span><br><span class="line">             &#x27;The new movie is so great&#x27;,</span><br><span class="line">             &#x27;Do you like pizza?&#x27;]</span><br><span class="line"></span><br><span class="line">#Compute embeddings</span><br><span class="line">embeddings = model.encode(sentences, convert_to_tensor=True)</span><br><span class="line"></span><br><span class="line">#Compute cosine-similarities for each sentence with each other sentence</span><br><span class="line">cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)</span><br><span class="line"></span><br><span class="line">#Find the pairs with the highest cosine similarity scores</span><br><span class="line">pairs = []</span><br><span class="line">for i in range(len(cosine_scores)-1):</span><br><span class="line">    for j in range(i+1, len(cosine_scores)):</span><br><span class="line">        pairs.append(&#123;&#x27;index&#x27;: [i, j], &#x27;score&#x27;: cosine_scores[i][j]&#125;)</span><br><span class="line"></span><br><span class="line">#Sort scores in decreasing order</span><br><span class="line">pairs = sorted(pairs, key=lambda x: x[&#x27;score&#x27;], reverse=True)</span><br><span class="line"></span><br><span class="line">for pair in pairs[0:10]:</span><br><span class="line">    i, j = pair[&#x27;index&#x27;]</span><br><span class="line">    print(&quot;&#123;&#125; \t\t &#123;&#125; \t\t Score: &#123;:.4f&#125;&quot;.format(sentences[i], sentences[j], pair[&#x27;score&#x27;]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">The new movie is awesome 		 The new movie is so great 		 Score: 0.9283</span><br><span class="line">The cat sits outside 		 The cat plays in the garden 		 Score: 0.6855</span><br><span class="line">I love pasta 		 Do you like pizza? 		 Score: 0.5420</span><br><span class="line">I love pasta 		 The new movie is awesome 		 Score: 0.2629</span><br><span class="line">I love pasta 		 The new movie is so great 		 Score: 0.2268</span><br><span class="line">The new movie is awesome 		 Do you like pizza? 		 Score: 0.1885</span><br><span class="line">A man is playing guitar 		 A woman watches TV 		 Score: 0.1759</span><br><span class="line">The new movie is so great 		 Do you like pizza? 		 Score: 0.1615</span><br><span class="line">The cat plays in the garden 		 A woman watches TV 		 Score: 0.1521</span><br><span class="line">The cat sits outside 		 The new movie is awesome 		 Score: 0.1475</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2></div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-19  <a class="commentCountImg" href="/2021/07/19/word-representation/#comment-container"><span class="display-none-class">18b6512fec030553b9c871f0394c8e23</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="18b6512fec030553b9c871f0394c8e23">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/19/word-representation/">文本表示</a></h1><div class="content"><p>文本表示的表示形式可以是单一数值（基本没人用），可以是向量（目前主流），好奇有没有高纬tensor表示的？下文是基于向量表示的。</p>
<h2 id="1-词语表示"><a href="#1-词语表示" class="headerlink" title="1.词语表示"></a>1.词语表示</h2><h3 id="1-1-one-hot"><a href="#1-1-one-hot" class="headerlink" title="1.1 one hot"></a>1.1 one hot</h3><p>举个例子，有样本如下：</p>
<p>​    Jane wants to go to Shenzhen.</p>
<p>​    Bob wants to go to Shanghai.</p>
<p>基于上述两个文档中出现的单词，构建如下一个词典：</p>
<p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p>
<p>那么wants 可以表示为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0,1,0,0,0,0,0]</span><br></pre></td></tr></table></figure>
<h3 id="1-2-word-embedding"><a href="#1-2-word-embedding" class="headerlink" title="1.2 word embedding"></a>1.2 word embedding</h3><p>词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度的向量空间当中，使得语意相似的词在向量空间上也会比较相近，举个例子，如</p>
<p><img src="/2021/07/19/word-representation/11.jpg" alt></p>
<p>上表为词向量矩阵，其中行表示不同特征，列表示不同词，Man可以表示为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[-1,0.01,0.03,0.09]</span><br></pre></td></tr></table></figure>
<p>性质：$emb_{Man}-emb_{Women}\approx  emb_{King}-emb_{Queen}$</p>
<p>常见的词向量矩阵构建方法有，word2vec，GloVe</p>
<h2 id="2-句子表示"><a href="#2-句子表示" class="headerlink" title="2.句子表示"></a>2.句子表示</h2><h3 id="2-1-词袋模型"><a href="#2-1-词袋模型" class="headerlink" title="2.1 词袋模型"></a>2.1 词袋模型</h3><p>词袋模型不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。</p>
<p>例句:</p>
<p>​    Jane wants to go to Shenzhen.</p>
<p>​    Bob wants to go to Shanghai.</p>
<p>基于上述两个文档中出现的单词，构建如下一个词典：</p>
<p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p>
<p>那么上面两个例句就可以用以下两个向量表示，其值为该词语出现的次数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1,1,2,1,1,0,0]</span><br><span class="line">[0,1,2,1,0,1,1]</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Sentence-Embedding"><a href="#2-2-Sentence-Embedding" class="headerlink" title="2.2 Sentence Embedding"></a>2.2 Sentence Embedding</h3><h4 id="2-2-1-评价工具"><a href="#2-2-1-评价工具" class="headerlink" title="2.2.1 评价工具"></a>2.2.1 评价工具</h4><p>SentEval  is a popular toolkit to evaluate the quality of sentence embeddings.</p>
<h4 id="2-2-2-常见方法"><a href="#2-2-2-常见方法" class="headerlink" title="2.2.2 常见方法"></a>2.2.2 常见方法</h4><p>sentence BERT</p>
<p>BERT-flow</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/444346578">https://zhuanlan.zhihu.com/p/444346578</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353187575">https://zhuanlan.zhihu.com/p/353187575</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0587bc01e414">https://www.jianshu.com/p/0587bc01e414</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chenyusheng0803/p/10978883.html">https://www.cnblogs.com/chenyusheng0803/p/10978883.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">469</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">447</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-29T14:55:38.000Z">2024-07-29</time></p><p class="title"><a href="/2024/07/29/skiplist/">skiplist跳表</a></p><p class="categories"><a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a> / <a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-11T13:10:43.000Z">2024-06-11</time></p><p class="title"><a href="/2024/06/11/web-search/">网页搜索</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T11:39:05.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/raid/">raid</a></p><p class="categories"><a href="/categories/c/">c++</a> / <a href="/categories/c/%E4%BC%98%E5%8C%96/">优化</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:23:12.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/shell/">shell</a></p><p class="categories"><a href="/categories/shell/">shell</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:09:27.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/vim/">vim</a></p><p class="categories"><a href="/categories/linux/">linux</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">77</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E9%A1%B5%E6%90%9C%E7%B4%A2/"><span class="tag">网页搜索</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>