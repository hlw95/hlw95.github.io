<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: 排序 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li><a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></li><li class="is-active"><a href="#" aria-current="page">排序</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-03-31  <a class="commentCountImg" href="/2022/03/31/ranking-survey/#comment-container"><span class="display-none-class">a3ba9d93381b27420c02cccce16a70f2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="a3ba9d93381b27420c02cccce16a70f2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>6 m  <i class="fas fa-pencil-alt"> </i>0.9 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/31/ranking-survey/">A Deep Look into Neural Ranking Models for Information Retrieval</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://par.nsf.gov/servlets/purl/10277191">https://par.nsf.gov/servlets/purl/10277191</a></p>
<h2 id="3-A-Unified-Model-Formulation"><a href="#3-A-Unified-Model-Formulation" class="headerlink" title="3 A Unified Model Formulation"></a>3 A Unified Model Formulation</h2><p> So a generalized LTR problem is to find the optimal ranking function f ∗ by minimizing the loss function over some labeled dataset</p>
<p><img src="/2022/03/31/ranking-survey/1.JPG" alt></p>
<p>f 是ranking function，s是query，t是候选集，y is the label set where labels represent grades</p>
<p>Without loss of generality, the ranking function f could be further abstracted by the following unified formulation</p>
<p><img src="/2022/03/31/ranking-survey/2.JPG" alt></p>
<p>ψ, ϕare representation functions which extract features from s and t respectively η is the interaction function which extracts features from (s, t) pair, and g is the evaluation function which computes the relevance score based on the feature representations.</p>
<h2 id="4-Model-Architecture"><a href="#4-Model-Architecture" class="headerlink" title="4. Model Architecture"></a>4. Model Architecture</h2><h3 id="4-1-Symmetric-vs-Asymmetric-Architectures"><a href="#4-1-Symmetric-vs-Asymmetric-Architectures" class="headerlink" title="4.1. Symmetric vs. Asymmetric Architectures"></a>4.1. Symmetric vs. Asymmetric Architectures</h3><p><strong>Symmetric Architecture:</strong> The inputs s and t are assumed to be homogeneous, so that symmetric network structure could be applied over the inputs</p>
<p><strong>Asymmetric Architecture</strong>: The inputs s and t are assumed to be heterogeneous, so that asymmetric network structures should be applied over the inputs</p>
<h3 id="4-2-Representation-focused-vs-Interaction-focused-Architectures"><a href="#4-2-Representation-focused-vs-Interaction-focused-Architectures" class="headerlink" title="4.2. Representation-focused vs. Interaction-focused Architectures"></a>4.2. Representation-focused vs. Interaction-focused Architectures</h3><p><img src="/2022/03/31/ranking-survey/3.JPG" alt></p>
<p><strong>Representation-focused Architecture</strong>: The underlying assumption of this type of architecture is that relevance depends on compositional meaning of the input texts. Therefore, models in this category usually define complex representation functions ϕ and ψ (i.e., deep neural networks), but no interaction function η</p>
<p><strong>Interaction-focused Architecture:</strong> The underlying assumption of this type of architecture is that relevance is in essence about the relation between the input texts, so it would be more effective to directly learn from interactions rather than from individual representations. Models in this category thus define the interaction function η rather than the representation functions ϕ and ψ</p>
<p><strong>Hybrid Architecture</strong>: In order to take advantage of both representation focused and interaction-focused architectures, a natural way is to adopt a hybrid architecture for feature learning. We find that there are two major hybrid strategies to integrate the two architectures, namely combined strategy and coupled strategy.</p>
<h3 id="4-3-Single-granularity-vs-Multi-granularity-Architecture"><a href="#4-3-Single-granularity-vs-Multi-granularity-Architecture" class="headerlink" title="4.3. Single-granularity vs. Multi-granularity Architecture"></a>4.3. Single-granularity vs. Multi-granularity Architecture</h3><p><strong>Single-granularity Architecture</strong>: The underlying assumption of the single granularity architecture is that relevance can be evaluated based on the high level features extracted by ϕ, ψ and η from the single-form text inputs.</p>
<p><strong>Multi-granularity Architecture:</strong> The underlying assumption of the multigranularity architecture is that relevance estimation requires multiple granularities of features, either from different-level feature abstraction or based on different types of language units of the inputs</p>
<h2 id="5-Model-Learning"><a href="#5-Model-Learning" class="headerlink" title="5. Model Learning"></a>5. Model Learning</h2><h3 id="5-1-Learning-objective"><a href="#5-1-Learning-objective" class="headerlink" title="5.1. Learning objective"></a>5.1. Learning objective</h3><p>Similar to other LTR algorithms, the learning objective of neural ranking models can be broadly categorized into three groups: pointwise, pairwise, and listwise.</p>
<h4 id="5-1-1-Pointwise-Ranking-Objective"><a href="#5-1-1-Pointwise-Ranking-Objective" class="headerlink" title="5.1.1. Pointwise Ranking Objective"></a>5.1.1. Pointwise Ranking Objective</h4><p>1 loss</p>
<p>The idea of pointwise ranking objectives is to simplify a ranking problem to a set of classification or regression problems</p>
<p><img src="/2022/03/31/ranking-survey/4.JPG" alt></p>
<p>a. Cross Entropy</p>
<p>For example, one of the most popular pointwise loss functions used in neural ranking models is Cross Entropy:</p>
<p><img src="/2022/03/31/ranking-survey/5.JPG" alt></p>
<p>b. Mean Squared Error</p>
<p>There are other pointwise loss functions such as Mean Squared Error for numerical labels, but they are more commonly used in recommendation tasks.</p>
<p>2 优缺点</p>
<p>a.advantages</p>
<p>First, it simple and easy to scale. Second, the outputs  <strong>have real meanings and value in practice.</strong> For instance, in sponsored search, a model learned with cross entropy loss and clickthrough rates can directly predict the probability of user clicks on search ads, which is more important than creating a good result list in some application scenarios. </p>
<p>b.disadvantages</p>
<p>less effective ，Because pointwise loss functions consider no document preference or order information, they do not guarantee to produce the best ranking list when the model loss reaches the global minimum. </p>
<h4 id="5-1-2-Pairwise-Ranking-Objective"><a href="#5-1-2-Pairwise-Ranking-Objective" class="headerlink" title="5.1.2. Pairwise Ranking Objective"></a>5.1.2. Pairwise Ranking Objective</h4><p>1 loss</p>
<p>Pairwise ranking objectives focus on optimizing the relative preferences between documents rather than their labels.</p>
<p><img src="/2022/03/31/ranking-survey/6.JPG" alt></p>
<p>a.Hinge loss</p>
<p>b.cross entropy</p>
<p>​        RankNet</p>
<p>2 优缺点</p>
<p>a.advantages</p>
<p>effective in many tasks</p>
<p>b.disadvantages</p>
<p>pairwise methods does not always lead to the improvement of final ranking metrics due to two reasons: (1) it is impossible to develop a ranking model that can correctly predict document preferences in all cases; and (2) in the computation of most existing ranking metrics, not all document pairs are equally important.</p>
<h4 id="5-1-3-Listwise-Ranking-Objective"><a href="#5-1-3-Listwise-Ranking-Objective" class="headerlink" title="5.1.3. Listwise Ranking Objective"></a>5.1.3. Listwise Ranking Objective</h4><p>1 loss</p>
<p>listwise loss functions compute ranking loss with each query and their candidate document list together</p>
<p><img src="/2022/03/31/ranking-survey/9.JPG" alt></p>
<p>a. ListMLE</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36478718/article/details/122598406">https://blog.csdn.net/qq_36478718/article/details/122598406</a></p>
<p><img src="/2022/03/31/ranking-survey/10.JPG" alt></p>
<p>b.Attention Rank function</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.05936">https://arxiv.org/abs/1804.05936</a></p>
<p><img src="/2022/03/31/ranking-survey/11.JPG" alt></p>
<p>c. softmax-based listwise</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.04415.pdf">https://arxiv.org/pdf/1811.04415.pdf</a></p>
<p>2 优缺点</p>
<p>a.advantages</p>
<p>While listwise ranking objectives are generally more effective than pairwise ranking objectives</p>
<p>b.disadvantages</p>
<p>their high computational cost often limits their applications. They are suitable for the re-ranking phase over a small set of candidate documents</p>
<h4 id="5-1-4-Multi-task-Learning-Objective"><a href="#5-1-4-Multi-task-Learning-Objective" class="headerlink" title="5.1.4. Multi-task Learning Objective"></a>5.1.4. Multi-task Learning Objective</h4><p> the optimization of neural ranking models may include the learning of multiple ranking or non-ranking objectives at the same time. </p>
<h3 id="5-2-Training-Strategies"><a href="#5-2-Training-Strategies" class="headerlink" title="5.2. Training Strategies"></a>5.2. Training Strategies</h3><p>1 Supervised learning</p>
<p>2 Weakly supervised learning</p>
<p>3 Semi-supervised learning</p>
<h2 id="6-Model-Comparison"><a href="#6-Model-Comparison" class="headerlink" title="6. Model Comparison"></a>6. Model Comparison</h2><p>比较了常见模型在不同应用的效果</p>
<p>1  Ad-hoc Retrieval</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44092699/article/details/106335971">https://blog.csdn.net/qq_44092699/article/details/106335971</a></p>
<p>Ad-hoc information retrieval refers to the task of returning information resources related to a user query formulated in natural language.</p>
<p>2 QA</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E6%8E%92%E5%BA%8F/">排序</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/ranking-survey/">ranking survey</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-01-17  <a class="commentCountImg" href="/2022/01/17/pointwise-pairwise/#comment-container"><span class="display-none-class">ef110b442bf69044fd6cf2d25969c151</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="ef110b442bf69044fd6cf2d25969c151">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/17/pointwise-pairwise/">pointwise vs pairwise</a></h1><div class="content"><p>pairwise算法聚焦于精确的预测每个文档之间的相关度，pairwise算法主要关心两个文档之间的顺序，相比pointwise的算法更加接近于排序的概念。</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/pairwise/">pairwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/pointwise-vs-pairwise/">pointwise vs pairwise</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2022-01-04  <a class="commentCountImg" href="/2022/01/04/ranknet-listnet/#comment-container"><span class="display-none-class">6eb9cb632fe7c732bdaa14f92c545576</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6eb9cb632fe7c732bdaa14f92c545576">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/04/ranknet-listnet/">ranknet对比listnet</a></h1><div class="content"><p>The ListNet method grows on the bases of RankNet, they both employ the Cross Entropy function as a loss function and Gradient Descendant as algorithm to train a Neural Network Model. While the ListNet uses document list as instances, RankNet uses document pairs.</p>
<p><img src="/2022/01/04/ranknet-listnet/1.JPG" alt></p>
<p>We investigated why the listwise method ListNet can outperform the pairwise methods of RankNet, Ranking SVM, and RankBoost.</p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p>
<p>1.for the pairwise approach the number of document pairs varies largely from query to query</p>
<p>2.The pairwise approach actually employs a ‘pairwise’ loss function, which might be too loose as an approximation of the performance measures</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/listwise/">listwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/ranknet%E5%AF%B9%E6%AF%94listnet/">ranknet对比listnet</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-21  <a class="commentCountImg" href="/2021/12/21/listnet/#comment-container"><span class="display-none-class">788c98b06edc306c413ac6cf44ad4c7f</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="788c98b06edc306c413ac6cf44ad4c7f">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/21/listnet/">Learning to Rank From Pairwise Approach to Listwise Approach(listnet)</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Mr_tyting/article/details/80554849">https://blog.csdn.net/Mr_tyting/article/details/80554849</a></p>
<p>核心思想为：</p>
<ol>
<li>Given two lists of scores（模型和人）</li>
<li>we can first calculate two permutation probability distributions from them（简化到用top1）</li>
<li>and then calculate the distance between the two distributions as the listwise loss function.（交叉熵）</li>
</ol>
<h2 id="4-Probability-Models"><a href="#4-Probability-Models" class="headerlink" title="4. Probability Models"></a>4. Probability Models</h2><h3 id="4-1-Permutation-Probability"><a href="#4-1-Permutation-Probability" class="headerlink" title="4.1. Permutation Probability"></a>4.1. Permutation Probability</h3><p><img src="/2021/12/21/listnet/1.JPG" alt></p>
<p><img src="/2021/12/21/listnet/2.JPG" alt></p>
<p>$\pi=(2,3,1) $指的是对象2排在第一位</p>
<p>上面是topn的形式</p>
<p>因为总共有n！次排序组合</p>
<h3 id="4-2-Top-One-Probability"><a href="#4-2-Top-One-Probability" class="headerlink" title="4.2. Top One Probability"></a>4.2. Top One Probability</h3><p><strong>topk：</strong></p>
<script type="math/tex; mode=display">
P_s(\pi)=\prod \limits_{j=1}^K\frac{\phi(S_{\pi(j)})}{\sum_{k=j}^n\phi(S_{\pi(k)})}</script><p>总共有N ! / ( N − k ) ! 种不同排列，大大减少了计算复杂度</p>
<p><strong>top1：</strong></p>
<p><img src="/2021/12/21/listnet/3.JPG" alt></p>
<p>此时有n种不同排列情况</p>
<p>概率分布的含义：对于每个j，分别都处于第一的概率是多少</p>
<h2 id="5-Learning-Method-ListNet"><a href="#5-Learning-Method-ListNet" class="headerlink" title="5.Learning Method: ListNet"></a>5.Learning Method: ListNet</h2><p>We employ a new learning method for optimizing the listwise loss function based on <strong>top one probability</strong>, with Neural Network as model and Gradient Descent as optimization algorithm. We refer to the method as ListNet.</p>
<p><img src="/2021/12/21/listnet/4.JPG" alt></p>
<p><img src="/2021/12/21/listnet/5.JPG" alt></p>
<p>$y^{(i)}$是真实的score list，有个疑问就是$y^{(i)}$怎么得到？关于这个，应该是先有真实的score list（人打），然后基于score list得到排序，参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66514492">https://zhuanlan.zhihu.com/p/66514492</a></p>
<p><img src="/2021/12/21/listnet/6.JPG" alt></p>
<p><img src="/2021/12/21/listnet/7.JPG" alt></p>
<h2 id="核心步骤"><a href="#核心步骤" class="headerlink" title="核心步骤"></a>核心步骤</h2><p>1.打标得到真实的score list，模型得到预测的score list</p>
<p>2.然后用softmax得到真实的和预测的score list的概率分布</p>
<p>3.然后用交叉熵计算两种概率分布的差距</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/listwise/">listwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/listnet/">listnet</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-20  <a class="commentCountImg" href="/2021/12/20/listwise/#comment-container"><span class="display-none-class">f88c01dcc7fa2396fe18595db004813c</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="f88c01dcc7fa2396fe18595db004813c">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/20/listwise/">listwise</a></h1><div class="content"><p>博客</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66514492">https://zhuanlan.zhihu.com/p/66514492</a></p>
<p>Listwise Approach to Learning to Rank - Theory and Algorithm（ListMLE）</p>
<p><a target="_blank" rel="noopener" href="http://icml2008.cs.helsinki.fi/papers/167.pdf">http://icml2008.cs.helsinki.fi/papers/167.pdf</a></p>
<p>Learning to Rank: From Pairwise Approach to Listwise Approach（ListNet） </p>
<p><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/listwise/">listwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/listwise/">listwise</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-20  <a class="commentCountImg" href="/2021/12/20/pairwise/#comment-container"><span class="display-none-class">a7b16f1fee5fc32820c885a95c98393a</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="a7b16f1fee5fc32820c885a95c98393a">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/20/pairwise/">pairwise</a></h1><div class="content"><p>pairwise learning to rank 的方法可以分为两大类。</p>
<p>第一类是基于<strong>打分函数</strong>，它们通过一些特殊的设计让模型依靠“样本对”的信息来学习得到每个样本的score。所以得到这类方法最后的全局排序结果很简单，就是用所有样本的score来排序即可。</p>
<p>另一类方法是基于<strong>优先函数</strong>的方法。这类方法的整个过程分为两个阶段，第一阶段是用机器学习模型来学习两个样本之间的优先关系，例如f(x1, x2)=1表示样本x1优先于x2（x1应该排在x2前面），f(x1, x2)=-1表示样本x2优先于x1（x1应该排在x2后面）。从题主的问题来看，可能问的是“当我们已经训练出了优先函数f之后，如何对所有样本进行排序，并且使该排序在最大程度上与f的结果一致”。这个问题在学界被称为Rank Aggregation（排列聚合）。</p>
<p>具体参考 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/389068269">https://www.zhihu.com/question/389068269</a></p>
<p>别的相关参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/235756fbf6b6">https://www.jianshu.com/p/235756fbf6b6</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/318300682">https://zhuanlan.zhihu.com/p/318300682</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65224450">https://zhuanlan.zhihu.com/p/65224450</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/65756030">https://zhuanlan.zhihu.com/p/65756030</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/389068269/answer/1180120736">https://www.zhihu.com/question/389068269/answer/1180120736</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/pairwise/">pairwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/pairwise/">pairwise</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-12-20  <a class="commentCountImg" href="/2021/12/20/pointwise/#comment-container"><span class="display-none-class">1ea5cdae2519042bc5e1c51ff4c4e818</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="1ea5cdae2519042bc5e1c51ff4c4e818">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/12/20/pointwise/">pointwise</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113302654#">https://zhuanlan.zhihu.com/p/113302654#</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/pointwise/">pointwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/pointwise/">pointwise</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-16  <a class="commentCountImg" href="/2021/11/16/tf-ranking/#comment-container"><span class="display-none-class">4113593d36ffcf1f3a94f57931141e13</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4113593d36ffcf1f3a94f57931141e13">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/16/tf-ranking/">tf ranking</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/52447211#">https://zhuanlan.zhihu.com/p/52447211#</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/ranking">https://github.com/tensorflow/ranking</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E5%B0%8F%E5%B8%AE%E6%89%8B/">小帮手</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/tf-ranking/">tf ranking</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-27  <a class="commentCountImg" href="/2021/09/27/lambdamart/#comment-container"><span class="display-none-class">1d21af6ac09030b0fa57649eb89cea30</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="1d21af6ac09030b0fa57649eb89cea30">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>6 m  <i class="fas fa-pencil-alt"> </i>0.9 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/27/lambdamart/">From RankNet to LambdaRank to LambdaMART</a></h1><div class="content"><h2 id="1-RankNet"><a href="#1-RankNet" class="headerlink" title="1.RankNet"></a>1.RankNet</h2><p>RankNet采用pairwise的方法进行模型训练。</p>
<p><strong>loss推导</strong></p>
<p>给定特定$query$下的两个文档$U_i$和$U_j$，其特征向量分别为$x_i$和$x_j$，经过RankNet进行前向计算得到对应的分数为$s_i=f(x_i)$和$s_j=f(x_j)$。用$U_i \rhd U_j$表示$U_i$比$U_j$排序更靠前。继而可以用下面的公式来表示$U_i$应该比$U_j$排序更靠前的概率：</p>
<script type="math/tex; mode=display">
P_{ij} \equiv P(U_i \rhd U_j) \equiv \frac{1}{1+e^{-\sigma(s_i-s_j)}}</script><p>定义$S_{ij} \in \{0,\pm1\}$为文档$i$和文档$j$被标记的标签之间的关联，即</p>
<script type="math/tex; mode=display">
S_{ij}=\left\{ \begin{aligned} 1&&     文档i比文档j更相关\\ 0&&    文档i和文档j相关性一致\\ -1&&   文档j比文档i更相关 \end{aligned} \right.</script><p>定义$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$U_i$应该比$U_j$排序更靠前的已知概率，则可以用交叉熵定义优化目标的损失函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
C&=-\overline{P}_{ij}log{P_{ij}}-(1-\overline{P}_{ij})log(1-P_{ij})
\\&=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})

\end{align*}</script><p>注意：$\sigma$是超参数</p>
<p><strong>ranknet 加速</strong></p>
<h2 id="2-LambdaRank"><a href="#2-LambdaRank" class="headerlink" title="2.LambdaRank"></a>2.LambdaRank</h2><p>ranket缺陷为只考虑pair的相对位置没有考虑二者在列表的整体位置</p>
<p>LambdaRank本质为ranknet基础上加入Listwise的指标，因此有人将LambdaRank归为listwise方法，也有归到pairwise方法</p>
<h3 id="2-1-RankNet的局限"><a href="#2-1-RankNet的局限" class="headerlink" title="2.1 RankNet的局限"></a>2.1 RankNet的局限</h3><p><img src="/2021/09/27/lambdamart/11.GIF" alt></p>
<h3 id="2-2-LambdaRank定义"><a href="#2-2-LambdaRank定义" class="headerlink" title="2.2 LambdaRank定义"></a>2.2 LambdaRank定义</h3><script type="math/tex; mode=display">
\begin{align*}
\frac{\partial{C}}{\partial{w_k}}&=\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}
\\&=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)
\\&=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)

\end{align*}
\\其中\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=-\frac{\partial{C}}{\partial{s_j}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)</script><p>上述公式可以进一步简化，即只考虑$S_{ij}=1$ （为什么可以？？？？？）</p>
<p>那么Lambda，$\lambda$，就是梯度</p>
<script type="math/tex; mode=display">
\lambda_{ij}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}</script><p>为了加强排序中前后顺序的重要性，Lambda在原基础上引入评价指标Z（如NDCG），把交换两个文档的位置引起的评价指标的变化$|\Delta Z_{ij}|$作为其中一个因子：</p>
<script type="math/tex; mode=display">
\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta Z_{ij}|</script><p>反推出 LambdaRank 的损失函数：</p>
<script type="math/tex; mode=display">
C=log(1+e^{\sigma (s_i-s_j)})|\Delta Z_{ij}|</script><h2 id="3-LambdaMART"><a href="#3-LambdaMART" class="headerlink" title="3.LambdaMART"></a>3.LambdaMART</h2><p>属于listwise，也有说pairwise。</p>
<p>LambdaMART=lambda($\lambda$)+mart(gbdt)</p>
<p>$\lambda$就是梯度，lambdarank就是一种loss，gbdt就是模型</p>
<p>lambdamart说白了就是利用gbdt计算lambdarank中s，或者说将lambdarank作为gbdt的loss</p>
<p>gbdt，lambdamart算法流程差异在于step1</p>
<p><strong>GBDT</strong>：</p>
<ol>
<li>初始化： $f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</li>
<li>for m=1 to M:<br>(a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \qquad i = 1,2 \cdots N$<br>(b). $\left \{ R_{jm} \right\}_1^J = \mathop{\arg\min}\limits_{\left \{ R_{jm} \right\}_1^J}\sum\limits_{i=1}^N \left [\tilde{y}_i - h_m(x_i\,;\,\left \{R_{jm},b_{jm} \right\}_1^J) \right]^2$<br>(c).  $\gamma_{jm} = \mathop{\arg\min}\limits_\gamma \sum\limits_{x_i \in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$<br>(d).  $f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^J \gamma_{jm}I(x \in R_{jm})$</li>
<li>输出$f_M(x)$</li>
</ol>
<p><strong>LambdaMART:</strong></p>
<p><img src="/2021/09/27/lambdamart/22.png" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/laolu1573/article/details/87372514">https://blog.csdn.net/laolu1573/article/details/87372514</a></p>
<p><a target="_blank" rel="noopener" href="https://liam.page/uploads/slides/lambdamart.pdf">https://liam.page/uploads/slides/lambdamart.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zpalyq110/article/details/79527653">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86354141">https://zhuanlan.zhihu.com/p/86354141</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/genyuan/p/9788294.html">https://www.cnblogs.com/genyuan/p/9788294.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/huagong_adu/article/details/40710305">https://blog.csdn.net/huagong_adu/article/details/40710305</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bentuwuying/p/6690836.html">https://www.cnblogs.com/bentuwuying/p/6690836.html</a></p>
<p>paper原文： <a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a78b3f52c221">https://www.jianshu.com/p/a78b3f52c221</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhoujialin/article/details/46697409">https://blog.csdn.net/zhoujialin/article/details/46697409</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/w28971023/article/details/45849659">https://blog.csdn.net/w28971023/article/details/45849659</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/listwise/">listwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/LambdaMART/">LambdaMART</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-21  <a class="commentCountImg" href="/2021/08/21/meituan/#comment-container"><span class="display-none-class">0621e1cfe84648350430593241d756d2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="0621e1cfe84648350430593241d756d2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/21/meituan/">BERT在美团搜索核心排序的探索和实践</a></h1><div class="content"><p>很有启发，抱着学习态度，mark一下</p>
<h2 id="模型层面"><a href="#模型层面" class="headerlink" title="模型层面"></a>模型层面</h2><p>整体结构如下</p>
<p><img src="/2021/08/21/meituan/1.png" alt></p>
<p>1 BERT预训练</p>
<p>2 多任务学习</p>
<p>​    场景层：根据业务场景进行划分，每个业务场景单独设计网络结构</p>
<p>3 联合训练</p>
<p>两个任务分别为：</p>
<p>​    1 相关性任务：相关性+NER（多任务增强相关性）</p>
<p>​    2 排序任务</p>
<p>怎么联合没看出来</p>
<p>之前是两阶段finetune： 1. 先相关性任务 2 然后排序任务</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html">https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html</a> </p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E6%8E%92%E5%BA%8F/">排序</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%8E%92%E5%BA%8F%E5%AD%A6%E4%B9%A0/">排序学习</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">1</a></li><li><a class="pagination-link" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">376</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">138</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">359</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-08-04T14:52:30.000Z">2022-08-04</time></p><p class="title"><a href="/2022/08/04/feature-scale/">feature scale</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-26T15:40:11.000Z">2022-07-26</time></p><p class="title"><a href="/2022/07/26/ml-iteration-analyze/">迭代分析</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%BF%AD%E4%BB%A3%E5%88%86%E6%9E%90/">迭代分析</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-15T15:01:42.000Z">2022-07-15</time></p><p class="title"><a href="/2022/07/15/indexing/">倒排索引</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95/">倒排索引</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-15T14:54:57.000Z">2022-07-15</time></p><p class="title"><a href="/2022/07/15/pre-ranking/">粗排</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/%E7%B2%97%E6%8E%92/">粗排</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-01T15:10:52.000Z">2022-07-01</time></p><p class="title"><a href="/2022/07/01/bert-serving/">bert_serving</a></p><p class="categories"><a href="/categories/NLP/">NLP</a> / <a href="/categories/NLP/%E5%B0%8F%E5%B8%AE%E6%89%8B/">小帮手</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/CTR/"><span class="level-start"><span class="level-item">CTR</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">68</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/PTM/"><span class="level-start"><span class="level-item">PTM</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Prompt/"><span class="level-start"><span class="level-item">Prompt</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Tokenization/"><span class="level-start"><span class="level-item">Tokenization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">29</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/04/"><span class="level-start"><span class="level-item">April 2022</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CTR%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"><span class="tag">CTR一些问题</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2022 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>