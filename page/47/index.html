<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-21  <a class="commentCountImg" href="/2021/07/21/word-similarity/#comment-container"><span class="display-none-class">b208022762ac5cc544ef49ae50f65ab2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="b208022762ac5cc544ef49ae50f65ab2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>18 m  <i class="fas fa-pencil-alt"> </i>2.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/21/word-similarity/">词语的文本相似度</a></h1><div class="content"><h2 id="一-基于词典"><a href="#一-基于词典" class="headerlink" title="一.基于词典"></a>一.基于词典</h2><p>人为构建，比较主观，不利于维护</p>
<h3 id="1-1-基于词林"><a href="#1-1-基于词林" class="headerlink" title="1.1 基于词林"></a>1.1 基于词林</h3><h4 id="1-1-1-结构"><a href="#1-1-1-结构" class="headerlink" title="1.1.1 结构"></a>1.1.1 结构</h4><p>扩展版同义词词林分为5层结构，如图，随着级别的递增，词义刻画越来越细，到了第五层，每个分类里词语数量已经不大，很多只有一个词语，已经不可再分，可以称为原子词群、原子类或原子节点。不同级别的分类结果可以为自然语言处理提供不同的服务，例如第四层的分类和第五层的分类在信息检索、文本分类、自动问答等研究领域得到应用。有研究证明，对词义进行有效扩展，或者对关键词做同义词替换可以明显改善信息检索、文本分类和自动问答系统的性能。</p>
<p><img src="/2021/07/21/word-similarity/cilin.JPG" alt></p>
<p>下载后的词典文件如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Aa01A01= 人 士 人物 人士 人氏 人选</span><br><span class="line">Aa01A02= 人类 生人 全人类</span><br><span class="line">Aa01A03= 人手 人员 人口 人丁 口 食指</span><br><span class="line">Aa01A04= 劳力 劳动力 工作者</span><br><span class="line">Aa01A05= 匹夫 个人</span><br></pre></td></tr></table></figure>
<p><img src="/2021/07/21/word-similarity/coder.JPG" alt></p>
<p>表中的编码位是按照从左到右的顺序排列。第八位的标记有3 种，分别是“=”、“#”、“@”， “=”代表“相等”、“同义”。末尾的“#”代表“不等”、“同类”，属于相关词语。末尾的“@”代表“自我封闭”、“独立”，它在词典中既没有同义词，也没有相关词。</p>
<p><strong>源码如下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">class WordSimilarity2010(SimilarBase):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">        本类根据下面的论文方法：</span><br><span class="line">        基于同义词词林的词语相似度计算方法，田久乐, 赵 蔚(东北师范大学 计算机科学与信息技术学院, 长春 130117 )</span><br><span class="line">        计算两个单词所有编码组合的相似度，取最大的一个</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(WordSimilarity2010, self).__init__()</span><br><span class="line">        self.a = 0.65</span><br><span class="line">        self.b = 0.8</span><br><span class="line">        self.c = 0.9</span><br><span class="line">        self.d = 0.96</span><br><span class="line">        self.e = 0.5</span><br><span class="line">        self.f = 0.1</span><br><span class="line">        self.degree = 180</span><br><span class="line">        self.PI = math.pi</span><br><span class="line"></span><br><span class="line">    def similarity(self, w1, w2):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        判断两个词的相似性。</span><br><span class="line">        :param w1: [string]</span><br><span class="line">        :param w2: [string]</span><br><span class="line">        :return: [float]0~1之间。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        code1 = self._data.get(w1, None)</span><br><span class="line">        code2 = self._data.get(w2, None)</span><br><span class="line"></span><br><span class="line">        if not code1 or not code2:</span><br><span class="line">            return 0  # 只要有一个不在库里则代表没有相似性。</span><br><span class="line"></span><br><span class="line">        # 最终返回的最大相似度</span><br><span class="line">        sim_max = 0</span><br><span class="line"></span><br><span class="line">        # 两个词可能对应多个编码</span><br><span class="line">        for c1 in code1:</span><br><span class="line">            for c2 in code2:</span><br><span class="line">                cur_sim = self.sim_by_code(c1, c2)</span><br><span class="line">                # print(c1, c2, &#x27;的相似度为：&#x27;, cur_sim)</span><br><span class="line">                if cur_sim &gt; sim_max:</span><br><span class="line">                    sim_max = cur_sim</span><br><span class="line"></span><br><span class="line">        return sim_max</span><br><span class="line"></span><br><span class="line">    def sim_by_code(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据编码计算相似度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # 先把code的层级信息提取出来</span><br><span class="line">        clayer1 = self._parse_code(c1)</span><br><span class="line">        clayer2 = self._parse_code(c2)</span><br><span class="line"></span><br><span class="line">        common_layer = self.get_common_layer(clayer1,clayer2)</span><br><span class="line">        length = len(common_layer)</span><br><span class="line"></span><br><span class="line">        # 如果有一个编码以&#x27;@&#x27;结尾，那么表示自我封闭，这个编码中只有一个词，直接返回f</span><br><span class="line">        if c1.endswith(&#x27;@&#x27;) or c2.endswith(&#x27;@&#x27;) or 0 == length:</span><br><span class="line">            return self.f</span><br><span class="line"></span><br><span class="line">        cur_sim = 0</span><br><span class="line">        if 6 &lt;= length:</span><br><span class="line">            # 如果前面七个字符相同，则第八个字符也相同，要么同为&#x27;=&#x27;，要么同为&#x27;#&#x27;&#x27;</span><br><span class="line">            if c1.endswith(&#x27;=&#x27;) and c2.endswith(&#x27;=&#x27;):</span><br><span class="line">                cur_sim = 1</span><br><span class="line">            elif c1.endswith(&#x27;#&#x27;) and c2.endswith(&#x27;#&#x27;):</span><br><span class="line">                cur_sim = self.e</span><br><span class="line">        else:</span><br><span class="line">            k = self.get_k(clayer1, clayer2)</span><br><span class="line">            n = self.get_n(common_layer)</span><br><span class="line">            if 1 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.a, n, k)</span><br><span class="line">            elif 2 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.b, n, k)</span><br><span class="line">            elif 3 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.c, n, k)</span><br><span class="line">            elif 4 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.d, n, k)</span><br><span class="line"></span><br><span class="line">        return cur_sim</span><br><span class="line"></span><br><span class="line">    def sim_formula(self, coeff, n, k):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        计算相似度的公式，不同的层系数不同</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return coeff * math.cos(n * self.PI / self.degree) * ((n - k + 1) / n)</span><br><span class="line"></span><br><span class="line">    def get_common_layer(self, ca, cb):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应的layer层</span><br><span class="line">        :param ca:     [list(str)] 分解后的编码。</span><br><span class="line">        :param cb:     [list(str)] 分解后的编码。</span><br><span class="line">        :return:   [list(str)]列表代表相应的根编码。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        common_layer = []</span><br><span class="line"></span><br><span class="line">        for i, j in zip(ca, cb):</span><br><span class="line">            if i == j:</span><br><span class="line">                common_layer.append(i)</span><br><span class="line">            else:</span><br><span class="line">                break</span><br><span class="line">        return common_layer</span><br><span class="line"></span><br><span class="line">    def get_k(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        返回两个编码对应分支的距离，相邻距离为1</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if c1[0] != c2[0]:</span><br><span class="line">            return abs(ord(c1[0]) - ord(c2[0]))</span><br><span class="line">        elif c1[1] != c2[1]:</span><br><span class="line">            return abs(ord(c1[1]) - ord(c2[1]))</span><br><span class="line">        elif c1[2] != c2[2]:</span><br><span class="line">            return abs(int(c1[2]) - int(c2[2]))</span><br><span class="line">        elif c1[3] != c2[3]:</span><br><span class="line">            return abs(ord(c1[3]) - ord(c2[3]))</span><br><span class="line">        else:</span><br><span class="line">            return abs(int(c1[4]) - int(c2[4]))</span><br><span class="line"></span><br><span class="line">    def get_n(self, common_layer):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应结点下有多少个同级子结点。</span><br><span class="line">        :param common_layer:    [listr(str)]相同的结点。</span><br><span class="line">        :return:    int</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        end_node = self._code_tree</span><br><span class="line">        for t_node_name in common_layer:</span><br><span class="line">            end_node = end_node[t_node_name]</span><br><span class="line"></span><br><span class="line">        if not isinstance(end_node, dict):</span><br><span class="line">            return end_node</span><br><span class="line">        return len(end_node.keys())</span><br></pre></td></tr></table></figure>
<h4 id="1-1-2-使用"><a href="#1-1-2-使用" class="headerlink" title="1.1.2 使用"></a>1.1.2 使用</h4><p>环境准备：pip install WordSimilarity</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from word_similarity import WordSimilarity2010</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">ws_tool = WordSimilarity2010()</span><br><span class="line">start = time.time()</span><br><span class="line">b_a = &quot;联系方式&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">b_a = &quot;手机&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 0</span><br><span class="line">运行时间：5.793571472167969e-05</span><br><span class="line">手机 电话 相似度为 0.30484094213212237</span><br><span class="line">运行时间：0.0001442432403564453</span><br></pre></td></tr></table></figure>
<h3 id="1-2-基于知网与词林的词语语义相似度计算"><a href="#1-2-基于知网与词林的词语语义相似度计算" class="headerlink" title="1.2 基于知网与词林的词语语义相似度计算"></a>1.2 基于知网与词林的词语语义相似度计算</h3><h4 id="1-2-1-原理"><a href="#1-2-1-原理" class="headerlink" title="1.2.1 原理"></a>1.2.1 原理</h4><p>综合了词林cilin与知网hownet的相似度计算方法，采用混合策略，混合策略具体可以参考源码，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">from hownet.howNet import How_Similarity</span><br><span class="line">from cilin.V3.ciLin import CilinSimilarity</span><br><span class="line"></span><br><span class="line">class HybridSim():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    混合相似度计算策略。使用了词林与知网词汇量的并集。扩大了词汇覆盖范围。</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    ci_lin = CilinSimilarity()  # 实例化词林相似度计算对象</span><br><span class="line">    how_net = How_Similarity()  # 实例化知网相似度计算对象</span><br><span class="line">    Common = ci_lin.vocab &amp; how_net.vocab</span><br><span class="line">    A = how_net.vocab - ci_lin.vocab</span><br><span class="line">    B = ci_lin.vocab - how_net.vocab</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def get_Final_sim(cls, w1, w2):</span><br><span class="line">        lin = cls.ci_lin.sim2018(w1, w2) if w1 in cls.ci_lin.vocab and w2 in cls.ci_lin.vocab else 0</span><br><span class="line">        how = cls.how_net.calc(w1, w2) if w1 in cls.how_net.vocab and w2 in cls.how_net.vocab else 0</span><br><span class="line"></span><br><span class="line">        if w1 in cls.Common and w2 in cls.Common:  # 两个词都被词林和知网共同收录。</span><br><span class="line">            # print(&#x27;两个词都被词林和知网共同收录。&#x27;, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(w1, w2, &#x27;词林改进版相似度：&#x27;, lin, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(&#x27;知网相似度结果为：&#x27;, how, end=&#x27;\t&#x27;)</span><br><span class="line">            return lin * 1 + how * 0  # 可以调节两者的权重，以获取更优结果！！</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.A:  # 两个词都只被知网收录。</span><br><span class="line">            return how</span><br><span class="line">        if w1 in cls.B and w2 in cls.B:  # 两个词都只被词林收录。</span><br><span class="line">            return lin</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.B:  # 一个只被词林收录，另一个只被知网收录。</span><br><span class="line">            print(&#x27;触发策略三，左词为知网，右词为词林&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.B:</span><br><span class="line">            print(&#x27;触发策略三，左词为词林，右词为知网&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（左知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（右知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.B and w2 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（左词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.B and w1 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（右词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        print(&#x27;对不起，词语可能未收录，无法计算相似度！&#x27;)</span><br><span class="line">        return -1</span><br></pre></td></tr></table></figure>
<h4 id="1-2-2-使用"><a href="#1-2-2-使用" class="headerlink" title="1.2.2 使用"></a>1.2.2 使用</h4><p>参考<a target="_blank" rel="noopener" href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from Hybrid_Sim import HybridSim</span><br><span class="line">from Pearson import *</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    print(&#x27;词林词汇量&#x27;, len(HybridSim.ci_lin.vocab ),&#x27;\t知网词汇量&#x27;, len(HybridSim.how_net.vocab))</span><br><span class="line">    print(&#x27;两者总词汇量&#x27;,len(HybridSim.ci_lin.vocab | HybridSim.how_net.vocab),&#x27;\t重叠词汇量&#x27;, len(HybridSim.Common))</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">词林词汇量 77498 	知网词汇量 53336</span><br><span class="line">两者总词汇量 85817 	重叠词汇量 45017</span><br><span class="line">对不起，词语可能未收录，无法计算相似度！</span><br><span class="line">联系方式 电话相似度为： -1</span><br><span class="line">运行时间：3.504753112792969e-05</span><br><span class="line">手机 电话相似度为： 1.0</span><br><span class="line">运行时间：0.019332408905029297</span><br></pre></td></tr></table></figure>
<h2 id="二-基于词向量"><a href="#二-基于词向量" class="headerlink" title="二.基于词向量"></a>二.基于词向量</h2><p>基于样本构建，利于维护</p>
<h3 id="2-1-基于word2vec"><a href="#2-1-基于word2vec" class="headerlink" title="2.1 基于word2vec"></a>2.1 基于word2vec</h3><h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><p>word2vec的原理和词向量获取过程不在此赘述，在本部分主要讲解基于word2vec的词向量如何计算词语相似度。源码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def similarity(self, w1, w2):</span><br><span class="line">    &quot;&quot;&quot;Compute cosine similarity between two keys.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    w1 : str</span><br><span class="line">        Input key.</span><br><span class="line">    w2 : str</span><br><span class="line">        Input key.</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    float</span><br><span class="line">        Cosine similarity between `w1` and `w2`.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-使用"><a href="#2-2-2-使用" class="headerlink" title="2.2.2 使用"></a>2.2.2 使用</h4><p>训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.Word2Vec.load(model_path)</span><br><span class="line">    start = time.time()</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    start = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 -0.014857853</span><br><span class="line">运行时间：-4.76837158203125e-07</span><br><span class="line">手机 电话 相似度为 0.1771852</span><br><span class="line">运行时间：0.0004227161407470703</span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_33741547/article/details/80016713">https://blog.csdn.net/sinat_33741547/article/details/80016713</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-20  <a class="commentCountImg" href="/2021/07/20/bert/#comment-container"><span class="display-none-class">9778d6a091288619b35b401f0aad2c15</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9778d6a091288619b35b401f0aad2c15">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.6 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/20/bert/">bert(Pre-training of Deep Bidirectional Transformers for Language Understanding)</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<h2 id="1-结构"><a href="#1-结构" class="headerlink" title="1 结构"></a>1 结构</h2><p><img src="/2021/07/20/bert/111.JPG" alt></p>
<p>整体结构如上图，基本单元为Transformer 的encoder部分。作者对结构的描述为：BERT’s model architecture is a multi-layer bidirectional Transformer encoder。</p>
<h2 id="2-Input-Output-Representations"><a href="#2-Input-Output-Representations" class="headerlink" title="2 Input/Output Representations"></a>2 Input/Output Representations</h2><p><img src="/2021/07/20/bert/bert_input.JPG" alt></p>
<p>[CLS]表征句子开始，[SEP]表示句子结束以及分割两个句子</p>
<p>Token Embedding为词向量的表示，Position Embedding为位置信息，Segment Embedding表示A，B两句话，最后的输入向量为三者相加。比起transformer多一个Segment Embedding。</p>
<p>具体例子：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p>
<h2 id="3-预训练任务"><a href="#3-预训练任务" class="headerlink" title="3 预训练任务"></a>3 预训练任务</h2><p><img src="/2021/07/20/bert/bert_pre.JPG" alt></p>
<p><strong>1 Masked LM</strong></p>
<p>standard conditional language models can only be trained left-to-right or right-to-left ,   since bidirectional conditioning would allow each word to indirectly “see itself”.In order to train a deep bidirectional representation,MLM</p>
<p>The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time    (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.</p>
<p><strong>2 Next Sentence Prediction (NSP)</strong></p>
<p> In order to train a model that understands sentence relationships</p>
<p>choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).</p>
<h2 id="4-Fine-tuning-BERT"><a href="#4-Fine-tuning-BERT" class="headerlink" title="4 Fine-tuning BERT"></a>4 Fine-tuning BERT</h2><p><img src="/2021/07/20/bert/bert_finetune.JPG" alt></p>
<p>For each task, we simply plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end.</p>
<p>输入: 可以为句子对或者单句，取决于特定任务</p>
<p>输出：At the output, the token representations are fed into an output layer for token level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.</p>
<h2 id="5-常见问题"><a href="#5-常见问题" class="headerlink" title="5 常见问题"></a>5 常见问题</h2><p><strong>1 bert为什么双向，gpt单向？</strong></p>
<p>1.结构的不同</p>
<p>因为BERT用了transformer的encoder，在编码某个token的时候同时利用了其上下文的token，但是gptT用了transformer的decoder，只能利用上文</p>
<p>2.预训练任务的不同</p>
<p><strong>2 为什么bert长度固定？</strong></p>
<p>因为bert是基于transformer encoder的，不同位置的词语都是并行的，所以长度要提前固定，不可变</p>
<p>bert的输入输出长度为max_length,大于截断，小于padding，max_length的最大值为512</p>
<p><strong>3 为什么bert需要补充位置信息？</strong></p>
<p>因为是并行，不像迭代，没有天然的位置信息，需要补充position embedding。</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-20  <a class="commentCountImg" href="/2021/07/20/tokenization/#comment-container"><span class="display-none-class">b0c7a467c3fd5c87f187be3c2b7bc0d9</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="b0c7a467c3fd5c87f187be3c2b7bc0d9">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>13 m  <i class="fas fa-pencil-alt"> </i>2.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/20/tokenization/">Tokenization</a></h1><div class="content"><p>对于中文和英文而言，由于语言差异导致算法也有差异。对于中文，存在字粒度和词粒度。对于英文，存在三个级别的粒度，character level，subword level，word level。下面主要阐述中文的词粒度和英文的subword level。</p>
<h2 id="一、中文"><a href="#一、中文" class="headerlink" title="一、中文"></a>一、中文</h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 原理</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/146792308">https://zhuanlan.zhihu.com/p/146792308</a></p>
<h3 id="1-2-常见中文分词工具"><a href="#1-2-常见中文分词工具" class="headerlink" title="1.2 常见中文分词工具"></a>1.2 常见中文分词工具</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># #####stanfordcorenlp</span><br><span class="line">from timeit import default_timer as timer</span><br><span class="line">from stanfordcorenlp import StanfordCoreNLP</span><br><span class="line">tic = timer()</span><br><span class="line">path=&quot;XXXXX&quot;</span><br><span class="line">nlp_zh = StanfordCoreNLP(path,lang=&#x27;zh&#x27;)#模型文件路径</span><br><span class="line">sentence = &quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;</span><br><span class="line">tic = timer()</span><br><span class="line">print (&#x27;Tokenize:&#x27;, nlp_zh.word_tokenize(sentence))</span><br><span class="line">toc = timer()</span><br><span class="line">print(toc - tic) # 输出的时间，秒为单位</span><br><span class="line">#########thulac</span><br><span class="line">import thulac</span><br><span class="line">thu1 = thulac.thulac(seg_only=True)  #默认模式</span><br><span class="line">tic = timer()</span><br><span class="line">text = thu1.cut(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;, text=True).split(&quot; &quot;)  #进行一句话分词</span><br><span class="line">toc = timer()</span><br><span class="line">print(text)</span><br><span class="line">print(toc - tic)</span><br><span class="line">####jieba</span><br><span class="line">import jieba</span><br><span class="line">tic = timer()</span><br><span class="line">print(jieba.lcut(str(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;)))</span><br><span class="line">toc = timer()</span><br><span class="line">print(toc - tic)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tokenize: [&#x27;搜索&#x27;, &#x27;引擎&#x27;, &#x27;会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串都&#x27;, &#x27;记录&#x27;, &#x27;下来&#x27;]</span><br><span class="line">运行时间：22.68650701455772</span><br><span class="line">[&#x27;搜索引擎会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每&#x27;, &#x27;次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串&#x27;, &#x27;都&#x27;, &#x27;记录&#x27;, &#x27;下&#x27;, &#x27;来&#x27;]</span><br><span class="line">运行时间：0.0016864966601133347</span><br><span class="line">[&#x27;搜索引擎&#x27;, &#x27;会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串&#x27;, &#x27;都&#x27;, &#x27;记录下来&#x27;]</span><br><span class="line">运行时间：0.9094752036035061</span><br></pre></td></tr></table></figure>
<p>观察结果，可以看出thulac分词效率最高，jieba分词的精度和效率比较平衡，stanfordcorenlp分词粒度很细，但是速度慢</p>
<h2 id="二、英文"><a href="#二、英文" class="headerlink" title="二、英文"></a>二、英文</h2><p>SubWord算法如今已成为一个重要的NLP模型的提升算法。其主要优势如下：</p>
<p>1.word level存在OOV问题，一旦碰到就是back off to a dictionary，无法很好地处理未知和罕见词汇<br>2.Character level可以解决OOV，但是相比于 word-level , Character-level 的输入句子变长，使得数据变得稀疏，而且对于远距离的依赖难以学到，训练速度降低。<br>常见的SubWord算法有：BPE，WordPiece，Unigram Language Model等</p>
<h3 id="2-1-BPE"><a href="#2-1-BPE" class="headerlink" title="2.1 BPE"></a>2.1 BPE</h3><p>全称为Byte Pair Encoding，算法来自paper《Neural Machine Translation of Rare Words with Subword Units》。</p>
<h4 id="2-1-1-构建BPE-subword词表"><a href="#2-1-1-构建BPE-subword词表" class="headerlink" title="2.1.1  构建BPE subword词表"></a>2.1.1  构建BPE subword词表</h4><p><strong>原理</strong></p>
<ol>
<li>准备足够大的训练语料</li>
<li>确定期望的subword词表大小</li>
<li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”并且统计单词频率。停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。具体来说，不加”&lt;/w&gt;”可以出现在词首，加了”&lt;/w&gt;”只能位于词尾。例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li>
<li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li>
<li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li>
</ol>
<p>注意，每次合并后词表可能出现3种变化：</p>
<ul>
<li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词都不是完全随着另一个字词的出现而紧跟着出现）</li>
<li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（只有一个字词完全随着另一个字词的出现而紧跟着出现）</li>
<li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li>
</ul>
<p><strong>例子：</strong></p>
<p>训练语料为：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w e s t &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d e s t &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”，输出：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w es t &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d es t &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”，输出：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w est &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d est &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>Iter 3, 以此类推，最高频连续字节对为”est”和”&lt;/w&gt;” ，合并后输出：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w est&lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d est&lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>……</p>
<p>Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。</p>
<p><strong>代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import re, collections</span><br><span class="line">def get_stats(vocab):</span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    for word, freq in vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        for i in range(len(symbols)-1):</span><br><span class="line">            pairs[symbols[i],symbols[i+1]] += freq</span><br><span class="line">    return pairs</span><br><span class="line">def merge_vocab(pair, v_in):</span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(&#x27; &#x27;.join(pair))</span><br><span class="line">    p = re.compile(r&#x27;(?&lt;!\S)&#x27; + bigram + r&#x27;(?!\S)&#x27;)</span><br><span class="line">    for word in v_in:</span><br><span class="line">        w_out = p.sub(&#x27;&#x27;.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    return v_out</span><br><span class="line">vocab = &#123;&#x27;l o w &lt;/w&gt;&#x27; : 5, &#x27;l o w e r &lt;/w&gt;&#x27; : 2,</span><br><span class="line">&#x27;n e w e s t &lt;/w&gt;&#x27;:6, &#x27;w i d e s t &lt;/w&gt;&#x27;:3&#125;</span><br><span class="line">num_merges = 10</span><br><span class="line">for i in range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;e&#x27;, &#x27;s&#x27;)</span><br><span class="line">(&#x27;es&#x27;, &#x27;t&#x27;)</span><br><span class="line">(&#x27;est&#x27;, &#x27;&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;l&#x27;, &#x27;o&#x27;)</span><br><span class="line">(&#x27;lo&#x27;, &#x27;w&#x27;)</span><br><span class="line">(&#x27;n&#x27;, &#x27;e&#x27;)</span><br><span class="line">(&#x27;ne&#x27;, &#x27;w&#x27;)</span><br><span class="line">(&#x27;new&#x27;, &#x27;est&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;low&#x27;, &#x27;&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;w&#x27;, &#x27;i&#x27;)</span><br></pre></td></tr></table></figure>
<h4 id="2-1-2-编解码"><a href="#2-1-2-编解码" class="headerlink" title="2.1.2 编解码"></a>2.1.2 编解码</h4><p><strong>编码</strong></p>
<p>1.将subword词表按照子词长度由大到小排序。</p>
<p>2.对于每个单词，遍历排好序的subword词表，寻找是否有token是当前单词的子字符串。最终，我们将迭代所有token，并将所有子字符串替换为token。 </p>
<p>3.如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子字符串替换为特殊token，如<unk>。</unk></p>
<p>例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure>
<p>编码的计算量很大。对于已知数据，我们可以pre-tokenize所有单词，并在词典中保存单词和tokenize的结果。如果存在字典中不存在的未知单词，可以应用上述编码方法对单词进行tokenize，然后将新单词以及tokenize的结果添加到字典中备用。</p>
<p><strong>解码</strong></p>
<p>将所有的subword拼在一起。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[[“the&lt;/w&gt;”], [“high”, “est&lt;/w&gt;”], [“moun”, “tain&lt;/w&gt;”]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br></pre></td></tr></table></figure>
<h4 id="2-1-3-和embedding结合"><a href="#2-1-3-和embedding结合" class="headerlink" title="2.1.3 和embedding结合"></a>2.1.3 和embedding结合</h4><p>1.构建词表，假设有subword词表：[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</p>
<p>2.编码，词语”highest”编码成[“high”, “est&lt;/w&gt;”]</p>
<p>3.向量表示，$[E_{high},\ E_{est(/w)}]$]</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p>
<h3 id="2-2-WordPiece"><a href="#2-2-WordPiece" class="headerlink" title="2.2 WordPiece"></a>2.2 WordPiece</h3><p>算法来自于《Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》</p>
<p>WordPiece算法可以看作是BPE的变种。不同点在于，WordPiece基于概率生成新的subword而不是下一最高频字节对。</p>
<h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><ol>
<li>准备足够大的训练语料</li>
<li>确定期望的subword词表大小</li>
<li>将单词拆分成字符序列</li>
<li>基于第3步数据训练语言模型</li>
<li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li>
<li>重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值</li>
</ol>
<h3 id="2-3-ULM"><a href="#2-3-ULM" class="headerlink" title="2.3 ULM"></a>2.3 ULM</h3><h3 id="2-4-char-n-gram"><a href="#2-4-char-n-gram" class="headerlink" title="2.4 char n-gram"></a>2.4 char n-gram</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p>
<h3 id="2-5-Byte-Level-BPE"><a href="#2-5-Byte-Level-BPE" class="headerlink" title="2.5 Byte-Level BPE"></a>2.5 Byte-Level BPE</h3><p>《Neural Machine Translation with Byte-Level Subwords》</p>
<p><img src="/2021/07/20/tokenization/1.JPG" alt></p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h2><p>我们在进行中文NLP任务的时候，目前基本都是字粒度；英文的话大多数是使用subword的wordpiece。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112444056">https://zhuanlan.zhihu.com/p/112444056</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38130825">https://zhuanlan.zhihu.com/p/38130825</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86965595">https://zhuanlan.zhihu.com/p/86965595</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhangxiaolinxin/article/details/107052054?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&amp;spm=1001.2101.3001.4242">https://blog.csdn.net/zhangxiaolinxin/article/details/107052054?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&amp;spm=1001.2101.3001.4242</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/mj-selina/p/13687291.html">https://www.cnblogs.com/mj-selina/p/13687291.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/Tokenization/">Tokenization</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/Tokenization/">Tokenization</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-20  <a class="commentCountImg" href="/2021/07/20/torch-cuda/#comment-container"><span class="display-none-class">a49a5bfffe20414104e0e4cabcefcc52</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="a49a5bfffe20414104e0e4cabcefcc52">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/20/torch-cuda/">TensorFlow，pytorch，cuda，cudnn，显卡驱动之间的区别以及对应关系</a></h1><div class="content"><h2 id="一-概念理解"><a href="#一-概念理解" class="headerlink" title="一.概念理解"></a>一.概念理解</h2><p>显卡驱动连接操作系统与底层硬件。</p>
<p>CUDA和NVIDIA的显卡驱动程序完全是两个不同的概念。CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。CUDA的本质是一个工具包（ToolKit）。</p>
<p>cuDNN是一个SDK，是一个专门用于神经网络的加速包，注意，它跟我们的CUDA没有一一对应的关系，即每一个版本的CUDA可能有好几个版本的cuDNN与之对应，但一般有一个最新版本的cuDNN版本与CUDA对应更好。</p>
<p>TensorFlow为谷歌推出的深度学习框架，pytorch是Facebook 推出的深度学习框架。</p>
<h2 id="二-版本对应关系"><a href="#二-版本对应关系" class="headerlink" title="二.版本对应关系"></a>二.版本对应关系</h2><p>深度学习框架基于GPU运算效率远高于CPU，但是需要满足框架的版本和cuda，cudnn以及显卡驱动版本匹配才可以正常工作。</p>
<h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><img src="/2021/07/20/torch-cuda/11.png" alt></p>
<h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><p><img src="/2021/07/20/torch-cuda/torch.JPG" alt></p>
<h3 id="cuDNN与CUDA"><a href="#cuDNN与CUDA" class="headerlink" title="cuDNN与CUDA"></a>cuDNN与CUDA</h3><p><img src="/2021/07/20/torch-cuda/cuda.JPG" alt></p>
<h3 id="CUDA和NVIDIA显卡驱动关系"><a href="#CUDA和NVIDIA显卡驱动关系" class="headerlink" title="CUDA和NVIDIA显卡驱动关系"></a>CUDA和NVIDIA显卡驱动关系</h3><p><img src="/2021/07/20/torch-cuda/22.png" alt></p>
<h2 id="三-常用命令"><a href="#三-常用命令" class="headerlink" title="三.常用命令"></a>三.常用命令</h2><p>查看GPU型号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia</span><br></pre></td></tr></table></figure>
<p>查看NVIDIA驱动版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure>
<p>Python 查看pytorch版本、判断CUDA是否可用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">print(torch.__version__) </span><br><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure>
<p>查看cuda版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/version.txt</span><br><span class="line">conda list | grep cuda</span><br></pre></td></tr></table></figure>
<p>Tensorflow中查看GPU是否可用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure>
<h2 id="四-参考文献"><a href="#四-参考文献" class="headerlink" title="四.参考文献"></a>四.参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/caiguanhong/article/details/112184290">https://blog.csdn.net/caiguanhong/article/details/112184290</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/">深度学习框架</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%A1%86%E6%9E%B6%E4%BE%9D%E8%B5%96/">框架依赖</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-19  <a class="commentCountImg" href="/2021/07/19/word-representation/#comment-container"><span class="display-none-class">18b6512fec030553b9c871f0394c8e23</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="18b6512fec030553b9c871f0394c8e23">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/19/word-representation/">文本表示</a></h1><div class="content"><p>文本表示的表示形式可以是单一数值（基本没人用），可以是向量（目前主流），好奇有没有高纬tensor表示的？下文是基于向量表示的。</p>
<h2 id="1-词语表示"><a href="#1-词语表示" class="headerlink" title="1.词语表示"></a>1.词语表示</h2><h3 id="1-1-one-hot"><a href="#1-1-one-hot" class="headerlink" title="1.1 one hot"></a>1.1 one hot</h3><p>举个例子，有样本如下：</p>
<p>​    Jane wants to go to Shenzhen.</p>
<p>​    Bob wants to go to Shanghai.</p>
<p>基于上述两个文档中出现的单词，构建如下一个词典：</p>
<p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p>
<p>那么wants 可以表示为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0,1,0,0,0,0,0]</span><br></pre></td></tr></table></figure>
<h3 id="1-2-word-embedding"><a href="#1-2-word-embedding" class="headerlink" title="1.2 word embedding"></a>1.2 word embedding</h3><p>词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度的向量空间当中，使得语意相似的词在向量空间上也会比较相近，举个例子，如</p>
<p><img src="/2021/07/19/word-representation/11.jpg" alt></p>
<p>上表为词向量矩阵，其中行表示不同特征，列表示不同词，Man可以表示为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[-1,0.01,0.03,0.09]</span><br></pre></td></tr></table></figure>
<p>性质：$emb_{Man}-emb_{Women}\approx  emb_{King}-emb_{Queen}$</p>
<p>常见的词向量矩阵构建方法有，word2vec，GloVe</p>
<h2 id="2-句子表示"><a href="#2-句子表示" class="headerlink" title="2.句子表示"></a>2.句子表示</h2><h3 id="2-1-词袋模型"><a href="#2-1-词袋模型" class="headerlink" title="2.1 词袋模型"></a>2.1 词袋模型</h3><p>词袋模型不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。</p>
<p>例句:</p>
<p>​    Jane wants to go to Shenzhen.</p>
<p>​    Bob wants to go to Shanghai.</p>
<p>基于上述两个文档中出现的单词，构建如下一个词典：</p>
<p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p>
<p>那么上面两个例句就可以用以下两个向量表示，其值为该词语出现的次数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1,1,2,1,1,0,0]</span><br><span class="line">[0,1,2,1,0,1,1]</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Sentence-Embedding"><a href="#2-2-Sentence-Embedding" class="headerlink" title="2.2 Sentence Embedding"></a>2.2 Sentence Embedding</h3><h4 id="2-2-1-评价工具"><a href="#2-2-1-评价工具" class="headerlink" title="2.2.1 评价工具"></a>2.2.1 评价工具</h4><p>SentEval  is a popular toolkit to evaluate the quality of sentence embeddings.</p>
<h4 id="2-2-2-常见方法"><a href="#2-2-2-常见方法" class="headerlink" title="2.2.2 常见方法"></a>2.2.2 常见方法</h4><p>sentence BERT</p>
<p>BERT-flow</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/444346578">https://zhuanlan.zhihu.com/p/444346578</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/353187575">https://zhuanlan.zhihu.com/p/353187575</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/0587bc01e414">https://www.jianshu.com/p/0587bc01e414</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chenyusheng0803/p/10978883.html">https://www.cnblogs.com/chenyusheng0803/p/10978883.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-19  <a class="commentCountImg" href="/2021/07/19/fasttext/#comment-container"><span class="display-none-class">4aed935a7f1d83aa4dfd2aabd7f6373b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4aed935a7f1d83aa4dfd2aabd7f6373b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>9 m  <i class="fas fa-pencil-alt"> </i>1.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/19/fasttext/">fasttext</a></h1><div class="content"><h2 id="1、文本分类"><a href="#1、文本分类" class="headerlink" title="1、文本分类"></a>1、文本分类</h2><h3 id="1-1-n-gram"><a href="#1-1-n-gram" class="headerlink" title="1.1 n-gram"></a>1.1 n-gram</h3><p>由于Bag of words不考虑词语的顺序，因此引入bag of n-gram。针对英文，词内的是char n-gram，用于词向量；词之间的是word n-gram，用于分类；对于中文，存在词粒度和字粒度。</p>
<p>举个例子，句子A为”今天天气真不错”，这里以词粒度举例，先分词为[“今天”，”天气”，”真“，”不错“]</p>
<p>uni-gram：今天   天气   真   不错</p>
<p>2-gram为：今天/天气    天气/真    真/不错</p>
<p>3-gram为：今天/天气/真      天气/真/不错</p>
<p>由于n-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采用了hashing trick的方式，如下图所示：</p>
<p><img src="/2021/07/19/fasttext/1.png" alt></p>
<p>用哈希的方式既能保证查找时O(1)的效率，又可能把内存消耗控制在O(buckets * dim)范围内。不过这种方法潜在的问题是存在哈希冲突，不同的n-gram可能会共享同一个embedding。如果buckets取的足够大，这种影响会很小。</p>
<p>代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">def build_dataset(config, ues_word):</span><br><span class="line">    if ues_word:</span><br><span class="line">        tokenizer = lambda x: x.split(&#x27; &#x27;)  # word-level</span><br><span class="line">    else:</span><br><span class="line">        tokenizer = lambda x: [y for y in x]  # char-level</span><br><span class="line">    if os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, &#x27;rb&#x27;))</span><br><span class="line">    else:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, &#x27;wb&#x27;))</span><br><span class="line">    print(f&quot;Vocab size: &#123;len(vocab)&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    def biGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        return (t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def triGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        t2 = sequence[t - 2] if t - 2 &gt;= 0 else 0</span><br><span class="line">        return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def load_dataset(path, pad_size=32):</span><br><span class="line">        contents = []</span><br><span class="line">        with open(path, &#x27;r&#x27;, encoding=&#x27;UTF-8&#x27;) as f:</span><br><span class="line">            for line in tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                if not lin:</span><br><span class="line">                    continue</span><br><span class="line">                content, label = lin.split(&#x27;\t&#x27;)</span><br><span class="line">                words_line = []</span><br><span class="line">                token = tokenizer(content)</span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                if pad_size:</span><br><span class="line">                    if len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([PAD] * (pad_size - len(token)))</span><br><span class="line">                    else:</span><br><span class="line">                        token = token[:pad_size]</span><br><span class="line">                        seq_len = pad_size</span><br><span class="line">                # word to id</span><br><span class="line">                for word in token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line"></span><br><span class="line">                # fasttext ngram</span><br><span class="line">                buckets = config.n_gram_vocab</span><br><span class="line">                bigram = []</span><br><span class="line">                trigram = []</span><br><span class="line">                # ------ngram------</span><br><span class="line">                for i in range(pad_size):</span><br><span class="line">                    bigram.append(biGramHash(words_line, i, buckets))</span><br><span class="line">                    trigram.append(triGramHash(words_line, i, buckets))</span><br><span class="line">                # -----------------</span><br><span class="line">                contents.append((words_line, int(label), seq_len, bigram, trigram))</span><br><span class="line">        return contents  # [([...], 0), ([...], 1), ...]</span><br><span class="line"></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size)</span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size)</span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size)</span><br><span class="line">    return vocab, train, dev, test</span><br></pre></td></tr></table></figure>
<h3 id="1-2-网络结构"><a href="#1-2-网络结构" class="headerlink" title="1.2 网络结构"></a>1.2 网络结构</h3><p><img src="/2021/07/19/fasttext/fasttext.JPG" alt="fasttext"></p>
<p>模型结构上word2vec的cbow模型很像</p>
<p>输入层：举个例子，输入文本”今天天气真不错”，词粒度的2-gram为</p>
<script type="math/tex; mode=display">
x_2=\begin{bmatrix}
emb_{今天/天气}，emb_{天气/真}，emb_{ 真/不错} \end{bmatrix},emb为词向量矩阵
\\x_{1},x_{2},...,x_{N}最后输入到中间层的形式为:
mean(\begin{bmatrix}x_1 \\ x_2 \\...\\x_N  \end{bmatrix}),其中mean为对每个x的列求平均</script><p>中间层：线形层+relu作为激活函数</p>
<p>输出层：为简单的线形层</p>
<p>代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        if config.embedding_pretrained is not None:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">        else:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)</span><br><span class="line">        self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed * 3, config.hidden_size)</span><br><span class="line">        # self.dropout2 = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line"></span><br><span class="line">        out_word = self.embedding(x[0])</span><br><span class="line">        out_bigram = self.embedding_ngram2(x[2])</span><br><span class="line">        out_trigram = self.embedding_ngram3(x[3])</span><br><span class="line">        out = torch.cat((out_word, out_bigram, out_trigram), -1)</span><br><span class="line"></span><br><span class="line">        out = out.mean(dim=1)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>
<h3 id="1-3-分层softmax"><a href="#1-3-分层softmax" class="headerlink" title="1.3 分层softmax"></a>1.3 分层softmax</h3><p>对于分类问题，神经网络的输出结果需要经过softmax将其转为概率分布后才可以利用交叉熵计算loss</p>
<p>由于普通softmax的计算效率比较低，计算效率为$O(Kd)$使用分层的softmax时间复杂度可以达到$dlogK$，$K$为分类的数量，$d$为向量的维度</p>
<h4 id="1-3-1-普通softmax"><a href="#1-3-1-普通softmax" class="headerlink" title="1.3.1 普通softmax"></a>1.3.1 普通softmax</h4><p>假设输出为$Y_{pred}=[y_1,y_2,…,y_K]$,则$P_{y_i}$为</p>
<script type="math/tex; mode=display">
P_{y_i}=\frac{e_{y_i}}{\sum_{j=0}^Ke^{y_j}}</script><p>其中$y_i$的维度为$d$，从公式可以看出计算效率为$O(Kd)$</p>
<h4 id="1-3-2-分层softmax"><a href="#1-3-2-分层softmax" class="headerlink" title="1.3.2 分层softmax"></a>1.3.2 分层softmax</h4><p>霍夫曼树可以参考 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/154356949">https://zhuanlan.zhihu.com/p/154356949</a></p>
<p>为什么要霍夫曼，普通的不行？</p>
<p>分层softmax核心思想为利用训练样本构建霍夫曼树，如下</p>
<p><img src="/2021/07/19/fasttext/11.png" alt="fasttext"></p>
<p>树的结构是根据不同类在样本中出现的频次构造的，即频次越大的节点距离根节点越近。$K$个不同的类组成所有的叶子节点，$K-1个$内部节点作为参数。从根节点到某个叶子节点$y_i$经过的节点和边形成一条路径，路径长度表示为 $L_{y_i}$,$n_{(y_i,j)}$表示路径上的节点，那么</p>
<script type="math/tex; mode=display">
P_{y_i}=\prod \limits_{j=1}^{L_{y_i}}P_{(n(y_{i},j),left\ or\ right)}
\\=\prod \limits_{j=0}^{L_{y_i}-1}\sigma(f(n(y_i,j+1)==LC(n(y_i,j))){\theta_{n(y_i,j)}^T} Y)
\\其中LC(n(y_i,j)表示n(y_i,j)的左孩子，\sigma 为SIGMOD函数，f(m)=\begin{equation}\left\{
\begin{aligned}
1 && if \ m==true \\
-1 & & \ else \\
\end{aligned}
\right.
\end{equation}</script><p>从公式可以看出时间复杂度降低至$dlogK$。</p>
<p>以图中$y_2$为例：</p>
<script type="math/tex; mode=display">
P_{y_2}=P_{(n(y_{2},1),left)}\cdot P_{(n(y_{2},2),left)}\cdot P_{(n(y_{2},3),right)}
\\=\sigma({\theta_{n(y_2,1)}^T} Y)\cdot \sigma({\theta_{n(y_2,2)}^T} Y)
\cdot \sigma({-\theta_{n(y_2,3)}^T} Y)</script><p>从根节点走到叶子节点 $y_2$ ，实际上是在做了3次逻辑回归。</p>
<h2 id="2-训练词向量"><a href="#2-训练词向量" class="headerlink" title="2.训练词向量"></a>2.训练词向量</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27009517/article/details/80676022">https://blog.csdn.net/qq_27009517/article/details/80676022</a></p>
<p><a target="_blank" rel="noopener" href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">http://alex.smola.org/papers/2009/Weinbergeretal09.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p>
<p>fasttext工具 <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-18  <a class="commentCountImg" href="/2021/07/18/transformer/#comment-container"><span class="display-none-class">4b6ab54568a681a24f6047500db0eb87</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4b6ab54568a681a24f6047500db0eb87">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.8 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/18/transformer/">transformer(attention is all your need)</a></h1><div class="content"><p>1.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. </p>
<p>2.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.</p>
<p><img src="/2021/07/18/transformer/transformer.JPG" alt></p>
<h2 id="1-Positional-Encoding"><a href="#1-Positional-Encoding" class="headerlink" title="1 Positional Encoding"></a>1 Positional Encoding</h2><p>in order for the model to <strong>make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>详细可参考 <a target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1453/">https://wmathor.com/index.php/archives/1453/</a></p>
<h2 id="2-Attention"><a href="#2-Attention" class="headerlink" title="2 Attention"></a>2 Attention</h2><p><img src="/2021/07/18/transformer/11.jpg" alt></p>
<p>其中不同颜色表示不同head，颜色深浅表示词的关联程度。</p>
<p>不同head表示不同应用场景 ，单一head表示某个场景下，各个字之间的关联程度</p>
<h3 id="1-Scaled-Dot-Product-Attention"><a href="#1-Scaled-Dot-Product-Attention" class="headerlink" title="1 Scaled Dot-Product Attention"></a>1 Scaled Dot-Product Attention</h3><p><img src="/2021/07/18/transformer/self-attention.JPG" alt></p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_{k}}})V</script><p>$d_{k}$  ： keys of dimension</p>
<p>为什么scale？We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</p>
<p><strong>Mask</strong></p>
<p>可以分为两类：Attention Mask和Padding Mask，接下来具体讲解。</p>
<p>1.Attention Mask</p>
<p>ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p>
<p>sotfmax前要mask，上三角mask掉</p>
<p><img src="/2021/07/18/transformer/11.jpeg" alt></p>
<p>2.Padding Mask</p>
<p>Padding位置上的信息是无效的，所以需要丢弃。</p>
<p>过程如下图示：</p>
<p><img src="/2021/07/18/transformer/44.jpg" alt></p>
<p><img src="/2021/07/18/transformer/66.jpg" alt></p>
<h3 id="2-Multi-Head-Attention"><a href="#2-Multi-Head-Attention" class="headerlink" title="2 Multi-Head Attention"></a>2 Multi-Head Attention</h3><p><img src="/2021/07/18/transformer/self-attention2.JPG" alt></p>
<p><strong>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</strong></p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head1,...,head_n)W^O
\\ where  \ head_{i}=Attetion(QW_{i}^{Q},kW_{i}^{K},VW_{i}^{V})</script><h3 id="3-Applications-of-Attention-in-our-Model"><a href="#3-Applications-of-Attention-in-our-Model" class="headerlink" title="3 Applications of Attention in our Model"></a>3 Applications of Attention in our Model</h3><p><strong>1.encoder-decoder attention layers</strong></p>
<p>结构：queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.</p>
<p>目的：This allows every position in the decoder to attend over all positions in the input sequence.</p>
<p><strong>2.encoder contains self-attention layers</strong></p>
<p>结构：keys, values and queries come from the same place</p>
<p>目的：Each position in the encoder can attend to all positions in the previous layer of the encoder.</p>
<p><strong>3.self-attention layers in the decoder</strong></p>
<p>结构：keys, values and queries come from the same place</p>
<p>目的：allow each position in the decoder to attend to all positions in the decoder up to and including that position</p>
<h2 id="3-Encoder-and-Decoder-Stacks"><a href="#3-Encoder-and-Decoder-Stacks" class="headerlink" title="3 Encoder and Decoder Stacks"></a>3 Encoder and Decoder Stacks</h2><h3 id="1-encoder"><a href="#1-encoder" class="headerlink" title="1 encoder"></a>1 encoder</h3><p>1).Input Embedding与Positional Encoding</p>
<script type="math/tex; mode=display">
X = \text{Input Embedding}+ \text{Positional Encoding}\\</script><p>2). multi-head attention</p>
<script type="math/tex; mode=display">
Q = \text{Linear}_q(X) = XW_{Q}\\
K = \text{Linear}_k(X) = XW_{K}\\
V = \text{Linear}_v(X) = XW_{V}\\
X_{attention} = \text{Attention}(Q,K,V)</script><p>3). 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
X_{attention} = X + X_{attention}\\
X_{attention} = \text{LayerNorm}(X_{attention})</script><p>4). FeedForward</p>
<script type="math/tex; mode=display">
X_{hidden} = \text{Linear}(\text{ReLU}(\text{Linear}(X_{attention})))</script><p>5). 残差连接与 Layer Normalization</p>
<script type="math/tex; mode=display">
X_{hidden} = X_{attention} + X_{hidden}\\
X_{hidden} = \text{LayerNorm}(X_{hidden})</script><p>其中$ X_{hidden} \in \mathbb{R}^{batch_size  \ <em> \  seq_len \  </em> \  embed_dim} $</p>
<h3 id="2-decoder"><a href="#2-decoder" class="headerlink" title="2 decoder"></a>2 decoder</h3><p>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：</p>
<ul>
<li>Masked Multi-Head Self-Attention</li>
<li>Multi-Head Encoder-Decoder Attention</li>
<li>FeedForward Network</li>
</ul>
<h2 id="4-常见问题"><a href="#4-常见问题" class="headerlink" title="4 常见问题"></a>4 常见问题</h2><p><strong>1 并行化</strong></p>
<p>训练encoder，decoder都并行，测试encoder并行，decoder不是并行</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/368592551">https://zhuanlan.zhihu.com/p/368592551</a></p>
<p><strong>2 self-attention和普通attention的区别</strong></p>
<p>取决于query和key是否在一个地方</p>
<p><strong>3 Why Self-Attention</strong></p>
<p>Motivating our use of self-attention we consider three desiderata.</p>
<p>1.One is the total <strong>computational complexity</strong> per layer.</p>
<p>2.Another is the amount of computation that can be <strong>parallelized</strong>, as measured by the minimum number of sequential operations required.</p>
<p>3.The third is the path length between <strong>long-range dependencies</strong> in the network</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<p>大佬详解： <a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/transformer/">transformer</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-18  <a class="commentCountImg" href="/2021/07/18/hexo-intro/#comment-container"><span class="display-none-class">9ed370344d73dc6521a92d1ee10ad72b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9ed370344d73dc6521a92d1ee10ad72b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/18/hexo-intro/">hexo_intro</a></h1><div class="content"><h2 id="1-部署"><a href="#1-部署" class="headerlink" title="1.部署"></a>1.部署</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure>
<h2 id="2-创建文章"><a href="#2-创建文章" class="headerlink" title="2.创建文章"></a>2.创建文章</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;XXX&quot;</span><br></pre></td></tr></table></figure>
<h2 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3.常见问题"></a>3.常见问题</h2><p><strong>Error: pandoc exited with code 7: pandoc: Unknown extension: smart</strong></p>
<p>解决：卸载pandoc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-pandoc —save</span><br></pre></td></tr></table></figure>
<p><strong>error：spawn failed</strong></p>
<p>1.删除<code>.deploy_git</code>文件夹</p>
<p>2.执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global core.autocrlf false</span><br></pre></td></tr></table></figure>
<p><strong>hexo 图片显示问题</strong></p>
<p>1、在_config.yml设置post_asset_folder为true</p>
<p>hexo new “paper_name”时会创建paper_name.md和paper_name的文件夹，将图片放在paper_name的文件夹</p>
<p>2、安装插件asset-image<br>npm install <a target="_blank" rel="noopener" href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a><br>3、设置图片为相对路径</p>
<p><img src="/2021/07/18/hexo-intro/11.png" alt></p>
<p>注意修改图片路径中的 \ 为 / ,并且不带 . 或者 . /</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/hexo/">hexo</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/hexo/">hexo</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-18  <a class="commentCountImg" href="/2021/07/18/classify-performance/#comment-container"><span class="display-none-class">ce9206e22a233f02b45bae16d7714ee3</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="ce9206e22a233f02b45bae16d7714ee3">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>11 m  <i class="fas fa-pencil-alt"> </i>1.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/18/classify-performance/">分类任务的衡量指标</a></h1><div class="content"><h2 id="一、二分类"><a href="#一、二分类" class="headerlink" title="一、二分类"></a>一、二分类</h2><h3 id="1-1-confusion-matrix"><a href="#1-1-confusion-matrix" class="headerlink" title="1.1 confusion matrix"></a>1.1 confusion matrix</h3><p><img src="/2021/07/18/classify-performance/22.png" alt></p>
<p><img src="/2021/07/18/classify-performance/11.png" alt></p>
<h3 id="1-2-accuracy"><a href="#1-2-accuracy" class="headerlink" title="1.2 accuracy"></a>1.2 accuracy</h3><script type="math/tex; mode=display">
accuracy={\frac{TP+TN}{TP+TN+FP+FN}}</script><p>accuracy 衡量全局分类正确的数量占总样本的比例</p>
<h3 id="1-3-precision"><a href="#1-3-precision" class="headerlink" title="1.3 precision"></a>1.3 precision</h3><script type="math/tex; mode=display">
precision={\frac{TP}{TP+FP}}</script><p>precision为预测正确正样本数占预测的全部正样本数的比例，即系统判定为正样本的正确率。通俗地说，假如医生给病人检查，医生判断病人有疾病，然后医生判断的正确率有多少。</p>
<h3 id="1-4-recall"><a href="#1-4-recall" class="headerlink" title="1.4 recall"></a>1.4 recall</h3><script type="math/tex; mode=display">
recall={\frac{TP}{TP+FN}}</script><p>recall为预测正确的正样本数量占真实正样本数量的比例，即衡量正样本的召回比例。通俗说，假如有一批病人，医生能从中找出病人的比例</p>
<h3 id="1-5-F1"><a href="#1-5-F1" class="headerlink" title="1.5 F1"></a>1.5 F1</h3><p>由于precision和recall往往是矛盾的，因此为了综合考虑二者，引入F1，即为precision和recall的调和平均</p>
<script type="math/tex; mode=display">
F_{1}={2\frac{precision\cdot recall}{precision+recall}}</script><p>当$precision$和$recall$的任一个值为0，$F_1$都为0</p>
<p>之所以采用调和平均，是因为调和平均数受极端值影响较大，更适合评价不平衡数据的分类问题</p>
<p>通用的F值表达式：</p>
<script type="math/tex; mode=display">
F_{\beta}={(1+\beta^2)\frac{precision\cdot recall}{\beta^2\cdot precision+recall}}</script><p>除了$F_1$分数之外，$F_2$ 分数和$F_{0.5}$分数在统计学中也得到大量的应用。其中，$F_2$分数中，召回率的权重高于精确率，而$F_{0.5}$分数中，精确率的权重高于召回率。</p>
<h3 id="1-6-ROC"><a href="#1-6-ROC" class="headerlink" title="1.6 ROC"></a>1.6 ROC</h3><p><img src="/2021/07/18/classify-performance/11.jfif" alt></p>
<p><img src="/2021/07/18/classify-performance/22.jpg" alt></p>
<p>roc曲线：接收者操作特征(receiver operating characteristic), roc曲线上每个点反映某个阈值下的FPR和TPR的组合。</p>
<p>横轴：$FPR$，叫做假正类率，表示预测为正例但真实情况为反例的占所有真实情况中反例的比率，公式为$FPR=\frac{FP}{TN+FP}$。</p>
<p>纵轴：$TPR$ ，叫做真正例率，表示预测为正例且真实情况为正例的占所有真实情况中正例的比率，公式为​</p>
<p>$TPR=\frac{TP}{TP+FN}$。</p>
<h3 id="1-7-AUC"><a href="#1-7-AUC" class="headerlink" title="1.7 AUC"></a>1.7 AUC</h3><p>$AUC$(Area under Curve)：ROC曲线下的面积，数值可以直观评价分类器的好坏，值越大越好，对于二分类，结果介于0.5和1之间，1为完美分类器，0.5是因为二分类分类效果最差也是0.5。</p>
<h2 id="二、多分类"><a href="#二、多分类" class="headerlink" title="二、多分类"></a>二、多分类</h2><h3 id="2-1-混淆矩阵"><a href="#2-1-混淆矩阵" class="headerlink" title="2.1 混淆矩阵"></a>2.1 混淆矩阵</h3><p><img src="/2021/07/18/classify-performance/11.jpg" alt></p>
<h3 id="2-2-accuracy"><a href="#2-2-accuracy" class="headerlink" title="2.2 accuracy"></a>2.2 accuracy</h3><script type="math/tex; mode=display">
accuracy=\frac{分类正确的样本数,即对角线上的数}{总样本数，即矩阵全部元素相加}</script><h3 id="2-3-某个类别的precision，recall，F1"><a href="#2-3-某个类别的precision，recall，F1" class="headerlink" title="2.3 某个类别的precision，recall，F1"></a>2.3 某个类别的precision，recall，F1</h3><p>与二分类公式一样</p>
<p><img src="/2021/07/18/classify-performance/case1.JPG" alt></p>
<script type="math/tex; mode=display">
precision_{pig}=\frac{20}{20+(10+40)}=\frac{2}{7}
\\recall_{pig}=\frac{20}{20+10}=\frac{2}{3}
\\F_{1pig}={2\frac{precision_{pig}\cdot recall_{pig}}{precision_{pig}+recall_{pig}}}</script><h3 id="2-4-系统的precision，recall，F1"><a href="#2-4-系统的precision，recall，F1" class="headerlink" title="2.4 系统的precision，recall，F1"></a>2.4 系统的precision，recall，F1</h3><p>系统的precision，recall，$F_1$需要综合考虑所有类别，即同时考虑猫、狗、猪的precision，recall，$F_1$。有如下几种方案：</p>
<h4 id="2-4-1-Macro-average"><a href="#2-4-1-Macro-average" class="headerlink" title="2.4.1 Macro average"></a>2.4.1 Macro average</h4><script type="math/tex; mode=display">
Macro-precision=\frac{precision_{cat}+precision_{dog}+precision_{pig}}{3}
\\Macro-recall=\frac{recall{cat}+recall{dog}+recall{pig}}{3}
\\Macro-F_{1}=\frac{F_{1cat}+F_{1dog}+F_{1pig}}{3}</script><h4 id="2-4-2-Weighted-average"><a href="#2-4-2-Weighted-average" class="headerlink" title="2.4.2 Weighted average"></a>2.4.2 Weighted average</h4><p>对macro的推广</p>
<script type="math/tex; mode=display">
Weighted-precision=W_{cat}\cdot precision_{cat}+W_{dog}\cdot precision_{dog}+W_{pig}\cdot precision_{pig}
\\Weighted-recall=W_{cat}\cdot recall{cat}+W_{dog}\cdot recall{dog}+W_{pig}\cdot recall{pig}
\\Weighted-F_{1}=W_{cat}\cdot F_{1cat}+W_{dog}\cdot F_{1dog}+W_{pig}\cdot F_{1pig}
\\W_{cat}:W_{dog}:W_{pig}=N_{cat}:N_{dog}:N_{pig},其中N为样本数量，W为权重</script><h4 id="2-4-3-Micro-average"><a href="#2-4-3-Micro-average" class="headerlink" title="2.4.3 Micro average"></a>2.4.3 Micro average</h4><script type="math/tex; mode=display">
Micro-precision={\frac{TP_{总}}{TP_{总}+FP_{总}}}={\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FP_{i}}}
\\Micro-recall={\frac{TP_{总}}{TP_{总}+FN_{总}}}={\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FN_{i}}}
\\Micro-F_{1}={2\frac{Micro-precision\cdot Micro-recall}{Micro-precision+Micro-recall}}</script><h3 id="2-5-ROC"><a href="#2-5-ROC" class="headerlink" title="2.5 ROC"></a>2.5 ROC</h3><p><img src="/2021/07/18/classify-performance/33.jpg" alt></p>
<p>对于多分类分类器整体效果的ROC如上micro或者macro曲线，其余3条描述单个类别的分类效果。对于多分类，ROC上的点，同样是某个阈值下的FPR和TPR的组合。</p>
<p>对于多分类的$FPR$,$TPR$，有几种计算方式</p>
<p>a. micro average</p>
<script type="math/tex; mode=display">
FPR_{micro } =\frac{FP_总}{TN_总+FP_总}=\frac{\sum_{i=1}^{n}FP_{i}}{\sum_{i=1}^{n}TN_{i}+\sum_{i=1}^{n}FP_{i}}\\
TPR_{micro }=\frac{TP_总}{TP_总+FN_总}=\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FN_{i}}
\\n表示类别数量，FP_i，TN_i，TP_i，FN_i为某个类别的FP，TN，TP，FN</script><p>b. macro average</p>
<script type="math/tex; mode=display">
FPR_{macro}=\frac{1}{n}\sum_{i=1}^{n}FPR_{i}\\
TPR_{macro}=\frac{1}{n}\sum_{i=1}^{n}TPR_{i}，其中FPR_i，TPR_i为某个类别的FPR和TPR</script><h3 id="2-6-AUC"><a href="#2-6-AUC" class="headerlink" title="2.6 AUC"></a>2.6 AUC</h3><p>$AUC$依旧为ROC曲线下的面积，对于多分类个人认为取值范围为[0,1]。</p>
<h2 id="三-代码"><a href="#三-代码" class="headerlink" title="三.代码"></a>三.代码</h2><p><strong>accuracy，precision，recall，F1</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import precision_recall_fscore_support, accuracy_score</span><br><span class="line">def eval_acc_f1(y_true, y_pred):</span><br><span class="line">    acc = accuracy_score(y_true, y_pred)</span><br><span class="line">    prf = precision_recall_fscore_support(y_true, y_pred, average=&quot;macro&quot;)</span><br><span class="line">    return acc, prf</span><br></pre></td></tr></table></figure>
<p><strong>ROC和AUC</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"># 引入必要的库</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from itertools import cycle</span><br><span class="line">from sklearn import svm, datasets</span><br><span class="line">from sklearn.metrics import roc_curve, auc</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import label_binarize</span><br><span class="line">from sklearn.multiclass import OneVsRestClassifier</span><br><span class="line">from scipy import interp</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"># 将标签二值化</span><br><span class="line">y = label_binarize(y, classes=[0, 1, 2])</span><br><span class="line"># 设置种类</span><br><span class="line">n_classes = y.shape[1]</span><br><span class="line"></span><br><span class="line"># 训练模型并预测</span><br><span class="line">random_state = np.random.RandomState(0)</span><br><span class="line">n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line"># shuffle and split training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)</span><br><span class="line"></span><br><span class="line"># Learn to predict each class against the other</span><br><span class="line">classifier = OneVsRestClassifier(svm.SVC(kernel=&#x27;linear&#x27;, probability=True,</span><br><span class="line">                                 random_state=random_state))</span><br><span class="line">y_score = classifier.fit(X_train, y_train).decision_function(X_test)</span><br><span class="line"></span><br><span class="line"># 计算每一类的ROC</span><br><span class="line">fpr = dict()</span><br><span class="line">tpr = dict()</span><br><span class="line">roc_auc = dict()</span><br><span class="line">for i in range(n_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"># Compute micro-average ROC curve and ROC area（方法二）</span><br><span class="line">fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())</span><br><span class="line">roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])</span><br><span class="line"></span><br><span class="line"># Compute macro-average ROC curve and ROC area（方法一）</span><br><span class="line"># First aggregate all false positive rates</span><br><span class="line">all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))</span><br><span class="line"># Then interpolate all ROC curves at this points</span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line">for i in range(n_classes):</span><br><span class="line">    mean_tpr += interp(all_fpr, fpr[i], tpr[i])</span><br><span class="line"># Finally average it and compute AUC</span><br><span class="line">mean_tpr /= n_classes</span><br><span class="line">fpr[&quot;macro&quot;] = all_fpr</span><br><span class="line">tpr[&quot;macro&quot;] = mean_tpr</span><br><span class="line">roc_auc[&quot;macro&quot;] = auc(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;])</span><br><span class="line"></span><br><span class="line"># Plot all ROC curves</span><br><span class="line">lw=2</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;],</span><br><span class="line">         label=&#x27;micro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span><br><span class="line">               &#x27;&#x27;.format(roc_auc[&quot;micro&quot;]),</span><br><span class="line">         color=&#x27;deeppink&#x27;, linestyle=&#x27;:&#x27;, linewidth=4)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;],</span><br><span class="line">         label=&#x27;macro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span><br><span class="line">               &#x27;&#x27;.format(roc_auc[&quot;macro&quot;]),</span><br><span class="line">         color=&#x27;navy&#x27;, linestyle=&#x27;:&#x27;, linewidth=4)</span><br><span class="line"></span><br><span class="line">colors = cycle([&#x27;aqua&#x27;, &#x27;darkorange&#x27;, &#x27;cornflowerblue&#x27;])</span><br><span class="line">for i, color in zip(range(n_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=lw,</span><br><span class="line">             label=&#x27;ROC curve of class &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span><br><span class="line">             &#x27;&#x27;.format(i, roc_auc[i]))</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [0, 1], &#x27;k--&#x27;, lw=lw)</span><br><span class="line">plt.xlim([0.0, 1.0])</span><br><span class="line">plt.ylim([0.0, 1.05])</span><br><span class="line">plt.xlabel(&#x27;False Positive Rate&#x27;)</span><br><span class="line">plt.ylabel(&#x27;True Positive Rate&#x27;)</span><br><span class="line">plt.title(&#x27;Some extension of Receiver operating characteristic to multi-class&#x27;)</span><br><span class="line">plt.legend(loc=&quot;lower right&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839">https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/147663370">https://zhuanlan.zhihu.com/p/147663370</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81202617">https://zhuanlan.zhihu.com/p/81202617</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266386193">https://zhuanlan.zhihu.com/p/266386193</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/">评价指标</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E7%9A%84%E8%A1%A1%E9%87%8F%E6%8C%87%E6%A0%87/">分类任务的衡量指标</a></div><hr></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/46/">Previous</a></div><div class="pagination-next is-invisible is-hidden-mobile"><a href="/page/48/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/46/">46</a></li><li><a class="pagination-link is-current" href="/page/47/">47</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">469</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">447</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-29T14:55:38.000Z">2024-07-29</time></p><p class="title"><a href="/2024/07/29/skiplist/">skiplist跳表</a></p><p class="categories"><a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a> / <a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-11T13:10:43.000Z">2024-06-11</time></p><p class="title"><a href="/2024/06/11/web-search/">网页搜索</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T11:39:05.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/raid/">raid</a></p><p class="categories"><a href="/categories/c/">c++</a> / <a href="/categories/c/%E4%BC%98%E5%8C%96/">优化</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:23:12.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/shell/">shell</a></p><p class="categories"><a href="/categories/shell/">shell</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:09:27.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/vim/">vim</a></p><p class="categories"><a href="/categories/linux/">linux</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">77</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E9%A1%B5%E6%90%9C%E7%B4%A2/"><span class="tag">网页搜索</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>