<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-03  <a class="commentCountImg" href="/2021/11/03/naive-bayse/#comment-container"><span class="display-none-class">b7455dcd04c216db363b6385a9ef8c8a</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="b7455dcd04c216db363b6385a9ef8c8a">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/03/naive-bayse/">分类算法之朴素贝叶斯</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6069267.html">https://www.cnblogs.com/pinard/p/6069267.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/">模型结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">朴素贝叶斯</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-01  <a class="commentCountImg" href="/2021/11/01/decay-time-series/#comment-container"><span class="display-none-class">5a81de4f2e6420fd2067d4d92289baab</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="5a81de4f2e6420fd2067d4d92289baab">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/01/decay-time-series/">时间序列预测滞后现象</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://www.dazhuanlan.com/bb_principessa/topics/1021683">https://www.dazhuanlan.com/bb_principessa/topics/1021683</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">时间序列预测</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%BB%9E%E5%90%8E/">滞后</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-11-01  <a class="commentCountImg" href="/2021/11/01/convtrans/#comment-container"><span class="display-none-class">296975bbe73a6fa756b39cb86fe8768f</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="296975bbe73a6fa756b39cb86fe8768f">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/11/01/convtrans/">Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</a></h1><div class="content"><p>原文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.00235">https://arxiv.org/abs/1907.00235</a></p>
<p>作者利用transformer做时间序列预测，发现了两个问题，然后提出了改进。一个问题是locality-agnostics，the point-wise dot product self-attention in canonical Transformer architecture is insensitive to local context。另外一个问题是memory bottleneck ：space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible.为了解决这两个问题，作者提出了convolutional self-attention和LogSparse Transformer。</p>
<h2 id="3-背景"><a href="#3-背景" class="headerlink" title="3.背景"></a>3.背景</h2><p><strong>问题定义</strong></p>
<p><img src="/2021/11/01/convtrans/4.GIF" alt></p>
<p>其中$\Phi$是参数，$\textbf{X}$是辅助输入，就是除了观测值以外的输入，$\textbf{Z}_{i,t}$表示序列$i$在时刻$t$的值</p>
<p>为了简化式子，定义了</p>
<p><img src="/2021/11/01/convtrans/5.GIF" alt></p>
<p>目标就变成了$\textbf{z}_t \sim f(\textbf{Y}_t)$</p>
<p><strong>Transformer</strong></p>
<p><img src="/2021/11/01/convtrans/6.GIF" alt></p>
<p>$h$表示某个头，$M$表示mask matrix</p>
<h2 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4.方法论"></a>4.方法论</h2><h3 id="4-1-Enhancing-the-locality-of-Transformer"><a href="#4-1-Enhancing-the-locality-of-Transformer" class="headerlink" title="4.1 Enhancing the locality of Transformer"></a>4.1 Enhancing the locality of Transformer</h3><p><img src="/2021/11/01/convtrans/1.GIF" alt></p>
<p>改进思想如上图所示，原版的transformer，利用了point-wise之间的相似度，万一存在异常点，就会造成偏差。改进方向就是将点和点之间的相似度变为local context based，也就是先利用卷积得到local的表示，然后基于local做Q和K的相似度。当卷积核的尺寸为1就退化为原版的transformer。</p>
<h3 id="4-2-Breaking-the-memory-bottleneck-of-Transformer"><a href="#4-2-Breaking-the-memory-bottleneck-of-Transformer" class="headerlink" title="4.2 Breaking the memory bottleneck of Transformer"></a>4.2 Breaking the memory bottleneck of Transformer</h3><p><img src="/2021/11/01/convtrans/2.GIF" alt></p>
<p>原来的transformer需要$O(L^2)$的空间复杂度，每层每个cell为$O(L)$，每层所有cell为$O(L^2)$，然后堆叠$h$层，$h$为常数，所以为$O(L^2)$，如图(a)。作者提出了LogSparse Transformer，如图（b）,空间复杂度为$O(L(logL)^2)$。首先每层每一个cell需要$logL$，每层所有cell就是$LlogL$，然后堆叠$logL$层，最后为$O(L(logL)^2)$。</p>
<p>对于LogSparse Transformer，筛选规则为：</p>
<p><img src="/2021/11/01/convtrans/3.GIF" alt></p>
<p>图（c）和图（d）是对（b）的改进。</p>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h2><p>评级指标:p-quantile loss $R_p$ with $p\in(0,1)$</p>
<p><img src="/2021/11/01/convtrans/7.GIF" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412800154">https://zhuanlan.zhihu.com/p/412800154</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">时间序列预测</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/convtrans/">convtrans</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-29  <a class="commentCountImg" href="/2021/10/29/basic-regression-classfify/#comment-container"><span class="display-none-class">7176fd258f62e74eb1c4afa31b970477</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="7176fd258f62e74eb1c4afa31b970477">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/29/basic-regression-classfify/">分类回归模型总结</a></h1><div class="content"><p>很多的深度模型都属于表示学习，是为了得到好的特征表示，比如文本表示之类的模型，有了好的特征表示，才能增强分类或者回归的效果。某些端到端的模型其实可以拆解成几个部分，比如前置的环节包括了特征提取，特征表示，然后顶层是分类或者回归层。下文总结的是纯粹的分类和回归的模型。</p>
<h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1.分类"></a>1.分类</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/169dc01f0589">https://www.jianshu.com/p/169dc01f0589</a></p>
<h2 id="2-回归"><a href="#2-回归" class="headerlink" title="2.回归"></a>2.回归</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ChenVast/article/details/82107490">https://blog.csdn.net/ChenVast/article/details/82107490</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/">模型结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%88%86%E7%B1%BB%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/">分类回归模型总结</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-29  <a class="commentCountImg" href="/2021/10/29/transformer-time-seties/#comment-container"><span class="display-none-class">53e0a4b4f3015f1a68551fd049b4034f</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="53e0a4b4f3015f1a68551fd049b4034f">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/29/transformer-time-seties/">Transformer时间序列预测</a></h1><div class="content"><p>1.基本的Transformer</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360829130">https://zhuanlan.zhihu.com/p/360829130</a></p>
<p>2.改进的Transformer</p>
<p>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.00235.pdf">https://arxiv.org/pdf/1907.00235.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/391337035">https://zhuanlan.zhihu.com/p/391337035</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">时间序列预测</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/Transformer%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/">Transformer时间序列预测</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/ch-word-bert/#comment-container"><span class="display-none-class">6fc64fa940d006e20b55dd3fa555894d</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6fc64fa940d006e20b55dd3fa555894d">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/ch-word-bert/">中文词粒度BERT</a></h1><div class="content"><p><strong>1 Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</strong></p>
<p>we find that charbased（字粒度） models consistently <strong>outperform</strong> wordbased （词粒度）models.</p>
<p>We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. </p>
<p><strong>2 腾讯中文词模型</strong></p>
<p>词模型在公开数据集的表现逊于字模型</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.05526.pdf">https://arxiv.org/pdf/1905.05526.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-06-27-17">https://www.jiqizhixin.com/articles/2019-06-27-17</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/text-matching/#comment-container"><span class="display-none-class">4f0c1edbb24c7f5c8f422d9a846dcbba</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4f0c1edbb24c7f5c8f422d9a846dcbba">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>10 m  <i class="fas fa-pencil-alt"> </i>1.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/text-matching/">文本匹配</a></h1><div class="content"><h2 id="1-无监督"><a href="#1-无监督" class="headerlink" title="1.无监督"></a>1.无监督</h2><h3 id="1-1-编辑距离"><a href="#1-1-编辑距离" class="headerlink" title="1.1 编辑距离"></a>1.1 编辑距离</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>编辑距离，英文名字为Levenshtein distance，通过描述一个字符串A需要多少次基本操作可以变成字符串B，来衡量两个字符串的相似度。</p>
<p>基本操作包括：增、删、改</p>
<p>增：字符串A为“AS”，字符串B为“ ASD“，字符串A-&gt;字符串B需要增加一个字符“D”</p>
<p>删：字符串A为“ASD”，字符串B为“ AS“，字符串A-&gt;字符串B需要删除一个字符“D”</p>
<p>改：字符串A为“ASX”，字符串B为“ ASD“，字符串A-&gt;字符串B需要将字符“X”变成字符“D”</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h4><p>实现过程使用动态规划，递推公式为</p>
<script type="math/tex; mode=display">
lev_{a,b}(i,j)=
\begin{equation}
f(x)=\left\{
\begin{aligned}
max(i,j) &  & if\ \min(i,j)=0
\\
min\left\{
\begin{aligned}
lev_{a,b}(i-1,j)+1 
\\
lev_{a,b}(i,j-1)+1 
\\
lev_{a,b}(i-1,j-1)+1_{(a_i\neq b_j)}
\end{aligned}
\right.
\end{aligned}
\right.
\end{equation}</script><p>$i$和$j$分别表示字符串$a$和字符串$b$的下标，$lev_{a,b}(i,j)$表示子串$a[:i]$到子串$b[:j]$的编辑距离。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def lev(str_a,str_b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    ED距离，用来衡量单词之间的相似度</span><br><span class="line">    :param str_a:</span><br><span class="line">    :param str_b:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    str_a=str_a.lower()</span><br><span class="line">    str_b=str_b.lower()</span><br><span class="line">    matrix_ed=np.zeros((len(str_a)+1,len(str_b)+1),dtype=np.int)</span><br><span class="line">    matrix_ed[0]=np.arange(len(str_b)+1)</span><br><span class="line">    matrix_ed[:,0] = np.arange(len(str_a) + 1)</span><br><span class="line">    for i in range(1,len(str_a)+1):</span><br><span class="line">        for j in range(1,len(str_b)+1):</span><br><span class="line">            # 表示删除a_i</span><br><span class="line">            dist_1 = matrix_ed[i - 1, j] + 1</span><br><span class="line">            # 表示插入b_i</span><br><span class="line">            dist_2 = matrix_ed[i, j - 1] + 1</span><br><span class="line">            # 表示替换b_i</span><br><span class="line">            dist_3 = matrix_ed[i - 1, j - 1] + (1 if str_a[i - 1] != str_b[j - 1] else 0)</span><br><span class="line">            #取最小距离</span><br><span class="line">            matrix_ed[i,j]=np.min([dist_1, dist_2, dist_3])</span><br><span class="line">    print(matrix_ed)</span><br><span class="line">    return matrix_ed[-1,-1]</span><br></pre></td></tr></table></figure>
<h3 id="1-2-TF-IDF"><a href="#1-2-TF-IDF" class="headerlink" title="1.2 TF-IDF"></a>1.2 TF-IDF</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>（1）TF</p>
<p>针对某个文本</p>
<p>$TF_{word}=\frac{word在文本中出现的次数}{文本中所有词的总数}$</p>
<p>（2）IDF</p>
<p>针对语料库</p>
<p>$IDF_{word}=log(\frac{语料库的文本总数}{包含该word的文本数+1})$</p>
<p>（3）TF-IDF</p>
<p>$TF-IDF_{word}=TF_{word}*IDF_{word}$</p>
<p>（4）TF-IDF VEC</p>
<p>现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”]</p>
<p> $VEC_{A}=[TF-IDF_{今天},TF-IDF_{天气}，TF-IDF_{真好},0,0]$</p>
<p>（5）计算两句话的文本相似度</p>
<p>假设词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”],现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],句子B:”天气不错呀”，分词后[“天气”,”不错呀”]</p>
<p>利用（3）得到句子A的TF-IDF VEC $VEC_{A}$，句子B的TF-IDF VEC $VEC_B$，利用余弦相似度计算文本相似度</p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import jieba</span><br><span class="line">import numpy  as np</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">from scipy.linalg import norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TF_IDF_Model(object):</span><br><span class="line">    def __init__(self, corpus_list):</span><br><span class="line"></span><br><span class="line">        self.documents_list = corpus_list</span><br><span class="line">        self.documents_number = len(corpus_list)</span><br><span class="line">        self.get_idf()</span><br><span class="line"></span><br><span class="line">    def get_idf(self):</span><br><span class="line">        df = &#123;&#125;</span><br><span class="line">        self.idf = &#123;&#125;</span><br><span class="line">        tf = []</span><br><span class="line">        for document in self.documents_list:</span><br><span class="line">            temp = &#123;&#125;</span><br><span class="line">            for word in document:</span><br><span class="line">                temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">            tf.append(temp)</span><br><span class="line">            for key in temp.keys():</span><br><span class="line">                df[key] = df.get(key, 0) + 1</span><br><span class="line">        for key, value in df.items():</span><br><span class="line">            self.idf[key] = np.log10(self.documents_number / (value + 1))</span><br><span class="line"></span><br><span class="line">    def get_tf(self, document):</span><br><span class="line">        document = list(jieba.cut(document))</span><br><span class="line">        # tf = []</span><br><span class="line">        temp = &#123;&#125;</span><br><span class="line">        for word in document:</span><br><span class="line">            temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">        # tf.append(temp)</span><br><span class="line">        return temp</span><br><span class="line"></span><br><span class="line">    def tf_idf_vec(self, text):</span><br><span class="line">        tf = self.get_tf(text)</span><br><span class="line">        word = list(self.idf.keys())</span><br><span class="line">        vec = [0] * len(self.idf)</span><br><span class="line">        text = list(jieba.cut(text))</span><br><span class="line">        for ele in text:</span><br><span class="line">            if ele in word:</span><br><span class="line">                vec[word.index(ele)] = tf[ele] * self.idf[ele]</span><br><span class="line">        return vec</span><br><span class="line"></span><br><span class="line">    def cal_similarty(self, sentence1, sentence2):</span><br><span class="line">        vec1 = self.tf_idf_vec(sentence1)</span><br><span class="line">        vec2 = self.tf_idf_vec(sentence2)</span><br><span class="line">        similarty = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))</span><br><span class="line">        return similarty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_model():</span><br><span class="line">    #####bulid corpus</span><br><span class="line">    corpus = pd.read_csv(corpus_path)</span><br><span class="line">    corpus_list = corpus[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    # corpus_list = corpus1[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    corpus_list = [list(jieba.cut(str(doc))) for doc in corpus_list]</span><br><span class="line">    tf_idf_model = TF_IDF_Model(corpus_list)</span><br><span class="line">    joblib.dump(tf_idf_model, model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_model(path):</span><br><span class="line">    tf_idf_model = joblib.load(path)</span><br><span class="line">    return tf_idf_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    from supercat.data_qualifier.tf_idf import TF_IDF_Model</span><br><span class="line">    ####</span><br><span class="line">    train_model()</span><br><span class="line">    ######</span><br><span class="line">    tf_idf_model = load_model(model_path)</span><br><span class="line">    sentence1=&quot;XXXX&quot;</span><br><span class="line">    sentence2=&quot;XXXX&quot;</span><br><span class="line">    print(tf_idf_model.get_tf(sentence1))</span><br><span class="line">    print(tf_idf_model.idf)</span><br><span class="line">    print(tf_idf_model.tf_idf_vec(sentence1))</span><br><span class="line">    print(tf_idf_model.cal_similarty(sentence1,sentence2))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-有监督"><a href="#2-有监督" class="headerlink" title="2.有监督"></a>2.有监督</h2><p><strong>基于表示的匹配方法</strong>：使用深度学习模型分别表征Query和Doc，通过计算向量相似度来作为语义匹配分数。微软的DSSM[26]及其扩展模型属于基于表示的语义匹配方法，美团搜索借鉴DSSM的双塔结构思想，左边塔输入Query信息，右边塔输入POI、品类信息，生成Query和Doc的高阶文本相关性、高阶品类相关性特征，应用于排序模型中取得了很好的效果。此外，比较有代表性的表示匹配模型还有百度提出 SimNet[27]，中科院提出的多视角循环神经网络匹配模型（MV-LSTM）[28]等。</p>
<p><strong>基于交互的匹配方法</strong>：这种方法不直接学习Query和Doc的语义表示向量，而是在神经网络底层就让Query和Doc提前交互，从而获得更好的文本向量表示，最后通过一个MLP网络获得语义匹配分数。代表性模型有华为提出的基于卷积神经网络的匹配模型ARC-II[29]，中科院提出的基于矩阵匹配的的层次化匹配模型MatchPyramid[30]。</p>
<p>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量，缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</p>
<p>匹配不同于排序，匹配是1对1的，排序是1对多</p>
<h3 id="2-1基于表示"><a href="#2-1基于表示" class="headerlink" title="2.1基于表示"></a>2.1基于表示</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138864580">https://zhuanlan.zhihu.com/p/138864580</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27590277/article/details/121391770">https://blog.csdn.net/qq_27590277/article/details/121391770</a></p>
<h3 id="2-2-基于交互"><a href="#2-2-基于交互" class="headerlink" title="2.2.基于交互"></a>2.2.基于交互</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/guofei_fly/article/details/107501276">https://blog.csdn.net/guofei_fly/article/details/107501276</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/transformer-survey/#comment-container"><span class="display-none-class">9f00e3da6337f991e5da2420e8a4f308</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9f00e3da6337f991e5da2420e8a4f308">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/transformer-survey/">transformer综述</a></h1><div class="content"><p>Transformer-XL</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860v3">https://arxiv.org/abs/1901.02860v3</a></p>
<p>RoFormer</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09864.pdf">https://arxiv.org/pdf/2104.09864.pdf</a></p>
<p>google2020出品的transformer的综述</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.06732.pdf">https://arxiv.org/pdf/2009.06732.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8/">特征提取器</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/transformer%E7%BB%BC%E8%BF%B0/">transformer综述</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/T5/#comment-container"><span class="display-none-class">9b456cc8675c74a5066445aae3f9352b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9b456cc8675c74a5066445aae3f9352b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/T5/">T5</a></h1><div class="content"><p><strong>T5</strong></p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.10683.pdf">https://arxiv.org/pdf/1910.10683.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/88438851">https://zhuanlan.zhihu.com/p/88438851</a></p>
<p><strong>mT5</strong></p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11934.pdf">https://arxiv.org/pdf/2010.11934.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/302380842">https://zhuanlan.zhihu.com/p/302380842</a></p>
<p><strong>Sentence-T5</strong>（文本表示新SOTA）</p>
<p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.08877.pdf">https://arxiv.org/pdf/2108.08877.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/403153114">https://zhuanlan.zhihu.com/p/403153114</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/PTM/">PTM</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/PTM/">PTM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-25  <a class="commentCountImg" href="/2021/10/25/Cold-start/#comment-container"><span class="display-none-class">a7ac42a42c2a413e0be9c11a44e5b1df</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="a7ac42a42c2a413e0be9c11a44e5b1df">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/25/Cold-start/">冷启动</a></h1><div class="content"><p>推荐系统冷启动</p>
<p>mark</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/79950668">https://zhuanlan.zhihu.com/p/79950668</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%86%B7%E5%90%AF%E5%8A%A8/">冷启动</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/29/">Previous</a></div><div class="pagination-next"><a href="/page/31/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/29/">29</a></li><li><a class="pagination-link is-current" href="/page/30/">30</a></li><li><a class="pagination-link" href="/page/31/">31</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/39/">39</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">387</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">370</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-12T14:14:32.000Z">2022-09-12</time></p><p class="title"><a href="/2022/09/12/python-cal-symbol/">运算</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-12T14:05:26.000Z">2022-09-12</time></p><p class="title"><a href="/2022/09/12/python-pattern/">正则</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-12T09:54:56.000Z">2022-09-12</time></p><p class="title"><a href="/2022/09/12/lamdba/">lambda</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-12T09:42:33.000Z">2022-09-12</time></p><p class="title"><a href="/2022/09/12/python-string/">字符串</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-09-12T07:31:39.000Z">2022-09-12</time></p><p class="title"><a href="/2022/09/12/python-iterator/">可迭代对象、迭代器与生成器</a></p><p class="categories"><a href="/categories/python/">python</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">68</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/NLP/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/PTM/"><span class="level-start"><span class="level-item">PTM</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Prompt/"><span class="level-start"><span class="level-item">Prompt</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/Tokenization/"><span class="level-start"><span class="level-item">Tokenization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96/"><span class="level-start"><span class="level-item">信息抽取</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2022/09/"><span class="level-start"><span class="level-item">September 2022</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/08/"><span class="level-start"><span class="level-item">August 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/07/"><span class="level-start"><span class="level-item">July 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">23</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">29</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIN/"><span class="tag">DIN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DMR/"><span class="tag">DMR</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2022 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>