<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-30  <a class="commentCountImg" href="/2021/09/30/FM/#comment-container"><span class="display-none-class">fd304f463f2eec2951b101ea45172581</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="fd304f463f2eec2951b101ea45172581">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.8 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/30/FM/">Factorization Machines</a></h1><div class="content"><p>原文地址 <a target="_blank" rel="noopener" href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf">https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/50426292">https://zhuanlan.zhihu.com/p/50426292</a></p>
<p>a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models</p>
<h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>In total, the advantages of our proposed FM are:<br>1) FMs <strong>allow parameter estimation under very sparse data</strong> where SVMs fail.<br>2) FMs have <strong>linear complexity</strong>, can be optimized in the primal and do not rely on support vectors like SVMs.<br>3) FMs are a <strong>general predictor</strong> that can work with any real valued feature vector. </p>
<h2 id="II-PREDICTION-UNDER-SPARSITY"><a href="#II-PREDICTION-UNDER-SPARSITY" class="headerlink" title="II. PREDICTION UNDER SPARSITY"></a>II. PREDICTION UNDER SPARSITY</h2><p><img src="/2021/09/30/FM/22.JPG" alt></p>
<p><img src="/2021/09/30/FM/11.JPG" alt></p>
<h2 id="III-FACTORIZATION-MACHINES-FM"><a href="#III-FACTORIZATION-MACHINES-FM" class="headerlink" title="III. FACTORIZATION MACHINES (FM)"></a>III. FACTORIZATION MACHINES (FM)</h2><h3 id="A-Factorization-Machine-Model"><a href="#A-Factorization-Machine-Model" class="headerlink" title="A. Factorization Machine Model"></a>A. Factorization Machine Model</h3><p><strong>1) 模型:</strong></p>
<script type="math/tex; mode=display">
\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n \bbox[border: 2px solid red]{w_{i,j}}x_ix_j</script><p>$x_i$表示第$i$个特征，但是针对上式，一个很大的问题，用户交互矩阵往往是比较稀疏的，这样就会导致对$w_{i,j}$的估算存在很大的问题。举个例子，假如想要估计Alice(A)和Star Trek(ST)的交互参数$w_{A,ST}$，由于训练集中没有实例同时满足$x_A$和$x_{ST}$非零，这会造成$w_{A,ST}=0$。因此这里使用了矩阵分解的思想：</p>
<script type="math/tex; mode=display">
\textbf{W} = \textbf{V}\textbf{V}^T,\textbf{V}=\begin{pmatrix} \textbf{v}_1 \\ \textbf{v}_2\\...\\\textbf{v}_n  \end{pmatrix} \in {R}^{n\times k}
\\\bbox[border: 2px solid red]{w_{i,j}=<\textbf{v}_i,\textbf{v}_j>=\sum_{f=1}^k v_{i,f}\cdot v_{j,f}}
\\\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n \bbox[border: 2px solid red]{<\textbf{v}_i,\textbf{v}_j>}x_ix_j</script><p><strong>2) 提升效率:</strong></p>
<p>直接计算上面的公式求解$\hat{y}(x)$的时间复杂度为$O ( k n^2 ) $，因为所有的特征交叉都需要计算。但是可以通过公式变换，将时间复杂度减少到$O(kn)$，如下公式推导</p>
<script type="math/tex; mode=display">
\begin{align*}

\\\sum_{i=1}^n\sum_{j=i+1}^n <\textbf{v}_i,\textbf{v}_j>x_ix_j&=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n <\textbf{v}_i,\textbf{v}_j>x_ix_j-\frac{1}{2}\sum_{i=1}^n\ <\textbf{v}_i,\textbf{v}_i>x_ix_i
\\&=...
\\&=\frac{1}{2}\sum_{f=1}^k((\sum_{i=1}^nv_{i,f}x_i)^2-\sum_{i=1}^nv_{i,f}^2x_i^2)

\end{align*}</script><h3 id="B-Factorization-Machines-as-Predictors"><a href="#B-Factorization-Machines-as-Predictors" class="headerlink" title="B. Factorization Machines as Predictors"></a>B. Factorization Machines as Predictors</h3><p>FM can be applied to a variety of prediction tasks. Among them are: Regression，Binary classification，Ranking</p>
<h3 id="C-Learning-Factorization-Machines"><a href="#C-Learning-Factorization-Machines" class="headerlink" title="C. Learning Factorization Machines"></a>C. Learning Factorization Machines</h3><p>the model parameters of FMs can be learned efficiently by gradient descent methods – e.g. stochastic gradient descent (SGD).The gradient of the FM model is:</p>
<script type="math/tex; mode=display">
\\\begin{equation}
\frac{\partial \hat{y}(x)}{\partial \theta}=\left\{
\begin{array}{rcl}
1& & {if \ \theta \ is \  w_0 }\\
x_i & & {if \ \theta \ is \  w_i}\\
x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 & & {if \ \theta \ is \  v_{i,f}}
\end{array} \right.
\end{equation}</script><h3 id="D-d-way-Factorization-Machine"><a href="#D-d-way-Factorization-Machine" class="headerlink" title="D. d-way Factorization Machine"></a>D. d-way Factorization Machine</h3><p>The 2-way FM described so far can easily be generalized to a d-way FM:</p>
<script type="math/tex; mode=display">
\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{l=2}^d\sum_{i_l=1}^n ...\sum_{i_l=i_{l-1}+1}^n (\prod \limits_{j=1}^lx_{i_j})(\sum_{f=1}^{k_l}\prod \limits_{j=1}^lv_{i_j,f}^{(l)})</script><p>直接计算上式的时间复杂度为$O(k_dn^d)$，利用类似上面的公式变形也可以将其降低为$O(k_d n )$</p>
<h3 id="E-Summary"><a href="#E-Summary" class="headerlink" title="E. Summary"></a>E. Summary</h3><p>FMs model all possible interactions between values in the feature vector $x$ using factorized interactions instead of full parametrized ones. This has two main advantages:</p>
<p>1) The interactions between values can be estimated even under high sparsity. Especially, it is possible to generalize to unobserved interactions.<br>2) The number of parameters as well as the time for prediction and learning is linear. </p>
<h2 id="IV-FMS-VS-SVMS"><a href="#IV-FMS-VS-SVMS" class="headerlink" title="IV. FMS VS. SVMS"></a>IV. FMS VS. SVMS</h2><h2 id="V-FMS-VS-OTHER-FACTORIZATION-MODELS"><a href="#V-FMS-VS-OTHER-FACTORIZATION-MODELS" class="headerlink" title="V. FMS VS. OTHER FACTORIZATION MODELS"></a>V. FMS VS. OTHER FACTORIZATION MODELS</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_26822029/article/details/103993243">https://blog.csdn.net/qq_26822029/article/details/103993243</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/">模型结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/FM/">FM</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-30  <a class="commentCountImg" href="/2021/09/30/sparsity-feature/#comment-container"><span class="display-none-class">0f5a5f655c5ed5274d387449e02b35aa</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="0f5a5f655c5ed5274d387449e02b35aa">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/30/sparsity-feature/">特征稀疏</a></h1><div class="content"><h3 id="What-are-sparse-features"><a href="#What-are-sparse-features" class="headerlink" title="What are sparse features?"></a>What are sparse features?</h3><p>Features with sparse data are features that have mostly zero values. This is different from features with missing data.</p>
<h3 id="Why-is-machine-learning-difficult-with-sparse-features"><a href="#Why-is-machine-learning-difficult-with-sparse-features" class="headerlink" title="Why is machine learning difficult with sparse features?"></a>Why is machine learning difficult with sparse features?</h3><p>Common problems with sparse features include:</p>
<ol>
<li>If the model has many sparse features, it will <strong>increase the space and time complexity</strong> of models. Linear regression models will fit more coefficients, and tree-based models will have greater depth to account for all features.</li>
<li>Model algorithms and diagnostic <strong>measures might behave in unknown ways</strong> if the features have sparse data. <a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1421">Kuss [2002]</a> shows that goodness-of-fit tests are flawed when the data is sparse.</li>
<li>If there are too many features, models fit the noise in the training data. This is called <strong>overfitting</strong>. When models overfit, they are unable to generalize to newer data when they are put in production. This negatively impacts the predictive power of models.</li>
<li>Some models may <strong>underestimate the importance of sparse features and given preference to denser features</strong> even though the sparse features may have predictive power. Tree-based models are notorious for behaving like this. For example, random forests overpredict the importance of features that have more categories than those features that have fewer categories.</li>
</ol>
<h3 id="Methods-for-dealing-with-sparse-features"><a href="#Methods-for-dealing-with-sparse-features" class="headerlink" title="Methods for dealing with sparse features"></a>Methods for dealing with sparse features</h3><ol>
<li><p>Removing features from the model</p>
</li>
<li><p>Make the features dense</p>
</li>
<li><p>Using models that are robust to sparse features</p>
</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html#:~:text= Methods for dealing with sparse features ,that are robust to sparse features More">https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html#:~:text=%20Methods%20for%20dealing%20with%20sparse%20features%20,that%20are%20robust%20to%20sparse%20features%20More%20</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">特征工程</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E7%89%B9%E5%BE%81%E7%A8%80%E7%96%8F/">特征稀疏</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-30  <a class="commentCountImg" href="/2021/09/30/felix/#comment-container"><span class="display-none-class">439b8b1d934392a5881d2cae5276da81</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="439b8b1d934392a5881d2cae5276da81">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/30/felix/">Felix Flexible Text Editing Through Tagging and Insertion</a></h1><div class="content"><p>google继lasertagger之后的又一篇text edit paper</p>
<p>In contrast to conventional sequence-to-sequence (seq2seq) models, FELIX is efficient in <strong>low-resource settings</strong> and <strong>fast</strong> at inference time, while being <strong>capable</strong> of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: <strong>tagging</strong> to decide on the subset of input tokens and their order in the output text and <strong>insertion</strong> to in-fill the missing tokens in the output not present in the input.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p><img src="/2021/09/30/felix/11.JPG" alt></p>
<p>In particular, we have designed FELIX with the following requirements in mind: Sample efficiency, Fast inference time, Flexible text editing</p>
<h2 id="2-Model-description"><a href="#2-Model-description" class="headerlink" title="2 Model description"></a>2 Model description</h2><p>FELIX decomposes the conditional probability of generating an output sequence $y$ from an input<br>$x$ as follows:</p>
<script type="math/tex; mode=display">
p(\textbf{y}|\textbf{x})=p_{ins}(\textbf{y}|\textbf{y}^m)p_{tag}(\textbf{y}^t,\pi|\textbf{x})</script><h3 id="2-1-Tagging-Model"><a href="#2-1-Tagging-Model" class="headerlink" title="2.1 Tagging Model"></a>2.1 Tagging Model</h3><p>trained to optimize both the tagging and pointing loss:</p>
<script type="math/tex; mode=display">
\mathcal{L}=\mathcal{L}_{pointing  }+\lambda\mathcal{L}_{tagging   }</script><p><strong>Tagging</strong> :</p>
<p>tag sequence $\textbf{y}^t$由3种tag组成：$KEEP$，$DELETE$，$INSERT (INS)$</p>
<p>Tags are predicted by applying a single feedforward layer $f$ to the output of the encoder $\textbf{h}^L$ (the source sentence is first encoded using a 12-layer BERT-base model). $\textbf{y}^t_i=argmax(f(\textbf{h}^L_i))$</p>
<p><strong>Pointing</strong>:</p>
<p><img src="/2021/09/30/felix/33.JPG" alt></p>
<p>Given a sequence $\textbf{x}$ and the predicted tags $\textbf{y}^t$ , the re-ordering model generates a permutation $\pi$ so that from $\pi$and  $\textbf{y}^t$ we can reconstruct the insertion model input $\textbf{y}^m$. Thus we have: </p>
<script type="math/tex; mode=display">
p(\textbf{y}^m|\textbf{x}) \approx \prod \limits_{i}p(\pi(i)|\textbf{x},\textbf{y}^t,i)p(\textbf{y}_i^t|\textbf{x})</script><p>Our implementation is based on a <strong>pointer network</strong>. The output of this model is a series of predicted pointers (source token → next target token)</p>
<p>The input to the Pointer layer at position $i$:</p>
<script type="math/tex; mode=display">
\textbf{h}^{L+1}_{i}=f([\textbf{h}^{L}_{i};e(\textbf{y}_i^t);e(\textbf{p}_i)])</script><p>其中$e(\textbf{y}_i^t)$is the embedding of the predicted tag，$e(\textbf{p}_i)$ is the positional embedding</p>
<p>The pointer network attends over all hidden states, as such:</p>
<script type="math/tex; mode=display">
p(\pi(i)|\textbf{h}_i^{L+1})=attention(\textbf{h}_i^{L+1},\textbf{h}_{\pi(i)}^{L+1})</script><p>其中$\textbf{h}_i^{L+1}$ as $Q $, $\textbf{h}_{\pi(i)}^{L+1}$ as $K$</p>
<p>When realizing the pointers, we use a constrained beam search</p>
<h3 id="2-2-Insertion-Model"><a href="#2-2-Insertion-Model" class="headerlink" title="2.2 Insertion Model"></a>2.2 Insertion Model</h3><p><img src="/2021/09/30/felix/22.JPG" alt></p>
<p>To represent masked token spans we consider two options: <strong>masking</strong> and <strong>infilling</strong>. In the former case the tagging model predicts how many tokens need to be inserted by specializing the $INSERT$ tag into $INS_k$, where $k$ translates the span into $ k$  $MASK$ tokens. For the infilling case the tagging model predicts a generic $INS$ tag. </p>
<p>Note that we preserve the deleted​ span in the input to the insertion model by enclosing it between $[REPL]$ and $[/REPL]$ tags.</p>
<p>our insertion model is also based on a 12-layer BERT-base and we can directly take advantage of the BERT-style pretrained checkpoints.</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.findings-emnlp.111.pdf">https://aclanthology.org/2020.findings-emnlp.111.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/">文本改写</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-28  <a class="commentCountImg" href="/2021/09/28/tao-emb-search/#comment-container"><span class="display-none-class">f3732a2aa8ad0bf847057df23a7462a0</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="f3732a2aa8ad0bf847057df23a7462a0">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>9 m  <i class="fas fa-pencil-alt"> </i>1.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/28/tao-emb-search/">Embedding based Product Retrieval in Taobao Search</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.09297.pdf">https://arxiv.org/pdf/2106.09297.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://xtf615.com/2021/10/07/taobao-ebr/">http://xtf615.com/2021/10/07/taobao-ebr/</a></p>
<h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1.INTRODUCTION"></a>1.INTRODUCTION</h2><p><img src="/2021/09/28/tao-emb-search/11.JPG" alt></p>
<p><img src="/2021/09/28/tao-emb-search/22.JPG" alt></p>
<p>框架是搜索系统主流的结构，即匹配/检索，粗排，精排，重排。</p>
<h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2.RELATED WORK"></a>2.RELATED WORK</h2><h3 id="2-1-Deep-Matching-in-Search"><a href="#2-1-Deep-Matching-in-Search" class="headerlink" title="2.1 Deep Matching in Search"></a>2.1 Deep Matching in Search</h3><p>fall into two categories: <strong>representation-based learning</strong> and <strong>interaction-based learning.</strong></p>
<p>Other than semantic and relevance matching, more complex factors/trade-offs, e.g., user personalization [2, 3, 10] and retrieval efficiency [5], need to be considered when applying deep models to a large-scale online retrieval system.</p>
<h3 id="2-2-Deep-Retrieval-in-Industry-Search"><a href="#2-2-Deep-Retrieval-in-Industry-Search" class="headerlink" title="2.2 Deep Retrieval in Industry Search"></a>2.2 Deep Retrieval in Industry Search</h3><p>Representation-based models with an ANN (approximate near neighbor) algorithm have become the mainstream trend to efficiently deploy neural retrieval models in industry.</p>
<h2 id="3-MODEL"><a href="#3-MODEL" class="headerlink" title="3 MODEL"></a>3 MODEL</h2><p>整体结构入下：</p>
<p><img src="/2021/09/28/tao-emb-search/33.JPG" alt></p>
<h3 id="3-1-Problem-Formulation"><a href="#3-1-Problem-Formulation" class="headerlink" title="3.1 Problem Formulation"></a>3.1 Problem Formulation</h3><p>$\mathcal{U}=\{u_1,…,u_u,…u_N\}$表示$N$个用户，$\mathcal{Q}=\{q_1,…,q_u,…q_N\}$表示与用户对应的$N$个query，$\mathcal{I}=\{i_1,…,i_u,…i_M\}$表示$M$个商品。将用户$u$的历史行为根据时间分成3个部分：1.real-time，before<br>the current time step，$\mathcal{R}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ 2.short-term, before $\mathcal{R}$ and within ten days,$\mathcal{S}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ 3.long-term sequences,before $\mathcal{S}$ and within one month,$\mathcal{L}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ ，$T$为时间长度。任务可以定义为：</p>
<script type="math/tex; mode=display">
z=\mathcal{F}(\phi(q_u,\mathcal{R}^u,\mathcal{S}^u,\mathcal{L}^u),\varphi(i))</script><p>其中$\mathcal{F}(\cdot),\phi(\cdot),\varphi(\cdot)$分别表示scoring function, query and behaviors encoder, and item encoder</p>
<h3 id="3-2-User-Tower"><a href="#3-2-User-Tower" class="headerlink" title="3.2 User Tower"></a>3.2 User Tower</h3><h4 id="3-2-1-Multi-Granular-Semantic-Unit"><a href="#3-2-1-Multi-Granular-Semantic-Unit" class="headerlink" title="3.2.1 Multi-Granular Semantic Unit"></a>3.2.1 Multi-Granular Semantic Unit</h4><p>挖掘query的语义，原始输入包含当前query和历史query</p>
<p>没有说明为什么这么设计，感觉就是工程试验的结论。有个疑问，直接用BERT等深度语言模型来挖掘query的语义不好吗？</p>
<p>query表示为$q_u=\{w_1^u,…,w_n^u\}$,例如{红色，连衣裙}，$w_u=\{c_1^u,…,c_m^u\}$,例如{红，色}，history query表示为$q_{his}=\{q_1^u,…,q_k^u\} $,例如{绿色，半身裙，黄色，长裙}，其中$w_n \in \mathbb{R}^{1\times d},c_m \in \mathbb{R}^{1\times d},q_k \in \mathbb{R}^{1\times d}$</p>
<script type="math/tex; mode=display">
q_{1\_gram}=mean\_pooling(c_1,...,c_m)
\\q_{2\_gram}=mean\_pooling(c_1c_2,...,c_{m-1}c_m)
\\q_{seq}=mean\_pooling(w_1,...,w_n)
\\q_{seq\_seq}=mean\_pooling(T_{rm}(w_1,...,w_n))
\\q_{his\_seq}=softmax(q_{seg}\cdot(q_{his})^{T})q_{his}
\\q_{mix}=q_{1\_gram}+q_{2\_gram}+q_{seq}+q_{seq\_seq}+q_{his\_seq}
\\Q_{mgs}=concat(q_{1\_gram},q_{2\_gram},q_{seq},q_{seq\_seq},q_{his\_seq},q_{mix})</script><p>其中𝑇𝑟𝑚,𝑚𝑒𝑎𝑛_𝑝𝑜𝑜𝑙𝑖𝑛𝑔, and 𝑐𝑜𝑛𝑐𝑎𝑡 denote the Transformer ,average, and vertical concatenation operation, respectively</p>
<h4 id="3-2-2-User-Behaviors-Attention"><a href="#3-2-2-User-Behaviors-Attention" class="headerlink" title="3.2.2 User Behaviors Attention"></a>3.2.2 User Behaviors Attention</h4><script type="math/tex; mode=display">
e_i^f=W_f\cdot x_i^{f} \in \mathbb{R}^{1\times d_f} \tag{9}
\\i_t^u=concat(\{e_i^f\ | \ f \in \mathcal{F} \})</script><p>其中$W_f$是embedding matrix，$x_i^{f}$是one-hot vector, $\mathcal{F}$是side information  (e.g., leaf category, first-level category, brand and,shop)</p>
<p><strong>real-time sequences</strong></p>
<p>User’s click_item</p>
<script type="math/tex; mode=display">
\mathcal{R}_{lstm}^u=LSTM(\mathcal{R}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}
\\\mathcal{R}_{self\_att}^u=multihead\_selfattention(\mathcal{R}_{lstm}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}
\\\mathcal{R}_{zero\_att}^u=\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \} \  \#  add \  a \ zero \ vector \ at \  the \ first \ position \ of \  \mathcal{R}_{self\_att}^u
\\H_{real}=softmax(Q_{mgs}\cdot\mathcal{R}_{zero\_att}^T)\cdot\mathcal{R}_{zero\_att}^T</script><p><strong>short-term sequences</strong></p>
<p>User’s click_item</p>
<script type="math/tex; mode=display">
\mathcal{S}_{self\_att}^u=multihead\_selfattention(\mathcal{S}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}
\\\mathcal{S}_{zero\_att}^u=\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \} 
\\H_{short}=softmax(Q_{mgs}\cdot\mathcal{S}_{zero\_att}^T)\cdot\mathcal{S}_{zero\_att}^T</script><p><strong>long-term sequence</strong></p>
<p>$\mathcal{L}^u$由四个部分构成，分别为$\mathcal{L}^{u}_{item},\mathcal{L}^{u}_{shop},\mathcal{L}^{u}_{leaf},\mathcal{L}^{u}_{brand}$,每个部分包含3个动作，分别为click，buy，collect。</p>
<script type="math/tex; mode=display">
\\ \mathcal{L}_{click\_item},\mathcal{L}_{buy\_item},\mathcal{L}_{collect\_item}  \rightarrow L^{T}_{item}
\\H_{a\_item}=softmax(Q_{mgs}\cdot  L^{T}_{item})\cdot L^{T}_{item}
\\H_{long}=H_{a\_item}+H_{a\_shop}+H_{a\_leaf}+H_{a\_brand}</script><h4 id="3-2-3-Fusion-of-Semantics-and-Personalization"><a href="#3-2-3-Fusion-of-Semantics-and-Personalization" class="headerlink" title="3.2.3 Fusion of Semantics and Personalization"></a>3.2.3 Fusion of Semantics and Personalization</h4><script type="math/tex; mode=display">
H_{qu}=Self\_Att^{first}([[cls],Q_{mgs},H_{real},H_{short},H_{long}]) \in \mathbb{R}^{1\times d}</script><h3 id="3-3-Item-Tower"><a href="#3-3-Item-Tower" class="headerlink" title="3.3 Item Tower"></a>3.3 Item Tower</h3><p>For the item tower, we experimentally use item ID and title to obtain the item representation 𝐻𝑖𝑡𝑒𝑚.Given the representation of item 𝑖’s ID, $e_i \in \mathbb{R}^{1\times d}$ , and its title segmentation result $T_i=\{w_1^{i},…,w_N^{i}\}$</p>
<script type="math/tex; mode=display">
H_{item}=e+tanh(W_t\cdot\frac{\sum_{i=1}^Nw_i}{N})</script><p>where $W_t$ is the transformation matrix. We empirically find that applying LSTM [12] or Transformer [27] to capture the context of the title is not as effective as simple mean-pooling since the title is stacked by keywords and lacks grammatical structure.</p>
<h3 id="3-4-Loss-Function"><a href="#3-4-Loss-Function" class="headerlink" title="3.4 Loss Function"></a>3.4 Loss Function</h3><p>adapt the softmax cross-entropy loss as the training objective</p>
<script type="math/tex; mode=display">
\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+}))}{\sum_{i^{'}\in I }exp(\mathcal{F}(q_u,i^{'}))}
\\L(\nabla )=-\sum_{i\in I}y_ilog(\hat{y_i})</script><p>where $\mathcal{F},I,i^+,q_u$denote the inner product, the full item pool, the item tower’s representation $H_{item}$, and the user tower’s representation $H_{qu}$, respectively.</p>
<h4 id="3-4-1-Smoothing-Noisy-Training-Data"><a href="#3-4-1-Smoothing-Noisy-Training-Data" class="headerlink" title="3.4.1 Smoothing Noisy Training Data"></a>3.4.1 Smoothing Noisy Training Data</h4><p>the softmax function with the temperature parameter $\tau$ is defined as follows</p>
<script type="math/tex; mode=display">
\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+})/\tau)}{\sum_{i^{'}\in I }exp(\mathcal{F}(q_u,i^{'})/\tau)}</script><p>If 𝜏-&gt;0, the fitted distribution is close to one hot distribution,If 𝜏-&gt;∞, the fitted distribution is close to a uniform distribution</p>
<h4 id="3-4-2-Generating-Relevance-improving-Hard-Negative-Samples"><a href="#3-4-2-Generating-Relevance-improving-Hard-Negative-Samples" class="headerlink" title="3.4.2 Generating Relevance-improving Hard Negative Samples"></a>3.4.2 Generating Relevance-improving Hard Negative Samples</h4><p>We first select the negative items of $i^-$ that have the top-𝑁 inner product scores with $q_u $ to form the hard sample set $I_{hard}$</p>
<script type="math/tex; mode=display">
I_{mix}=\alpha i^++(1-\alpha)I_{hard}</script><p>其中$\alpha\in \mathbb{R}^{N\times1}$is sampled from the uniform distribution 𝑈 (𝑎, 𝑏) (0 ≤ 𝑎 &lt; 𝑏 ≤ 1).</p>
<script type="math/tex; mode=display">
\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+})/\tau)}{\sum_{i^{'}\in (I\cup I_{mix}) }exp(\mathcal{F}(q_u,i^{'})/\tau)}</script></div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%90%91%E9%87%8F/">向量</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/Embedding-based-Product-Retrieval-in-Taobao-Search/">Embedding based Product Retrieval in Taobao Search</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-27  <a class="commentCountImg" href="/2021/09/27/lambdamart/#comment-container"><span class="display-none-class">1d21af6ac09030b0fa57649eb89cea30</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="1d21af6ac09030b0fa57649eb89cea30">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>6 m  <i class="fas fa-pencil-alt"> </i>0.9 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/27/lambdamart/">From RankNet to LambdaRank to LambdaMART</a></h1><div class="content"><h2 id="1-RankNet"><a href="#1-RankNet" class="headerlink" title="1.RankNet"></a>1.RankNet</h2><p>RankNet采用pairwise的方法进行模型训练。</p>
<p><strong>loss推导</strong></p>
<p>给定特定$query$下的两个文档$U_i$和$U_j$，其特征向量分别为$x_i$和$x_j$，经过RankNet进行前向计算得到对应的分数为$s_i=f(x_i)$和$s_j=f(x_j)$。用$U_i \rhd U_j$表示$U_i$比$U_j$排序更靠前。继而可以用下面的公式来表示$U_i$应该比$U_j$排序更靠前的概率：</p>
<script type="math/tex; mode=display">
P_{ij} \equiv P(U_i \rhd U_j) \equiv \frac{1}{1+e^{-\sigma(s_i-s_j)}}</script><p>定义$S_{ij} \in \{0,\pm1\}$为文档$i$和文档$j$被标记的标签之间的关联，即</p>
<script type="math/tex; mode=display">
S_{ij}=\left\{ \begin{aligned} 1&&     文档i比文档j更相关\\ 0&&    文档i和文档j相关性一致\\ -1&&   文档j比文档i更相关 \end{aligned} \right.</script><p>定义$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$U_i$应该比$U_j$排序更靠前的已知概率，则可以用交叉熵定义优化目标的损失函数：</p>
<script type="math/tex; mode=display">
\begin{align*}
C&=-\overline{P}_{ij}log{P_{ij}}-(1-\overline{P}_{ij})log(1-P_{ij})
\\&=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})

\end{align*}</script><p>注意：$\sigma$是超参数</p>
<p><strong>ranknet 加速</strong></p>
<h2 id="2-LambdaRank"><a href="#2-LambdaRank" class="headerlink" title="2.LambdaRank"></a>2.LambdaRank</h2><p>ranket缺陷为只考虑pair的相对位置没有考虑二者在列表的整体位置</p>
<p>LambdaRank本质为ranknet基础上加入Listwise的指标，因此有人将LambdaRank归为listwise方法，也有归到pairwise方法</p>
<h3 id="2-1-RankNet的局限"><a href="#2-1-RankNet的局限" class="headerlink" title="2.1 RankNet的局限"></a>2.1 RankNet的局限</h3><p><img src="/2021/09/27/lambdamart/11.GIF" alt></p>
<h3 id="2-2-LambdaRank定义"><a href="#2-2-LambdaRank定义" class="headerlink" title="2.2 LambdaRank定义"></a>2.2 LambdaRank定义</h3><script type="math/tex; mode=display">
\begin{align*}
\frac{\partial{C}}{\partial{w_k}}&=\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}
\\&=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)
\\&=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)

\end{align*}
\\其中\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=-\frac{\partial{C}}{\partial{s_j}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)</script><p>上述公式可以进一步简化，即只考虑$S_{ij}=1$ （为什么可以？？？？？）</p>
<p>那么Lambda，$\lambda$，就是梯度</p>
<script type="math/tex; mode=display">
\lambda_{ij}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}</script><p>为了加强排序中前后顺序的重要性，Lambda在原基础上引入评价指标Z（如NDCG），把交换两个文档的位置引起的评价指标的变化$|\Delta Z_{ij}|$作为其中一个因子：</p>
<script type="math/tex; mode=display">
\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta Z_{ij}|</script><p>反推出 LambdaRank 的损失函数：</p>
<script type="math/tex; mode=display">
C=log(1+e^{\sigma (s_i-s_j)})|\Delta Z_{ij}|</script><h2 id="3-LambdaMART"><a href="#3-LambdaMART" class="headerlink" title="3.LambdaMART"></a>3.LambdaMART</h2><p>属于listwise，也有说pairwise。</p>
<p>LambdaMART=lambda($\lambda$)+mart(gbdt)</p>
<p>$\lambda$就是梯度，lambdarank就是一种loss，gbdt就是模型</p>
<p>lambdamart说白了就是利用gbdt计算lambdarank中s，或者说将lambdarank作为gbdt的loss</p>
<p>gbdt，lambdamart算法流程差异在于step1</p>
<p><strong>GBDT</strong>：</p>
<ol>
<li>初始化： $f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</li>
<li>for m=1 to M:<br>(a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \qquad i = 1,2 \cdots N$<br>(b). $\left \{ R_{jm} \right\}_1^J = \mathop{\arg\min}\limits_{\left \{ R_{jm} \right\}_1^J}\sum\limits_{i=1}^N \left [\tilde{y}_i - h_m(x_i\,;\,\left \{R_{jm},b_{jm} \right\}_1^J) \right]^2$<br>(c).  $\gamma_{jm} = \mathop{\arg\min}\limits_\gamma \sum\limits_{x_i \in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$<br>(d).  $f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^J \gamma_{jm}I(x \in R_{jm})$</li>
<li>输出$f_M(x)$</li>
</ol>
<p><strong>LambdaMART:</strong></p>
<p><img src="/2021/09/27/lambdamart/22.png" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/laolu1573/article/details/87372514">https://blog.csdn.net/laolu1573/article/details/87372514</a></p>
<p><a target="_blank" rel="noopener" href="https://liam.page/uploads/slides/lambdamart.pdf">https://liam.page/uploads/slides/lambdamart.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zpalyq110/article/details/79527653">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86354141">https://zhuanlan.zhihu.com/p/86354141</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/genyuan/p/9788294.html">https://www.cnblogs.com/genyuan/p/9788294.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/huagong_adu/article/details/40710305">https://blog.csdn.net/huagong_adu/article/details/40710305</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bentuwuying/p/6690836.html">https://www.cnblogs.com/bentuwuying/p/6690836.html</a></p>
<p>paper原文： <a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a78b3f52c221">https://www.jianshu.com/p/a78b3f52c221</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zhoujialin/article/details/46697409">https://blog.csdn.net/zhoujialin/article/details/46697409</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/w28971023/article/details/45849659">https://blog.csdn.net/w28971023/article/details/45849659</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/LTR/">LTR</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/LTR/listwise/">listwise</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/LambdaMART/">LambdaMART</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-24  <a class="commentCountImg" href="/2021/09/24/monotonous-stack/#comment-container"><span class="display-none-class">09f84f16c33992af67a531338f94c503</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="09f84f16c33992af67a531338f94c503">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>3 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/24/monotonous-stack/">单调栈</a></h1><div class="content"><p><strong>分类</strong>：</p>
<p>单调栈也分为<strong>单调递增栈</strong>和<strong>单调递减栈</strong></p>
<ul>
<li>单调递增栈：单调递增栈就是从栈底到栈顶数据是从大到小</li>
<li>单调递减栈：单调递减栈就是从栈底到栈顶数据是从小到大</li>
</ul>
<p><strong>举例：</strong>单调递增栈</p>
<p>现在有一组数10，3，7，4，12。从左到右依次入栈，则如果栈为空或入栈元素值小于栈顶元素值，则入栈；否则，如果入栈则会破坏栈的单调性，则需要把比入栈元素小的元素全部出栈。</p>
<p>10入栈时，栈为空，直接入栈，栈内元素为10。</p>
<p>3入栈时，栈顶元素10比3大，则入栈，栈内元素为10，3。</p>
<p>7入栈时，栈顶元素3比7小，则栈顶元素出栈，此时栈顶元素为10，比7大，则7入栈，栈内元素为10，7。</p>
<p>4入栈时，栈顶元素7比4大，则入栈，栈内元素为10，7，4。</p>
<p>12入栈时，栈顶元素4比12小，4出栈，此时栈顶元素为7，仍比12小，栈顶元素7继续出栈，此时栈顶元素为10，仍比12小，10出栈，此时栈为空，12入栈，栈内元素为12。</p>
<p><strong>伪代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">stack&lt;int&gt; st;</span><br><span class="line">//单调递增栈</span><br><span class="line">for (遍历这个数组)</span><br><span class="line">&#123;</span><br><span class="line">	if (栈空 || 栈顶元素大于等于当前比较元素)</span><br><span class="line">	&#123;</span><br><span class="line">		入栈;</span><br><span class="line">	&#125;</span><br><span class="line">	else</span><br><span class="line">	&#123;</span><br><span class="line">		while (栈不为空 &amp;&amp; 栈顶元素小于当前元素)</span><br><span class="line">		&#123;</span><br><span class="line">			栈顶元素出栈;</span><br><span class="line">			更新结果;</span><br><span class="line">		&#125;</span><br><span class="line">		当前数据入栈;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>应用</strong>：</p>
<p>主要用于$O(n)$ 解决NGE问题（Next Greater Element）</p>
<ul>
<li>比当前元素更大的下一个元素</li>
<li>比当前元素更大的前一个元素</li>
<li>比当前元素更小的下一个元素</li>
<li>比当前元素更小的前一个元素</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/lucky52529/article/details/89155694">https://blog.csdn.net/lucky52529/article/details/89155694</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%8D%95%E8%B0%83%E6%A0%88/">单调栈</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-24  <a class="commentCountImg" href="/2021/09/24/gbdt/#comment-container"><span class="display-none-class">9c041ef5a76b762fb250215043b389e9</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="9c041ef5a76b762fb250215043b389e9">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/24/gbdt/">GBDT</a></h1><div class="content"><p>GBDT （Gradient Boosting Decison Tree）=Gradient Boosting+cart回归树</p>
<p>注意是cart回归树，不是cart分类树</p>
<p>说白了就是gradient boosting基学习器为cart回归树</p>
<h2 id="gradient-boosting算法流程"><a href="#gradient-boosting算法流程" class="headerlink" title="gradient boosting算法流程:"></a>gradient boosting算法流程:</h2><p>1.初始化：$f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</p>
<p>2.for m=1 to M:<br>    (a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \quad i = 1,2 \cdots N$<br>    (b).  通过最小化平方误差，用基学习器$h_m(x)$拟合$\tilde{y_i}$，$w_m = \mathop{\arg\min}\limits_w \sum\limits_{i=1}^{N} \left[\tilde{y}_i - h_m(x_i\,;\,w) \right]^2$<br>    (c).  使用line search确定步长$ρ_m$，使$L$最小，$\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))$<br>    (d).  $f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)$</p>
<p>3.输出$f_M(x)$</p>
<h2 id="GBDT算法流程："><a href="#GBDT算法流程：" class="headerlink" title="GBDT算法流程："></a>GBDT算法流程：</h2><ol>
<li>初始化： $f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</li>
<li>for m=1 to M:<br>(a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \qquad i = 1,2 \cdots N$<br>(b). $\left \{ R_{jm} \right\}_1^J = \mathop{\arg\min}\limits_{\left \{ R_{jm} \right\}_1^J}\sum\limits_{i=1}^N \left [\tilde{y}_i - h_m(x_i\,;\,\left \{R_{jm},b_{jm} \right\}_1^J) \right]^2$<br>(c).  $\gamma_{jm} = \mathop{\arg\min}\limits_\gamma \sum\limits_{x_i \in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$<br>(d).  $f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^J \gamma_{jm}I(x \in R_{jm})$</li>
<li>输出$f_M(x)$</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zpalyq110/article/details/79527653">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86354141">https://zhuanlan.zhihu.com/p/86354141</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting/">boosting</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/GBDT/">GBDT</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-23  <a class="commentCountImg" href="/2021/09/23/gradient_boosting/#comment-container"><span class="display-none-class">5a2eab6117a810045f8814ed348d2914</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="5a2eab6117a810045f8814ed348d2914">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.4 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/23/gradient_boosting/">gradient boosting</a></h1><div class="content"><h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>Gradient Boosting为boosting算法的一种，采用和AdaBoost同样的加法模型，在第m次迭代中，前m-1个基学习器都是固定的，即</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x) + \rho_m h_m(x)  \tag{1}</script><p>核心思想是得到基学习器$h_m(x)$和权重$p_m$</p>
<p><strong>参数空间的梯度下降</strong>很常见，即</p>
<script type="math/tex; mode=display">
\theta = \theta - \alpha \cdot \frac{\partial}{\partial \theta}L(\theta)</script><p>若将$f(x)$当成参数，则同样可以使用<strong>函数空间的梯度下降法</strong>：</p>
<script type="math/tex; mode=display">
f_m(x) = f_{m-1}(x) - \rho_m \cdot \frac{\partial}{\partial f_{m-1}(x)}L(y,f_{m-1}(x)) \tag{2}</script><p>对比（1）（2），我们发现$h_m(x) \approx -\frac{\partial L(y,f_{m-1}(x))}{\partial f_{m-1}(x)}$</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程:"></a>算法流程:</h2><p>1.初始化：$f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</p>
<p>2.for m=1 to M:<br>    (a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \quad i = 1,2 \cdots N$<br>    (b).  通过最小化平方误差，用基学习器$h_m(x)$拟合$\tilde{y_i}$，$w_m = \mathop{\arg\min}\limits_w \sum\limits_{i=1}^{N} \left[\tilde{y}_i - h_m(x_i\,;\,w) \right]^2$<br>    (c).  使用line search确定步长$ρ_m$，使$L$最小，$\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))$<br>    (d).  $f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)$</p>
<p>3.输出$f_M(x)$</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhubinwang/p/5170087.html">https://www.cnblogs.com/zhubinwang/p/5170087.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/massquantity/p/9174746.html">https://www.cnblogs.com/massquantity/p/9174746.html</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting/">boosting</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/gradient-boosting/">gradient boosting</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-23  <a class="commentCountImg" href="/2021/09/23/decison-tree/#comment-container"><span class="display-none-class">88139e860b94cc8e241f92b9e138f7dd</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="88139e860b94cc8e241f92b9e138f7dd">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>11 m  <i class="fas fa-pencil-alt"> </i>1.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/23/decison-tree/">决策树</a></h1><div class="content"><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h2><p>基本上决策树都是二叉树</p>
<p><img src="/2021/09/23/decison-tree/22.png" alt></p>
<p><img src="/2021/09/23/decison-tree/33.png" alt></p>
<p>两幅图意思一样</p>
<h2 id="2-决策树-vs-逻辑回归"><a href="#2-决策树-vs-逻辑回归" class="headerlink" title="2.决策树 vs 逻辑回归"></a>2.决策树 vs 逻辑回归</h2><p><img src="/2021/09/23/decison-tree/11.png" alt></p>
<p>最大的差异上图就可以看出，左边为逻辑回归的决策面，右边为决策树的决策面</p>
<h2 id="3-构建算法"><a href="#3-构建算法" class="headerlink" title="3.构建算法"></a>3.构建算法</h2><p>常见方法的有ID3，C4.5，CART，总结如下</p>
<p><img src="/2021/09/23/decison-tree/11.png" alt></p>
<h3 id="3-1-ID3"><a href="#3-1-ID3" class="headerlink" title="3.1 ID3"></a>3.1 ID3</h3><p>假设训练数据集为 $D$,∣$D∣$表示其大小。设有$K$个分类$ C_1,C_2,…,C_K$。设特征集为$\textbf{A}$,假设某个特征$ A$ 有$ n$ 个不同的取值 $\{a_1,a_2,…,a_n\}$,根据特征$A$的取值将 $D$ 划分成$n$个子集。记子集 $D_i$中属于类$ C_k$的样本集合为 $D_{ik}$。</p>
<p>数据集$D$的经验熵$H(D)$:</p>
<script type="math/tex; mode=display">
H(D) = - \sum_{j=1}^K \frac {|C_j|}{|D|} \log \frac {|C_j|}{|D|}</script><p>特征$A$对数据集$ D$的经验条件熵$H(D|A)$</p>
<script type="math/tex; mode=display">
% <![CDATA[
\begin{equation*}
\begin{split}
H(D|A) &= \sum_{i=1}^n \frac {|D_i|}{|D|} H(D_i) \\
& = - \sum_{i=1}^n  \frac {|D_i|}{|D|}\sum_{j=1}^K \frac {|D_{ij}|}{|D_i|} \log \frac {|D_{ij}|}{|D_i|} \\
\end{split}
\end{equation*} %]]></script><p><strong>信息增益</strong>$g(D,A)$</p>
<script type="math/tex; mode=display">
g(D,A)=H(D)-H(D|A)</script><p><strong>算法流程</strong>:</p>
<ol>
<li>若$D$中所有实例都属于同一类 $C_k$,则 $T$ 为单节点树,并将类 $C_k$作为该节点的类标记,返回$T$.</li>
<li>若$\textbf{A}=\phi$,则$T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$.</li>
<li>否则,按照信息增益的算法,计算每个特征对$D$的信息增益,取信息增益最大的特征 $A_g$.</li>
<li>如果$A_g&lt; \varepsilon$,则置 $T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$.</li>
<li>否则,对$A_g$的每一可能值 $a_i$,依$A_g=a_i$将$D$分成若干非空子集$D_i$</li>
<li>以$D_i$为训练集,以$\textbf{A}- A_g $为特征集,递归地调用步骤1到步骤5,得到子树 $T_i$,全部 $T_i$构成$T$,返回$T$.</li>
</ol>
<h3 id="3-2-C4-5"><a href="#3-2-C4-5" class="headerlink" title="3.2 C4.5"></a>3.2 C4.5</h3><p> <strong>C4.5算法流程与ID3相类似</strong>，只不过将信息增益改为信息增益比，以解决偏向取值较多的属性的问题，另外它可以处理连续型属性。</p>
<p>分裂信息 $SplitInformation(D,A)$</p>
<script type="math/tex; mode=display">
SplitInformation(D,A) = -\sum_{i=1}^n \frac {|D_i|}{|D|} \log \frac {|D_i|}{|D|}</script><p><strong>信息增益比</strong> $GainRatio(D, A)$</p>
<script type="math/tex; mode=display">
GainRatio(D, A) = \frac {g(D, A)} {SplitInformation(D, A)}</script><h3 id="3-3-CART"><a href="#3-3-CART" class="headerlink" title="3.3 CART"></a>3.3 CART</h3><h4 id="3-3-1-CART分类树"><a href="#3-3-1-CART分类树" class="headerlink" title="3.3.1 CART分类树"></a>3.3.1 CART分类树</h4><p>CART分类树算法使用基尼系数来代替信息增益（比），基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。</p>
<p>对于样本$D$，个数为$|D|$，假设$K$个类别，第$k$个类别的数量为$|C_k|$，则样本$D$的基尼系数表达式：</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2</script><p>对于样本$D$，个数为$|D|$，根据特征$A$的某个值$a$，把$D$分成$|D_1|$和$|D_2|$，则在特征$A=a$的条件下，样本$D$的<strong>基尼系数</strong>表达式为：</p>
<script type="math/tex; mode=display">
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p><strong>算法流程</strong>：算法输入训练集$D$，基尼系数的阈值，样本个数阈值。输出的是决策树$T$。</p>
<p>(1)、对于当前节点的数据集为$D$，如果样本个数小于阈值或没有特征，则当前节点停止递归，返回决策树。</p>
<p>(2)、计算样本集$D$的基尼系数，如果基尼系数小于阈值，则当前节点停止递归，返回决策树。</p>
<p>(3)、计算当前节点所有特征的各个特征值对数据集$D$的基尼系数</p>
<p>(4)、在计算出来的所有基尼系数中，选择基尼系数最小的特征$A$和对应的特征值$a$，并把数据集划分成两部分$D_1$和$D_2$，同时建立当前节点的左右节点，左节点的数据集$D$为$D_1$，右节点的数据集$D$为$D_2$。</p>
<p>(5)、对左右的子节点递归的调用1-4步，生成决策树。</p>
<h4 id="3-3-2-CART回归树"><a href="#3-3-2-CART回归树" class="headerlink" title="3.3.2 CART回归树"></a>3.3.2 CART回归树</h4><p>对回归树用<strong>平方误差</strong>最小化准则</p>
<p><strong>算法流程</strong>：输入为训练数据$D$，输出为回归树$f(x)$</p>
<p>(1) 选择最优的切分变量$j$和切分点$s$，遍历$j$，对固定的$j$遍历$s$，求解</p>
<script type="math/tex; mode=display">
\min  \limits_{j,s} \ [\min \limits_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min \limits_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]</script><p>(2) 用选定的$(j,s)$划分区域并决定输出值</p>
<script type="math/tex; mode=display">
R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}> s\}
\\\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,m=1,2</script><p>(3) 继续对两个子区域调用步骤（1）（2），直至满足停止条件</p>
<p>(4) 将输入空间划分成$M$个区域$R_1,R_2,…,R_M$，生成回归树</p>
<script type="math/tex; mode=display">
f(x)=\sum_{m=1}^{M}\hat{c}_mI(x\in R_m)</script><h3 id="3-4-多变量决策树"><a href="#3-4-多变量决策树" class="headerlink" title="3.4 多变量决策树"></a>3.4 多变量决策树</h3><p>无论ID3，C4.5，CART都是选择<strong>一个最优的特征</strong>做分类决策，但大多数，分类决策不是由某一个特征决定，而是一组特征。这样得到的决策树更加准确，这种决策树叫多变量决策树(multi-variate decision tree)。在选择最优特征的时，多变量决策树不是选择某一个最优特征，而是选择<strong>一个最优的特征线性组合</strong>做决策。</p>
<p>代表算法OC1。</p>
<p><img src="/2021/09/23/decison-tree/1.png" alt></p>
<h2 id="4-剪枝"><a href="#4-剪枝" class="headerlink" title="4.剪枝"></a>4.剪枝</h2><p>剪枝(pruning)是解决决策树过拟合的主要手段，通过剪枝可以大大提升决策树的泛化能力。通常，剪枝处理可分为：预剪枝，后剪枝。</p>
<ul>
<li>预剪枝：通过启发式方法，在生成决策树过程中对划分进行考察，若当前结点的划分影响决策树的泛化性能，则停止划分，并将其标记为叶节点</li>
<li>后剪枝：对已有的决策树，自底向上的对非叶结点进行考察，若该结点对应的子树替换为叶结点能提升决策树的泛化能力，则将改子树替换为叶结点</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89607509">https://zhuanlan.zhihu.com/p/89607509</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wxquare/p/5379970.html">https://www.cnblogs.com/wxquare/p/5379970.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43468807/article/details/105969232">https://blog.csdn.net/qq_43468807/article/details/105969232</a></p>
<p><a target="_blank" rel="noopener" href="http://leijun00.github.io/2014/09/decision-tree/">http://leijun00.github.io/2014/09/decision-tree/</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89607509">https://zhuanlan.zhihu.com/p/89607509</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wxquare/p/5379970.html">https://www.cnblogs.com/wxquare/p/5379970.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43468807/article/details/105969232">https://blog.csdn.net/qq_43468807/article/details/105969232</a></p>
<p><a target="_blank" rel="noopener" href="http://leijun00.github.io/2014/09/decision-tree/">http://leijun00.github.io/2014/09/decision-tree/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wqbin/p/11689709.html">https://www.cnblogs.com/wqbin/p/11689709.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/keye/p/10564914.html">https://www.cnblogs.com/keye/p/10564914.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/keye/p/10601501.html">https://www.cnblogs.com/keye/p/10601501.html</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1486712">https://cloud.tencent.com/developer/article/1486712</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44132485/article/details/106502422">https://blog.csdn.net/weixin_44132485/article/details/106502422</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/">模型结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-17  <a class="commentCountImg" href="/2021/09/17/doc-bert/#comment-container"><span class="display-none-class">497ee909ce15293540c44e2680c35f52</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="497ee909ce15293540c44e2680c35f52">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/17/doc-bert/">HIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICATION</a></h1><div class="content"><p>原版BERT的最大输入为512，为了使得BERT能解决超长文本的问题，作者在finetune阶段提出了两种策略来弥补这个问题，即利用BERT+LSTM或者BERT+transformer。</p>
<p>核心步骤：</p>
<p>1.split the input sequence into segments of a fixed size with overlap.</p>
<p>2.For each of these segments, we obtain H or P from BERT model.</p>
<p><img src="/2021/09/17/doc-bert/11.JPG" alt></p>
<p>3.We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer.//replacing the LSTM recurrent layer in favor of a small Transformer model</p>
<p>4.Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.</p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/">文本分类</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E8%B6%85%E9%95%BF%E6%96%87%E6%9C%AC/">超长文本</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/41/">Previous</a></div><div class="pagination-next"><a href="/page/43/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/41/">41</a></li><li><a class="pagination-link is-current" href="/page/42/">42</a></li><li><a class="pagination-link" href="/page/43/">43</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/47/">47</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">469</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">139</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">447</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-29T14:55:38.000Z">2024-07-29</time></p><p class="title"><a href="/2024/07/29/skiplist/">skiplist跳表</a></p><p class="categories"><a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a> / <a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-06-11T13:10:43.000Z">2024-06-11</time></p><p class="title"><a href="/2024/06/11/web-search/">网页搜索</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T11:39:05.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/raid/">raid</a></p><p class="categories"><a href="/categories/c/">c++</a> / <a href="/categories/c/%E4%BC%98%E5%8C%96/">优化</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:23:12.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/shell/">shell</a></p><p class="categories"><a href="/categories/shell/">shell</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-03T09:09:27.000Z">2024-05-03</time></p><p class="title"><a href="/2024/05/03/vim/">vim</a></p><p class="categories"><a href="/categories/linux/">linux</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/07/"><span class="level-start"><span class="level-item">July 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/06/"><span class="level-start"><span class="level-item">June 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/05/"><span class="level-start"><span class="level-item">May 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">77</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BD%91%E9%A1%B5%E6%90%9C%E7%B4%A2/"><span class="tag">网页搜索</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>