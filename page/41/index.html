<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-10  <a class="commentCountImg" href="/2021/08/10/leetcode_algorith-tech/#comment-container"><span class="display-none-class">744a04bfda424efee762dfa29699be92</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="744a04bfda424efee762dfa29699be92">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>a minute  <i class="fas fa-pencil-alt"> </i>0.1 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/10/leetcode_algorith-tech/">leetcode常见套路</a></h1><div class="content"><h2 id="一-常见算法"><a href="#一-常见算法" class="headerlink" title="一.常见算法"></a>一.常见算法</h2><p>分治策略，动态规划，回溯，分支限界，贪心策略</p>
<h2 id="二-巧用数据结构"><a href="#二-巧用数据结构" class="headerlink" title="二.巧用数据结构"></a>二.巧用数据结构</h2><p>普通栈、单调栈</p>
<p>队列</p>
<p>堆</p>
<p>字典树</p>
<h2 id="三-技巧"><a href="#三-技巧" class="headerlink" title="三.技巧"></a>三.技巧</h2><p>双指针/滑窗，二分查找，排序，快慢指针，取余，位运算，倍增（<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/divide-two-integers/">29. 两数相除</a>），递归，时空转化（hashtable），dfs/bfs</p>
<h2 id="四-套路选择"><a href="#四-套路选择" class="headerlink" title="四 套路选择"></a>四 套路选择</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/358653377">https://zhuanlan.zhihu.com/p/358653377</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/341176507">https://zhuanlan.zhihu.com/p/341176507</a></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/358653377">https://zhuanlan.zhihu.com/p/358653377</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/341176507">https://zhuanlan.zhihu.com/p/341176507</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/leetcode/">leetcode</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/leetcode/">leetcode</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-10  <a class="commentCountImg" href="/2021/08/10/graph-nn-survey/#comment-container"><span class="display-none-class">8f57e6bc9da60e4c16e6e1af2c61d167</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="8f57e6bc9da60e4c16e6e1af2c61d167">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>12 m  <i class="fas fa-pencil-alt"> </i>1.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/10/graph-nn-survey/">A Comprehensive Survey on Graph Neural Networks</a></h1><div class="content"><p> there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms.</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>虽然深度学习技术可以捕获欧式空间数据的隐藏模式，但是目前很多应用是基于图的，这是非欧空间的数据。图数据的复杂性给现有的技术带来了很大的挑战。这是因为图数据可以是不规则的，一个图可能有不同数量的无序结点，一个结点可能有不同数量的邻接结点。这会使得一些基本操作，比如卷积，在图领域无法很好的捕获特征。除此之外，目前机器学习算法有一个重要的假设，就是假设各个结点是相互独立的，然而，图中存在很多复杂的连接信息，主要用来表征结点间的互相关性。为了解决以上问题，衍生了很多图神经网络技术。举个例子，比如，图卷积。下图对比了传统的2D卷积和图卷积。二者最大的区别在于邻接结点，一个有序一个无序，一个尺寸固定一个尺寸可变。</p>
<p><img src="/2021/08/10/graph-nn-survey/gra1.JPG" alt></p>
<h2 id="2-背景和定义"><a href="#2-背景和定义" class="headerlink" title="2.背景和定义"></a>2.背景和定义</h2><h3 id="A-背景"><a href="#A-背景" class="headerlink" title="A. 背景"></a>A. 背景</h3><p><strong>Graph neural networks vs network embedding</strong></p>
<p>The main distinction between GNNs and network embedding is that GNNs are a group of neural network models which are designed for various tasks while network embedding covers various kinds of methods targeting the same task.</p>
<p><strong>Graph neural networks vs graph kernel methods</strong></p>
<p>The difference is that this mapping function of graph kernel methods is deterministic rather than learnable. </p>
<p>GNNs are much more efficient than graph kernel methods.</p>
<h3 id="B-定义"><a href="#B-定义" class="headerlink" title="B. 定义"></a>B. 定义</h3><p><img src="/2021/08/10/graph-nn-survey/gra2.JPG" alt></p>
<p>上表为本文的notation。</p>
<p>1.图</p>
<p>$ {G}=(V,E) $表示一个图。$N(v)=\{u\in V|(v,u)\in E\}$表示结点$v$的邻接结点。$\textbf{A}$是邻接矩阵，如果$A_{ij}=1$,那么表示$e_{ij}\in E$；如果$A_{ij}=0$,那么表示$e_{ij} \notin E$。$\textbf{X} \in \mathbb{R}^{n \times d} $是结点特征矩阵，$\textbf{X}^{e} \in \mathbb{R}^{m \times c}$是边特征矩阵。</p>
<p>2.有向图</p>
<p>A graph is undirected if and only if the adjacency matrix is symmetric.</p>
<p>3.时空图</p>
<p>A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time.</p>
<p>$G^{(t)}=(V,E,\textbf{X}^{(t)})，\textbf{X}^{(t)} \in \mathbb{R}^{n \times d}$</p>
<h2 id="3-分类和框架"><a href="#3-分类和框架" class="headerlink" title="3.分类和框架"></a>3.分类和框架</h2><h3 id="3-1-GNN分类"><a href="#3-1-GNN分类" class="headerlink" title="3.1 GNN分类"></a>3.1 GNN分类</h3><p>作者把GNN分成以下4类，分别为RecGNNs，ConvGNNs , GAEs, STGNNs。</p>
<p><strong>RecGNNs（Recurrent graph neural networks）</strong></p>
<p>RecGNNs aim to learn node representations with recurrent neural architectures. They assume a node in a graph constantly exchanges information message with its neighbors until a stable equilibrium is reached.</p>
<p><strong>ConvGNNs（Convolutional graph neural networks ）</strong> </p>
<p>The main idea is to generate a node $v$’s representation by aggregating its own features $\textbf{x}_v$ and neighbors’ features $\textbf{x}_u,u\in N(v)$。Different from RecGNNs, ConvGNNs stack multiple graph convolutional layers to extract high-level node representations.</p>
<p><strong>GAEs（Graph autoencoders）</strong></p>
<p>are unsupervised learning frameworks which encode nodes/graphs into a latent vector space and reconstruct graph data from the encoded information. GAEs are used to learn network embeddings and<br>graph generative distributions.</p>
<p><strong>STGNNs（Spatial-temporal graph neural networks）</strong></p>
<p>aim to learn hidden patterns from spatial-temporal graphs. The key idea of STGNNs is to consider spatial dependency and temporal dependency at the same time.</p>
<p><img src="/2021/08/10/graph-nn-survey/gra3.JPG" alt></p>
<p><img src="/2021/08/10/graph-nn-survey/gra4.JPG" alt></p>
<h3 id="3-2-框架"><a href="#3-2-框架" class="headerlink" title="3.2 框架"></a>3.2 框架</h3><p>With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms:</p>
<p>Node-level outputs relate to node regression and node classification tasks.</p>
<p>Edge-level outputs relate to the edge classification and link prediction tasks.</p>
<p>Graph-level outputs relate to the graph classification task. </p>
<p><strong>Training Frameworks：</strong></p>
<p>1.Semi-supervised learning for node-level classification</p>
<p>2.Supervised learning for graph-level classification</p>
<p>3.Unsupervised learning for graph embedding</p>
<h2 id="4-RecGNNs"><a href="#4-RecGNNs" class="headerlink" title="4.RecGNNs"></a>4.RecGNNs</h2><p>RecGNNs apply the <strong>same set of parameters</strong> recurrently over nodes in a graph to extract high-level node representations. 接下来介绍几种RecGNNs 结构。</p>
<p><strong>GNN*</strong></p>
<p>Based on an information diffusion mechanism,  GNN* updates nodes’ states by exchanging neighborhood information recurrently until a stable equilibrium is reached.</p>
<p>结点的hidden state is recurrently updated by</p>
<script type="math/tex; mode=display">
\textbf{h}_v^{(t)}=\sum_{u\in N(v)}f(\textbf{x}_v,\textbf{x}^e_{(v,u)},\textbf{x}_{u},\textbf{h}_{u}^{(t-1)})</script><p>$\textbf{h}_v^0$随机初始化。$f(\cdot)$是 parametric function，must be a contraction mapping, which shrinks the distance between two points after projecting them into a latent space.</p>
<p>训练过程分为两步，更新结点表示和更新参数，交替进行使得loss收敛。When a convergence criterion is satisfied, the last step node hidden states are forwarded to a readout layer.</p>
<p><strong>GraphESN</strong></p>
<p>GraphESN使用ESN提高GNN*的训练效率。GraphESN包含encoder和output output。encoder随机初始化并且不需要训练。It implements a contractive state transition function to recurrently update node states until the global graph state reaches convergence. Afterward, the output layer is trained by taking the fixed node states as inputs.</p>
<p><strong>Gated Graph Neural Networks (GGNNs)</strong></p>
<script type="math/tex; mode=display">
\textbf{h}_{v}^t=GRU(\textbf{h}_{v}^{t-1},\sum_{u\in N(v)}\textbf{W}h_{u}^t)</script><p>The adavantage is that it no longer needs to constrain parameters to ensure convergence. However, the downside of training by BPTT is that it sacrifices efficiency both in time and memory.</p>
<p><strong>GGNN</strong></p>
<p>RecGNNs 利用GRU作为循环函数</p>
<script type="math/tex; mode=display">
\textbf{h}_v^{(t)}=GRU(\textbf{h}_v^{(t-1)},\sum_{u\in N(v)}\textbf{W}\textbf{h}_u^{(t-1)})</script><p>其中$\textbf{h}_v^{(0)}=\textbf{x}_v$。</p>
<p>GGNN uses the back-propagation through time (BPTT) algorithm to learn the model parameters.</p>
<p>对于大的图不适用。</p>
<p><strong>SSE</strong></p>
<p>proposes a learning algorithm that is more scalable to large graphs</p>
<script type="math/tex; mode=display">
\textbf{h}_{v}^{(t)}=(1-\alpha)\textbf{h}_{v}^{（t-1）}+\alpha \textbf{W}_1 \sigma(\textbf{W}_2[\textbf{x}_v,\sum_{u\in N(v)}[\textbf{h}_u^{t-1},\textbf{x}_u]])</script><p>其中$\alpha$为超参数，$\sigma(\cdot)$为sigmoid函数。</p>
<h2 id="5-ConvGNNs"><a href="#5-ConvGNNs" class="headerlink" title="5.ConvGNNs"></a>5.ConvGNNs</h2><p><img src="/2021/08/10/graph-nn-survey/gra6.JPG" alt></p>
<p><img src="/2021/08/10/graph-nn-survey/1.JPG" alt></p>
<p>ConvGNNs与RecGNNs 主要区别在于上图。</p>
<p>ConvGNNs fall into two categories, <strong>spectral-based</strong> and <strong>spatial-based</strong>. Spectral based approaches <strong>define graph convolutions by introducing filters</strong> from the perspective of graph signal processing [82] where the graph convolutional operation is interpreted as removing noises from graph signals. Spatial-based approaches inherit ideas from RecGNNs to <strong>define graph convolutions by information propagation</strong>.  spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality.</p>
<h3 id="5-1-Spectral-based-ConvGNNs"><a href="#5-1-Spectral-based-ConvGNNs" class="headerlink" title="5.1 Spectral-based ConvGNNs"></a>5.1 Spectral-based ConvGNNs</h3><h3 id="5-2-Spatial-based-ConvGNNs"><a href="#5-2-Spatial-based-ConvGNNs" class="headerlink" title="5.2 Spatial-based ConvGNNs"></a>5.2 Spatial-based ConvGNNs</h3><p>罗列几个基本的结构。</p>
<p><strong>NN4G</strong></p>
<script type="math/tex; mode=display">
\textbf{h}_{v}^{(k)}=f(\textbf{W}^{(k)^T}\textbf{x}_v+\sum_{i=1}^{k-1}\sum_{u\in N(v) }\Theta^{(k)^{T}}\textbf{h}_{u}^{(k-1)})</script><p>其中$f(\cdot)$是激活函数，$\textbf{h}_{v}^{(0)}=0$，可以使用矩阵形式表达为：</p>
<script type="math/tex; mode=display">
\textbf{H}^{(k)}=f(\textbf{X}\textbf{W}^{(k)}+\sum_{i=1}^{k-1}\textbf{A}\textbf{H}^{k-1}\Theta^{(k)})</script><p><strong>DCNN</strong></p>
<p>regards graph convolutions as a diffusion process.</p>
<script type="math/tex; mode=display">
\textbf{H}^{(k)}=f(\textbf{W}^{(k)}\odot\textbf{P}^k\textbf{X}    )</script><p>其中$f(\cdot)$是激活函数。probability transition matrix $\textbf{P}\in\mathbb{R}^{n\times n},\textbf{P} = \textbf{D}^{-1}\textbf{A}$。</p>
<p>DCNN concatenates $\textbf{H}^{(1)},\textbf{H}^{(2)},…,\textbf{H}^{(K)}$together as the final model outputs.</p>
<p><strong>PGC-DGCNN</strong></p>
<p><strong>MPNN</strong></p>
<h3 id="5-3-Graph-Pooling-Modules"><a href="#5-3-Graph-Pooling-Modules" class="headerlink" title="5.3 Graph Pooling Modules"></a>5.3 Graph Pooling Modules</h3><p>After a GNN generates node features, we can use them for the final task. But using all these features directly can be computationally challenging, thus, a <strong>down-sampling strategy</strong> is needed. Depending on the objective and the role it plays in the network, different names are given to this strategy: (1) <strong>the pooling operation</strong> aims to reduce the size of parameters by down-sampling the nodes to generate smaller representations and thus avoid overfitting, permutation invariance, and computational complexity issues; (2) <strong>the readout operation</strong> is mainly used to generate graph-level representation based on node representations. <strong>Their mechanism is very similar. In this chapter, we use pooling to refer to all kinds of down-sampling strategies applied to GNNs.</strong></p>
<p>mean/max/sum pooling is the most <strong>primitive and effective way</strong> ：</p>
<script type="math/tex; mode=display">
\textbf{h}_G=mean/max/sum(\textbf{h}_1^{(K)},\textbf{h}_2^{(K)},...,\textbf{h}_n^{(K)})</script><p>$K$ is the index of the last graph convolutional layer.</p>
<p> some works [17], [27], [46] also use <strong>attention</strong> mechanisms to enhance the mean/sum pooling.</p>
<p>[101] propose the <strong>Set2Set</strong> method to generate a memory that increases with the size of the input.</p>
<p>还有<strong>SortPooling，DiffPool</strong>等</p>
<h2 id="6-GAEs"><a href="#6-GAEs" class="headerlink" title="6.GAEs"></a>6.GAEs</h2><h2 id="7-STGNNs"><a href="#7-STGNNs" class="headerlink" title="7.STGNNs"></a>7.STGNNs</h2><h2 id="8-APPLICATIONS"><a href="#8-APPLICATIONS" class="headerlink" title="8.APPLICATIONS"></a>8.APPLICATIONS</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.00596v4">https://arxiv.org/abs/1901.00596v4</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/GNN/">GNN</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/GNN%E7%BB%BC%E8%BF%B0/">GNN综述</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-06  <a class="commentCountImg" href="/2021/08/06/kg-bert/#comment-container"><span class="display-none-class">d959049bb39f5499e0cb44a98e0854ae</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="d959049bb39f5499e0cb44a98e0854ae">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/06/kg-bert/">KG-BERT BERT for Knowledge Graph Completion</a></h1><div class="content"><p>原文 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.03193.pdf">https://arxiv.org/pdf/1909.03193.pdf</a></p>
<h2 id="一-背景补充"><a href="#一-背景补充" class="headerlink" title="一.背景补充"></a>一.背景补充</h2><p><img src="/2021/08/06/kg-bert/11.GIF" alt></p>
<p>知识图谱普遍存在不完备的问题。以上图为例，黑色的箭头表示已经存在的关系，红色的虚线则是缺失的关系。知识图谱补全是基于图谱里已有的关系去推理出缺失的关系。由于BERT在NLP取得的成绩，作者将其迁移到知识图谱补全的应用上。</p>
<h2 id="二-结构"><a href="#二-结构" class="headerlink" title="二.结构"></a>二.结构</h2><p>作者设计了两种训练方式的KG - BERT, 可以运用到不同的知识图谱补全任务当中。</p>
<p><strong>2.1 Illustrations of fine-tuning KG-BERT for predicting the plausibility of a triple</strong></p>
<p><img src="/2021/08/06/kg-bert/kgbert1.JPG" alt></p>
<p>输入由三部分组成，$Head$，$Relation$，$Tail$。举个例子，$Head$可以是“Steven Paul Jobs was an American business magnate,entrepreneur and investor.” 或者“Steve Jobs”，$Relation$可以是“founded”，$Tail$可以是“Apple Inc. is an American multinational technology company headquartered in Cupertino, California.”或者“Apple Inc.”。用$[SEP]$分隔实体和关系。输入为3个向量的sum，即token, segment 和position embeddings。对于segment，实体的segment Embedding为$e_A$，而关系的segment Embedding为$e_B$。对于position ，相同position的不同token使用相同的position embedding。</p>
<p>对于输入的三元组$\tau=(h,r,t)$，目标函数为：</p>
<script type="math/tex; mode=display">
S_{\tau}=f(h,r,t)=sigmoid(CW^T)，S_{\tau} \in \mathbb{R}^2,S_{\tau 0},
S_{\tau 1} \in [0,1]</script><p>损失函数是$S$和$y$的交叉熵：</p>
<script type="math/tex; mode=display">
L=-\sum_{\tau \in D^{+}\cup D^{-}}(y_{\tau}log(S_{\tau0})+(1-y_{\tau}log(S_{\tau1})))</script><p>其中$y_{\tau}\in \{0,1\}$是标签。</p>
<p>关于负样本的构造，作者是将正样本的$Head$或者$Tail$变成随机替换成别的，如下</p>
<script type="math/tex; mode=display">
D^{-}=\{(h^{'},r,t)|h^{'}\in E\cap h^{'}\neq h \cap(h^{'},r,t)\notin D^{+} \}\\\cup\{(h,r,t^{'})|t^{'}\in E\cap t^{'}\neq t \cap(h,r,t^{'})\notin D^{+}\}</script><p>其中$E$为实体的集合。</p>
<p><strong>2.2 Illustrations of fine-tuning KG-BERT for predicting the relation between two entities</strong></p>
<p><img src="/2021/08/06/kg-bert/kgbert2.JPG" alt></p>
<p>作者发现直接使用两个实体去预测关系，效果优于使用两个实体和一个随机关系（这里本人认为一个随机的关系本来就是错误特征，感觉肯定会影响预测结果）。这里和2.1结构的差异在于：1.输入从实体加关系的三输入变成基于实体的双输入2.输出从二分类变成多分类</p>
<p>目标函数为：</p>
<script type="math/tex; mode=display">
S_{\tau}^{'}=f(h,r,t)=softmax(CW^{'T})</script><p>损失函数为$S^{‘}$和$y^{‘}$的交叉熵：</p>
<script type="math/tex; mode=display">
L^{'}=-\sum_{\tau \in D^{+}}\sum_{i=1}^{R}y_{\tau i}^{'}log(s^{'}_{\tau i})</script><h2 id="三-实验"><a href="#三-实验" class="headerlink" title="三.实验"></a>三.实验</h2><p>setting： We choose pre-trained BERT-Base model with 12 layers, 12 self-attention heads and H = 768 as the initialization of KG-BERT, then fine tune KG-BERT with Adam implemented in BERT. </p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://github.com/yao8839836/kg-bert">https://github.com/yao8839836/kg-bert</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/355391327">https://zhuanlan.zhihu.com/p/355391327</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/">知识图谱</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/KG-BERT/">KG-BERT</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-06  <a class="commentCountImg" href="/2021/08/06/search-rank-init/#comment-container"><span class="display-none-class">54c38d42e6b06e14ec6cde05f228369a</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="54c38d42e6b06e14ec6cde05f228369a">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>fast  <i class="fas fa-pencil-alt"> </i>0.0 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/06/search-rank-init/">搜索系统</a></h1><div class="content"><p>综述</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/112719984">https://zhuanlan.zhihu.com/p/112719984</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/382001982">https://zhuanlan.zhihu.com/p/382001982</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/davidwang456/articles/10251599.html">https://www.cnblogs.com/davidwang456/articles/10251599.html</a></p>
<p>DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.05649.pdf">https://arxiv.org/pdf/1710.05649.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/word2vec/#comment-container"><span class="display-none-class">5557345d5f80a07d7c4cb69fb82373ba</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="5557345d5f80a07d7c4cb69fb82373ba">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/word2vec/">word2vec</a></h1><div class="content"><h2 id="一-原理"><a href="#一-原理" class="headerlink" title="一.原理"></a>一.原理</h2><p><strong>两种训练模型</strong></p>
<ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<p><strong>训练技巧</strong></p>
<p>hierarchical softmax 和 negative sampling</p>
<h2 id="二-代码"><a href="#二-代码" class="headerlink" title="二.代码"></a>二.代码</h2><p><strong>训练代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure>
<p><strong>词向量矩阵</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.KeyedVectors.load_word2vec_format(model_path,binary=True)</span><br><span class="line">    print(model.vectors)   ##(779845, 400))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model.index_to_key)</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model[&quot;的&quot;])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[-1.3980628e+00, -4.6281612e-01,  5.8368486e-01, ...,         5.3952241e-01,  4.4697687e-01,  1.3505782e+00],       [ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, ...,         1.1110669e+00,  2.1992767e-01,  7.0457202e-01],       [-8.5650706e-01,  8.2832746e-02, -8.4218192e-01, ...,         2.1654253e+00,  6.4846051e-01, -5.7714492e-01],       ...,       [ 7.5072781e-03, -1.3543828e-02,  2.3101490e-02, ...,         4.2363801e-03, -5.6749382e-03,  6.3404259e-03],       [-2.6244391e-04, -3.0459568e-02,  5.9752418e-03, ...,         1.7844304e-02, -4.7109672e-04,  7.7916058e-03],       [ 7.2062697e-04, -6.5988898e-03,  1.1346856e-02, ...,        -3.7340564e-03, -1.8825980e-02,  2.7245486e-03]], dtype=float32)</span><br><span class="line"></span><br><span class="line">[&#x27;，&#x27;, &#x27;的&#x27;, &#x27;。&#x27;, &#x27;、&#x27;, &#x27;０&#x27;, &#x27;１&#x27;, &#x27;在&#x27;, &#x27;”&#x27;, &#x27;２&#x27;, &#x27;了&#x27;, &#x27;“&#x27;, &#x27;和&#x27;, &#x27;是&#x27;, &#x27;５&#x27;, ...]</span><br><span class="line"></span><br><span class="line">array([ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, -3.6405793e-01,        1.0851435e-01,  4.9507666e-02, -7.1219063e-01, -5.4614645e-01,       -1.3581418e+00,  3.0274218e-01,  6.1700332e-01,  3.5553512e-01,        1.6602433e+00,  7.5298291e-01, -1.4151905e-01, -2.1077128e-01,       -2.6325354e-01,  1.6108564e+00, -4.6750236e-01, -1.6261842e+00,        1.3063166e-01,  8.0702168e-01,  4.0011466e-01,  1.2198541e+00,       -6.2879241e-01,  ... 2.1928079e-01,  7.1725255e-01, -2.3430648e-01, -1.2066336e+00,        9.7590965e-01, -1.5906478e-01, -3.5802779e-01, -3.8005975e-01,        1.9056025e-01,  1.1110669e+00,  2.1992767e-01,  7.0457202e-01],      dtype=float32)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1301.3781v3">https://arxiv.org/abs/1301.3781v3</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1405.4053">https://arxiv.org/abs/1405.4053</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/short-chinese-text-match/#comment-container"><span class="display-none-class">e42e73069e4b8f9b20abc430afce93b1</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="e42e73069e4b8f9b20abc430afce93b1">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>8 m  <i class="fas fa-pencil-alt"> </i>1.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/short-chinese-text-match/">Neural Graph Matching Networks for Chinese Short Text Matching</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.547.pdf">https://aclanthology.org/2020.acl-main.547.pdf</a></p>
<h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>对于中文短文本匹配，通常基于词粒度而不是字粒度。但是分词结果可能是错误的、模糊的或不一致的，从而损害最终的匹配性能。比如下图：字符序列“南京市长江大桥”经过不同的分词可能表达为不同的意思。</p>
<p><img src="/2021/08/04/short-chinese-text-match/lattice.JPG" alt></p>
<p>为了解决这个问题，作者提出了一种基于图神经网络的中文短文本匹配方法。不是将句子分割成一个单词序列，而是保留所有可能的分割路径，形成一个Lattice（segment1，segment2，segment3），如上图所示。</p>
<h2 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2.问题定义"></a>2.问题定义</h2><p>将两个待匹配中文短文本分别定义为$S_a=\left \{ C_1^a,C_2^a,…,C_{t_a}^a \right \}$，$S_b=\left \{ C_1^b,C_2^b,…,C_{t_b}^b \right \}$，其中$C_i^a$表示句子$a$第$i$个字，$C_j^b$表示句子$b$第$j$个字，$t_a$，$t_b$分别表示两个句子的长度。$f(S_a,S_b)$是目标函数，输出为两个文本的匹配度。词格图用$G=(\nu,\xi)$表示，其中$\nu$是节点集，包括所有字符序列。$\xi$表示边集，如果$\nu$中两个顶点$v_i$和$v_j$相邻，那么就存在一个边为$e_{ij}$。$N_{fw}(v_i)$表示节点$v_i$ 正向的所有可达节点的集合,$N_{bw}(v_i)$表示节点$v_i$ 反向的所有可达节点的集合。句子$a$的词格图为$G^a(\nu_a,\xi_a)$，句子$b$的词格图为$G^b(\nu_b,\xi_b)$。</p>
<h2 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h2><p><img src="/2021/08/04/short-chinese-text-match/entire1.JPG" alt></p>
<p>模型分成3个部分，1.语言节点表示 2.图神经匹配 3.相关性分类器</p>
<h3 id="3-1-语言节点表示"><a href="#3-1-语言节点表示" class="headerlink" title="3.1 语言节点表示"></a>3.1 语言节点表示</h3><p>这一部分基于BERT的结构。BERT的token表示基于字粒度，可以得到$\left \{ [CLS],C_1^a,C_2^a,…,C_{ta}^a,[SEP],C_1^b,C_2^b,…,C_{t_b}^b,[SEP] \right \}$,如上图所示。BERT的输出为各个字的Embedding，$ \left \{\textbf{C}^{CLS},\textbf{C}_1^a,\textbf{C}_2^a,…,\textbf{C}_{t_a}^a,\textbf{C}^{SEP},\textbf{C}_1^b,\textbf{C}_2^b,…,\textbf{C}_{t_b},\textbf{C}^{SEP} \right \}$。</p>
<h3 id="3-2-图神经匹配"><a href="#3-2-图神经匹配" class="headerlink" title="3.2 图神经匹配"></a>3.2 图神经匹配</h3><p><strong>初始化</strong>：假设节点$v_i$包含$n_i$个连续字符，起始字符位置为$s_i$，即$ \left \{C_{s_i},C_{s_{i+1}},…,C_{s_{i}+n_i-1} \right \}$，这里$v_i$表示句子$a$或者$b$的结点。$V_i=\sum_{k=0}^{n_i-1}\textbf{U}_{s_i+k}\odot\textbf{C}_{s_i+k}$，其中$\odot$表示两个向量对应各个元素相乘。特征识别分数向量$\textbf{U}_{s_i+k}=softmax(FFN(\textbf{C}_{s_i+k}))$，$FFN$为两层。$h$为结点的向量表示，将$h_i^0$等于$V_i$</p>
<p><strong>Message Propagation</strong> : 对于第$l$次迭代，$G_a$中某个结点$v_i$由如下四个部分组成</p>
<script type="math/tex; mode=display">
m_i^{fw}=\sum_{v_j \in N_{fw}(v_i)}\alpha_{ij}(W^{fw}h_j^{l-1}),
\\m_i^{bw}=\sum_{v_k \in N_{bw}(v_i)}\alpha_{ik}(W^{bw}h_k^{l-1}),
\\m_i^{b1}=\sum_{v_m \in V^b}\alpha_{im}(W^{fw}h_m^{l-1}),
\\m_i^{b2}=\sum_{v_q \in V^b}\alpha_{iq}(W^{bw}h_q^{l-1})，</script><p>其中$\alpha_{ij},\alpha_{ik},\alpha_{im},\alpha_{iq}$是注意力系数，$W^{fw},W^{bw}$是注意力系数参数</p>
<p>然后定义两种信息为$m_i^{self}\triangleq[m_i^{fw},m_i^{bw}]，m_i^{cross}\triangleq[m_i^{b1},m_i^{b2}]$</p>
<p><strong>Representation Updating</strong>：得到两种信息后，需要更新结点$ v_i$的向量表示</p>
<script type="math/tex; mode=display">
d_k=cosine(w_k^{cos}\odot m_i^{self},w_k^{cos}\odot m_i^{cross})</script><p>其中$w_k^{cos}$为参数，$d_k$为multi-perspective cosine distance，可以衡量两种信息的距离，$k \in \left \{ 1,2,3,…P\right\}$，$P$是视角的数量。</p>
<script type="math/tex; mode=display">
h_i^l=FFN([m_i^{self},\textbf{d}_i])</script><p>其中$\textbf{d}_i\triangleq[d_1,d_2,…,d_P]$,$FFN$两层。</p>
<p><strong>句子的图级别表示</strong>：</p>
<p>总共经历了$L$次迭代（layer），得到$h_i^L$为结点$v_i$最终的向量表示（$h_i^L$includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph)</p>
<p>最终，两个句子的图级别表示分别为</p>
<script type="math/tex; mode=display">
g^a=attentive pooling(\left \{ h_{1a}^L,h_{2a}^L,...,h_{node-num_a a}^L \right \}),
\\g^b=attentive pooling(\left \{ h_{1b}^L,h_{2b}^L,...,h_{node-num_b b}^L \right \})</script><h3 id="3-3-分类器"><a href="#3-3-分类器" class="headerlink" title="3.3 分类器"></a>3.3 分类器</h3><p>得到$g^a,g^b$后，两句子的相似度可以用分类器衡量：</p>
<script type="math/tex; mode=display">
P=FFN([g^a,g^b,g^a \odot g^b,|g^a-g^b|])</script><p>其中$P \in [0,1]$。</p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4.实验结果"></a>4.实验结果</h2><p><img src="/2021/08/04/short-chinese-text-match/22.GIF" alt></p>
<p><img src="/2021/08/04/short-chinese-text-match/33.GIF" alt></p>
<p>lattice和JIEBA+PKU的区别？</p>
<p>JIEBA+PKU is a small lattice graph generated by merging two word segmentation results</p>
<p>lattice：overall lattice，应该是全部的组合</p>
<p>两者效果差不多是因为Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence).</p>
<p><img src="/2021/08/04/short-chinese-text-match/11.GIF" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43390809/article/details/114077216">https://blog.csdn.net/qq_43390809/article/details/114077216</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-02  <a class="commentCountImg" href="/2021/08/02/trie-tree/#comment-container"><span class="display-none-class">965c27c0899efd84889dab9fc80a80f0</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="965c27c0899efd84889dab9fc80a80f0">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>5 m  <i class="fas fa-pencil-alt"> </i>0.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/02/trie-tree/">字典树</a></h1><div class="content"><h2 id="一-核心思想"><a href="#一-核心思想" class="headerlink" title="一.核心思想"></a>一.核心思想</h2><p>Trie tree，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较。Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。字典树的查询时间复杂度是O (L)，L是待查字符串的长度。如果是普通的线性表结构，那么查询效率为O（NL），N为待查数据集的大小。</p>
<p>假设有b，abc，abd，bcd，abcd，efg，hii 这6个单词,那我们创建字典树如下：</p>
<p><img src="/2021/08/02/trie-tree/11.png" alt></p>
<h2 id="二-应用"><a href="#二-应用" class="headerlink" title="二.应用"></a>二.应用</h2><p><strong>目的</strong>：利用汉语拼音缩写还原中文汉字</p>
<p><strong>准备</strong>：数据集（包含中文汉字以及对应汉语缩写）</p>
<p><strong>思想</strong>：1.基于汉语拼音缩写<strong>检索</strong>出数据集中对应的所有中文汉字 2.基于中文汉字出现频次排序，将<strong>top1</strong>作为汉语拼音的还原结果</p>
<p><strong>代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">import  pandas as pd</span><br><span class="line">import pickle</span><br><span class="line">import os</span><br><span class="line">import joblib</span><br><span class="line"></span><br><span class="line">class TrieNode(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Initialize your data structure here.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.data = &#123;&#125;###字母字符</span><br><span class="line">        self.data1=&#123;&#125;###中文</span><br><span class="line">        self.is_word = False###标识是否汉字</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Trie(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.root = TrieNode()</span><br><span class="line"></span><br><span class="line">    def insert(self, word,word1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Inserts a word into the trie.</span><br><span class="line">        :type word: str</span><br><span class="line">        :rtype: void</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in word:</span><br><span class="line">            child = node.data.get(letter)</span><br><span class="line">            if not child:</span><br><span class="line">                node.data[letter] = TrieNode()</span><br><span class="line">            node = node.data[letter]</span><br><span class="line">        node.is_word = True</span><br><span class="line">        if word1 not in node.data1:</span><br><span class="line">            node.data1[word1]=1</span><br><span class="line">        else:</span><br><span class="line">            node.data1[word1]+=1</span><br><span class="line"></span><br><span class="line">    def search(self, word):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns if the word is in the trie.</span><br><span class="line">        :type word: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in word:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            if not node:</span><br><span class="line">                return False</span><br><span class="line">        return node.is_word</span><br><span class="line"></span><br><span class="line">    def starts_with(self, prefix):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns if there is any word in the trie</span><br><span class="line">        that starts with the given prefix.</span><br><span class="line">        :type prefix: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            if not node:</span><br><span class="line">                return False</span><br><span class="line">        return True</span><br><span class="line"></span><br><span class="line">    def get_start(self, prefix):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns words started with prefix</span><br><span class="line">        :param prefix:</span><br><span class="line">        :return: words (list)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        def _get_key(pre, pre_node):</span><br><span class="line">            words_list = []</span><br><span class="line">            if pre_node.is_word:</span><br><span class="line">                words_list.append([pre,pre_node.data1])</span><br><span class="line">            for x in pre_node.data.keys():</span><br><span class="line">                words_list.extend(_get_key(pre + str(x), pre_node.data.get(x)))</span><br><span class="line">            return words_list</span><br><span class="line"></span><br><span class="line">        words = []</span><br><span class="line">        if not self.starts_with(prefix):</span><br><span class="line">            return words</span><br><span class="line">        # if self.search(prefix):</span><br><span class="line">        #     words.append(prefix)</span><br><span class="line">        #     return words</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">        return _get_key(prefix, node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def find_result(self,string):</span><br><span class="line">        result =self.get_start(string)</span><br><span class="line">        result = sort_by_value(result[0][1])</span><br><span class="line">        result.reverse()</span><br><span class="line">        return result[0]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">def sort_by_value(d):</span><br><span class="line">    return sorted(d.items(), key=lambda k: k[1])  # k[1] 取到字典的值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_tree(data,save_path):</span><br><span class="line"></span><br><span class="line">    trie = Trie()</span><br><span class="line">    for element in data.values:</span><br><span class="line">        trie.insert(element[0], element[1])</span><br><span class="line">    joblib.dump(trie, save_path)</span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line">def load_tree(path):</span><br><span class="line">    trie = joblib.load(path)</span><br><span class="line">    return trie</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    ###</span><br><span class="line">    build_tree(data,save_path)</span><br><span class="line">    ###</span><br><span class="line">    tree=load_tree(save_path)</span><br><span class="line">    print(tree.find_result(&quot;XXXXXXXX&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28891541">https://zhuanlan.zhihu.com/p/28891541</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/">基础算法</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">数据结构</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/">字典树</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-28  <a class="commentCountImg" href="/2021/07/28/word-emb-add/#comment-container"><span class="display-none-class">27d8132878abc1f5fae21650cfc4df15</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="27d8132878abc1f5fae21650cfc4df15">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>2 m  <i class="fas fa-pencil-alt"> </i>0.3 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/28/word-emb-add/">nlp中使用预训练的词向量和随机初始化的词向量的区别在哪里？</a></h1><div class="content"><p>当你训练数据<strong>不充足</strong>的时候，可以直接使用别人已经预训练好的词向量，也可以根据自己的训练数据微调(fine-tuning)预训练词向量，也可以把词向量和整个模型一块训练，但是通常预训练的词向量我们不会再在训练的过程中进行更新。</p>
<p>当你的训练数据<strong>比较充足</strong>的时候，并且想让词向量能更好的捕捉自己的训练数据的语义信息时，应该使用随机初始化的词向量。当然，随机初始化的词向量必须要在训练网络的过程中不断进行更新，就和神经网络的权重参数一样进行训练。</p>
<p>例子：</p>
<p>1.直观展示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">###random</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">###from pretrain</span><br><span class="line">weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])</span><br><span class="line">embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">print(embedding.weight)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.1754,  1.6604, -1.5025, -1.0980, -0.4718],</span><br><span class="line">        [-1.1276,  0.1408, -1.0746, -1.2768, -0.6789]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.7366,  0.0607,  0.6151,  0.2282,  0.3878],</span><br><span class="line">        [-1.1365,  0.1844, -1.1191, -0.8787, -0.5121]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[1.0000, 2.3000, 3.0000],</span><br><span class="line">        [4.0000, 5.1000, 6.3000]])</span><br></pre></td></tr></table></figure>
<p>2.n-gram</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/337950427">https://www.zhihu.com/question/337950427</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/lasertagger/#comment-container"><span class="display-none-class">cb752746ba01f81f9394f0e9d77bdc74</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="cb752746ba01f81f9394f0e9d77bdc74">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>18 m  <i class="fas fa-pencil-alt"> </i>2.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/lasertagger/">LASERTAGGER</a></h1><div class="content"><h2 id="一-摘要"><a href="#一-摘要" class="headerlink" title="一. 摘要"></a>一. 摘要</h2><p>对于某一些文本生成任务，输入和输出的文本有很多的重叠部分，如果还是采用encoder-decoder的文本生成模型去从零开始生成，其实是很浪费和没必要的，并且会导致两个问题：1：生成模型的幻觉问题(就是模型胡说八道) ；2：出现叠词(部分片段一致)。</p>
<p>基于上面的考虑，作者提出了lasertagger模型，通过几个常用的操作：keep token、delete token、 add token，给输入序列的每个token打上标签，使得文本生成任务转化为了序列标注任务。</p>
<p>通过这种方式，相较于encoder-decoder模型的优势有如下：1、推理的速度更快 2、在较小的数据集上性能优于seq2seq baseline，在大数据集上和baseline持平（因为输入和输出的文本有很多的重叠部分，对于这种情况，lasertagger的候选词库比较小，因为对于重叠部分的词，词库只需要添加keep，而传统encoder-decoder的候选词库依然很大，因为对于重叠部分的词，词库需要添加对应的词）</p>
<h2 id="二-主要贡献"><a href="#二-主要贡献" class="headerlink" title="二.主要贡献"></a>二.主要贡献</h2><p>1、通过输入和输出文本，自动去提取需要add的token</p>
<p>2、通过输入文本，输出文本和tag集，给训练的输入序列打上标签</p>
<p>3、提出了两个版本，$LASERTAGGER_{AR}$( bert+transformer decoder )和$LASERTAGGER_{FF}$( bert+desen+softmax )</p>
<h2 id="三-整体流程"><a href="#三-整体流程" class="headerlink" title="三. 整体流程"></a>三. 整体流程</h2><p><img src="/2021/07/27/lasertagger/entire.JPG" alt></p>
<p>其实就是两个过程，一.将输入文本变编码成特殊标注，二.将标注解码成文本</p>
<h2 id="四-文本标注"><a href="#四-文本标注" class="headerlink" title="四. 文本标注"></a>四. 文本标注</h2><h3 id="4-1-Tag集构建（也就是label集构建）"><a href="#4-1-Tag集构建（也就是label集构建）" class="headerlink" title="4.1 Tag集构建（也就是label集构建）"></a>4.1 Tag集构建（也就是label集构建）</h3><p>一般情况，tag分为两个大类： base tag $B$和 add tag $P$。对于base tag，就是$KEEP$或者$DELETE$当前token；对于add tag，就是要添加一个词到token前面，添加的词来源于词表$V$。实际在工程中，将$B$和$P$结合来表示，即$^{P}B$，总的tag数量大约等于$B$的数量乘以$P$的数量，即$2|V|$。对于某些任务可以引入特定的tag，比如对于句子融合，可以引入$SWAP$,如下图。</p>
<p><img src="/2021/07/27/lasertagger/case.JPG" alt></p>
<h4 id="4-1-1-词表V的构建"><a href="#4-1-1-词表V的构建" class="headerlink" title="4.1.1 词表V的构建"></a>4.1.1 词表V的构建</h4><p><strong>构建目标：</strong></p>
<ol>
<li>最小化词汇表规模；</li>
<li>最大化目标词语的比例</li>
</ol>
<p>限制词汇表的词组数量可以减少相应输出的决策量；最大化目标词语的比例可以防止模型添加无效词。</p>
<p><strong>构建过程：</strong></p>
<p>通过$LCS$算法（longest common sequence，最长公共子序列，注意和最长公共子串不是一回事），找出输入和输出序列的最长公共子序列，输出剩下的序列，就是需要$add$的token，添加到词表$V$，词表中的词基于词频排序,然后选择$l$个常用的。</p>
<p>举个例子：soruce为“12345678”，target为”1264591”</p>
<p>​                    最长公共子序列为[‘1’, ‘2’, ‘4’, ‘5’]</p>
<p>​                    需要$add$的token为 [‘6’, ‘91’]</p>
<p><strong>源码</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def _lcs_table(source, target):</span><br><span class="line">  &quot;&quot;&quot;Returns the Longest Common Subsequence dynamic programming table.&quot;&quot;&quot;</span><br><span class="line">  rows = len(source)</span><br><span class="line">  cols = len(target)</span><br><span class="line">  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]</span><br><span class="line">  for i in range(1, rows + 1):</span><br><span class="line">    for j in range(1, cols + 1):</span><br><span class="line">      if source[i - 1] == target[j - 1]:</span><br><span class="line">        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1</span><br><span class="line">      else:</span><br><span class="line">        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])</span><br><span class="line">  return lcs_table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _backtrack(table, source, target, i, j):</span><br><span class="line">  &quot;&quot;&quot;Backtracks the Longest Common Subsequence table to reconstruct the LCS.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    table: Precomputed LCS table.</span><br><span class="line">    source: List of source tokens.</span><br><span class="line">    target: List of target tokens.</span><br><span class="line">    i: Current row index.</span><br><span class="line">    j: Current column index.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    List of tokens corresponding to LCS.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if i == 0 or j == 0:</span><br><span class="line">    return []</span><br><span class="line">  if source[i - 1] == target[j - 1]:</span><br><span class="line">    # Append the aligned token to output.</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j - 1) + [target[j - 1]]</span><br><span class="line">  if table[i][j - 1] &gt; table[i - 1][j]:</span><br><span class="line">    return _backtrack(table, source, target, i, j - 1)</span><br><span class="line">  else:</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j)</span><br><span class="line"></span><br><span class="line">def _compute_lcs(source, target):</span><br><span class="line">  # s1=&#123;1,3,4,5,6,7,7,8&#125;,s2=&#123;3,5,7,4,8,6,7,8,2&#125; return 35778</span><br><span class="line">  table = _lcs_table(source, target)</span><br><span class="line">  return _backtrack(table, source, target, len(source), len(target))</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def _get_added_phrases(source: Text, target: Text) -&gt; Sequence[Text]:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the phrases that need to be added to the source to get the target.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sep = &#x27;&#x27;</span><br><span class="line">    source_tokens = utils.get_token_list(source.lower())</span><br><span class="line">    target_tokens = utils.get_token_list(target.lower())</span><br><span class="line">    #compute Longest Common Subsequence</span><br><span class="line">    kept_tokens = _compute_lcs(source_tokens, target_tokens)</span><br><span class="line">    added_phrases = []</span><br><span class="line">    kept_idx = 0</span><br><span class="line">    phrase = []</span><br><span class="line">    for token in target_tokens:</span><br><span class="line">        if kept_idx &lt; len(kept_tokens) and token == kept_tokens[kept_idx]:</span><br><span class="line">            kept_idx += 1</span><br><span class="line">            if phrase:</span><br><span class="line">                added_phrases.append(sep.join(phrase))</span><br><span class="line">                phrase = []</span><br><span class="line">        else:</span><br><span class="line">            phrase.append(token)</span><br><span class="line">    if phrase:</span><br><span class="line">        added_phrases.append(sep.join(phrase))</span><br><span class="line">    return added_phrases</span><br></pre></td></tr></table></figure>
<p>词表位于文件label_map.txt.log，本人基于自己的数据集，内容如下所示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Idx Frequency  Coverage (%)   Phrase</span><br><span class="line">1  19 94.22  址</span><br><span class="line">2  15 95.27  单位</span><br><span class="line">3  8  95.76  地</span><br><span class="line">4  6  96.17  执勤</span><br></pre></td></tr></table></figure>
<h4 id="4-1-2-tag集"><a href="#4-1-2-tag集" class="headerlink" title="4.1.2 tag集"></a>4.1.2 tag集</h4><p>本人基于自己的数据集，得到的候选tag如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEEP</span><br><span class="line">DELETE</span><br><span class="line">KEEP|址</span><br><span class="line">DELETE|址</span><br><span class="line">KEEP|单位</span><br><span class="line">DELETE|单位</span><br><span class="line">KEEP|地</span><br><span class="line">DELETE|地</span><br><span class="line">KEEP|执勤</span><br><span class="line">DELETE|执勤</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Converting-Training-Targets-into-Tags"><a href="#4-2-Converting-Training-Targets-into-Tags" class="headerlink" title="4.2 Converting Training Targets into Tags"></a>4.2 Converting Training Targets into Tags</h3><p><strong>paper上的伪代码：</strong></p>
<p><img src="/2021/07/27/lasertagger/al1.JPG" alt></p>
<p>采用贪心策略，核心思想就是遍历$t$，先和$s$匹配，匹配上就$keep$，然后$i_t+j$，得到潜在的$add \ phrase \ p=t(i_t:i_t+j-1) $，然后判断$t(i_t+j)==s(i_s)\ and \ p\in V $</p>
<p><strong>源码</strong>：</p>
<p>和伪代码有一点不同，差异在于#####之间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">def _compute_single_tag(</span><br><span class="line">        self, source_token, target_token_idx,</span><br><span class="line">        target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes a single tag.</span><br><span class="line"></span><br><span class="line">    The tag may match multiple target tokens (via tag.added_phrase) so we return</span><br><span class="line">    the next unmatched target token.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_token: The token to be tagged.</span><br><span class="line">      target_token_idx: Index of the current target tag.</span><br><span class="line">      target_tokens: List of all target tokens.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A tuple with (1) the computed tag and (2) the next target_token_idx.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    source_token = source_token.lower()</span><br><span class="line">    target_token = target_tokens[target_token_idx].lower()</span><br><span class="line">    if source_token == target_token:</span><br><span class="line">        return tagging.Tag(&#x27;KEEP&#x27;), target_token_idx + 1</span><br><span class="line">    # source_token!=target_token</span><br><span class="line">    added_phrase = &#x27;&#x27;</span><br><span class="line">    for num_added_tokens in range(1, self._max_added_phrase_length + 1):</span><br><span class="line">        if target_token not in self._token_vocabulary:</span><br><span class="line">            break</span><br><span class="line">        added_phrase += (&#x27; &#x27; if added_phrase else &#x27;&#x27;) + target_token</span><br><span class="line">        next_target_token_idx = target_token_idx + num_added_tokens</span><br><span class="line">        if next_target_token_idx &gt;= len(target_tokens):</span><br><span class="line">            break</span><br><span class="line">        target_token = target_tokens[next_target_token_idx].lower()</span><br><span class="line">        if (source_token == target_token and</span><br><span class="line">                added_phrase in self._phrase_vocabulary):</span><br><span class="line">            return tagging.Tag(&#x27;KEEP|&#x27; + added_phrase), next_target_token_idx + 1</span><br><span class="line">    return tagging.Tag(&#x27;DELETE&#x27;), target_token_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        ####################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        ########################################################################################</span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<p><strong>缺陷</strong>：</p>
<p>对于一些情况，无法还原，举个例子：</p>
<p>​        source：证件有效期截止日期  target：证件日期格式</p>
<p>​        得不到tag结果</p>
<p>可以补充策略来修复bug</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        #########################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        #######################################################################################</span><br><span class="line"></span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    ####fix bug by lavine</span><br><span class="line"></span><br><span class="line">    ###strategy1</span><br><span class="line">    added_phrase = &quot;&quot;.join(target_tokens[target_token_idx:])</span><br><span class="line">    if added_phrase in self._phrase_vocabulary:</span><br><span class="line">        tags[-1] = tagging.Tag(&#x27;DELETE|&#x27; + added_phrase)</span><br><span class="line">        print(&#x27;&#x27;.join(source_tokens))</span><br><span class="line">        print(&#x27;&#x27;.join(target_tokens))</span><br><span class="line">        print(str([str(tag) for tag in tags] if tags != None else None))</span><br><span class="line">        return tags</span><br><span class="line">    ###strategy2</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<h3 id="4-3-模型结构"><a href="#4-3-模型结构" class="headerlink" title="4.3 模型结构"></a>4.3 模型结构</h3><p><img src="/2021/07/27/lasertagger/fr.JPG" alt></p>
<p>模型主要包含两个部分：1.encoder:generates activation vectors for each element in the input sequence 2.decoder：converts encoder activations into tag labels</p>
<h4 id="4-3-1-encoder"><a href="#4-3-1-encoder" class="headerlink" title="4.3.1 encoder"></a>4.3.1 encoder</h4><p>由于$BERT$在sentence encoding tasks上做到state-of-the-art，所以使用$BERT$ 作为encoder部分。作者选择了$BERT_{base}$,包含12个self-attention层</p>
<h4 id="4-3-2-decoder"><a href="#4-3-2-decoder" class="headerlink" title="4.3.2 decoder"></a>4.3.2 decoder</h4><p>在$BERT$原文中，对于标注任务采取了非常简单的decoder结构，即采用一层feed-forward作为decoder，把这种组合叫做$LASERTAGGER_{FF}$，这种结构的缺点在于预测的标注词相互独立，没有考虑标注词的关联性。</p>
<p>为了考虑标注词的关联性，decode使用了Transformer decoder，单向连接，记作$LASERTAGGER_{AR}$，这种encoder和decoder的组合的有点像BERT结合GPT的感觉decoder 和encoder在以下方面交流：(i) through a full attention over the sequence of encoder activations (ii) by directly consuming the encoder activation at the current step</p>
<h2 id="五-realize"><a href="#五-realize" class="headerlink" title="五.realize"></a>五.realize</h2><p>对于基本的tag，比如$KEEP$，$DELETE$，$ADD$，$realize$就是根据输入和tag直接转换就行；对于特殊的tag，需要一些特定操作，看情况维护规则。</p>
<h2 id="六-loss"><a href="#六-loss" class="headerlink" title="六 loss"></a>六 loss</h2><p>假设句子长度为n，tag数量为m, loss为n个m分类任务的和</p>
<h2 id="七-评价指标"><a href="#七-评价指标" class="headerlink" title="七.评价指标"></a>七.评价指标</h2><p>评价指标，不同任务不同评价指标</p>
<p>1 Sentence Fusion</p>
<p>Exact score ：percentage of exactly correctly predicted fusions（类似accuracy）</p>
<p>SARI ：average F1 scores of the added, kept, and deleted n-grams</p>
<p>2 Split and Rephrase</p>
<p>SARI</p>
<p>3 Abstractive Summarization</p>
<p>ROUGE-L</p>
<p>4 Grammatical Error Correction (GEC)</p>
<p>precision and recall, F0:5</p>
<h2 id="八-实验结果"><a href="#八-实验结果" class="headerlink" title="八.实验结果"></a>八.实验结果</h2><p><strong>baseline</strong>： based on Transformer where both the encoder and decoder replicate the $BERT_{base}$ architecture</p>
<p><strong>速度</strong>：1.$LASERTAGGER_{AR} $is already 10x faster than comparable-in-accuracy $SEQ2SEQ_{BERT}$ baseline. This difference is due to the former model using a 1-layer decoder (instead of 12 layers) and no encoder-decoder cross attention. 2.$LASERTAGGER_{FF}$ is more than 100x faster</p>
<p>其余结果参考paper</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01187.pdf">https://arxiv.org/pdf/1909.01187.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/lasertagger">https://github.com/google-research/lasertagger</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348109034">https://zhuanlan.zhihu.com/p/348109034</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/">文本改写</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/sentence-bert/#comment-container"><span class="display-none-class">6886a6840e722c992035060d31a77303</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="6886a6840e722c992035060d31a77303">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.6 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/sentence-bert/">Sentence-BERT Sentence Embeddings using Siamese BERT-Networks</a></h1><div class="content"><p>paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p>
<p>giit: <a target="_blank" rel="noopener" href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications">https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications</a></p>
<h2 id="1-贡献"><a href="#1-贡献" class="headerlink" title="1.贡献"></a>1.贡献</h2><p>基于bert利用孪生结构或者三胞胎结构训练，使得产生在低维空间可用的句子Embedding。对于文本匹配任务，可以离线计算句子Embedding，然后基于句子Embedding在线匹配，可实现快速高精度的匹配。</p>
<h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/07/27/sentence-bert/s-bert1.JPG" alt></p>
<p>文章提出三种结构和目标函数，三胞胎结构作者没有画图</p>
<p>1.Classification Objective Function</p>
<script type="math/tex; mode=display">
loss=cross-entropy(softmax(W_t(u,v,|u-v|)),y_{true})</script><p>2.Regression Objective Function</p>
<script type="math/tex; mode=display">
loss=MSE(cosine-sim(u, v),y_{true})</script><p>3.Triplet Objective Function</p>
<script type="math/tex; mode=display">
loss=max(||s_a-s_p||-||s_a-s_n||+\sigma,0)</script><p>$||.||$计算向量距离，$s_a$为样本本身，$s_p$为正样本，$s_n$为负样本，$\sigma$使得正样本至少比负样本距离样本近$\sigma$。</p>
<p>对于pooling，文章提出三种策略</p>
<p>1.Using the output of the CLS-token<br>2.computing the mean of all output vectors (MEAN_strategy)<br>3.computing a max-over-time of the output vectors (MAX_strategy). The default configuration is MEAN.</p>
<h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h2><h3 id="3-1-Unsupervised-STS"><a href="#3-1-Unsupervised-STS" class="headerlink" title="3.1 Unsupervised STS"></a>3.1 Unsupervised STS</h3><p><img src="/2021/07/27/sentence-bert/11.JPG" alt></p>
<h3 id="3-2-Supervised-STS"><a href="#3-2-Supervised-STS" class="headerlink" title="3.2 Supervised STS"></a>3.2 Supervised STS</h3><p><img src="/2021/07/27/sentence-bert/22.JPG" alt></p>
<h3 id="3-3-Argument-Facet-Similarity"><a href="#3-3-Argument-Facet-Similarity" class="headerlink" title="3.3 Argument Facet Similarity"></a>3.3 Argument Facet Similarity</h3><p><img src="/2021/07/27/sentence-bert/33.JPG" alt></p>
<h3 id="3-4-Wikipedia-Sections-Distinction"><a href="#3-4-Wikipedia-Sections-Distinction" class="headerlink" title="3.4 Wikipedia Sections Distinction"></a>3.4 Wikipedia Sections Distinction</h3><p>We use the Triplet Objective</p>
<p><img src="/2021/07/27/sentence-bert/44.JPG" alt></p>
<h2 id="4-代码"><a href="#4-代码" class="headerlink" title="4.代码"></a>4.代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from sentence_bert.sentence_transformers import SentenceTransformer, util</span><br><span class="line"></span><br><span class="line">###load model</span><br><span class="line">model = SentenceTransformer(model_path)</span><br><span class="line"></span><br><span class="line"># Single list of sentences</span><br><span class="line">sentences = [&#x27;The cat sits outside&#x27;,</span><br><span class="line">             &#x27;A man is playing guitar&#x27;,</span><br><span class="line">             &#x27;I love pasta&#x27;,</span><br><span class="line">             &#x27;The new movie is awesome&#x27;,</span><br><span class="line">             &#x27;The cat plays in the garden&#x27;,</span><br><span class="line">             &#x27;A woman watches TV&#x27;,</span><br><span class="line">             &#x27;The new movie is so great&#x27;,</span><br><span class="line">             &#x27;Do you like pizza?&#x27;]</span><br><span class="line"></span><br><span class="line">#Compute embeddings</span><br><span class="line">embeddings = model.encode(sentences, convert_to_tensor=True)</span><br><span class="line"></span><br><span class="line">#Compute cosine-similarities for each sentence with each other sentence</span><br><span class="line">cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)</span><br><span class="line"></span><br><span class="line">#Find the pairs with the highest cosine similarity scores</span><br><span class="line">pairs = []</span><br><span class="line">for i in range(len(cosine_scores)-1):</span><br><span class="line">    for j in range(i+1, len(cosine_scores)):</span><br><span class="line">        pairs.append(&#123;&#x27;index&#x27;: [i, j], &#x27;score&#x27;: cosine_scores[i][j]&#125;)</span><br><span class="line"></span><br><span class="line">#Sort scores in decreasing order</span><br><span class="line">pairs = sorted(pairs, key=lambda x: x[&#x27;score&#x27;], reverse=True)</span><br><span class="line"></span><br><span class="line">for pair in pairs[0:10]:</span><br><span class="line">    i, j = pair[&#x27;index&#x27;]</span><br><span class="line">    print(&quot;&#123;&#125; \t\t &#123;&#125; \t\t Score: &#123;:.4f&#125;&quot;.format(sentences[i], sentences[j], pair[&#x27;score&#x27;]))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">The new movie is awesome 		 The new movie is so great 		 Score: 0.9283</span><br><span class="line">The cat sits outside 		 The cat plays in the garden 		 Score: 0.6855</span><br><span class="line">I love pasta 		 Do you like pizza? 		 Score: 0.5420</span><br><span class="line">I love pasta 		 The new movie is awesome 		 Score: 0.2629</span><br><span class="line">I love pasta 		 The new movie is so great 		 Score: 0.2268</span><br><span class="line">The new movie is awesome 		 Do you like pizza? 		 Score: 0.1885</span><br><span class="line">A man is playing guitar 		 A woman watches TV 		 Score: 0.1759</span><br><span class="line">The new movie is so great 		 Do you like pizza? 		 Score: 0.1615</span><br><span class="line">The cat plays in the garden 		 A woman watches TV 		 Score: 0.1521</span><br><span class="line">The cat sits outside 		 The new movie is awesome 		 Score: 0.1475</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2></div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">文本表示</a></div><hr></div></article></div><!--!--><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/40/">Previous</a></div><div class="pagination-next"><a href="/page/42/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/40/">40</a></li><li><a class="pagination-link is-current" href="/page/41/">41</a></li><li><a class="pagination-link" href="/page/42/">42</a></li><li><a class="pagination-link" href="/page/43/">43</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">425</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">137</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">405</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:56:21.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/jar/">jar</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:56:07.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-compile/">java编译</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T06:29:22.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/gc/">gc</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T05:37:12.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-optimize/">java性能优化</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-14T05:36:18.000Z">2024-04-14</time></p><p class="title"><a href="/2024/04/14/java-mem/">java内存泄露</a></p><p class="categories"><a href="/categories/java/">java</a> / <a href="/categories/java/java/">java</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">37</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>