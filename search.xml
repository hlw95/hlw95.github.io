<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>c++语法</title>
      <link href="/2023/05/03/cpp-lang/"/>
      <url>/2023/05/03/cpp-lang/</url>
      
        <content type="html"><![CDATA[<h2 id="1-头文件"><a href="#1-头文件" class="headerlink" title="1 头文件"></a>1 头文件</h2><p><a href="https://www.runoob.com/w3cnote/cpp-header.html">https://www.runoob.com/w3cnote/cpp-header.html</a></p><h3 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h3><p>方便代码复用</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/* math.h */</span><br><span class="line">double f1();</span><br><span class="line">double f2(double);</span><br><span class="line">/* end of math.h */</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/* math.cpp */</span><br><span class="line">double f1()</span><br><span class="line">&#123;</span><br><span class="line">    //do something here....</span><br><span class="line">    return;</span><br><span class="line">&#125;</span><br><span class="line">double f2(double a)</span><br><span class="line">&#123;</span><br><span class="line">    //do something here...</span><br><span class="line">    return a * a;</span><br><span class="line">&#125;</span><br><span class="line">/* end of math.cpp */</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/* main.cpp */</span><br><span class="line">#include &quot;math.h&quot;</span><br><span class="line">main()</span><br><span class="line">&#123;</span><br><span class="line">    int number1 = f1();</span><br><span class="line">    int number2 = f2(number1);</span><br><span class="line">&#125;</span><br><span class="line">/* end of main.cpp */</span><br></pre></td></tr></table></figure><p>include 的作用就是把每一个它出现的地方，替换成它后面所写的那个文件的内容。简单的文本替换，别无其他。（预编译）</p><p>1 预编译</p><p>main.cpp首先include “math.h”，就是把math.h的东西全部替换过来</p><p>2 编译 </p><p>main.cpp f1，f2的声明 变 符号表</p><p>3 链接</p><p>根据符号表就可以在math.cpp找到f1，f2的实现</p><h3 id="头文件中应该写什么"><a href="#头文件中应该写什么" class="headerlink" title="头文件中应该写什么"></a>头文件中应该写什么</h3><p>.h文件中能包含：</p><ul><li>类成员数据的声明，但不能赋值</li><li>类静态数据成员的定义和赋值，但不建议，只是个声明就好。</li><li>类的成员函数的声明</li><li>非类成员函数的声明</li><li>常数的定义：如：constint a=5;</li><li>静态函数的定义</li><li>类的内联函数的定义</li></ul><p>不能包含：</p><ul><li>1.所有非静态变量（不是类的数据成员）的声明</li><li>2.默认命名空间声明不要放在头文件，using namespace std;等应放在.cpp中，在 .h 文件中使用 std::string</li></ul><h2 id="2-内联函数"><a href="#2-内联函数" class="headerlink" title="2 内联函数"></a>2 内联函数</h2><p><a href="https://zhuanlan.zhihu.com/p/375828786">https://zhuanlan.zhihu.com/p/375828786</a></p><p><img src="/2023/05/03/cpp-lang/image-20240310170013989.png" alt="image-20240310170013989"></p><p>好处:效率</p><p>坏处:代码膨胀</p><h2 id="3-存储类"><a href="#3-存储类" class="headerlink" title="3 存储类"></a>3 存储类</h2><p><a href="https://zhuanlan.zhihu.com/p/61853231">https://zhuanlan.zhihu.com/p/61853231</a>    </p><h3 id="static"><a href="#static" class="headerlink" title="static"></a>static</h3><p><strong>1 变量</strong></p><p>只能在定义时初始化，以后不行   如果没有显式初始化，会被程序自动初始化为0    静态成员变量必须类外初始化</p><p>生命周期和全局变量一样  程序结束才释放</p><p>就一个 公用</p><p>作用域：</p><p>​    静态成员变量：类内</p><p>​    静态全局变量：本文件</p><p>​    静态局部变量：函数内</p><p><strong>2 函数</strong></p><p>静态成员函数：和类绑定 和对象没有关系</p><p>静态函数：只能在本文件用</p><h3 id="extern"><a href="#extern" class="headerlink" title="extern"></a>extern</h3><p><a href="https://www.51cto.com/article/768503.html">https://www.51cto.com/article/768503.html</a></p><p><a href="https://blog.csdn.net/w2865673691/article/details/13018563">https://blog.csdn.net/w2865673691/article/details/13018563</a></p><p><a href="https://blog.csdn.net/xingjiarong/article/details/47656339">https://blog.csdn.net/xingjiarong/article/details/47656339</a></p><p><strong>extern</strong>   </p><p>原理：就是先声明 告诉编译器定义在别的地方</p><p>默认值：全局变量和函数默认 extern</p><p>作用：在本文件使用别的文件的变量和函数   和include 的区别？？？</p><p><strong>extern “c”</strong>  </p><p>表示按照c语言的方式去编译</p><h2 id="4-内存分配"><a href="#4-内存分配" class="headerlink" title="4 内存分配"></a>4 内存分配</h2><p><a href="https://developer.aliyun.com/article/343804">https://developer.aliyun.com/article/343804</a></p><h2 id="5-函数"><a href="#5-函数" class="headerlink" title="5 函数"></a>5 函数</h2><h3 id="函数声明"><a href="#函数声明" class="headerlink" title="函数声明"></a>函数声明</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return_type function_name( parameter list );</span><br></pre></td></tr></table></figure><h3 id="函数参数"><a href="#函数参数" class="headerlink" title="函数参数"></a>函数参数</h3><p>传值 传指针 传引用</p><p>参数的默认值</p><p>double&amp; setValues(int&amp; i) </p><p>int &amp; i = 传入的变量</p><h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><p>原理</p><p><a href="https://blog.csdn.net/jmh1996/article/details/78384083">https://blog.csdn.net/jmh1996/article/details/78384083</a></p><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><p><a href="https://www.runoob.com/cplusplus/cpp-functions.html">https://www.runoob.com/cplusplus/cpp-functions.html</a></p><h2 id="7-STL"><a href="#7-STL" class="headerlink" title="7 STL"></a>7 STL</h2><h2 id="8-静态库-动态库-静态链接-动态链接"><a href="#8-静态库-动态库-静态链接-动态链接" class="headerlink" title="8 静态库 动态库  静态链接 动态链接"></a>8 静态库 动态库  静态链接 动态链接</h2><p><a href="https://www.runoob.com/w3cnote/cpp-static-library-and-dynamic-library.html">https://www.runoob.com/w3cnote/cpp-static-library-and-dynamic-library.html</a></p><h4 id="静态库-动态库"><a href="#静态库-动态库" class="headerlink" title="静态库 动态库"></a>静态库 动态库</h4><p>库是写好的现有的，成熟的，可以复用的代码  和python的 包一样</p><p>可以编译生成静态库和动态库</p><p>使用？？？</p><h4 id="静态链接-动态链接"><a href="#静态链接-动态链接" class="headerlink" title="静态链接 动态链接"></a>静态链接 动态链接</h4><p>静态链接：静态库在程序编译时会被连接到目标代码中，程序运行时将不再需要该静态库，因此体积较大。</p><p> 动态链接：动态库在程序编译时并不会被连接到目标代码中，而是在程序运行是才被载入，因此在程序运行时还需要动态库存在，因此代码体积较小.不同的应用程序如果调用相同的库，那么在内存里只需要有一份该共享库的实例。带来好处的同时，也会有问题！如经典的DLL Hell问题，关于如何规避动态库管理问题</p><h2 id="9-声明-定义"><a href="#9-声明-定义" class="headerlink" title="9 声明 定义"></a>9 声明 定义</h2><p><a href="https://bbs.huaweicloud.com/blogs/292599">https://bbs.huaweicloud.com/blogs/292599</a></p><p><strong>变量</strong></p><p>声明：告诉编译器变量的名称和类型，而不分配内存</p><p>定义：给变量分配内存，可以为变量赋初值</p><p><strong>函数</strong></p><p>函数只要有实现即为定义，否则为声明</p><p><strong>注意</strong></p><p>声明要在使用前面 否则报错</p><h2 id="10-数组"><a href="#10-数组" class="headerlink" title="10 数组"></a>10 数组</h2><p>数组名就是指向第一个元素的地址</p><p>数组作为函数返回值   返回指针</p><p>数组作为函数参数 传指针 </p><h2 id="11-指针"><a href="#11-指针" class="headerlink" title="11 指针"></a>11 指针</h2><h3 id="1-裸指针"><a href="#1-裸指针" class="headerlink" title="1 裸指针"></a>1 裸指针</h3><h4 id="野指针"><a href="#野指针" class="headerlink" title="野指针"></a>野指针</h4><p><a href="https://zhuanlan.zhihu.com/p/337060273">https://zhuanlan.zhihu.com/p/337060273</a></p><h4 id="指针运算"><a href="#指针运算" class="headerlink" title="指针运算"></a>指针运算</h4><p>指针是一个用数值表示的地址  所以可以数值运算</p><p>可以+ -  （地址偏移取决于指针类型）比较</p><h4 id="指针数组"><a href="#指针数组" class="headerlink" title="指针数组"></a>指针数组</h4><p>int *ptr[MAX];</p><p>在这里，把 ptr 声明为一个数组，由 MAX 个整数指针组成</p><h4 id="指向指针的指针"><a href="#指向指针的指针" class="headerlink" title="指向指针的指针"></a>指向指针的指针</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int **var;</span><br></pre></td></tr></table></figure><h2 id="12-引用"><a href="#12-引用" class="headerlink" title="12 引用"></a>12 引用</h2><h3 id="函数返回值"><a href="#函数返回值" class="headerlink" title="函数返回值"></a>函数返回值</h3><p>原理？？？</p><h2 id="13-左值右值"><a href="#13-左值右值" class="headerlink" title="13 左值右值"></a>13 左值右值</h2><p><a href="https://nettee.github.io/posts/2018/Understanding-lvalues-and-rvalues-in-C-and-C/">https://nettee.github.io/posts/2018/Understanding-lvalues-and-rvalues-in-C-and-C/</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a; <span class="comment">// a 为左值</span></span><br><span class="line">a = <span class="number">3</span>; <span class="comment">// 3 为右值</span></span><br></pre></td></tr></table></figure><h2 id="14-类中定义自身类的对象"><a href="#14-类中定义自身类的对象" class="headerlink" title="14 类中定义自身类的对象"></a>14 类中定义自身类的对象</h2><p><a href="https://blog.csdn.net/digitalkee/article/details/104526994">https://blog.csdn.net/digitalkee/article/details/104526994</a></p><p><a href="https://blog.csdn.net/shltsh/article/details/45949347">https://blog.csdn.net/shltsh/article/details/45949347</a></p><p>指针和引用可以</p><p>static对象可以 </p><p>non-static对象 只有普通函数可以  成员变量不行  构造函数不行</p><h2 id="15-变量作用域"><a href="#15-变量作用域" class="headerlink" title="15 变量作用域"></a>15 变量作用域</h2><ul><li><strong>局部作用域</strong>：在函数内部声明的变量具有局部作用域，它们只能在函数内部访问。<strong>局部变量在函数每次被调用时被创建，在函数执行完后被销毁。</strong></li><li><strong>全局作用域</strong>：在所有函数和代码块之外声明的变量具有全局作用域，它们可以被程序中的任何函数访问。<strong>全局变量在程序开始时被创建，在程序结束时被销毁。</strong></li><li><strong>类作用域</strong>：在类内部声明的变量具有类作用域，它们可以被类的所有成员函数访问。<strong>类作用域变量的生命周期与类的生命周期相同**</strong>。</li></ul>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++语法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++简介</title>
      <link href="/2023/04/16/cpp_init/"/>
      <url>/2023/04/16/cpp_init/</url>
      
        <content type="html"><![CDATA[<h2 id="1-构建工具"><a href="#1-构建工具" class="headerlink" title="1 构建工具"></a>1 构建工具</h2><h3 id="编译过程"><a href="#编译过程" class="headerlink" title="编译过程"></a>编译过程</h3><p><img src="/2023/04/16/cpp_init/1.PNG" alt></p><p><a href="https://developer.aliyun.com/article/1154672">https://developer.aliyun.com/article/1154672</a></p><h3 id="命令行编译"><a href="#命令行编译" class="headerlink" title="命令行编译"></a>命令行编译</h3><h3 id="为什么要构建工具呢？"><a href="#为什么要构建工具呢？" class="headerlink" title="为什么要构建工具呢？"></a>为什么要构建工具呢？</h3><p>当你的程序只有一个源文件时，直接就可以用gcc命令编译它。但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大，为什么呢？<br>因为各个文件之间还涉及到互相访问与链接，错综复杂的关系一个一个处理很麻烦，很容易出错，素衣需要一个工具来制定一个很好的编译规则，这就是make的作用了</p><h3 id="构建工具"><a href="#构建工具" class="headerlink" title="构建工具"></a>构建工具</h3><p>cmake</p><p>bazel</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><h4 id="1-vs-Visual-Studio-为什么可以直接编译一个工程"><a href="#1-vs-Visual-Studio-为什么可以直接编译一个工程" class="headerlink" title="1 vs(Visual Studio)为什么可以直接编译一个工程"></a>1 vs(Visual Studio)为什么可以直接编译一个工程</h4><p>因为vs创建的是一个工程，所有添加的文件都有链接管理</p><h2 id="2-ide"><a href="#2-ide" class="headerlink" title="2 ide"></a>2 ide</h2><p>clion</p><p>vscode</p>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> c++简介 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python多版本兼容</title>
      <link href="/2022/10/05/python-multi-version/"/>
      <url>/2022/10/05/python-multi-version/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/sunyllove/p/13521471.html">https://www.cnblogs.com/sunyllove/p/13521471.html</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python多版本兼容 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多线程</title>
      <link href="/2022/10/04/python-multi-thread/"/>
      <url>/2022/10/04/python-multi-thread/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多线程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设计模式</title>
      <link href="/2022/10/04/python-designmode/"/>
      <url>/2022/10/04/python-designmode/</url>
      
        <content type="html"><![CDATA[<h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><p>SOLID</p><p><a href="https://www.cnblogs.com/wuyuegb2312/p/7011708.html">https://www.cnblogs.com/wuyuegb2312/p/7011708.html</a></p><h2 id="23种设计模式"><a href="#23种设计模式" class="headerlink" title="23种设计模式"></a>23种设计模式</h2><p>设计模式是经过总结、优化的，对我们经常会碰到的一些编程问题的<strong>可重用解决方案</strong></p><p><a href="https://blog.csdn.net/weicao1990/article/details/79108193">https://blog.csdn.net/weicao1990/article/details/79108193</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>异常处理</title>
      <link href="/2022/09/18/python-exception/"/>
      <url>/2022/09/18/python-exception/</url>
      
        <content type="html"><![CDATA[<h2 id="try-exception-finally"><a href="#try-exception-finally" class="headerlink" title="try-exception-finally"></a>try-exception-finally</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">   XXX</span><br><span class="line">exception XXX:</span><br><span class="line">   XXXXX</span><br><span class="line">finally:</span><br><span class="line">XXXX</span><br></pre></td></tr></table></figure><h2 id="raise"><a href="#raise" class="headerlink" title="raise"></a>raise</h2><p><a href="https://blog.csdn.net/qq_35541155/article/details/108809130">https://blog.csdn.net/qq_35541155/article/details/108809130</a></p><p>一般情况，遇到特定代码错误，才会触发特定异常，raise可以人工触发特定异常，比如下面，没有raise 除非try里面有ValueError的代码错误，except才会捕获，有了raise后，可以人工触发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">    a = input(&quot;输入一个数：&quot;)</span><br><span class="line">    if(not a.isdigit()):</span><br><span class="line">        raise ValueError(&quot;a 必须是数字&quot;)</span><br><span class="line">except ValueError as e:</span><br><span class="line">    print(&quot;引发异常：&quot;,repr(e))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 异常处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变量</title>
      <link href="/2022/09/18/python-variable/"/>
      <url>/2022/09/18/python-variable/</url>
      
        <content type="html"><![CDATA[<h2 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h2><h2 id="变量类型"><a href="#变量类型" class="headerlink" title="变量类型"></a>变量类型</h2><p>LEGB</p><ul><li>locals 包括局部变量和形参</li><li>enclosing 嵌套函数的名字空间（闭包中常见）</li><li>globals 全局变量</li><li>builtins 内置模块的名字空间</li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 变量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运算</title>
      <link href="/2022/09/12/python-cal-symbol/"/>
      <url>/2022/09/12/python-cal-symbol/</url>
      
        <content type="html"><![CDATA[<h2 id="1-逻辑运算"><a href="#1-逻辑运算" class="headerlink" title="1. 逻辑运算"></a>1. 逻辑运算</h2><div class="table-container"><table><thead><tr><th>operation</th><th>result</th></tr></thead><tbody><tr><td>x or y</td><td>if x is false，then y ,else x</td></tr><tr><td>x and y</td><td>if x is false，then x ,else y</td></tr><tr><td>not x</td><td>if x is false，then True , else False</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 运算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则</title>
      <link href="/2022/09/12/python-pattern/"/>
      <url>/2022/09/12/python-pattern/</url>
      
        <content type="html"><![CDATA[<h2 id="1-re-match"><a href="#1-re-match" class="headerlink" title="1 re.match"></a>1 re.match</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matchObj/None = re.match(pattern, string, flags=0)</span><br></pre></td></tr></table></figure><h2 id="2-re-search"><a href="#2-re-search" class="headerlink" title="2 re.search"></a>2 re.search</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matchObj/None = re.search(pattern, string, flags=0)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lambda</title>
      <link href="/2022/09/12/lamdba/"/>
      <url>/2022/09/12/lamdba/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def test01(a,b,c,d):</span><br><span class="line">    return a*b*c*d</span><br><span class="line">print(test01(1,2,3,4))</span><br><span class="line"></span><br><span class="line">#相当于下面这个函数</span><br><span class="line"></span><br><span class="line">f=lambda a,b,c,d:a*b*c*d</span><br><span class="line">print(f(1,2,3,4))  </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print([f(2) for f in [lambda x:i*x for i in range(5)]])</span><br><span class="line">print([f(2) for f in [lambda x,i=i:i*x for i in range(5)]])</span><br><span class="line">print([(lambda x:i*x)(2) for i in range(5)])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[8, 8, 8, 8, 8]</span><br><span class="line">[0, 2, 4, 6, 8]</span><br><span class="line">[0, 2, 4, 6, 8]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">先声5个 然后调用  取最后i</span><br><span class="line">先声5个 然后调用  但是指定i了</span><br><span class="line">一边声明 一边调用</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lambda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字符串</title>
      <link href="/2022/09/12/python-string/"/>
      <url>/2022/09/12/python-string/</url>
      
        <content type="html"><![CDATA[<h2 id="1-格式化"><a href="#1-格式化" class="headerlink" title="1 格式化"></a>1 格式化</h2><p><a href="https://blog.csdn.net/zjbyough/article/details/96466658">https://blog.csdn.net/zjbyough/article/details/96466658</a></p><h3 id="f-string"><a href="#f-string" class="headerlink" title="f-string"></a>f-string</h3><p><a href="https://blog.csdn.net/sunxb10/article/details/81036693">https://blog.csdn.net/sunxb10/article/details/81036693</a></p><h3 id><a href="#" class="headerlink" title="%"></a>%</h3><h3 id="format"><a href="#format" class="headerlink" title="format"></a>format</h3>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字符串 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可迭代对象、迭代器与生成器</title>
      <link href="/2022/09/12/python-iterator/"/>
      <url>/2022/09/12/python-iterator/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/09/12/python-iterator/1.png" alt></p><h2 id="可迭代对象"><a href="#可迭代对象" class="headerlink" title="可迭代对象"></a>可迭代对象</h2><p><strong> iter </strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class IsIterable:</span><br><span class="line">        def __iter__(self):</span><br><span class="line">            return self</span><br></pre></td></tr></table></figure><h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><p><strong> iter </strong>  +<strong> next </strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; class MyIterator:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        迭代器类</span><br><span class="line">        Author：可乐python说</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        def __init__(self):</span><br><span class="line">            self.num = 0</span><br><span class="line">        def __iter__(self):</span><br><span class="line">            return self</span><br><span class="line">        def __next__(self):</span><br><span class="line">            return_num = self.num</span><br><span class="line">            # 只要值大于等于6，就停止迭代</span><br><span class="line">            if return_num &gt;= 6:</span><br><span class="line">                raise StopIteration</span><br><span class="line">            self.num += 2</span><br><span class="line">            return return_num</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; my_iterator = MyIterator()</span><br><span class="line">&gt;&gt;&gt; next(my_iterator)</span><br><span class="line">0</span><br><span class="line">&gt;&gt;&gt; next(my_iterator)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; next(my_iterator)</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt; next(my_iterator)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure><h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><h3 id="生成器函数"><a href="#生成器函数" class="headerlink" title="生成器函数"></a>生成器函数</h3><p>定义与常规函数相同，区别在于，它使用 <code>yield 语句</code> 而不是 <code>return 语句</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def my_generator():</span><br><span class="line">        my_num = 0</span><br><span class="line">        while my_num &lt; 5:</span><br><span class="line">            yield my_num</span><br><span class="line">            my_num += 1</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;generator_ = my_generator()</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line">0</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line">1</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line">2</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line">3</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line">4</span><br><span class="line">&gt;&gt;&gt; next(generator_)</span><br><span class="line"># 终止迭代则会抛出 StopIteration 异常</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">StopIteration</span><br></pre></td></tr></table></figure><h3 id="生成器表达式"><a href="#生成器表达式" class="headerlink" title="生成器表达式"></a>生成器表达式</h3><p>使用 <code>()</code> 包裹，而不是【】</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">my_list = [i for i in range(1000000)]</span><br><span class="line">print(&quot;列表消耗的内存：&#123;&#125;&quot;.format(sys.getsizeof(my_list)))</span><br><span class="line"></span><br><span class="line">my_generator = (i for i in range(1000000))</span><br><span class="line">print(&quot;生成器消耗的内存：&#123;&#125;&quot;.format(sys.getsizeof(my_generator)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">列表消耗的内存：8448728</span><br><span class="line">生成器消耗的内存：112</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.runoob.com/python3/python3-iterator-generator.html">https://www.runoob.com/python3/python3-iterator-generator.html</a></p><p><a href="https://kelepython.readthedocs.io/zh/latest/c01/c01_11.html">https://kelepython.readthedocs.io/zh/latest/c01/c01_11.html</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 可迭代对象、迭代器与生成器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python不可变对象和可变对象</title>
      <link href="/2022/09/12/python-changed_obj/"/>
      <url>/2022/09/12/python-changed_obj/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/34395671">https://zhuanlan.zhihu.com/p/34395671</a></p><ul><li>可变对象：list dict set</li><li>不可变对象：tuple string int float bool</li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python不可变对象和可变对象 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>闭包</title>
      <link href="/2022/09/12/closure/"/>
      <url>/2022/09/12/closure/</url>
      
        <content type="html"><![CDATA[<p>调用函数A，返回函数B给你，函数B就是闭包，A的参数就是自由变量</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 闭包 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>装饰器</title>
      <link href="/2022/09/12/decorator/"/>
      <url>/2022/09/12/decorator/</url>
      
        <content type="html"><![CDATA[<h2 id="装饰器"><a href="#装饰器" class="headerlink" title="装饰器"></a>装饰器</h2><p>修饰符号：@</p><p>本质是闭包</p><p><a href="https://www.runoob.com/w3cnote/python-func-decorators.html">https://www.runoob.com/w3cnote/python-func-decorators.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def a_new_decorator(a_func):</span><br><span class="line"> </span><br><span class="line">    def wrapTheFunction():</span><br><span class="line">        print(&quot;I am doing some boring work before executing a_func()&quot;)</span><br><span class="line"> </span><br><span class="line">        a_func()</span><br><span class="line"> </span><br><span class="line">        print(&quot;I am doing some boring work after executing a_func()&quot;)</span><br><span class="line"> </span><br><span class="line">    return wrapTheFunction</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@a_new_decorator </span><br><span class="line">def a_function_requiring_decoration():     </span><br><span class="line">    &quot;&quot;&quot;Hey you! Decorate me!&quot;&quot;&quot;</span><br><span class="line">    print(&quot;I am the function which needs some decoration to &quot;</span><br><span class="line">          &quot;remove my foul smell&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration=a_new_decorator(a_function_requiring_decoration) </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line">#outputs: I am doing some boring work before executing a_func()</span><br><span class="line">#         I am the function which needs some decoration to remove my foul smell</span><br><span class="line">#         I am doing some boring work after executing a_func()</span><br><span class="line"> </span><br></pre></td></tr></table></figure><h2 id="多装饰器"><a href="#多装饰器" class="headerlink" title="多装饰器"></a>多装饰器</h2><p>顺序为从下至上</p><h2 id="常见装饰器"><a href="#常见装饰器" class="headerlink" title="常见装饰器"></a>常见装饰器</h2><h3 id="property"><a href="#property" class="headerlink" title="@property"></a>@property</h3><p>将<strong>方法</strong>转换为相同名称的<strong>只读属性</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class dataset(object):</span><br><span class="line">    @property</span><br><span class="line">    def fun1(self):</span><br><span class="line">        return 13</span><br><span class="line">    def fun2(self):</span><br><span class="line">        return 13</span><br><span class="line"></span><br><span class="line">a=dataset()</span><br><span class="line">print(a.fun1)</span><br><span class="line">print(a.fun2())</span><br></pre></td></tr></table></figure><h3 id="staticmethod"><a href="#staticmethod" class="headerlink" title="@staticmethod"></a>@staticmethod</h3><h3 id="classmthod"><a href="#classmthod" class="headerlink" title="@classmthod"></a>@classmthod</h3><p><a href="https://zhuanlan.zhihu.com/p/35643573">https://zhuanlan.zhihu.com/p/35643573</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 装饰器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>前馈神经网络</title>
      <link href="/2022/08/20/ffn/"/>
      <url>/2022/08/20/ffn/</url>
      
        <content type="html"><![CDATA[<p>Feedforward neural network，FNN</p><p>和全连接神经网络的区别？？？</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 前馈神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>感知机</title>
      <link href="/2022/08/20/perceptron/"/>
      <url>/2022/08/20/perceptron/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/08/20/perceptron/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 感知机 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>逻辑回归</title>
      <link href="/2022/08/20/logistic-reg/"/>
      <url>/2022/08/20/logistic-reg/</url>
      
        <content type="html"><![CDATA[<p>逻辑回归（lr）= 线性回归+sigmoid</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>feature scale</title>
      <link href="/2022/08/04/feature-scale/"/>
      <url>/2022/08/04/feature-scale/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 特征工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> feature scale </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>迭代分析</title>
      <link href="/2022/07/26/ml-iteration-analyze/"/>
      <url>/2022/07/26/ml-iteration-analyze/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 迭代分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 迭代分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>倒排索引</title>
      <link href="/2022/07/15/indexing/"/>
      <url>/2022/07/15/indexing/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 召回 </category>
          
          <category> 倒排索引 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 倒排索引 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>粗排</title>
      <link href="/2022/07/15/pre-ranking/"/>
      <url>/2022/07/15/pre-ranking/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 排序 </category>
          
          <category> 粗排 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 粗排 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bert_serving</title>
      <link href="/2022/07/01/bert-serving/"/>
      <url>/2022/07/01/bert-serving/</url>
      
        <content type="html"><![CDATA[<p>bert词向量服务，生成词向量并聚类可视化</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/61323d366f7c">https://www.jianshu.com/p/61323d366f7c</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bert_serving </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>细粒度NLP任务</title>
      <link href="/2022/06/30/micro-Grained/"/>
      <url>/2022/06/30/micro-Grained/</url>
      
        <content type="html"><![CDATA[<p>AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization （ByteDance AI Lab）</p><p><a href="https://arxiv.org/pdf/2008.11869.pdf">https://arxiv.org/pdf/2008.11869.pdf</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 细粒度NLP任务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bertviz:attention可视化工具</title>
      <link href="/2022/06/30/bertviz/"/>
      <url>/2022/06/30/bertviz/</url>
      
        <content type="html"><![CDATA[<p>看不同layer，不同head的attention</p><p>注意：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from bertviz.neuron_view import show</span><br><span class="line">from bertviz.transformers_neuron_view import BertModel, BertTokenizer</span><br><span class="line">model1=BertModel.from_pretrained(path)</span><br><span class="line">model_type = &#x27;bert&#x27;</span><br><span class="line"></span><br><span class="line">show(model1, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)</span><br><span class="line">可以</span><br><span class="line">###########################</span><br><span class="line"></span><br><span class="line">from bertviz.neuron_view import show</span><br><span class="line">from transformers import BertTokenizer, BertModel</span><br><span class="line">model1=BertModel.from_pretrained(path)</span><br><span class="line">model_type = &#x27;bert&#x27;</span><br><span class="line"></span><br><span class="line">show(model1, model_type, tokenizer, sentence_a, sentence_b, layer=4, head=3)</span><br><span class="line">报错</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/457043243">https://zhuanlan.zhihu.com/p/457043243</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bertviz:attention可视化工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>prompt trick</title>
      <link href="/2022/06/27/prompt-trick/"/>
      <url>/2022/06/27/prompt-trick/</url>
      
        <content type="html"><![CDATA[<h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>通过模板使得预测任务与预训练模型的训练任务相统一，拉近预训练任务目标与下游微调目标的差距</p><h2 id="和finetune差异"><a href="#和finetune差异" class="headerlink" title="和finetune差异"></a>和finetune差异</h2><p>finetune：PTM<strong>向下兼容</strong>specific task</p><p>prompt：specific task<strong>向上兼容</strong>PTM</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>由于其当前预测任务与预训练模型的训练任务相统一，所以我们可以在训练数据较少，甚至没有的情况下去完成当前任务，总结一下，其比较适合的应用场景：</p><ol><li>zero-shot</li><li>few-shot</li><li>冷启动</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/424888379">https://zhuanlan.zhihu.com/p/424888379</a></p><p><a href="https://zhuanlan.zhihu.com/p/440169921">https://zhuanlan.zhihu.com/p/440169921</a>    </p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Prompt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prompt trick </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jupyter notebook</title>
      <link href="/2022/06/22/jupyter-notebook/"/>
      <url>/2022/06/22/jupyter-notebook/</url>
      
        <content type="html"><![CDATA[<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p><a href="https://blog.csdn.net/weixin_41149572/article/details/114640624">https://blog.csdn.net/weixin_41149572/article/details/114640624</a></p><h2 id="传参"><a href="#传参" class="headerlink" title="传参"></a>传参</h2><p>和.py唯一区别</p><p>jupyter：args=parser.pars_args(argew=[传入参数])</p><p>py：args=parser.pars_args()</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/weixin_41149572/article/details/114640624">https://blog.csdn.net/weixin_41149572/article/details/114640624</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jupyter notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</title>
      <link href="/2022/06/17/chinesebert/"/>
      <url>/2022/06/17/chinesebert/</url>
      
        <content type="html"><![CDATA[<p>考虑字形和拼音的中文PTM</p><h2 id="1-模型结构"><a href="#1-模型结构" class="headerlink" title="1 模型结构"></a>1 模型结构</h2><p><img src="/2022/06/17/chinesebert/1.JPG" alt></p><p><strong>变动在bert的输入</strong></p><p>原来Char embedding+Position embedding+segment embedding-&gt; 现在 Fusion embedding+Position embedding （omit the segment embedding）</p><p>Char embedding +Glyph ( 字形 ) embedding +Pinyin （拼音）embedding -》Fusion embedding</p><h2 id="2-预训练任务"><a href="#2-预训练任务" class="headerlink" title="2 预训练任务"></a>2 预训练任务</h2><p> Whole Word Masking (WWM) and Char Masking (CM)</p><h2 id="3-使用"><a href="#3-使用" class="headerlink" title="3 使用"></a>3 使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from datasets.bert_dataset import BertDataset</span><br><span class="line">&gt;&gt;&gt; from models.modeling_glycebert import GlyceBertModel</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; tokenizer = BertDataset([CHINESEBERT_PATH])</span><br><span class="line">&gt;&gt;&gt; chinese_bert = GlyceBertModel.from_pretrained([CHINESEBERT_PATH])</span><br><span class="line">&gt;&gt;&gt; sentence = &#x27;我喜欢猫&#x27;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; input_ids, pinyin_ids = tokenizer.tokenize_sentence(sentence)</span><br><span class="line">&gt;&gt;&gt; length = input_ids.shape[0]</span><br><span class="line">&gt;&gt;&gt; input_ids = input_ids.view(1, length)</span><br><span class="line">&gt;&gt;&gt; pinyin_ids = pinyin_ids.view(1, length, 8)</span><br><span class="line">&gt;&gt;&gt; output_hidden = chinese_bert.forward(input_ids, pinyin_ids)[0]</span><br><span class="line">&gt;&gt;&gt; print(output_hidden)</span><br><span class="line">tensor([[[ 0.0287, -0.0126,  0.0389,  ...,  0.0228, -0.0677, -0.1519],</span><br><span class="line">         [ 0.0144, -0.2494, -0.1853,  ...,  0.0673,  0.0424, -0.1074],</span><br><span class="line">         [ 0.0839, -0.2989, -0.2421,  ...,  0.0454, -0.1474, -0.1736],</span><br><span class="line">         [-0.0499, -0.2983, -0.1604,  ..., -0.0550, -0.1863,  0.0226],</span><br><span class="line">         [ 0.1428, -0.0682, -0.1310,  ..., -0.1126,  0.0440, -0.1782],</span><br><span class="line">         [ 0.0287, -0.0126,  0.0389,  ...,  0.0228, -0.0677, -0.1519]]],</span><br><span class="line">       grad_fn=&lt;NativeLayerNormBackward&gt;)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/ShannonAI/ChineseBert">https://github.com/ShannonAI/ChineseBert</a></p><p><a href="https://arxiv.org/pdf/2106.16038.pdf">https://arxiv.org/pdf/2106.16038.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>哈希表</title>
      <link href="/2022/06/16/hashtable/"/>
      <url>/2022/06/16/hashtable/</url>
      
        <content type="html"><![CDATA[<h2 id="1-哈希函数"><a href="#1-哈希函数" class="headerlink" title="1 哈希函数"></a>1 哈希函数</h2><p><img src="/2022/06/16/hashtable/1.jpg" alt></p><h2 id="2-哈希冲突"><a href="#2-哈希冲突" class="headerlink" title="2 哈希冲突"></a>2 哈希冲突</h2><p><a href="https://blog.51cto.com/u_15077556/3984082">https://blog.51cto.com/u_15077556/3984082</a></p><p><a href="https://www.jianshu.com/p/585f8882bbfb">https://www.jianshu.com/p/585f8882bbfb</a></p><h2 id="3-效率"><a href="#3-效率" class="headerlink" title="3 效率"></a>3 效率</h2><p>插入和查找的时间复杂度都是为O(1)</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/144296454">https://zhuanlan.zhihu.com/p/144296454</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 哈希表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针</title>
      <link href="/2022/06/16/two-pointer/"/>
      <url>/2022/06/16/two-pointer/</url>
      
        <content type="html"><![CDATA[<p>指针$i$，指针$j$，序列长度为$n$</p><p>1 $O(n^2)$ </p><p>$i$总共遍历$n$，$j$总共遍历$n^2$</p><p>1 $O(n )$</p><p>$i$总共遍历$n$，$j$总共遍历$n$</p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 双指针 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>finetune</title>
      <link href="/2022/06/11/finetune/"/>
      <url>/2022/06/11/finetune/</url>
      
        <content type="html"><![CDATA[<h2 id="1-使用哪些层参与下游任务"><a href="#1-使用哪些层参与下游任务" class="headerlink" title="1 使用哪些层参与下游任务"></a>1 使用哪些层参与下游任务</h2><p>使用哪些层参与下游任务</p><p>选择的层model1+下游任务model2</p><p>对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。</p><p>用$\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\cdot)$为特定的任务模型。有以下几种方法选择representation:</p><p><strong>a) Embedding Only</strong></p><p>choose only the pre-trained static embeddings，即$g(\textbf{H}^{1})$</p><p><strong>b) Top Layer</strong></p><p>选择顶层的representation，然后接入特定的任务模型，即$g(\textbf{H}^{L})$</p><p><strong>c) All Layers</strong></p><p>输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下</p><script type="math/tex; mode=display">g(\textbf{r}_t)=g(\gamma \sum_{l=1}^{L}\alpha_l\textbf{H}^{(l)})</script><p>其中$\alpha$ is the softmax-normalized weight for layer $l$ and $\gamma$ is a scalar to scale the vectors output by pre-trained model</p><h2 id="2-参数是否固定"><a href="#2-参数是否固定" class="headerlink" title="2 参数是否固定"></a>2 参数是否固定</h2><p>总共有两种常用的模型迁移方式：<strong>feature extraction</strong> (where the pre-trained parameters are frozen), and <strong>fine-tuning</strong> (where the pre-trained parameters are unfrozen and fine-tuned).</p><p><img src="/2022/06/11/finetune/ptm7.JPG" alt></p><h2 id="3-Fine-Tuning-Strategies"><a href="#3-Fine-Tuning-Strategies" class="headerlink" title="3 Fine-Tuning Strategies"></a>3 Fine-Tuning Strategies</h2><p><strong>Two-stage fine-tuning</strong></p><p>第一阶段为中间任务，第二阶段为目标任务</p><p><strong>Multi-task fine-tuning</strong></p><p>multi-task learning and pre-training are complementary technologies.</p><p><strong>Fine-tuning with extra adaptation modules</strong></p><p>The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed.</p><p><strong>Others</strong></p><p>self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> finetune </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多任务学习</title>
      <link href="/2022/06/11/multi-task-learning/"/>
      <url>/2022/06/11/multi-task-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h2><p>1 减少模型数量，相似任务可以共享</p><p>2 可以提升单一任务的效果</p><p>微软的MT-DNN[33]已经证明基于预训练模型的多任务Fine-tuning可以提升各项子任务效果</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/2022/06/11/multi-task-learning/1.jpg" alt></p><h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><p>设计loss是重点</p><p>除了各个子任务的损失直接相加，还有别的方式吗</p><p>辅助任务怎么设计</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/397196665">https://zhuanlan.zhihu.com/p/397196665</a></p><p><a href="https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html">https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 多任务学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多任务学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实体关系抽取</title>
      <link href="/2022/06/11/relation-extract/"/>
      <url>/2022/06/11/relation-extract/</url>
      
        <content type="html"><![CDATA[<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/77868938">https://zhuanlan.zhihu.com/p/77868938</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 信息抽取 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体关系抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>text Span抽取</title>
      <link href="/2022/06/11/txt-span/"/>
      <url>/2022/06/11/txt-span/</url>
      
        <content type="html"><![CDATA[<p>基于问题在段落中寻找答案</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 问题：苏轼是哪里人？</span><br><span class="line">2 描述：苏轼是北宋著名的文学家与政治家，眉州眉山人。</span><br><span class="line">3 标签：眉州眉山人</span><br></pre></td></tr></table></figure><p>bert中的SQuAD问答任务</p><h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><p>引入start 和 end 标签</p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/2022/06/11/txt-span/1.jpg" alt></p><h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sequence_output = all_encoder_outputs[-1] #[src_len, batch_size, hidden_size]</span><br><span class="line">logits = self.qa_outputs(sequence_output)  # [src_len, batch_size,2] </span><br><span class="line">start_logits, end_logits = logits.split(1, dim=-1)</span><br><span class="line">start_logits = start_logits.squeeze(-1).transpose(0, 1)  # [batch_size,src_len]</span><br><span class="line">end_logits = end_logits.squeeze(-1).transpose(0, 1)  # [batch_size,src_len]</span><br><span class="line">loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)</span><br><span class="line">start_loss = loss_fct(start_logits, start_positions)</span><br><span class="line">end_loss = loss_fct(end_logits, end_positions)</span><br><span class="line">final_loss=(start_loss + end_loss) / 2</span><br></pre></td></tr></table></figure><p>模型输出为：  [src_len, batch_size,2] </p><p>两个（start 和 end ）src_len分类的平均</p><h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><p>假设候选文本长度为n，输出n个2分类结果，选出最大的start概率和end概率最为start和end label</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/77868938">https://zhuanlan.zhihu.com/p/77868938</a></p><p><a href="https://blog.csdn.net/guangyacyb/article/details/105526482">https://blog.csdn.net/guangyacyb/article/details/105526482</a></p><p><a href="https://zhuanlan.zhihu.com/p/473157694">https://zhuanlan.zhihu.com/p/473157694</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 信息抽取 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> text Span抽取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>天池新闻推荐</title>
      <link href="/2022/06/09/tianchi-recommend/"/>
      <url>/2022/06/09/tianchi-recommend/</url>
      
        <content type="html"><![CDATA[<p>目标: 为不同用户（测试为5万）分别推荐top5的新闻文章（总数36万）</p><p>标签：不同用户在不同时间的点击新闻</p><p>特征：</p><p>整体框架也是：多路召回+排序</p><h2 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中</span><br><span class="line">user_multi_recall_dict =  &#123;&#x27;itemcf_sim_itemcf_recall&#x27;: &#123;&#125;,</span><br><span class="line">                           &#x27;embedding_sim_item_recall&#x27;: &#123;&#125;,</span><br><span class="line">                           &#x27;youtubednn_recall&#x27;: &#123;&#125;,</span><br><span class="line">                           &#x27;youtubednn_usercf_recall&#x27;: &#123;&#125;, </span><br><span class="line">                           &#x27;cold_start_recall&#x27;: &#123;&#125;&#125;</span><br></pre></td></tr></table></figure><ol><li><p>基于itemcf计算的item之间的相似度sim进行的召回</p></li><li><p>基于embedding搜索得到的item之间的相似度进行的召回</p></li><li>YoutubeDNN召回</li><li>YoutubeDNN得到的user之间的相似度进行的召回</li><li>基于冷启动策略的召回</li></ol><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>排序</p><ol><li>LGB的排序模型</li><li>LGB的分类模型</li><li>深度学习的分类模型DIN</li></ol><p>模型集成</p><ol><li>输出结果加权融合</li><li>Staking</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tianchi.aliyun.com/notebook-ai/detail">https://tianchi.aliyun.com/notebook-ai/detail</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 天池新闻推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GB, GBDT, XGBoost, LightGBM</title>
      <link href="/2022/06/09/boost-relation/"/>
      <url>/2022/06/09/boost-relation/</url>
      
        <content type="html"><![CDATA[<p>GB：boosting算法，是一种算法思想</p><p>GBDT ：说白了就是gradient boosting基学习器为cart回归树</p><p>XGBoost：GBDT的高效实现</p><p>LightGBM：GBDT的高效实现，对XGBoost改进</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/765efe2b951a">https://www.jianshu.com/p/765efe2b951a</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> boosting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GB, GBDT, XGBoost, LightGBM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>序列标注</title>
      <link href="/2022/06/09/sequence-annotation/"/>
      <url>/2022/06/09/sequence-annotation/</url>
      
        <content type="html"><![CDATA[<p>序列标注（Sequence Tagging）是NLP中最基础的任务，应用十分广泛，如分词、词性标注（POS tagging）、命名实体识别（Named Entity Recognition，NER）、关键词抽取、语义角色标注（Semantic Role Labeling）、槽位抽取（Slot Filling）等实质上都属于序列标注的范畴。</p><h2 id="标注方式"><a href="#标注方式" class="headerlink" title="标注方式"></a>标注方式</h2><p><a href="https://zhuanlan.zhihu.com/p/147537898#">https://zhuanlan.zhihu.com/p/147537898#</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/268579769">https://zhuanlan.zhihu.com/p/268579769</a></p><p><a href="https://zhuanlan.zhihu.com/p/147537898#">https://zhuanlan.zhihu.com/p/147537898#</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 序列标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>lightgbm</title>
      <link href="/2022/06/09/lightgbm/"/>
      <url>/2022/06/09/lightgbm/</url>
      
        <content type="html"><![CDATA[<p>Light  gradient boosting machine</p><h2 id="用于排序"><a href="#用于排序" class="headerlink" title="用于排序"></a>用于排序</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import lightgbm as lgb</span><br><span class="line">lgb.LGBMRanker</span><br></pre></td></tr></table></figure><p>原理</p><p>LGBMRanker模型和这个LambdaMART的原理很像</p><p><a href="https://blog.csdn.net/wuzhongqiang/article/details/110521519">https://blog.csdn.net/wuzhongqiang/article/details/110521519</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/99069186">https://zhuanlan.zhihu.com/p/99069186</a></p><p><a href="https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w">https://mp.weixin.qq.com/s/XxFHmxV4_iDq8ksFuZM02w</a></p><p><a href="https://www.jianshu.com/p/765efe2b951a">https://www.jianshu.com/p/765efe2b951a</a></p><p><a href="https://blog.csdn.net/wuzhongqiang/article/details/110521519">https://blog.csdn.net/wuzhongqiang/article/details/110521519</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> boosting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lightgbm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多标签</title>
      <link href="/2022/06/08/multi-label/"/>
      <url>/2022/06/08/multi-label/</url>
      
        <content type="html"><![CDATA[<h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1 分类"></a>1 分类</h2><p><strong>标签</strong></p><p><img src="/2022/06/08/multi-label/1.png" alt></p><p><strong>方法</strong></p><p>1 转成多个2分类</p><p>2 直接多标签分类</p><p><strong>loss：</strong></p><p>有个疑问，直接softmax+交叉熵不行吗？？？</p><p><a href="https://zhuanlan.zhihu.com/p/385475273">https://zhuanlan.zhihu.com/p/385475273</a></p><p> <a href="https://zhuanlan.zhihu.com/p/138117543">https://zhuanlan.zhihu.com/p/138117543</a></p><p>1 原来的交叉熵</p><p><img src="/2022/06/08/multi-label/2.svg" alt></p><p>m为样本总和，q为类别数量</p><p>2  “softmax+交叉熵”推广</p><p><img src="/2022/06/08/multi-label/3.jpg" alt></p><p><strong>评价指标</strong></p><p><a href="https://zhuanlan.zhihu.com/p/385475273">https://zhuanlan.zhihu.com/p/385475273</a></p><h2 id="2-文本翻译"><a href="#2-文本翻译" class="headerlink" title="2 文本翻译"></a>2 文本翻译</h2><p>一句英文输入，有多个版本的中文翻译，这种一对多怎么训练？？？？</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/138117543">https://zhuanlan.zhihu.com/p/138117543</a></p><p><a href="https://www.jianshu.com/p/ac3bec3dde3e">https://www.jianshu.com/p/ac3bec3dde3e</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 多标签 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多标签 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>postman</title>
      <link href="/2022/06/08/postman/"/>
      <url>/2022/06/08/postman/</url>
      
        <content type="html"><![CDATA[<h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>后端服务启动后，需要前端请求才会有反馈，真实场景是需要前端点击触发，为了简化，利用postman模拟前端的请求</p><p>真实前端/postman 《—》后端</p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>服务器</p><p>1 后端启应用</p><p>postman</p><p>2 填写后端请求地址</p><p>3 发送数据</p><p>body raw json</p><p>{。。。。}</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1 setting 设置 ssl certificate verification 为off</p>]]></content>
      
      
      <categories>
          
          <category> web前后端 </category>
          
          <category> postman </category>
          
      </categories>
      
      
        <tags>
            
            <tag> postman </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flask</title>
      <link href="/2022/06/08/flask/"/>
      <url>/2022/06/08/flask/</url>
      
        <content type="html"><![CDATA[<p>python web应用框架，后端</p><p><strong>编写逻辑</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from flask import Flask, request, jsonify</span><br><span class="line">##创建应用</span><br><span class="line">app1 = Flask(__name__)</span><br><span class="line">###绑定函数</span><br><span class="line">@app1.route(&#x27;URL rule&#x27;, methods=[&#x27;post&#x27;])</span><br><span class="line">def process_element_data():</span><br><span class="line"></span><br><span class="line">return</span><br><span class="line">##启动应用</span><br><span class="line">app1.run(....)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>执行逻辑</strong><br>1 启动应用<br>2 前端发送请求<br>3 根据URL rule找函数，返回结果给前端</p>]]></content>
      
      
      <categories>
          
          <category> web前后端 </category>
          
          <category> flask </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>oov怎么解决</title>
      <link href="/2022/06/07/oov/"/>
      <url>/2022/06/07/oov/</url>
      
        <content type="html"><![CDATA[<p>oov：Out-Of-Vocabulary</p><p>中文：采用字粒度</p><p>英文：subword</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> oov怎么解决 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>checkpoints_iterator</title>
      <link href="/2022/06/06/checkpoints-iterator/"/>
      <url>/2022/06/06/checkpoints-iterator/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for ckpt in tf.contrib.training.checkpoints_iterator(</span><br><span class="line">        FLAGS.output_dir, timeout=FLAGS.eval_timeout)</span><br></pre></td></tr></table></figure><p>持续捕捉最新的checkpoint，两次捕捉之间的最大等待时间为 eval_timeout</p><p>什么用？</p><p>用于训练验证并行，之前是交替</p><p>一个设备训练，产生checkpoint，一个设备捕捉验证</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> checkpoints_iterator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Graph和Session</title>
      <link href="/2022/06/02/graph-session/"/>
      <url>/2022/06/02/graph-session/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/06/02/graph-session/1.webp" alt></p><p>graph定义了计算方式（计算流程），本身不会进行任何计算</p><p>session帮助graph计算</p><p>可以定义多个graph，例如一个graph实现z = x + y，另一个graph实现u = 2 * v</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/b636de7c251a">https://www.jianshu.com/p/b636de7c251a</a></p><p><a href="https://blog.csdn.net/qq_40242197/article/details/105315219">https://blog.csdn.net/qq_40242197/article/details/105315219</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph和Session </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>early stop</title>
      <link href="/2022/06/01/early-stop/"/>
      <url>/2022/06/01/early-stop/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/06/01/early-stop/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> early stop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>训练,验证同步进行</title>
      <link href="/2022/06/01/train-with-valid/"/>
      <url>/2022/06/01/train-with-valid/</url>
      
        <content type="html"><![CDATA[<p>参数没变，只是选择了训练中验证最好的模型，类似early stop</p><p>不同时行不行？不同时可能无法得到最优的step，有可能过拟合</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 训练,验证同步进行 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超长文本处理</title>
      <link href="/2022/05/31/long-text/"/>
      <url>/2022/05/31/long-text/</url>
      
        <content type="html"><![CDATA[<p>bert最大长度固定，默认为512 </p><p><strong>数据层面：</strong></p><p>1 直接截断：太粗暴，可能把重要的丢了</p><p>2 抽取重要部分</p><p>3 分段+拼接</p><p>​    问题很多，怎么训练？？怎么预测？？？</p><p><strong>模型层面：</strong></p><p>transformer-xl based的ptm，比如xlnet    </p><p>传统rnn based的seq2seq</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhihu.com/question/395903256">https://www.zhihu.com/question/395903256</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 超长文本处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>词表特殊词的含义</title>
      <link href="/2022/05/31/dict-word/"/>
      <url>/2022/05/31/dict-word/</url>
      
        <content type="html"><![CDATA[<p>[PAD]：要将句子处理为特定的长度，就要在句子前或后补[PAD]</p><p>[CLS]：句子的开始</p><p>[SEP]：分开两个输入句子</p><p>[mask] ：遮盖句子中的一些单词</p><p>[UNK]：标记词典内没有的词</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 词表特殊词的含义 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>递归</title>
      <link href="/2022/05/31/recusive/"/>
      <url>/2022/05/31/recusive/</url>
      
        <content type="html"><![CDATA[<p>自己调用自己</p><p>注意出口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def function():</span><br><span class="line">1 最小子问题，出口，特解</span><br><span class="line">2 通释</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 递归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练任务</title>
      <link href="/2022/05/29/pretrain-task/"/>
      <url>/2022/05/29/pretrain-task/</url>
      
        <content type="html"><![CDATA[<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p><img src="/2022/05/29/pretrain-task/2.JPG" alt></p><p><img src="/2022/05/29/pretrain-task/1.JPG" alt></p><p>TLM : Translation Language Modeling</p><p>DAE: Denoising Autoencoder</p><p>CTL: Contrastive Learning</p><p>RTD： Replaced Token Detection </p><p>SOP：Sentence Order Prediction</p><p>DIM：Deep InfoMAx</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/360892229">https://zhuanlan.zhihu.com/p/360892229</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预训练任务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bp算法</title>
      <link href="/2022/05/28/bp/"/>
      <url>/2022/05/28/bp/</url>
      
        <content type="html"><![CDATA[<p>Error Back Propagation</p><p>两个过程：1.信号的正向传播 2.误差的反向传播</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bp算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集划分</title>
      <link href="/2022/05/28/data-split/"/>
      <url>/2022/05/28/data-split/</url>
      
        <content type="html"><![CDATA[<p>训练</p><p>验证: 调参</p><p>测试</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数据构造 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据集划分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化</title>
      <link href="/2022/05/24/python_optimization/"/>
      <url>/2022/05/24/python_optimization/</url>
      
        <content type="html"><![CDATA[<h2 id="1-list，dict，set的时间复杂度"><a href="#1-list，dict，set的时间复杂度" class="headerlink" title="1 list，dict，set的时间复杂度"></a>1 list，dict，set的时间复杂度</h2><h3 id="x-in-s"><a href="#x-in-s" class="headerlink" title="x in s"></a>x in s</h3><p>list：O(n)</p><p>dict：O(1)</p><p>set：O(1)</p><h2 id="2-字符串格式化速度对比"><a href="#2-字符串格式化速度对比" class="headerlink" title="2.字符串格式化速度对比"></a>2.字符串格式化速度对比</h2><p>f-string 》 + 》% 》format</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/jmh1996/article/details/78481365">https://blog.csdn.net/jmh1996/article/details/78481365</a></p><p><a href="https://www.cnblogs.com/tintinsoft/articles/9743765.html">https://www.cnblogs.com/tintinsoft/articles/9743765.html</a></p><p><a href="https://blog.csdn.net/weixin_48629601/article/details/107532754">https://blog.csdn.net/weixin_48629601/article/details/107532754</a></p><p><a href="https://blog.csdn.net/ACBattle/article/details/97012800">https://blog.csdn.net/ACBattle/article/details/97012800</a></p><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>递归,迭代</title>
      <link href="/2022/05/23/recusive-iteration/"/>
      <url>/2022/05/23/recusive-iteration/</url>
      
        <content type="html"><![CDATA[<p>首先，迭代（有递推过程）区别于非递归</p><p><strong>关系</strong></p><p>迭代可以转换为递归，但递归不一定能转换为迭代</p><p>递归一定可以非递归表示</p><p><strong>递归，迭代对比</strong></p><p>空间复杂度：递归需要额外开销</p><p>时间复杂度：</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/32bcc45efd32">https://www.jianshu.com/p/32bcc45efd32</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 递归,迭代 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习模型部署</title>
      <link href="/2022/05/23/model-deploy/"/>
      <url>/2022/05/23/model-deploy/</url>
      
        <content type="html"><![CDATA[<h2 id="0-OONX"><a href="#0-OONX" class="headerlink" title="0 OONX"></a>0 OONX</h2><p>Open Neural Network Exchange</p><p>跨框架的模型<strong>中间表达</strong>，模型的统一存储形式,ONNX 模型一般用于中间部署阶段</p><h2 id="1-pytorch"><a href="#1-pytorch" class="headerlink" title="1 pytorch"></a>1 pytorch</h2><p>libtorch</p><h2 id="2-TensorFlow"><a href="#2-TensorFlow" class="headerlink" title="2 TensorFlow"></a>2 TensorFlow</h2><p>Tensorflow Serving</p><p>TensorFlow Lite</p><h2 id="3-TensorRT"><a href="#3-TensorRT" class="headerlink" title="3 TensorRT"></a>3 TensorRT</h2><p>加速用的</p><p>NVIDIA® TensorRT™ is an SDK for optimizing trained deep learning models to enable high-performance inference. </p><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html</a></p><h2 id="4-NCNN"><a href="#4-NCNN" class="headerlink" title="4 NCNN"></a>4 NCNN</h2><p>ncnn is a high-performance neural network inference computing framework optimized for <strong>mobile</strong> platforms.</p><h2 id="5-TVM"><a href="#5-TVM" class="headerlink" title="5 TVM"></a>5 TVM</h2><p>加速</p><p>ApacheTVM是一个面向CPU、GPU和机器学习加速器的开源机器学习编译器框架。它旨在使机器学习工程师能够在任何后端设备高效地优化并运行计算</p><p><a href="https://chinese.tvm.wiki/tutorial/introduction.html#an-overview-of-tvm-and-model-optimization">https://chinese.tvm.wiki/tutorial/introduction.html#an-overview-of-tvm-and-model-optimization</a></p><h2 id="6-onnxruntime"><a href="#6-onnxruntime" class="headerlink" title="6 onnxruntime"></a>6 onnxruntime</h2><p> 加速用的</p><h2 id="7-组合使用"><a href="#7-组合使用" class="headerlink" title="7 组合使用"></a>7 组合使用</h2><p><img src="/2022/05/23/model-deploy/1.jpg" alt></p><p>1 TensorFlow -&gt; oonx-&gt;onnxruntime</p><p><a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/346511883">https://zhuanlan.zhihu.com/p/346511883</a></p><p><a href="https://zhuanlan.zhihu.com/p/423551635">https://zhuanlan.zhihu.com/p/423551635</a></p><p><a href="https://hub.fastgit.org/onnx/tutorials">https://hub.fastgit.org/onnx/tutorials</a></p><p><a href="https://blog.csdn.net/zxgmlcj/article/details/103279869">https://blog.csdn.net/zxgmlcj/article/details/103279869</a></p><p><a href="https://hub.fastgit.org/microsoft/onnxruntime">https://hub.fastgit.org/microsoft/onnxruntime</a></p><p><a href="https://hub.fastgit.org/onnx/onnx">https://hub.fastgit.org/onnx/onnx</a></p><p><a href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html</a></p><p><a href="https://chinese.tvm">https://chinese.tvm</a>.</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 部署 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习模型部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>phoenix</title>
      <link href="/2022/05/22/phoenix/"/>
      <url>/2022/05/22/phoenix/</url>
      
        <content type="html"><![CDATA[<p>在hbase上构建SQL层，使得hbase 能够使用标准SQL管理数据，Phoenix中的sql语句还是有些不同的</p><h2 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h2><p><a href="https://www.hangge.com/blog/cache/detail_2980.html">https://www.hangge.com/blog/cache/detail_2980.html</a></p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>使用python2，python2 sqlline.py</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1 KeeperErrorCode = NoNode for /hbase    或者   Retrieve cluster id failed</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.hangge.com/blog/cache/detail_2980.html">https://www.hangge.com/blog/cache/detail_2980.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> phoenix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> phoenix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mysql</title>
      <link href="/2022/05/21/mysql/"/>
      <url>/2022/05/21/mysql/</url>
      
        <content type="html"><![CDATA[<h2 id="开机自启"><a href="#开机自启" class="headerlink" title="开机自启"></a>开机自启</h2><h2 id="配置Binlog"><a href="#配置Binlog" class="headerlink" title="配置Binlog"></a>配置Binlog</h2><p>flinkcdc需要用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1 sudo vim /etc/my.cnf</span><br><span class="line">2 写入</span><br><span class="line">server id = 1 </span><br><span class="line">log-bin=mysql-bin </span><br><span class="line">binlog_format=row </span><br><span class="line">binlog-do-db=gmall2021</span><br><span class="line">binlog-do-db=gmall2021_realtime</span><br><span class="line">注意：binlog-do-db 根据自己的情况进行修 改，指定具体要同步的数据库，多个就写多行</span><br><span class="line">3 重启mysql</span><br><span class="line">sudo systemctl restart mysqld</span><br><span class="line">4 查看有没有生效</span><br><span class="line"> cd /var/lib</span><br><span class="line"> sudo ls -l mysql | grep bin</span><br><span class="line"> 修改mysql库里数据，观察&quot;mysql-bin.最新&quot;文件大小的变化</span><br></pre></td></tr></table></figure><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1 Job for mysqld.service failed because the control process exited with error code</p><p><a href="https://blog.csdn.net/qq_41179691/article/details/104598293">https://blog.csdn.net/qq_41179691/article/details/104598293</a></p><p>2 navicat连不上服务器的mysql</p><p><a href="https://blog.csdn.net/u014264373/article/details/85564524">https://blog.csdn.net/u014264373/article/details/85564524</a></p><p><a href="https://blog.csdn.net/MTbaby/article/details/56836986">https://blog.csdn.net/MTbaby/article/details/56836986</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 关系型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Negative Sampling 负采样</title>
      <link href="/2022/05/21/Negative-sample/"/>
      <url>/2022/05/21/Negative-sample/</url>
      
        <content type="html"><![CDATA[<p>首先区别于欠采样 ( under sampling )和过采样 (oversampling)</p><p><strong>作用</strong>：</p><p>减少计算量，调高训练效率</p><p><strong>是什么</strong></p><p>负采样，顾名思义，就是从一堆负样本中采样出一部分负样本，用于模型的训练。</p><p>1 作用在训练时候</p><p>也就是说在训练的时候采样</p><p> 一个全连接网络为100X10X100,多分类，100选1，也就是说输出层只有一个正样本，99个负样本，为了减少计算量，每次只选部分负样本，比如5个，那么梯度更新的时候，只更新正样本和5个负样本的，这样还剩94个就不更新了</p><p>2 作用在训练前面</p><p>也就是说在训练前，样本已经采好了</p><p><strong>分类</strong></p><p>在负采样过程中，有几个问题需要重点考虑：（1）这么多负样本中，到底需要采出哪一部分作为负样本呢（2）需要采出多大数量的负样本？</p><p><a href="https://kaiyuan.blog.csdn.net/article/details/122264543">https://kaiyuan.blog.csdn.net/article/details/122264543</a> </p><p><a href="https://zhuanlan.zhihu.com/p/456088223">https://zhuanlan.zhihu.com/p/456088223</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://kaiyuan.blog.csdn.net/article/details/122264543">https://kaiyuan.blog.csdn.net/article/details/122264543</a> </p><p><a href="https://blog.csdn.net/ningyanggege/article/details/87869393">https://blog.csdn.net/ningyanggege/article/details/87869393</a></p><p><a href="https://zhuanlan.zhihu.com/p/456088223">https://zhuanlan.zhihu.com/p/456088223</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数据构造 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Negative Sampling 负采样 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>auxiliary loss(辅助损失)</title>
      <link href="/2022/05/21/auxiliary-loss/"/>
      <url>/2022/05/21/auxiliary-loss/</url>
      
        <content type="html"><![CDATA[<p>多任务学习中经常可以看到</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.nowcoder.net/n/600d5e96508d4b818009227fdc9b8cbc">https://blog.nowcoder.net/n/600d5e96508d4b818009227fdc9b8cbc</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> loss </category>
          
      </categories>
      
      
        <tags>
            
            <tag> auxiliary loss(辅助损失） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见题目</title>
      <link href="/2022/05/19/leet-normal/"/>
      <url>/2022/05/19/leet-normal/</url>
      
        <content type="html"><![CDATA[<h2 id="1-topk"><a href="#1-topk" class="headerlink" title="1 topk"></a>1 topk</h2><p><strong>快排</strong></p><p>O(n)</p><p>每排一次，就知道基准的位置，就可以得出TopK是在基准左边部分还是右边部分，因此不需要全部排序</p><p><strong>堆排</strong></p><p>O(k+nlog(k))</p><h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 给定一个字符串s和一个字符串p，请问最少去掉s中的多少个字符，才能使得p是s的子串呢？</span><br><span class="line"># 解答要求时间限制：1000ms, 内存限制：100MB</span><br><span class="line"></span><br><span class="line"># 输入</span><br><span class="line"># 两行，第一行为字符串s，第二行为字符串p。(s和p只包含小写英文字母,s的长度不超过2000,p的长度不超过10,且保证有解）</span><br><span class="line"></span><br><span class="line"># 输出</span><br><span class="line"># 最少去掉的字符个数。</span><br><span class="line"></span><br><span class="line"># 样例</span><br><span class="line"># 输入样例 1</span><br><span class="line">#     axb</span><br><span class="line">#     ab</span><br><span class="line"># 输出</span><br><span class="line">#     1</span><br><span class="line"></span><br><span class="line"># 输入样例 2</span><br><span class="line">#     axabc</span><br><span class="line">#     abc</span><br><span class="line"># 输出</span><br><span class="line">#     0</span><br><span class="line"></span><br><span class="line"># 输入样例 3</span><br><span class="line">#     axbacxbc</span><br><span class="line">#     abc</span><br><span class="line"># 输出</span><br><span class="line">#     2</span><br><span class="line"></span><br><span class="line">#dp[i][j]  表示 s[:i]最少需要删几个p[:j]是它的子串 O(mn) O(mn)</span><br><span class="line">###</span><br><span class="line"></span><br><span class="line">def minedit(str1,str2):</span><br><span class="line">    m=len(str1)</span><br><span class="line">    n=len(str2)</span><br><span class="line">    # max_num=9999</span><br><span class="line">    dp=[[None for i in range(n+1)]for j in range(m+1)]</span><br><span class="line">    dp[0][0]=0</span><br><span class="line">    for i in range(m):</span><br><span class="line">        dp[i][0]=0</span><br><span class="line">    for i in range(1,m+1):</span><br><span class="line">        for j in range(1,n+1):</span><br><span class="line">            if str1[i-1]==str2[j-1]:</span><br><span class="line">                dp[i][j]=dp[i-1][j-1]</span><br><span class="line">            else:</span><br><span class="line">                ##</span><br><span class="line">                if dp[i-1][j]!=None:</span><br><span class="line">                    dp[i][j]=dp[i-1][j]+1</span><br><span class="line">    result=None</span><br><span class="line">    for i in range(n,m+1):</span><br><span class="line">        if result==None:</span><br><span class="line">            result=dp[i][n]</span><br><span class="line">        else:</span><br><span class="line">            result=min(result,dp[i][n])</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">print(minedit(&quot;axbacxbc&quot;,&quot;abc&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常见题目 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Evaluation of Text Generation A Survey</title>
      <link href="/2022/05/18/text-generate-metric-survey/"/>
      <url>/2022/05/18/text-generate-metric-survey/</url>
      
        <content type="html"><![CDATA[<p>从3个维度将评价指标分类</p><p><strong>1 Human-Centric Evaluation Methods</strong></p><p>​    gold standard   expensive to execute</p><p><strong>2 Untrained Automatic Evaluation Metrics</strong></p><p>widely used</p><p>汇总</p><p><img src="/2022/05/18/text-generate-metric-survey/1.JPG" alt></p><p>property： 应该是说这个方法的关注点</p><p><strong>3 Untrained Automatic Evaluation Metrics</strong></p><p>overfitting and `gaming of the metric.’</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/2006.14799">https://arxiv.org/abs/2006.14799</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Evaluation of Text Generation A Survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>basic algorithm</title>
      <link href="/2022/05/18/basic-algorithm/"/>
      <url>/2022/05/18/basic-algorithm/</url>
      
        <content type="html"><![CDATA[<h2 id="快排"><a href="#快排" class="headerlink" title="快排"></a>快排</h2><p>核心思想：取一个数作为基准，比这个数小的放在左边，大的放在右边，然后左边重复，右边重复。</p><p>基准元素，一般来说选取有几种方法</p><ul><li>取第一个元素</li><li>取最后一个元素</li><li>取第中间位置元素</li></ul><p>实现方式：一般递归+双指针</p><p>伪代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1：首先取序列第一个元素为基准元素pivot=R[low]。i=low,j=high。</span><br><span class="line">2：从后向前扫描，找小于等于pivot的数，如果找到，R[i]与R[j]交换，i++。</span><br><span class="line">3：从前往后扫描，找大于pivot的数，如果找到，R[i]与R[j]交换，j--。 </span><br><span class="line">4: 重复2~3，直到i=j,返回该位置mid=i,该位置正好为pivot元素。 完成一趟排序后，以mid为界，将序列分为两部分，左序列都比pivot小，有序列都比pivot大，然后再分别对这两个子序列进行快速排序。</span><br></pre></td></tr></table></figure><p>时间复杂度（0nlogn）</p><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/63227573">https://zhuanlan.zhihu.com/p/63227573</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> basic algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java学习网站</title>
      <link href="/2022/05/16/java-tech/"/>
      <url>/2022/05/16/java-tech/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.pdai.tech/">https://www.pdai.tech/</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java学习网站 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink cdc</title>
      <link href="/2022/05/15/flink-cdc/"/>
      <url>/2022/05/15/flink-cdc/</url>
      
        <content type="html"><![CDATA[<p>CDC是 Change Data Capture(变更数据获取 )的简称。 核心思想是，监测并捕获数据库的变动（包括数据或数据表的插入 、 更新 以及 删除等），将这些变更按发生的顺序完整记录下来，写入到消息中间件中以供其他服务进行订阅及消费。</p><p>Flink社区开发了 flink-cdc-connectors 组件，这是一个可以直接从 MySQL、 PostgreSQL 等数据库直接 读取全量数据 和 增量变更数据 的 source 组件。</p><p>说白了就是连接数据库，然后实时监控变化</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink cdc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx</title>
      <link href="/2022/05/15/nginxx/"/>
      <url>/2022/05/15/nginxx/</url>
      
        <content type="html"><![CDATA[<h2 id="0-概念"><a href="#0-概念" class="headerlink" title="0 概念"></a>0 概念</h2><p>正向代理：代理的是客户端</p><p>反向代理：代理的是服务器端</p><p>概念</p><h2 id="1-作用"><a href="#1-作用" class="headerlink" title="1 作用"></a>1 作用</h2><p><a href="https://zhuanlan.zhihu.com/p/54793789">https://zhuanlan.zhihu.com/p/54793789</a></p><p>1、静态HTTP服务器</p><p>2、反向代理服务器</p><p>3、负载均衡</p><p>负载均衡通常是指将请求”均匀”分摊到集群中多个服务器节点上执行</p><p>4、虚拟主机</p><p>5、FastCGI</p><h2 id="2-问题"><a href="#2-问题" class="headerlink" title="2 问题"></a>2 问题</h2><p>1 nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lsof -i :80 |grep nginx |grep -v grep|awk &#x27;&#123;print $2&#125;&#x27;</span><br><span class="line">kill -9 XXX XXX</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/fmwind/article/details/120786375">https://blog.csdn.net/fmwind/article/details/120786375</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型联合训练</title>
      <link href="/2022/05/11/train-union/"/>
      <url>/2022/05/11/train-union/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www4.comp.polyu.edu.hk/~csxmwu/papers/KDD-2021-Taobao-Search.pdf">https://www4.comp.polyu.edu.hk/~csxmwu/papers/KDD-2021-Taobao-Search.pdf</a></p><p>user tower（model1），item tower（model2） 利用softmax cross entropy as the trainning objective</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 模型联合训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并行度设置</title>
      <link href="/2022/05/11/flink-parallel/"/>
      <url>/2022/05/11/flink-parallel/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/hongzhen91/article/details/90812686">https://blog.csdn.net/hongzhen91/article/details/90812686</a></p><p>一个任务的并行实例(线程)数目就被称为该任务的并行度</p><p>并行度设置层次</p><p>1 Operator Level（算子层次）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">setParallelism</span><br><span class="line"></span><br><span class="line">reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123;</span><br><span class="line">                        // 将累加器更新为当前最大的pv统计值，然后向下游发送累加器的值</span><br><span class="line">                        return value1.f1 &gt; value2.f1 ? value1 : value2;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).setParallelism(5)</span><br><span class="line">                .print();</span><br><span class="line">                </span><br><span class="line">(Mary,1)</span><br><span class="line">(Bob,1)</span><br><span class="line">(Mary,2)</span><br><span class="line">(Bob,2)</span><br><span class="line">(Mary,3)</span><br><span class="line">(Bob,3)</span><br><span class="line">(Mary,4)</span><br><span class="line">(Bob,4)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">keyBy(r -&gt; true) // 为每一条数据分配同一个key，将聚合结果发送到一条流中去</span><br><span class="line">                .reduce(new ReduceFunction&lt;Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">                    @Override</span><br><span class="line">                    public Tuple2&lt;String, Long&gt; reduce(Tuple2&lt;String, Long&gt; value1, Tuple2&lt;String, Long&gt; value2) throws Exception &#123;</span><br><span class="line">                        // 将累加器更新为当前最大的pv统计值，然后向下游发送累加器的值</span><br><span class="line">                        return value1.f1 &gt; value2.f1 ? value1 : value2;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;)</span><br><span class="line">                .print().setParallelism(5);</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">2&gt; (Bob,2)</span><br><span class="line">1&gt; (Mary,2)</span><br><span class="line">1&gt; (Bob,4)</span><br><span class="line">3&gt; (Mary,3)</span><br><span class="line">5&gt; (Bob,1)</span><br><span class="line">5&gt; (Mary,4)</span><br><span class="line">4&gt; (Mary,1)</span><br><span class="line">4&gt; (Bob,3)</span><br></pre></td></tr></table></figure><p>2Execution Environment Level（执行环境层次）</p><p>3Client Level（客户端层次）</p><p>4System Level（系统层次）</p><p>优先级1&gt;2&gt;3&gt;4</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 并行度设置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>物理分区</title>
      <link href="/2022/05/09/phisical-partition/"/>
      <url>/2022/05/09/phisical-partition/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/wdh01/p/16038278.html">https://www.cnblogs.com/wdh01/p/16038278.html</a></p><p>首先和逻辑分区区别开，逻辑分区包括keyBy等算子</p><p>逻辑分区只不过将数据按照key分组，哪个key分到哪个task，系统自动控制，万一分配不均，会发生数据倾斜</p><p>物理分区就是按一定逻辑将数据分配到不同Task，可以缓解数据倾斜</p><p>source（1）-》不同物理分区方式（3）-》slot</p><p><strong>分类</strong></p><p>1 随机分区 random</p><p>2 轮询分区round-robin</p><p>3 重缩放分区 rescale</p><p>4 分局分区 global</p><p>5 自定义 custom</p><p>6 广播 </p><p>不完全算物理分区方式</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 物理分区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>流批选择</title>
      <link href="/2022/05/07/flink-batch-stream/"/>
      <url>/2022/05/07/flink-batch-stream/</url>
      
        <content type="html"><![CDATA[<p><strong>之前版本</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//流</span><br><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">//批</span><br><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br></pre></td></tr></table></figure><p><strong>现在版本</strong></p><p>通过执行模式 execution mode选择</p><p>1 流处理 streaming 默认</p><p>2 批处理 batch</p><p>3 自动 automatic</p><p>（1） 通过命令行 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flink run -Dexecution.runtime-mode=BATCH/../..</span><br></pre></td></tr></table></figure><p>（2）代码    </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env.setRuntimeMode(RuntimeExecutionMode.STREAMING);</span><br><span class="line">       env.setRuntimeMode(RuntimeExecutionMode.BATCH);</span><br><span class="line">       env.setRuntimeMode(RuntimeExecutionMode.AUTOMATIC);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 流批选择 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hql增删改查</title>
      <link href="/2022/05/06/hql-crud/"/>
      <url>/2022/05/06/hql-crud/</url>
      
        <content type="html"><![CDATA[<h2 id="1增"><a href="#1增" class="headerlink" title="1增"></a>1增</h2><p>insert </p><p>load</p><h2 id="2删"><a href="#2删" class="headerlink" title="2删"></a>2删</h2><p>1 表</p><p>Drop 表结构都没有了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE IF EXISTS employee;</span><br></pre></td></tr></table></figure><p>2 记录</p><p>没有DELETE </p><p> <strong>TRUNCATE</strong> </p><p>所有记录</p><p>truncate table employees;</p><p><strong>INSERT OVERWRITE</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE dpc_test SELECT * FROM dpc_test WHERE age is not null;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3改"><a href="#3改" class="headerlink" title="3改"></a>3改</h2><p>1 update </p><p>针对记录</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update student set id=&#x27;444&#x27; where name=&#x27;tom&#x27;;</span><br></pre></td></tr></table></figure><p>2 Alter </p><p>表结构</p><h2 id="4查"><a href="#4查" class="headerlink" title="4查"></a>4查</h2><p>select</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hql增删改查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建表</title>
      <link href="/2022/05/05/sql-buildtable/"/>
      <url>/2022/05/05/sql-buildtable/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS `runoob_tbl`(</span><br><span class="line">   `runoob_id` INT UNSIGNED AUTO_INCREMENT,</span><br><span class="line">   `runoob_title` VARCHAR(100) NOT NULL,</span><br><span class="line">   `runoob_author` VARCHAR(40) NOT NULL,</span><br><span class="line">   `submission_date` DATE,</span><br><span class="line">   PRIMARY KEY ( `runoob_id` )</span><br><span class="line">)ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><h2 id="字段数据类型"><a href="#字段数据类型" class="headerlink" title="字段数据类型"></a>字段数据类型</h2><p><a href="https://www.w3school.com.cn/sql/sql_datatypes.asp">https://www.w3school.com.cn/sql/sql_datatypes.asp</a></p><p>array</p><p><a href="https://www.educba.com/array-in-sql/">https://www.educba.com/array-in-sql/</a></p><h2 id="约束（Constraints）"><a href="#约束（Constraints）" class="headerlink" title="约束（Constraints）"></a>约束（Constraints）</h2><ul><li><strong>NOT NULL</strong> - 指示某列不能存储 NULL 值。</li><li><strong>UNIQUE</strong> - 保证某列的每行必须有唯一的值。</li><li><strong>PRIMARY KEY</strong> - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。</li><li><strong>FOREIGN KEY</strong> - 保证一个表中的数据匹配另一个表中的值的参照完整性。</li><li><strong>CHECK</strong> - 保证列中的值符合指定的条件。</li><li><strong>DEFAULT</strong> - 规定没有给列赋值时的默认值。</li></ul><h2 id="自增字段"><a href="#自增字段" class="headerlink" title="自增字段"></a>自增字段</h2><p> AUTO INCREMENT </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Persons</span><br><span class="line">(</span><br><span class="line">ID int NOT NULL AUTO_INCREMENT,</span><br><span class="line">LastName varchar(255) NOT NULL,</span><br><span class="line">FirstName varchar(255),</span><br><span class="line">Address varchar(255),</span><br><span class="line">City varchar(255),</span><br><span class="line">PRIMARY KEY (ID)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>开始值是 1，每条新记录递增 1</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 建表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多线程编程</title>
      <link href="/2022/05/03/java-multi-thread/"/>
      <url>/2022/05/03/java-multi-thread/</url>
      
        <content type="html"><![CDATA[<p>进程 &gt; 线程</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 多线程编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多线程编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>网络编程</title>
      <link href="/2022/05/03/java-net-program/"/>
      <url>/2022/05/03/java-net-program/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 网络编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据结构汇总</title>
      <link href="/2022/05/01/data-struct/"/>
      <url>/2022/05/01/data-struct/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/m0_37568814/article/details/81288756">https://blog.csdn.net/m0_37568814/article/details/81288756</a></p><p><a href="https://zhuanlan.zhihu.com/p/138523723">https://zhuanlan.zhihu.com/p/138523723</a></p><h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1 分类"></a>1 分类</h2><p><img src="/2022/05/01/data-struct/1.png" alt></p><p>数据结构包括：逻辑结构，存储结构，数据运算</p><p>数据的<strong>逻辑结构</strong>主要分为线性结构和非线性结构。</p><ul><li><strong>线性结构</strong>：数据结构的元素之间存在一对一线性关系，所有结点都最多只有一个直接前趋结点和一个直接后继结点。常见的有数组、队列、链表、栈。</li><li><strong>非线性结构</strong>：各个结点之间具有多个对应关系，一个结点可能有多个直接前趋结点和多个直接后继结点。常见的有多维数组、广义表、树结构和图结构等。</li></ul><p>数据的<strong>物理结构</strong>（以后我都统一称<strong>存储结构</strong>），表示数据元素之间的逻辑关系，一种数据结构的逻辑结构根据需要可以表示成多种存储结构，常用的存储结构有：</p><ul><li><strong>顺序存储</strong>：存储顺序是连续的，在内存中用一组地址连续的存储单元依次存储线性表的各个数据元素。</li><li><strong>链式存储</strong>：在内存中的存储元素不一定是连续的，用任意地址的存储单元存储元素，元素节点存放数据元素以及通过指针指向相邻元素的地址信息。</li><li><strong>索引存储</strong>：除建立存储结点信息外，还建立附加的索引表来标识节点的地址。索引表由若干索引项组成。</li><li><strong>散列存储</strong>：又称Hash存储，由节点的关键码值决定节点的存储地址。</li></ul><h2 id="2-常用的数据结构"><a href="#2-常用的数据结构" class="headerlink" title="2 常用的数据结构"></a>2 常用的数据结构</h2><ul><li>数组（Array）</li><li>队列（Queue）</li><li>链表（Linked List）</li><li>栈（Stack）</li><li>树（Tree）</li><li>散列表（Hash）</li><li>堆（Heap）</li><li>图（Graph）</li></ul>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构汇总 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jvm</title>
      <link href="/2022/05/01/jvm/"/>
      <url>/2022/05/01/jvm/</url>
      
        <content type="html"><![CDATA[<h2 id="1-JDK、JRE、JVM"><a href="#1-JDK、JRE、JVM" class="headerlink" title="1. JDK、JRE、JVM"></a>1. JDK、JRE、JVM</h2><h2 id="0-汇总"><a href="#0-汇总" class="headerlink" title="0 汇总"></a>0 汇总</h2><p><a href="https://github.com/doocs/jvm">https://github.com/doocs/jvm</a></p><h2 id="1-JDK、JRE、JVM-1"><a href="#1-JDK、JRE、JVM-1" class="headerlink" title="1 JDK、JRE、JVM"></a>1 JDK、JRE、JVM</h2><p><a href="https://its401.com/article/weixin_45797022/104963478">https://its401.com/article/weixin_45797022/104963478</a></p><p><img src="/2022/05/01/jvm/1.png" alt></p><p><img src="/2022/05/01/jvm/2.png" alt></p><p><strong>三者的区别与联系</strong></p><ol><li><p>JDK用于开发，是给开发人员用的</p></li><li><p>JRE 用于运行java程序，普通用户用</p></li><li><p>JVM是java实现跨平台的最核心的部分，普通用户用</p></li></ol><h2 id="2-java内存区域，java-memory-model"><a href="#2-java内存区域，java-memory-model" class="headerlink" title="2  java内存区域，java memory model"></a>2  java内存区域，java memory model</h2><p>二者不同</p><p><a href="https://blog.csdn.net/javazejian/article/details/72772461">https://blog.csdn.net/javazejian/article/details/72772461</a></p><p>1 内存区域</p><p><a href="https://cloud.tencent.com/developer/article/1748395">https://cloud.tencent.com/developer/article/1748395</a></p><p>string 的 内存分配</p><p><a href="https://blog.csdn.net/liufangbaishi2014/article/details/52238881">https://blog.csdn.net/liufangbaishi2014/article/details/52238881</a></p><p>2 jmm</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> jvm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>任务生成和分配</title>
      <link href="/2022/04/25/flink-task-assign/"/>
      <url>/2022/04/25/flink-task-assign/</url>
      
        <content type="html"><![CDATA[<p>main代码  -》 数据流图（dataflow graph，logical streamgraph） -》 作业图（jobgraph）-》执行图（executiongraph）-&gt; 物理图（physical graph）</p><p><img src="/2022/04/25/flink-task-assign/2.png" alt></p><p><img src="/2022/04/25/flink-task-assign/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 任务生成和分配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>任务槽 task slots</title>
      <link href="/2022/04/25/flink-task-slot/"/>
      <url>/2022/04/25/flink-task-slot/</url>
      
        <content type="html"><![CDATA[<h3 id="slot共享"><a href="#slot共享" class="headerlink" title="slot共享"></a>slot共享</h3><p><img src="/2022/04/25/flink-task-slot/1.png" alt></p><p>并行度：算子的子任务个数</p><p>程序的并行度：最大算子并行度</p><p>假设：</p><p>设置全局并行度为6，保持sink为1</p><p>source，map(6)   -》keyby。。。(6)  -》sink(1)</p><p>总共有13个子任务</p><p>2 个taskmanger , 每个taskmanger 3个slot</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 任务槽 task slots </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算子链</title>
      <link href="/2022/04/25/flink-operator-chain/"/>
      <url>/2022/04/25/flink-operator-chain/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/04/25/flink-operator-chain/1.png" alt></p><p><img src="/2022/04/25/flink-operator-chain/2.png" alt></p><p>多个算子合并</p><p>合并条件：1 并行度相同的算子 2 一对一 one to one</p><p>好处：1. 减少线程之间的切换和缓存区的数据交换 2 减少时延 3 提高吞吐量</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算子链 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>datastream</title>
      <link href="/2022/04/24/datastream/"/>
      <url>/2022/04/24/datastream/</url>
      
        <content type="html"><![CDATA[<h2 id="1-转换算子-Transformation"><a href="#1-转换算子-Transformation" class="headerlink" title="1 转换算子 Transformation"></a>1 转换算子 Transformation</h2><p>function分类：普通的，rich</p><p>怎么写function：</p><ol><li>自定义</li><li>匿名类</li><li>lambda表达式</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">package com.atguigu.chapter05;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Copyright (c) 2020-2030 尚硅谷 All Rights Reserved</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Project:  FlinkTutorial</span><br><span class="line"> * &lt;p&gt;</span><br><span class="line"> * Created by  wushengran</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.common.typeinfo.Types;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStream;</span><br><span class="line">import org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"></span><br><span class="line">public class TransReturnTypeTest &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        env.setParallelism(1);</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;Event&gt; clicks = env.fromElements(</span><br><span class="line">                new Event(&quot;Mary&quot;, &quot;./home&quot;, 1000L),</span><br><span class="line">                new Event(&quot;Bob&quot;, &quot;./cart&quot;, 2000L)</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        // 想要转换成二元组类型，需要进行以下处理</span><br><span class="line">        // 1) 使用显式的 &quot;.returns(...)&quot;</span><br><span class="line">        DataStream&lt;Tuple2&lt;String, Long&gt;&gt; stream3 = clicks</span><br><span class="line">                .map( event -&gt; Tuple2.of(event.user, 1L) )</span><br><span class="line">                .returns(Types.TUPLE(Types.STRING, Types.LONG));</span><br><span class="line">        stream3.print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        // 2) 使用类来替代Lambda表达式</span><br><span class="line">        clicks.map(new MyTuple2Mapper())</span><br><span class="line">                .print();</span><br><span class="line"></span><br><span class="line">        // 3) 使用匿名类来代替Lambda表达式</span><br><span class="line">        clicks.map(new MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, Long&gt; map(Event value) throws Exception &#123;</span><br><span class="line">                return Tuple2.of(value.user, 1L);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).print();</span><br><span class="line"></span><br><span class="line">        env.execute();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 自定义MapFunction的实现类</span><br><span class="line">    public static class MyTuple2Mapper implements MapFunction&lt;Event, Tuple2&lt;String, Long&gt;&gt;&#123;</span><br><span class="line">        @Override</span><br><span class="line">        public Tuple2&lt;String, Long&gt; map(Event value) throws Exception &#123;</span><br><span class="line">            return Tuple2.of(value.user, 1L);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>max maxby 区别</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">DataStreamSource&lt;Event&gt; stream = env.fromElements(</span><br><span class="line">                new Event(&quot;Mary&quot;, &quot;./home&quot;, 5000L),</span><br><span class="line">                new Event(&quot;Bob&quot;, &quot;./cart&quot;, 2000L),</span><br><span class="line">                new Event(&quot;Mary&quot;, &quot;./cart&quot;, 3000L),</span><br><span class="line">                new Event(&quot;ss&quot;, &quot;./fav&quot;, 4000L),</span><br><span class="line">                new Event(&quot;Mary&quot;, &quot;./fav&quot;, 10000L)</span><br><span class="line">        );</span><br><span class="line">                </span><br><span class="line">stream.keyBy(e -&gt; e.user)</span><br><span class="line">//                .maxBy(&quot;timestamp&quot;)</span><br><span class="line">                .maxBy(&quot;timestamp&quot;)    // 指定字段名称</span><br><span class="line">                .print(&quot;maxBy:&quot;);</span><br><span class="line">        stream.keyBy(e -&gt; e.user)</span><br><span class="line">//                .(&quot;timestamp&quot;)</span><br><span class="line">                .max(&quot;timestamp&quot;)    // 指定字段名称</span><br><span class="line">                .print(&quot;max:&quot;);       </span><br><span class="line">               </span><br><span class="line">               </span><br><span class="line">max:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./home&#x27;, timestamp=1970-01-01 08:00:05.0&#125;</span><br><span class="line">maxBy:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./home&#x27;, timestamp=1970-01-01 08:00:05.0&#125;</span><br><span class="line">max:&gt; Event&#123;user=&#x27;Bob&#x27;, url=&#x27;./cart&#x27;, timestamp=1970-01-01 08:00:02.0&#125;</span><br><span class="line">maxBy:&gt; Event&#123;user=&#x27;Bob&#x27;, url=&#x27;./cart&#x27;, timestamp=1970-01-01 08:00:02.0&#125;</span><br><span class="line">max:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./home&#x27;, timestamp=1970-01-01 08:00:05.0&#125;</span><br><span class="line">max:&gt; Event&#123;user=&#x27;ss&#x27;, url=&#x27;./fav&#x27;, timestamp=1970-01-01 08:00:04.0&#125;</span><br><span class="line">maxBy:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./home&#x27;, timestamp=1970-01-01 08:00:05.0&#125;</span><br><span class="line">max:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./home&#x27;, timestamp=1970-01-01 08:00:10.0&#125;</span><br><span class="line">maxBy:&gt; Event&#123;user=&#x27;ss&#x27;, url=&#x27;./fav&#x27;, timestamp=1970-01-01 08:00:04.0&#125;</span><br><span class="line">maxBy:&gt; Event&#123;user=&#x27;Mary&#x27;, url=&#x27;./fav&#x27;, timestamp=1970-01-01 08:00:10.0&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">max部分替换，maxby全部替换</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-窗口"><a href="#2-窗口" class="headerlink" title="2 窗口"></a>2 窗口</h2><h4 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h4><p><img src="/2022/04/24/datastream/2.jpg" alt></p><p>窗口[0-10）中有11,12,但是11,12并不在窗口[0-10）处理，而是在对应的窗口[10,20)处理</p><h4 id="2-窗口的分类"><a href="#2-窗口的分类" class="headerlink" title="2 窗口的分类"></a>2 窗口的分类</h4><ol><li>按照驱动类型分类</li></ol><p>（1）时间窗口 Time Window</p><p>（2）计数窗口 Count Window</p><ol><li>按照窗口分配数据的规则分类</li></ol><p>（1）滚动窗口 Tumbling Windows</p><p>（2）滑动窗口 Sliding Windows</p><p>（3）会话窗口 Session Windows</p><p>（4）全局窗口 Global Windows</p><h4 id="3-使用"><a href="#3-使用" class="headerlink" title="3 使用"></a>3 使用</h4><p><img src="/2022/04/24/datastream/3.jpg" alt></p><h4 id="4-迟到数据的处理"><a href="#4-迟到数据的处理" class="headerlink" title="4 迟到数据的处理"></a>4 迟到数据的处理</h4><p> 窗口中 的迟到数据默认会被丢弃，这导致计算结果不够准确</p><p>1 设置水位线延迟时间</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">assignTimestampsAndWatermarks(WatermarkStrategy.&lt;Event&gt;forBoundedOutOfOrderness(Duration.ofSeconds(2))</span><br><span class="line">        .withTimestampAssigner(new SerializableTimestampAssigner&lt;Event&gt;() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public long extractTimestamp(Event element, long recordTimestamp) &#123;</span><br><span class="line">                return element.timestamp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;));</span><br></pre></td></tr></table></figure><p>2 允许窗口处理迟到数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.allowedLateness(Time.minutes(1))</span><br></pre></td></tr></table></figure><p>3 将迟到数据放入侧输出流</p><p>收集关窗之后的迟到数据，然后手动处理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.sideOutputLateData(outputTag)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> datastream </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink优化</title>
      <link href="/2022/04/23/flink-optimization/"/>
      <url>/2022/04/23/flink-optimization/</url>
      
        <content type="html"><![CDATA[<p><a href="https://shopify.engineering/optimizing-apache-flink-applications-tips">https://shopify.engineering/optimizing-apache-flink-applications-tips</a></p><p><a href="https://cloud.tencent.com/developer/article/1897249">https://cloud.tencent.com/developer/article/1897249</a></p><h2 id="1-广播"><a href="#1-广播" class="headerlink" title="1 广播"></a>1 广播</h2><p><img src="/2022/04/23/flink-optimization/1.png" alt></p><p><a href="https://blog.csdn.net/weixin_44318830/article/details/107678101">https://blog.csdn.net/weixin_44318830/article/details/107678101</a></p><p>广播是一种操作</p><p>如果不使用广播，每一个 Task 都会拷贝一份数据集，造成内存资源浪费 ; 广播后，每个节点存一份,不同的Task 都可以在节点上获取到</p><p>1 广播变量 </p><p><a href="https://blog.csdn.net/yang_shibiao/article/details/118662134">https://blog.csdn.net/yang_shibiao/article/details/118662134</a></p><p>2 广播流 </p><p>BroadcastStream</p><p>3 广播状态  </p><p>BroadcastState</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot</title>
      <link href="/2022/04/23/springboot/"/>
      <url>/2022/04/23/springboot/</url>
      
        <content type="html"><![CDATA[<p>java框架，对spring的改进</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 框架 </category>
          
          <category> springboot </category>
          
      </categories>
      
      
        <tags>
            
            <tag> springboot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sugar</title>
      <link href="/2022/04/23/sugar/"/>
      <url>/2022/04/23/sugar/</url>
      
        <content type="html"><![CDATA[<p>百度的BI可视化工具</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 可视化报表 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sugar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>clickhouse</title>
      <link href="/2022/04/23/clickhouse/"/>
      <url>/2022/04/23/clickhouse/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/139948625">https://zhuanlan.zhihu.com/p/139948625</a></p><p>关系型，分布式，完全列式</p><p>列式指得是列存储</p><p><img src="/2022/04/23/clickhouse/1.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 关系型 </category>
          
          <category> clickhouse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> clickhouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDC（Change Data Capture）工具对比</title>
      <link href="/2022/04/23/cdc/"/>
      <url>/2022/04/23/cdc/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/u013411339/article/details/120917907">https://blog.csdn.net/u013411339/article/details/120917907</a></p><p><img src="/2022/04/23/cdc/1.webp" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> 基础组件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CDC（Change Data Capture）工具对比 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据集成</title>
      <link href="/2022/04/23/data-ensemble/"/>
      <url>/2022/04/23/data-ensemble/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/269780713">https://zhuanlan.zhihu.com/p/269780713</a></p><p><a href="https://help.aliyun.com/document_detail/137663.html">https://help.aliyun.com/document_detail/137663.html</a></p><p><img src="https://pic3.zhimg.com/80/v2-e23e36683b5c47f8480f8a3b3f083152_720w.jpg" alt="img"></p><p>异构数据集成    </p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数据集成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据集成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark优化手段</title>
      <link href="/2022/04/19/spark-optimazation-operation/"/>
      <url>/2022/04/19/spark-optimazation-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="1-多个map合并（存疑）"><a href="#1-多个map合并（存疑）" class="headerlink" title="1.多个map合并（存疑）"></a>1.多个map合并（存疑）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.map().map() -&gt;  rdd1.map() </span><br></pre></td></tr></table></figure><h2 id="2-减少job数量"><a href="#2-减少job数量" class="headerlink" title="2.减少job数量"></a>2.减少job数量</h2><p>也就是减少action</p><p>说白了就是多个action操作，transformation逻辑可以写一起，最后action</p><h2 id="3-广播大变量"><a href="#3-广播大变量" class="headerlink" title="3 广播大变量"></a>3 广播大变量</h2><p>首先广播是操作，大变量是对象</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark优化手段 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实时数仓案例（电商）</title>
      <link href="/2022/04/14/real-datawarehouse-case/"/>
      <url>/2022/04/14/real-datawarehouse-case/</url>
      
        <content type="html"><![CDATA[<h2 id="0-架构"><a href="#0-架构" class="headerlink" title="0 架构"></a>0 架构</h2><p><img src="/2022/04/14/real-datawarehouse-case/1.JPG" alt></p><h2 id="1-ods"><a href="#1-ods" class="headerlink" title="1 ods"></a>1 ods</h2><p>1 日志数据</p><p>前端（jar，产生日志数据）-》Nginx（集群间负载均衡）-》日志服务器（springboot，采集数据，jar）-》log，ods(kafka)</p><p>本地测试，本地起应用  -》  单机部署，单服务器起应用   -》   集群部署，集群起应用</p><p>2 业务数据</p><p>前端，jar，产生业务数据-》mysql,配置什么同步-》flinkcdc-》ods(kafka)</p><h2 id="2-dim、dwd"><a href="#2-dim、dwd" class="headerlink" title="2 dim、dwd"></a>2 dim、dwd</h2><p><strong>1 用户行为日志</strong></p><p>ods(Kafka)-&gt; flink -&gt;  dwd(kafka) </p><p>1 识别新老用户</p><p>业务需要</p><p>2 日志数据拆分</p><p>这3类日志，结构不同，写回Kafka不同主题</p><p><strong>2 业务数据</strong></p><p>ods(kafka)  -&gt;  flink -&gt;   1 维度数据，dim（HBASE） 2 事实数据  dwd(kafka)</p><p>1 ETL</p><p>过滤控制</p><p>2 动态分流</p><p>维度数据到hbase 事实数据到kafka</p><p>怎么分流？</p><p>ods的表里面哪些是维度表，哪些是事实表，需要提前知道表的分类信息，后面才可以分流。业务库的表会变化，表的分类信息实时更新，需要动态同步。这里将表的分类信息存在mysql，利用广播流发送。</p><h2 id="3-dwm"><a href="#3-dwm" class="headerlink" title="3 dwm"></a>3 dwm</h2><p>dmd（kafka）-&gt; flink -&gt; dwm（kafka）</p><p>1 访问uv计算</p><p>UV，unique visitor</p><p>2 跳出明细计算</p><p>跳出率=跳出次数 / 访问次数</p><p>3 订单主题表</p><p>4 支付主题表</p><h2 id="4-dws"><a href="#4-dws" class="headerlink" title="4 dws"></a>4 dws</h2><p>dwm（kafka）-&gt; flink -&gt; dws（clickhouse）</p><p>1 访客主题宽表</p><p>2 商品主题宽表</p><p>3 地区主题表</p><p>4 关键词主题表</p><h2 id="5-ads"><a href="#5-ads" class="headerlink" title="5 ads"></a>5 ads</h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 实时数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时数仓案例（电商） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>距离/相似度</title>
      <link href="/2022/04/13/distance/"/>
      <url>/2022/04/13/distance/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/138107999">https://zhuanlan.zhihu.com/p/138107999</a></p><p><a href="https://blog.csdn.net/txwh0820/article/details/51791739">https://blog.csdn.net/txwh0820/article/details/51791739</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数学基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 距离/相似度 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch常见问题</title>
      <link href="/2022/04/12/pytorch-problem/"/>
      <url>/2022/04/12/pytorch-problem/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Gather-function-not-implemented-for-CPU-tensors"><a href="#1-Gather-function-not-implemented-for-CPU-tensors" class="headerlink" title="1  Gather function not implemented for CPU tensors"></a>1  Gather function not implemented for CPU tensors</h2><p>多卡训练时候，net的forward里面存在Tensor变成其它类型的操作，比如变成numpy，list</p><p>解决：改成Tensor操作</p><h2 id="2-RuntimeError-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn"><a href="#2-RuntimeError-element-0-of-tensors-does-not-require-grad-and-does-not-have-a-grad-fn" class="headerlink" title="2 RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"></a>2 RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</h2><p><a href="https://blog.csdn.net/weixin_41990278/article/details/90311313">https://blog.csdn.net/weixin_41990278/article/details/90311313</a></p><p><a href="https://blog.csdn.net/wu_xin1/article/details/116502378">https://blog.csdn.net/wu_xin1/article/details/116502378</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka常见命令</title>
      <link href="/2022/04/11/kafka-command/"/>
      <url>/2022/04/11/kafka-command/</url>
      
        <content type="html"><![CDATA[<p>启动命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/kafka/bin/kafka-server-start.sh -daemon /opt/module/kafka/config/server.properties</span><br></pre></td></tr></table></figure><p>关闭命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/kafka/bin/kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure><p><img src="/2022/04/11/kafka-command/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka常见命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分层结构</title>
      <link href="/2022/04/10/realware-multi-layer/"/>
      <url>/2022/04/10/realware-multi-layer/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/04/10/realware-multi-layer/1.JPG" alt></p><p><img src="/2022/04/10/realware-multi-layer/2.JPG" alt></p><h2 id="DWM"><a href="#DWM" class="headerlink" title="DWM"></a>DWM</h2><p><a href="https://blog.csdn.net/jianghuaijie/article/details/122009653">https://blog.csdn.net/jianghuaijie/article/details/122009653</a></p><p><strong>作用</strong></p><p>DWM层的定位是什么，DWM层主要服务DWS，因为部分需求直接从DWD层到DWS层中间会有一定的计算量，而且这部分计算的结果很有可能被多个DWS层主题复用</p><p><strong>构建</strong></p><p>分主题</p><h2 id="dwt"><a href="#dwt" class="headerlink" title="dwt"></a>dwt</h2><p>实时数仓没有dwt，因为dwt是累计统计，实时系统不适用</p><h2 id="dws"><a href="#dws" class="headerlink" title="dws"></a>dws</h2><p><strong>作用</strong></p><p>轻度聚合，生成一系列的中间表，提升公共指标的复用性，减少重复加工</p><p>分主题，便于管理</p><p><strong>构建</strong></p><p>分主题</p><p>宽表</p><p>轻度聚合</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 实时数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实时数仓分层 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python bif(内置函数)</title>
      <link href="/2022/04/10/python-bif/"/>
      <url>/2022/04/10/python-bif/</url>
      
        <content type="html"><![CDATA[<h2 id="1-ord（）"><a href="#1-ord（）" class="headerlink" title="1 ord（）"></a>1 ord（）</h2><p>如何获取ascii码，使用内置函数ord（）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ord(&quot;A&quot;)</span><br><span class="line">65</span><br><span class="line">ord(&quot;a&quot;)</span><br><span class="line">97</span><br></pre></td></tr></table></figure><h4 id="804-唯一摩尔斯密码词"><a href="#804-唯一摩尔斯密码词" class="headerlink" title="804. 唯一摩尔斯密码词"></a><a href="https://leetcode-cn.com/problems/unique-morse-code-words/">804. 唯一摩尔斯密码词</a></h4><h2 id="2-sorted"><a href="#2-sorted" class="headerlink" title="2  sorted()"></a>2  sorted()</h2><p><a href="https://www.runoob.com/python3/python3-func-sorted.html">https://www.runoob.com/python3/python3-func-sorted.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sorted([-3,4,2],key=abs)</span><br><span class="line">Out[5]: [2, -3, 4]</span><br><span class="line">sorted([-3,4,2])</span><br><span class="line">Out[6]: [-3, 2, 4]</span><br></pre></td></tr></table></figure><h2 id="3-iter-和-next函数"><a href="#3-iter-和-next函数" class="headerlink" title="3 iter 和 next函数"></a>3 iter 和 next函数</h2><h2 id="4-dir"><a href="#4-dir" class="headerlink" title="4 dir"></a>4 dir</h2><p><a href="https://www.runoob.com/python/python-func-dir.html">https://www.runoob.com/python/python-func-dir.html</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python bif </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息抽取 Information Extraction</title>
      <link href="/2022/04/07/information-extract/"/>
      <url>/2022/04/07/information-extract/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>信息抽取是基于已有信息筛选出目标信息，不是无中生有，生成是有无中生有的能力</p><p>信息抽取主要包括三个子任务：命名实体识别、关系抽取、事件抽取。</p><p>实体抽取又称命名实体识别，其目的是从文本中抽取实体信息元素。想要从文本中进行实体抽取，首先需要从文本中识别和定位实体，然后再将识别的实体分类到预定义的类别中去<strong>。</strong></p><p>关系抽取是知识抽取的重要子任务之一，面向非结构化文本数据， 关系抽取是从文本中抽取出两个或者多个实体之间的语义关系。关系抽取与实体抽取密切相关，一般在识别出文本中的实体后，再抽取实体之间可能存在的关系，也有很多联合模型同时将这两个任务一起做了的；</p><p>事件抽取是指 从自然语言文本中抽取出用户感兴趣的事件信息，并以结构化的形式呈 现出来，例如事件发生的时间、地点、发生原因、参与者等。跟关系抽取有重叠的地方，同样也可以分为流水线方法和联合抽取方法。</p><p>例子：</p><p>1.NER命名实体识别 （实体抽取）：从文本中检测出命名实体，并将其分类到预定义的类别中，例如人物、组织、地点、时间等。图中高灰色记的文字就是命名实体，在一般情况下，命名实体识别是知识抽取其他任务的基础。<br>2.关系抽取 ：从文本中识别抽取实体及实体之间的关系。例如，从句子“[王思聪] 是万达集团董事长[王健林]的独子”中识别出实体“[王健林]”和“[王思 聪]”之间具有“父子”关系。<br>3.事件抽取：识别文本中关于事件的信息，并以结构化的形式呈现。例如，从恐怖袭击事件的新闻报道中识别袭击发生的地点、时间、袭击目标和受害人等信息。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/183966841">https://zhuanlan.zhihu.com/p/183966841</a></p><p><a href="https://zhuanlan.zhihu.com/p/376898772">https://zhuanlan.zhihu.com/p/376898772</a></p><p><a href="https://zhuanlan.zhihu.com/p/352513650">https://zhuanlan.zhihu.com/p/352513650</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 信息抽取 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息抽取 Information Extraction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive优化</title>
      <link href="/2022/04/06/hive-optimazation/"/>
      <url>/2022/04/06/hive-optimazation/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yu0_zhang0/article/details/81776459">https://blog.csdn.net/yu0_zhang0/article/details/81776459</a></p><h2 id="1-索引"><a href="#1-索引" class="headerlink" title="1 索引"></a>1 索引</h2><p><a href="https://www.jianshu.com/p/28b825367ba1">https://www.jianshu.com/p/28b825367ba1</a></p><p><a href="https://www.jianshu.com/p/d53f528daca7">https://www.jianshu.com/p/d53f528daca7</a></p><p>Hive索引的目标是提高对表的某些列进行查询查找的速度。</p><p>索引所能提供的查询速度的提高是以存储索引的磁盘空间为代价的。</p><p>Hive 3.0开始将 移除index的功能，取而代之的是Hive 2.3版本开始的物化视图，自动重写的物化视图替代了index的功能。</p><h2 id="2-物化视图"><a href="#2-物化视图" class="headerlink" title="2 物化视图"></a>2 物化视图</h2><p><a href="https://blog.csdn.net/u011447164/article/details/105790713">https://blog.csdn.net/u011447164/article/details/105790713</a></p><p>区别于普通视图</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create materialized view view2</span><br><span class="line">as</span><br><span class="line">select dept.deptno,dept.dname,emp.ename</span><br><span class="line">from emp,dept</span><br><span class="line">where emp.deptno=dept.deptno;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dfs，bfs</title>
      <link href="/2022/04/06/dfs/"/>
      <url>/2022/04/06/dfs/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/a753d5c733ec">https://www.jianshu.com/p/a753d5c733ec</a></p><h2 id="dfs"><a href="#dfs" class="headerlink" title="dfs"></a>dfs</h2><p>1 递归实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def dfs(当前节点,子节点)</span><br><span class="line">    for node in 子节点集合</span><br><span class="line">        dfs(node,new 子节点)</span><br></pre></td></tr></table></figure><h4 id="310-最小高度树"><a href="#310-最小高度树" class="headerlink" title="310. 最小高度树"></a><a href="https://leetcode-cn.com/problems/minimum-height-trees/">310. 最小高度树</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#### root 当前节点 path子节点 depth 深度</span><br><span class="line">dfs(root,path,depth)</span><br></pre></td></tr></table></figure><h4 id="695-岛屿的最大面积"><a href="#695-岛屿的最大面积" class="headerlink" title="695. 岛屿的最大面积"></a><a href="https://leetcode-cn.com/problems/max-area-of-island/">695. 岛屿的最大面积</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">###grid全部格点 cur_i，cur_j 当前节点的位置，用+1-1可以确定子节点</span><br><span class="line">dfs(self, grid, cur_i, cur_j) :</span><br></pre></td></tr></table></figure><h2 id="bfs"><a href="#bfs" class="headerlink" title="bfs"></a>bfs</h2><p>1 迭代实现</p><p>root进队，root出队，root的子节点【a,b,c】进队，a出队，a的子节点进队，b出队，b的子节点进队。。。</p><p>访问顺序（出队顺序）：root a b c 。。。 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def levelOrder(self, root: &#x27;Node&#x27;) -&gt; List[List[int]]:</span><br><span class="line">        if root==None:</span><br><span class="line">            return []</span><br><span class="line">        ans = []</span><br><span class="line">        q = [root]</span><br><span class="line">        while q:</span><br><span class="line">            # length = len(q)</span><br><span class="line">            # arr = []</span><br><span class="line">            # for _ in range(length):</span><br><span class="line">            node = q.pop(0)</span><br><span class="line">            # arr.append(node.val)</span><br><span class="line">            for chd in node.children:</span><br><span class="line">                q.append(chd)</span><br><span class="line">            # if arr:</span><br><span class="line">            ans.append(node.val)</span><br><span class="line">        return ans</span><br></pre></td></tr></table></figure><h4 id="429-N-叉树的层序遍历"><a href="#429-N-叉树的层序遍历" class="headerlink" title="429. N 叉树的层序遍历"></a><a href="https://leetcode-cn.com/problems/n-ary-tree-level-order-traversal/">429. N 叉树的层序遍历</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def levelOrder(self, root: &#x27;Node&#x27;) -&gt; List[List[int]]:</span><br><span class="line">        ans = []  ##结果</span><br><span class="line">        q = [root] ##队列</span><br><span class="line">        while q:</span><br><span class="line">            length = len(q)</span><br><span class="line">            arr = [] #出队</span><br><span class="line">            for _ in range(length):</span><br><span class="line">                node = q.pop(0)</span><br><span class="line">                if not node: </span><br><span class="line">                    continue</span><br><span class="line">                arr.append(node.val)</span><br><span class="line">                for chd in node.children:</span><br><span class="line">                    q.append(chd)</span><br><span class="line">            if arr:</span><br><span class="line">                ans.append(arr)</span><br><span class="line">        return ans</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="应用选择"><a href="#应用选择" class="headerlink" title="应用选择"></a>应用选择</h2><p>时间复杂度都为O（n）,n为全部节点</p><ol><li>BFS是用来搜索最短径路的解是比较合适的，比如求最少步数的解，最少交换次数的解，因为BFS搜索过程中遇到的解一定是离根最近的，所以遇到一个解，一定就是最优解，此时搜索算法可以终止。这个时候不适宜使用dfs，因为DFS搜索到的解不一定是离根最近的，只有全局搜索完毕，才能从所有解中找出离根的最近的解。（当然这个DFS的不足，可以使用迭代加深搜索ID-DFS去弥补）</li><li>DFS适合搜索全部的解，而正因为要搜索全部的解，那么BFS搜索过程中，遇到离根最近的解，并没有什么用，也必须遍历完整棵搜索树；DFS搜索会搜索全部，但是相比之下 DFS不用记录过多信息，所以搜素全部解的问题，DFS显然更加合适。空间优劣上，DFS是有优势的，DFS不需要保存搜索过程中的状态，而BFS在搜索过程中需要保存搜索过的状态，而且一般情况需要一个队列来记录。</li></ol><h2 id="和回溯的关系"><a href="#和回溯的关系" class="headerlink" title="和回溯的关系"></a>和回溯的关系</h2><p>回溯是算法思想，dfs和bfs是搜索解空间的手段</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/e81a16a6f771">https://www.jianshu.com/p/e81a16a6f771</a></p><p><a href="https://blog.csdn.net/weixin_41894030/article/details/95317440">https://blog.csdn.net/weixin_41894030/article/details/95317440</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dfs，bfs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp教材</title>
      <link href="/2022/04/06/nlp-reference/"/>
      <url>/2022/04/06/nlp-reference/</url>
      
        <content type="html"><![CDATA[<p>《embeddings  in natural language processing》</p><p><a href="http://josecamachocollados.com/book_embNLP_draft.pdf">http://josecamachocollados.com/book_embNLP_draft.pdf</a></p><p>《Speech and Language Processing》</p><p><a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp教材 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark oom(out of memory)问题</title>
      <link href="/2022/04/05/spark-oom/"/>
      <url>/2022/04/05/spark-oom/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/yhb315279058/article/details/51035631">https://blog.csdn.net/yhb315279058/article/details/51035631</a></p><p><a href="https://www.cnblogs.com/yanshw/p/11988347.html">https://www.cnblogs.com/yanshw/p/11988347.html</a></p><h2 id="1-driver内存不够"><a href="#1-driver内存不够" class="headerlink" title="1 driver内存不够"></a>1 driver内存不够</h2><p>增加 Driver 内存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M). </span><br></pre></td></tr></table></figure><p>1 读入数据太大 </p><p>解决思路是增加 Driver 内存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from pyspark import SparkContext</span><br><span class="line">sc = SparkContext(master=&#x27;yarn&#x27;)</span><br><span class="line">rdd = sc.parallelize(range(300000000))</span><br><span class="line"># spark-submit --master yarn-client  --driver-memory 512M  driver_oom.py    内存溢出</span><br><span class="line"># spark-submit --master yarn-client  --driver-memory 3G  driver_oom.py  可以执行</span><br></pre></td></tr></table></figure><p>2 数据回传太大，也就是聚合到driver的数据太大</p><p>解决思路是分区输出，具体做法是 foreach</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd = sc.parallelize(range(100))</span><br><span class="line">rdd.flatMap(lambda x: [&#x27;%d&#x27;%x*50 for _ in range(100000)]).collect()     # 内存溢出</span><br><span class="line"></span><br><span class="line">def func(x): print(x)</span><br><span class="line">rdd.flatMap(lambda x: [&#x27;%d&#x27;%x*50 for _ in range(100000)]).foreach(func) # 分区输出</span><br></pre></td></tr></table></figure><h2 id="2-excutor内存不够"><a href="#2-excutor内存不够" class="headerlink" title="2 excutor内存不够"></a>2 excutor内存不够</h2><p>通用的解决办法就是增加 Executor 内存 但这并不一定是最好的办法</p><p>1 map 过程产生大量对象</p><p>解决思路是减少每个 task 的大小，从而减少每个 task 的输出</p><p>具体做法是在 会产生大量对象的 map 操作前 添加 repartition(重新分区) 方法，分区成更小的块传入 map</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd.flatMap(lambda x: [&#x27;%d&#x27;%x*50 for _ in range(100000000)]).count()      # 100 * 100000000 个对象，内存溢出</span><br><span class="line">rdd.flatMap(lambda x: len([&#x27;%d&#x27;%x*50 for _ in range(100000000)])).sum()     # 内存溢出</span><br><span class="line"></span><br><span class="line">rdd.repartition(1000000).flatMap(lambda x: [&#x27;%d&#x27;%x*50 for _ in range(100000000)]).count()   </span><br></pre></td></tr></table></figure><p>2 shuffle导致</p><p>shuffle内存溢出的情况可以说都是shuffle后发生数据倾斜，单个文件过大导致 </p><p>参考数据倾斜解决方案</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark oom(out of memory)问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark容错机制</title>
      <link href="/2022/04/04/spark-error-telerant/"/>
      <url>/2022/04/04/spark-error-telerant/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/JasonDing1354/article/details/46882585">https://blog.csdn.net/JasonDing1354/article/details/46882585</a></p><p><a href="https://blog.csdn.net/dengxing1234/article/details/73613484">https://blog.csdn.net/dengxing1234/article/details/73613484</a></p><p>容错指的是一个系统在部分模块出现故障时还能否持续的对外提供服务</p><h2 id="1-Lineage机制"><a href="#1-Lineage机制" class="headerlink" title="1 Lineage机制"></a>1 Lineage机制</h2><h2 id="2-Checkpoint机制"><a href="#2-Checkpoint机制" class="headerlink" title="2 Checkpoint机制"></a>2 Checkpoint机制</h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark容错机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>电商业务</title>
      <link href="/2022/04/04/e-commerce-business-knowleage/"/>
      <url>/2022/04/04/e-commerce-business-knowleage/</url>
      
        <content type="html"><![CDATA[<p>1  SPU，SKU </p><p>SPU（Standard Product Unit）：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。</p><p>SKU = Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。</p><p>例如：iPhoneX手机就是SPU。一台银色、128G内存的、支持联通网络的iPhoneX，就是SKU。</p>]]></content>
      
      
      <categories>
          
          <category> 业务 </category>
          
          <category> 电商 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电商业务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>维度退化</title>
      <link href="/2022/04/04/degenetate-dimension/"/>
      <url>/2022/04/04/degenetate-dimension/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/wzy0623/article/details/49797421">https://blog.csdn.net/wzy0623/article/details/49797421</a></p><p><a href="https://www.google.com/search?q=Degenerate+Dimension&amp;sourceid=chrome&amp;ie=UTF-8">https://www.google.com/search?q=Degenerate+Dimension&amp;sourceid=chrome&amp;ie=UTF-8</a></p><p><a href="https://www.jamesserra.com/archive/2011/11/degenerate-dimensions/">https://www.jamesserra.com/archive/2011/11/degenerate-dimensions/</a></p><p><a href="https://blog.csdn.net/yezonghui/article/details/120131320">https://blog.csdn.net/yezonghui/article/details/120131320</a></p><p>当一个维度没有数据仓库需要的任何数据的时候就可以退化此维度，需要把退化的相关数据迁移到事实表中，然后删除退化的维度。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 维度退化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hql常见操作</title>
      <link href="/2022/04/02/hql-common-operation/"/>
      <url>/2022/04/02/hql-common-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="1-with…as…"><a href="#1-with…as…" class="headerlink" title="1 with…as…"></a>1 with…as…</h2><p><a href="https://www.jianshu.com/p/d518e9f5d5f9">https://www.jianshu.com/p/d518e9f5d5f9</a></p><p>1 好处</p><p>​    a. 提高代码可读性</p><p>​            结构清晰</p><p>​    b. 优化执行速度</p><p>​            子查询结果存在内存中，不需要重复计算        </p><p>2 用法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with table_name as(子查询语句) 其他sql;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">with temp as (</span><br><span class="line">    select * from xxx</span><br><span class="line">)</span><br><span class="line">select * from temp;</span><br></pre></td></tr></table></figure><h2 id="2-视图"><a href="#2-视图" class="headerlink" title="2 视图"></a>2 视图</h2><p>与基本表不同，它是一个虚表。在数据库中，存放的只是视图的定义，而不存放视图包含的数据项，这些项目仍然存放在原来的基本表结构中。</p><p>视图是只读的，不能向视图中插入或加载或改变数据</p><p>作用：</p><p>1 便捷</p><p>通过引入视图机制，用户可以将注意力集中在其关心的数据上（而非全部数据），这样就大大提高了用户效率与用户满意度，而且如果这些数据来源于多个基本表结构，或者数据不仅来自于基本表结构，还有一部分数据来源于其他视图，并且搜索条件又比较复杂时，需要编写的查询语句就会比较烦琐，此时定义视图就可以使数据的查询语句变得简单可行。</p><p>2 安全</p><p>定义视图可以将表与表之间的复杂的操作连接和搜索条件对用户不可见，用户只需要简单地对一个视图进行查询即可，故增加了数据的安全性，但不能提高查询效率。</p><p>创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">AS SELECT ...</span><br><span class="line"></span><br><span class="line">hive&gt; CREATE VIEW emp_30000 AS</span><br><span class="line">   &gt; SELECT * FROM employee</span><br><span class="line">   &gt; WHERE salary&gt;30000;</span><br><span class="line">   </span><br></pre></td></tr></table></figure><p>删除</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP VIEW view_name</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hql常见操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>函数</title>
      <link href="/2022/04/02/hql-function/"/>
      <url>/2022/04/02/hql-function/</url>
      
        <content type="html"><![CDATA[<h2 id="1-系统函数"><a href="#1-系统函数" class="headerlink" title="1 系统函数"></a>1 系统函数</h2><p><a href="https://www.studytime.xin/article/hive-knowledge-function.html">https://www.studytime.xin/article/hive-knowledge-function.html</a></p><p>1 get_json_object</p><p><a href="https://blog.csdn.net/weixin_43412569/article/details/105290637">https://blog.csdn.net/weixin_43412569/article/details/105290637</a></p><p><img src="/2022/04/02/hql-function/1.JPG" alt></p><p>2 nvl</p><p>空值判断转换函数</p><p><a href="https://blog.csdn.net/a850661962/article/details/101209028">https://blog.csdn.net/a850661962/article/details/101209028</a></p><p>3 coalesce</p><p><a href="https://blog.csdn.net/yilulvxing/article/details/86595725">https://blog.csdn.net/yilulvxing/article/details/86595725</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select coalesce(success_cnt,period,1) from tableA</span><br></pre></td></tr></table></figure><p>当success_cnt不为null，那么无论period是否为null，都将返回success_cnt的真实值（因为success_cnt是第一个参数），当success_cnt为null，而period不为null的时候，返回period的真实值。只有当success_cnt和period均为null的时候，将返回1。</p><p>4 collect_list和collect_set</p><p><a href="https://blog.csdn.net/weixin_30230059/article/details/113324945">https://blog.csdn.net/weixin_30230059/article/details/113324945</a></p><p><a href="https://blog.csdn.net/qq_44104303/article/details/117551807">https://blog.csdn.net/qq_44104303/article/details/117551807</a></p><p>它们都是将分组中的某列转为一个数组返回，不同的是collect_list不去重而collect_set去重。</p><p>5 named_struct</p><p><a href="https://blog.csdn.net/weixin_43597208/article/details/117554838">https://blog.csdn.net/weixin_43597208/article/details/117554838</a></p><p>做字段拼接</p><p>区别于struct，struct 是集合数据类型，一般用于建表，named_struct是字段拼接函数，一般用于查询</p><p>6 array_contains()</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array_contains(array，值)</span><br></pre></td></tr></table></figure><p>判断array中是否包含某个值，包含返回true，不包含返回false</p><p>7 cast</p><p><a href="https://www.jianshu.com/p/999176fa2730">https://www.jianshu.com/p/999176fa2730</a></p><p>显式的将一个类型的数据转换成另一个数据类型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Cast(字段名 as 转换的类型 )</span><br></pre></td></tr></table></figure><h2 id="2-用户自定义函数"><a href="#2-用户自定义函数" class="headerlink" title="2 用户自定义函数"></a>2 用户自定义函数</h2><p>UDF(User-Defined-Function)：单入单出<br>UDTF(User-Defined Table-Generating Functions)：单入多出<br>UDAF(User Defined Aggregation Function)：多入单出<br><a href="https://blog.csdn.net/qq_40579464/article/details/105903405">https://blog.csdn.net/qq_40579464/article/details/105903405</a></p><p>1.编写代码</p><p>jar不能随意编写，需要和hive对齐接口，可以借助工具import org.apache.hadoop.hive.ql.exec.UDF;</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 public class classname extends UDF</span><br><span class="line">2 编写evalute</span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/eyeofeagle/article/details/83904147">https://blog.csdn.net/eyeofeagle/article/details/83904147</a></p><p>2.打包<br>3.导入hive<br>复制到hdfs上<br>Hive安装目录的lib目录下<br>4.创建关联<br>add jar hdfs://localhost:9000/user/root/hiveudf.jar<br>create temporary function my_lower as ‘com.example.hive.<a href="https://so.csdn.net/so/search?q=udf&amp;spm=1001.2101.3001.7020">udf</a>.LowerCase’;<br>5.使用</p><p>hql udf的使用和普通内置函数一样，比如有udf1</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select udf1（col1） from table1</span><br></pre></td></tr></table></figure><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hql函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>加载数据</title>
      <link href="/2022/04/02/hql-loaddata/"/>
      <url>/2022/04/02/hql-loaddata/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/bjlhx/p/6946422.html">https://www.cnblogs.com/bjlhx/p/6946422.html</a></p><p><a href="https://blog.csdn.net/m0_49092046/article/details/109251015">https://blog.csdn.net/m0_49092046/article/details/109251015</a></p><h2 id="1-load"><a href="#1-load" class="headerlink" title="1 load"></a>1 load</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data [local] inpath ‘/opt/module/datas/student.txt’ [overwrite] | into table tabName [partition (partcol1=val1,…)];</span><br></pre></td></tr></table></figure><p>（1）load data:表示加载数据<br>（2）local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表<br>（3）inpath:表示加载数据的路径<br>（4）overwrite:表示覆盖表中已有数据，否则表示追加<br>（5）into table:表示加载到哪张表<br>（6）tabName:表示具体的表<br>（7）partition:表示上传到指定分区</p><p>例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data inpath &#x27;/origin_data/gmall/log/topic_log/2020-06-14&#x27; into table ods_log partition(dt=&#x27;2020-06-14&#x27;)</span><br></pre></td></tr></table></figure><h2 id="2-INSERT"><a href="#2-INSERT" class="headerlink" title="2 INSERT"></a>2 INSERT</h2><p><a href="https://help.aliyun.com/document_detail/73775.html">https://help.aliyun.com/document_detail/73775.html</a></p><p>insert into 和insert overwrite</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">insert into table student  partition(month=&#x27;20201022&#x27;) values(1,&#x27;zhangsan&#x27;);</span><br><span class="line">2 </span><br><span class="line">insert overwrite table student partition(month=&#x27;20201023&#x27;)</span><br><span class="line">select id, name from student where month=&#x27;20201023&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hql加载数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建表</title>
      <link href="/2022/04/02/hql-build/"/>
      <url>/2022/04/02/hql-build/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/4f60f3c923fe">https://www.jianshu.com/p/4f60f3c923fe</a></p><h2 id="0-CREATE-TABLE"><a href="#0-CREATE-TABLE" class="headerlink" title="0 CREATE  TABLE"></a>0 CREATE  TABLE</h2><p><a href="https://blog.csdn.net/Thomson617/article/details/86153924">https://blog.csdn.net/Thomson617/article/details/86153924</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">CREATE EXTERNAL TABLE dim_sku_info (</span><br><span class="line">    `id` STRING COMMENT &#x27;商品id&#x27;,</span><br><span class="line">    `price` DECIMAL(16,2) COMMENT &#x27;商品价格&#x27;,</span><br><span class="line">    `sku_name` STRING COMMENT &#x27;商品名称&#x27;,</span><br><span class="line">    `sku_desc` STRING COMMENT &#x27;商品描述&#x27;,</span><br><span class="line">    `weight` DECIMAL(16,2) COMMENT &#x27;重量&#x27;,</span><br><span class="line">    `is_sale` BOOLEAN COMMENT &#x27;是否在售&#x27;,</span><br><span class="line">    `spu_id` STRING COMMENT &#x27;spu编号&#x27;,</span><br><span class="line">    `spu_name` STRING COMMENT &#x27;spu名称&#x27;,</span><br><span class="line">    `category3_id` STRING COMMENT &#x27;三级分类id&#x27;,</span><br><span class="line">    `category3_name` STRING COMMENT &#x27;三级分类名称&#x27;,</span><br><span class="line">    `category2_id` STRING COMMENT &#x27;二级分类id&#x27;,</span><br><span class="line">    `category2_name` STRING COMMENT &#x27;二级分类名称&#x27;,</span><br><span class="line">    `category1_id` STRING COMMENT &#x27;一级分类id&#x27;,</span><br><span class="line">    `category1_name` STRING COMMENT &#x27;一级分类名称&#x27;,</span><br><span class="line">    `tm_id` STRING COMMENT &#x27;品牌id&#x27;,</span><br><span class="line">    `tm_name` STRING COMMENT &#x27;品牌名称&#x27;,</span><br><span class="line">    `sku_attr_values` ARRAY&lt;STRUCT&lt;attr_id:STRING,value_id:STRING,attr_name:STRING,value_name:STRING&gt;&gt; COMMENT &#x27;平台属性&#x27;,</span><br><span class="line">    `sku_sale_attr_values` ARRAY&lt;STRUCT&lt;sale_attr_id:STRING,sale_attr_value_id:STRING,sale_attr_name:STRING,sale_attr_value_name:STRING&gt;&gt; COMMENT &#x27;销售属性&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;创建时间&#x27;</span><br><span class="line">) COMMENT &#x27;商品维度表&#x27;</span><br></pre></td></tr></table></figure><p>1  EXTERNAL</p><p>关键字可以让用户创建一个外部表，默认是内部表</p><p>2 字段的数据类型</p><p><a href="https://blog.csdn.net/weixin_46941961/article/details/108551512">https://blog.csdn.net/weixin_46941961/article/details/108551512</a></p><p><a href="https://blog.csdn.net/weixin_43215250/article/details/90034169">https://blog.csdn.net/weixin_43215250/article/details/90034169</a></p><p>集合数据类型：Array、Map和Struct</p><h2 id="1-分区"><a href="#1-分区" class="headerlink" title="1.分区"></a>1.分区</h2><p><a href="https://www.jianshu.com/p/5dbbaea8ff41">https://www.jianshu.com/p/5dbbaea8ff41</a></p><p>PARTITIONED BY (<code>dt</code> string)</p><p>0 分类</p><p>静态分区SP（static partition）<br>动态分区DP（dynamic partition）</p><p>静态分区与动态分区的主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。</p><p>1 静态分区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">DROP TABLE IF EXISTS dwd_display_log;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_display_log(</span><br><span class="line">    `area_code` STRING COMMENT &#x27;地区编码&#x27;,</span><br><span class="line">    `brand` STRING COMMENT &#x27;手机品牌&#x27;,</span><br><span class="line">    `channel` STRING COMMENT &#x27;渠道&#x27;,</span><br><span class="line">    `is_new` STRING COMMENT &#x27;是否首次启动&#x27;,</span><br><span class="line">    `model` STRING COMMENT &#x27;手机型号&#x27;,</span><br><span class="line">    `mid_id` STRING COMMENT &#x27;设备id&#x27;,</span><br><span class="line">    `os` STRING COMMENT &#x27;操作系统&#x27;,</span><br><span class="line">    `user_id` STRING COMMENT &#x27;会员id&#x27;,</span><br><span class="line">    `version_code` STRING COMMENT &#x27;app版本号&#x27;,</span><br><span class="line">    `during_time` BIGINT COMMENT &#x27;app版本号&#x27;,</span><br><span class="line">    `page_item` STRING COMMENT &#x27;目标id &#x27;,</span><br><span class="line">    `page_item_type` STRING COMMENT &#x27;目标类型&#x27;,</span><br><span class="line">    `last_page_id` STRING COMMENT &#x27;上页类型&#x27;,</span><br><span class="line">    `page_id` STRING COMMENT &#x27;页面ID &#x27;,</span><br><span class="line">    `source_type` STRING COMMENT &#x27;来源类型&#x27;,</span><br><span class="line">    `ts` BIGINT COMMENT &#x27;app版本号&#x27;,</span><br><span class="line">    `display_type` STRING COMMENT &#x27;曝光类型&#x27;,</span><br><span class="line">    `item` STRING COMMENT &#x27;曝光对象id &#x27;,</span><br><span class="line">    `item_type` STRING COMMENT &#x27;app版本号&#x27;,</span><br><span class="line">    `order` BIGINT COMMENT &#x27;曝光顺序&#x27;,</span><br><span class="line">    `pos_id` BIGINT COMMENT &#x27;曝光位置&#x27;</span><br><span class="line">) COMMENT &#x27;曝光日志表&#x27;</span><br><span class="line">PARTITIONED BY (`dt` STRING)</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">LOCATION &#x27;/warehouse/gmall/dwd/dwd_display_log&#x27;</span><br><span class="line">TBLPROPERTIES(&#x27;parquet.compression&#x27;=&#x27;lzo&#x27;);</span><br><span class="line">--加载数据</span><br><span class="line">insert overwrite table dwd_display_log partition(dt=&#x27;2020-06-14&#x27;)</span><br><span class="line">select</span><br><span class="line">    get_json_object(line,&#x27;$.common.ar&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.ba&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.ch&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.is_new&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.md&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.mid&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.os&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.uid&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.common.vc&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.during_time&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.item&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.item_type&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.last_page_id&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.page_id&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.page.source_type&#x27;),</span><br><span class="line">    get_json_object(line,&#x27;$.ts&#x27;),</span><br><span class="line">    get_json_object(display,&#x27;$.display_type&#x27;),</span><br><span class="line">    get_json_object(display,&#x27;$.item&#x27;),</span><br><span class="line">    get_json_object(display,&#x27;$.item_type&#x27;),</span><br><span class="line">    get_json_object(display,&#x27;$.order&#x27;),</span><br><span class="line">    get_json_object(display,&#x27;$.pos_id&#x27;)</span><br><span class="line">from ods_log lateral view explode_json_array(get_json_object(line,&#x27;$.displays&#x27;)) tmp as display</span><br><span class="line">where dt=&#x27;2020-06-14&#x27;</span><br><span class="line">and get_json_object(line,&#x27;$.displays&#x27;) is not null;</span><br></pre></td></tr></table></figure><p>2 动态分区</p><p>注意分区字段dt数据来源于date_format(create_time,’yyyy-MM-dd’)</p><p>和静态分区比较，建表的时候没区别，加载数据有区别</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">--建表</span><br><span class="line">DROP TABLE IF EXISTS dwd_comment_info;</span><br><span class="line">CREATE EXTERNAL TABLE dwd_comment_info(</span><br><span class="line">    `id` STRING COMMENT &#x27;编号&#x27;,</span><br><span class="line">    `user_id` STRING COMMENT &#x27;用户ID&#x27;,</span><br><span class="line">    `sku_id` STRING COMMENT &#x27;商品sku&#x27;,</span><br><span class="line">    `spu_id` STRING COMMENT &#x27;商品spu&#x27;,</span><br><span class="line">    `order_id` STRING COMMENT &#x27;订单ID&#x27;,</span><br><span class="line">    `appraise` STRING COMMENT &#x27;评价(好评、中评、差评、默认评价)&#x27;,</span><br><span class="line">    `create_time` STRING COMMENT &#x27;评价时间&#x27;</span><br><span class="line">) COMMENT &#x27;评价事实表&#x27;</span><br><span class="line">PARTITIONED BY (`dt` STRING)</span><br><span class="line">STORED AS PARQUET</span><br><span class="line">LOCATION &#x27;/warehouse/gmall/dwd/dwd_comment_info/&#x27;</span><br><span class="line">TBLPROPERTIES (&quot;parquet.compression&quot;=&quot;lzo&quot;);</span><br><span class="line">--加载数据</span><br><span class="line">insert overwrite table dwd_comment_info partition (dt)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    user_id,</span><br><span class="line">    sku_id,</span><br><span class="line">    spu_id,</span><br><span class="line">    order_id,</span><br><span class="line">    appraise,</span><br><span class="line">    create_time,</span><br><span class="line">    date_format(create_time,&#x27;yyyy-MM-dd&#x27;)</span><br><span class="line">from ods_comment_info</span><br><span class="line">where dt=&#x27;2020-0</span><br></pre></td></tr></table></figure><h2 id="2-LOCATION"><a href="#2-LOCATION" class="headerlink" title="2 LOCATION"></a>2 LOCATION</h2><p>LOCATION ‘/warehouse/gmall/ods/ods_log’</p><p>指定数据在hdfs上的存储位置</p><h2 id="3-ROW-FORMAT"><a href="#3-ROW-FORMAT" class="headerlink" title="3 ROW FORMAT"></a>3 ROW FORMAT</h2><p><a href="https://www.imooc.com/article/12213">https://www.imooc.com/article/12213</a></p><p><a href="https://blog.csdn.net/S_Running_snail/article/details/84258162">https://blog.csdn.net/S_Running_snail/article/details/84258162</a></p><p>指定数据切分格式</p><p>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’</p><h2 id="4-STORED-AS"><a href="#4-STORED-AS" class="headerlink" title="4 STORED AS"></a>4 STORED AS</h2><p><a href="https://blog.csdn.net/ZZQHELLO2018/article/details/106175887">https://blog.csdn.net/ZZQHELLO2018/article/details/106175887</a></p><p>指定存储方式</p><p>行式存储：TEXTFILE 、SEQUENCEFILE   列式存储： ORC、PARQUET</p><h2 id="5-TBLPROPERTIES"><a href="#5-TBLPROPERTIES" class="headerlink" title="5 TBLPROPERTIES"></a>5 TBLPROPERTIES</h2><p><a href="https://blog.csdn.net/yangguosb/article/details/83651073">https://blog.csdn.net/yangguosb/article/details/83651073</a></p><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable</a></p><p>TBLPROPERTIES是表的一些属性，HIVE内置了一部分属性，使用者也可以在创建表时进行自定义；</p><p>TBLPROPERTIES (“parquet.compression”=”lzo”); </p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hql建表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql增删改查</title>
      <link href="/2022/04/02/sql-crud/"/>
      <url>/2022/04/02/sql-crud/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/Zhangxichao100/article/details/55099118">https://blog.csdn.net/Zhangxichao100/article/details/55099118</a></p><h2 id="1-增"><a href="#1-增" class="headerlink" title="1 增"></a>1 增</h2><p>1 insert</p><p>insert into table (姓名,性别,出生日期) values (‘王伟华’,’男’,’1983/6/15’)</p><p>insert into table (‘姓名’,’地址’,’电子邮件’)select name,address,email     from Strdents </p><p>2 SELECT INTO</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT column_name</span><br><span class="line">INTO newtable</span><br><span class="line">FROM table1;</span><br></pre></td></tr></table></figure><h2 id="2-删"><a href="#2-删" class="headerlink" title="2 删"></a>2 删</h2><p>1 delete</p><p>删除数据某些数据</p><p>delete from table where name=’王伟华’</p><p>2 truncate  </p><p>删除整个表的数据</p><p>truncate table addressList</p><h2 id="3-改"><a href="#3-改" class="headerlink" title="3 改"></a>3 改</h2><p>1 update</p><p>update table set 年龄=18 where 姓名=’王伟华’ </p><h2 id="4-查"><a href="#4-查" class="headerlink" title="4 查"></a>4 查</h2><p> 1 select</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql增删改查 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓主题和主题域</title>
      <link href="/2022/04/02/datawarehouse-subject/"/>
      <url>/2022/04/02/datawarehouse-subject/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_22473611/article/details/116702667">https://blog.csdn.net/qq_22473611/article/details/116702667</a></p><h2 id="1-主题域、主题、实体的关系"><a href="#1-主题域、主题、实体的关系" class="headerlink" title="1 主题域、主题、实体的关系"></a>1 主题域、主题、实体的关系</h2><p><img src="/2022/04/02/datawarehouse-subject/1.png" alt></p><p>主题域下面可以有多个主题，主题还可以划分成更多的子主题，主题和主题之间的建设可能会有交叉现象，而实体则是不可划分的最小单位</p><h2 id="2-主题域划分"><a href="#2-主题域划分" class="headerlink" title="2 主题域划分"></a>2 主题域划分</h2><p>1  按照业务系统划分</p><p><img src="/2022/04/02/datawarehouse-subject/2.png" alt></p><p>2 按照业务过程划分</p><p><img src="/2022/04/02/datawarehouse-subject/3.png" alt></p><p>3 按照部门划分</p><p><img src="/2022/04/02/datawarehouse-subject/4.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓主题和主题域 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ad hoc和routing</title>
      <link href="/2022/04/01/ad-doc-routing/"/>
      <url>/2022/04/01/ad-doc-routing/</url>
      
        <content type="html"><![CDATA[<h2 id="0-区别"><a href="#0-区别" class="headerlink" title="0 区别"></a>0 区别</h2><p><a href="https://blog.csdn.net/memray/article/details/41149633">https://blog.csdn.net/memray/article/details/41149633</a></p><p>ad hoc类似于图书馆里的书籍检索，即书籍库(数据库)相对稳定不变，不同用户的查询要求是千变万化的。</p><p>routing的情况与ad hoc相对，用户的查询要求相对稳定。在routing中，查询常常称为pro file，也就是通常所说的兴趣，用户的兴趣在一段时间内是稳定不变的，但是数据库(更确切的说，是数据流)是不断变化的。这种任务很像我们所说的新闻定制什么的，比如用户喜欢体育，这个兴趣在一段时间内是不变的，而体育新闻在不断变化。</p><h2 id="1-ad-hoc"><a href="#1-ad-hoc" class="headerlink" title="1 ad hoc"></a>1 ad hoc</h2><p><a href="https://ils.unc.edu/courses/2017_fall/inls509_002/lectures/03-IntroductionToAdhocRetrieval.pdf">https://ils.unc.edu/courses/2017_fall/inls509_002/lectures/03-IntroductionToAdhocRetrieval.pdf</a></p><p>Ad-hoc Retrieval 是什么</p><p>Given a query and a corpus, find the relevant items </p><p>‣ query: textual description of information need </p><p>‣ corpus: a collection of textual documents </p><p>‣ relevance: satisfaction of the user’s information need </p><h2 id="2-routing"><a href="#2-routing" class="headerlink" title="2 routing"></a>2 routing</h2>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 搜索系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ad hoc和routing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>collections</title>
      <link href="/2022/04/01/python-counter/"/>
      <url>/2022/04/01/python-counter/</url>
      
        <content type="html"><![CDATA[<p>1 Counter</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from collections import Counter</span><br><span class="line">Counter([-3,4,2,2])</span><br><span class="line">Out[8]: Counter(&#123;-3: 1, 4: 1, 2: 2&#125;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> collections </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Deep Look into Neural Ranking Models for Information Retrieval</title>
      <link href="/2022/03/31/ranking-survey/"/>
      <url>/2022/03/31/ranking-survey/</url>
      
        <content type="html"><![CDATA[<p><a href="https://par.nsf.gov/servlets/purl/10277191">https://par.nsf.gov/servlets/purl/10277191</a></p><h2 id="3-A-Unified-Model-Formulation"><a href="#3-A-Unified-Model-Formulation" class="headerlink" title="3 A Unified Model Formulation"></a>3 A Unified Model Formulation</h2><p> So a generalized LTR problem is to find the optimal ranking function f ∗ by minimizing the loss function over some labeled dataset</p><p><img src="/2022/03/31/ranking-survey/1.JPG" alt></p><p>f 是ranking function，s是query，t是候选集，y is the label set where labels represent grades</p><p>Without loss of generality, the ranking function f could be further abstracted by the following unified formulation</p><p><img src="/2022/03/31/ranking-survey/2.JPG" alt></p><p>ψ, ϕare representation functions which extract features from s and t respectively η is the interaction function which extracts features from (s, t) pair, and g is the evaluation function which computes the relevance score based on the feature representations.</p><h2 id="4-Model-Architecture"><a href="#4-Model-Architecture" class="headerlink" title="4. Model Architecture"></a>4. Model Architecture</h2><h3 id="4-1-Symmetric-vs-Asymmetric-Architectures"><a href="#4-1-Symmetric-vs-Asymmetric-Architectures" class="headerlink" title="4.1. Symmetric vs. Asymmetric Architectures"></a>4.1. Symmetric vs. Asymmetric Architectures</h3><p><strong>Symmetric Architecture:</strong> The inputs s and t are assumed to be homogeneous, so that symmetric network structure could be applied over the inputs</p><p><strong>Asymmetric Architecture</strong>: The inputs s and t are assumed to be heterogeneous, so that asymmetric network structures should be applied over the inputs</p><h3 id="4-2-Representation-focused-vs-Interaction-focused-Architectures"><a href="#4-2-Representation-focused-vs-Interaction-focused-Architectures" class="headerlink" title="4.2. Representation-focused vs. Interaction-focused Architectures"></a>4.2. Representation-focused vs. Interaction-focused Architectures</h3><p><img src="/2022/03/31/ranking-survey/3.JPG" alt></p><p><strong>Representation-focused Architecture</strong>: The underlying assumption of this type of architecture is that relevance depends on compositional meaning of the input texts. Therefore, models in this category usually define complex representation functions ϕ and ψ (i.e., deep neural networks), but no interaction function η</p><p><strong>Interaction-focused Architecture:</strong> The underlying assumption of this type of architecture is that relevance is in essence about the relation between the input texts, so it would be more effective to directly learn from interactions rather than from individual representations. Models in this category thus define the interaction function η rather than the representation functions ϕ and ψ</p><p><strong>Hybrid Architecture</strong>: In order to take advantage of both representation focused and interaction-focused architectures, a natural way is to adopt a hybrid architecture for feature learning. We find that there are two major hybrid strategies to integrate the two architectures, namely combined strategy and coupled strategy.</p><h3 id="4-3-Single-granularity-vs-Multi-granularity-Architecture"><a href="#4-3-Single-granularity-vs-Multi-granularity-Architecture" class="headerlink" title="4.3. Single-granularity vs. Multi-granularity Architecture"></a>4.3. Single-granularity vs. Multi-granularity Architecture</h3><p><strong>Single-granularity Architecture</strong>: The underlying assumption of the single granularity architecture is that relevance can be evaluated based on the high level features extracted by ϕ, ψ and η from the single-form text inputs.</p><p><strong>Multi-granularity Architecture:</strong> The underlying assumption of the multigranularity architecture is that relevance estimation requires multiple granularities of features, either from different-level feature abstraction or based on different types of language units of the inputs</p><h2 id="5-Model-Learning"><a href="#5-Model-Learning" class="headerlink" title="5. Model Learning"></a>5. Model Learning</h2><h3 id="5-1-Learning-objective"><a href="#5-1-Learning-objective" class="headerlink" title="5.1. Learning objective"></a>5.1. Learning objective</h3><p>Similar to other LTR algorithms, the learning objective of neural ranking models can be broadly categorized into three groups: pointwise, pairwise, and listwise.</p><h4 id="5-1-1-Pointwise-Ranking-Objective"><a href="#5-1-1-Pointwise-Ranking-Objective" class="headerlink" title="5.1.1. Pointwise Ranking Objective"></a>5.1.1. Pointwise Ranking Objective</h4><p>1 loss</p><p>The idea of pointwise ranking objectives is to simplify a ranking problem to a set of classification or regression problems</p><p><img src="/2022/03/31/ranking-survey/4.JPG" alt></p><p>a. Cross Entropy</p><p>For example, one of the most popular pointwise loss functions used in neural ranking models is Cross Entropy:</p><p><img src="/2022/03/31/ranking-survey/5.JPG" alt></p><p>b. Mean Squared Error</p><p>There are other pointwise loss functions such as Mean Squared Error for numerical labels, but they are more commonly used in recommendation tasks.</p><p>2 优缺点</p><p>a.advantages</p><p>First, it simple and easy to scale. Second, the outputs  <strong>have real meanings and value in practice.</strong> For instance, in sponsored search, a model learned with cross entropy loss and clickthrough rates can directly predict the probability of user clicks on search ads, which is more important than creating a good result list in some application scenarios. </p><p>b.disadvantages</p><p>less effective ，Because pointwise loss functions consider no document preference or order information, they do not guarantee to produce the best ranking list when the model loss reaches the global minimum. </p><h4 id="5-1-2-Pairwise-Ranking-Objective"><a href="#5-1-2-Pairwise-Ranking-Objective" class="headerlink" title="5.1.2. Pairwise Ranking Objective"></a>5.1.2. Pairwise Ranking Objective</h4><p>1 loss</p><p>Pairwise ranking objectives focus on optimizing the relative preferences between documents rather than their labels.</p><p><img src="/2022/03/31/ranking-survey/6.JPG" alt></p><p>a.Hinge loss</p><p>b.cross entropy</p><p>​        RankNet</p><p>2 优缺点</p><p>a.advantages</p><p>effective in many tasks</p><p>b.disadvantages</p><p>pairwise methods does not always lead to the improvement of final ranking metrics due to two reasons: (1) it is impossible to develop a ranking model that can correctly predict document preferences in all cases; and (2) in the computation of most existing ranking metrics, not all document pairs are equally important.</p><h4 id="5-1-3-Listwise-Ranking-Objective"><a href="#5-1-3-Listwise-Ranking-Objective" class="headerlink" title="5.1.3. Listwise Ranking Objective"></a>5.1.3. Listwise Ranking Objective</h4><p>1 loss</p><p>listwise loss functions compute ranking loss with each query and their candidate document list together</p><p><img src="/2022/03/31/ranking-survey/9.JPG" alt></p><p>a. ListMLE</p><p><a href="https://blog.csdn.net/qq_36478718/article/details/122598406">https://blog.csdn.net/qq_36478718/article/details/122598406</a></p><p><img src="/2022/03/31/ranking-survey/10.JPG" alt></p><p>b.Attention Rank function</p><p><a href="https://arxiv.org/abs/1804.05936">https://arxiv.org/abs/1804.05936</a></p><p><img src="/2022/03/31/ranking-survey/11.JPG" alt></p><p>c. softmax-based listwise</p><p><a href="https://arxiv.org/pdf/1811.04415.pdf">https://arxiv.org/pdf/1811.04415.pdf</a></p><p>2 优缺点</p><p>a.advantages</p><p>While listwise ranking objectives are generally more effective than pairwise ranking objectives</p><p>b.disadvantages</p><p>their high computational cost often limits their applications. They are suitable for the re-ranking phase over a small set of candidate documents</p><h4 id="5-1-4-Multi-task-Learning-Objective"><a href="#5-1-4-Multi-task-Learning-Objective" class="headerlink" title="5.1.4. Multi-task Learning Objective"></a>5.1.4. Multi-task Learning Objective</h4><p> the optimization of neural ranking models may include the learning of multiple ranking or non-ranking objectives at the same time. </p><h3 id="5-2-Training-Strategies"><a href="#5-2-Training-Strategies" class="headerlink" title="5.2. Training Strategies"></a>5.2. Training Strategies</h3><p>1 Supervised learning</p><p>2 Weakly supervised learning</p><p>3 Semi-supervised learning</p><h2 id="6-Model-Comparison"><a href="#6-Model-Comparison" class="headerlink" title="6. Model Comparison"></a>6. Model Comparison</h2><p>比较了常见模型在不同应用的效果</p><p>1  Ad-hoc Retrieval</p><p><a href="https://blog.csdn.net/qq_44092699/article/details/106335971">https://blog.csdn.net/qq_44092699/article/details/106335971</a></p><p>Ad-hoc information retrieval refers to the task of returning information resources related to a user query formulated in natural language.</p><p>2 QA</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> 排序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ranking survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive MetaStore</title>
      <link href="/2022/03/30/hive-metadatastore/"/>
      <url>/2022/03/30/hive-metadatastore/</url>
      
        <content type="html"><![CDATA[<h2 id="1-描述"><a href="#1-描述" class="headerlink" title="1 描述"></a>1 描述</h2><p>Hive MetaStore - It is a central repository that stores all the structure information of various tables and partitions in the warehouse. It also includes metadata of column and its type information, the serializers and deserializers which is used to read and write data and the corresponding HDFS files where the data is stored.</p><h2 id="2-Hive的元数据存储-Metastore三种配置方式"><a href="#2-Hive的元数据存储-Metastore三种配置方式" class="headerlink" title="2 Hive的元数据存储(Metastore三种配置方式)"></a>2 Hive的元数据存储(Metastore三种配置方式)</h2><p>Embedded，Local，Remote</p><p><a href="https://blog.csdn.net/epitomizelu/article/details/117091656">https://blog.csdn.net/epitomizelu/article/details/117091656</a></p><p><a href="https://zhuanlan.zhihu.com/p/473378621">https://zhuanlan.zhihu.com/p/473378621</a></p><p><a href="https://blog.csdn.net/qq_40990732/article/details/80914873">https://blog.csdn.net/qq_40990732/article/details/80914873</a></p><h2 id="3-Hive元数据库介绍"><a href="#3-Hive元数据库介绍" class="headerlink" title="3 Hive元数据库介绍"></a>3 Hive元数据库介绍</h2><p><a href="https://blog.csdn.net/victorzzzz/article/details/81874674">https://blog.csdn.net/victorzzzz/article/details/81874674</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive MetaStore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分库分表</title>
      <link href="/2022/03/30/database-split-table-base/"/>
      <url>/2022/03/30/database-split-table-base/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/4487756131">https://www.zhihu.com/question/4487756131</a></p><p><a href="https://zhuanlan.zhihu.com/p/137368446">https://zhuanlan.zhihu.com/p/137368446</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分库分表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>open set recognization</title>
      <link href="/2022/03/29/open-set-recognization/"/>
      <url>/2022/03/29/open-set-recognization/</url>
      
        <content type="html"><![CDATA[<p>1 A Survey on Open Set Recognition</p><p><a href="https://arxiv.org/abs/2109.00893">https://arxiv.org/abs/2109.00893</a></p><p>2 Open-Set Recognition: A Good Closed-Set Classifier is All You Need</p><p><a href="https://arxiv.org/abs/2110.06207">https://arxiv.org/abs/2110.06207</a></p><p>3 Recent Advances in Open Set Recognition: A Survey</p><p><a href="https://arxiv.org/abs/1811.08581">https://arxiv.org/abs/1811.08581</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> open set recognization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> open set recognization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ptm之间的联系</title>
      <link href="/2022/03/28/ptm-relation/"/>
      <url>/2022/03/28/ptm-relation/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/03/28/ptm-relation/1.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ptm之间的联系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Table API和SQL</title>
      <link href="/2022/03/27/flink-tableapi-sql/"/>
      <url>/2022/03/27/flink-tableapi-sql/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_45366499/article/details/115449175">https://blog.csdn.net/weixin_45366499/article/details/115449175</a></p><h2 id="0-原理"><a href="#0-原理" class="headerlink" title="0 原理"></a>0 原理</h2><p>1 动态表</p><p>flink中的表是动态表</p><p>静态表：hive，mysql等</p><p>动态表：不断更新</p><p>2 持续查询</p><p><img src="/2022/03/27/flink-tableapi-sql/2.jpg" alt></p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>Apache Flink 有两种关系型 API 来做流批统一处理：Table API 和 SQL。</p><p>Table API 是用于 Scala 和 Java 语言的查询 API，它可以用一种非常直观的方式来 组合使用选取、过滤、join 等关系型算子。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table maryClickTable = eventTable</span><br><span class="line">.where($(&quot;user&quot;).isEqual(&quot;alice&quot;))</span><br><span class="line">.select($(&quot;url&quot;), $(&quot;user&quot;));</span><br></pre></td></tr></table></figure><p>SQL 是基于 Apache Calcite 来实现的标准 SQL</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Table urlCountTable = tableEnv.sqlQuery(</span><br><span class="line">&quot;SELECT user, COUNT(url) FROM EventTable GROUP BY user&quot;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h2 id="2-框架"><a href="#2-框架" class="headerlink" title="2 框架"></a>2 框架</h2><p>表环境和流执行环境不同</p><p><img src="/2022/03/27/flink-tableapi-sql/1.JPG" alt></p><h2 id="3-流表相互转化"><a href="#3-流表相互转化" class="headerlink" title="3 流表相互转化"></a>3 流表相互转化</h2><p>stream 《——》table</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tableEnv表环境</span><br><span class="line">// 将数据流eventstream转换成表eventTable</span><br><span class="line">Table eventTable = tableEnv.fromDataStream(eventstream);</span><br><span class="line"></span><br><span class="line">// 将表visitTable转换成数据流，打印输出</span><br><span class="line">tableEnv.toDataStream(visitTable).print();</span><br></pre></td></tr></table></figure><h2 id="4-连接外部系统"><a href="#4-连接外部系统" class="headerlink" title="4 连接外部系统"></a>4 连接外部系统</h2><p>可以在创建表的时候用 WITH子句指定连接器connector</p><h2 id="5-客户端"><a href="#5-客户端" class="headerlink" title="5 客户端"></a>5 客户端</h2><p>./bin/sql client.sh</p><h2 id="6-时间属性"><a href="#6-时间属性" class="headerlink" title="6 时间属性"></a>6 时间属性</h2><p> 事件事件、处理事件</p><ol><li>在创建表的 DDL中定义</li><li>在数据流转换为表时定义</li></ol><h2 id="7-窗口"><a href="#7-窗口" class="headerlink" title="7 窗口"></a>7 窗口</h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Table API和SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>容错机制</title>
      <link href="/2022/03/27/fault-tolerance/"/>
      <url>/2022/03/27/fault-tolerance/</url>
      
        <content type="html"><![CDATA[<p>在分布式架构中，当某个节点出现故障，其他节点基本不受影响。这时只需要重启应用，恢复之前某个时间点的状态继续处理就可以了。这一切看似简单，可是在实时流处理中，我们不仅需要保证故障后能够重启继续运行，还要保证结果的正确性、故障恢复的速度、对处理性能的影响，这就需要在架构上做出更加精巧的设计。<br>在Flink中，有一套完整的容错机制（ fault tolerance）来保证故障后的恢复，其中最重要的就是检查点（ checkpoint）。在第九章中，我们已经介绍过检查点的基本概念和用途，接下来我 们就深入探讨一下检查点的原理和 Flink的容错机制。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 容错机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>状态编程</title>
      <link href="/2022/03/27/flink-state-program/"/>
      <url>/2022/03/27/flink-state-program/</url>
      
        <content type="html"><![CDATA[<h2 id="0-状态管理机制"><a href="#0-状态管理机制" class="headerlink" title="0 状态管理机制"></a>0 状态管理机制</h2><h2 id="1-算子任务分类"><a href="#1-算子任务分类" class="headerlink" title="1 算子任务分类"></a>1 算子任务分类</h2><p>1 无状态</p><p><img src="/2022/03/27/flink-state-program/1.JPG" alt></p><p>2 有状态</p><p><img src="/2022/03/27/flink-state-program/2.JPG" alt></p><h2 id="2-状态分类"><a href="#2-状态分类" class="headerlink" title="2 状态分类"></a>2 状态分类</h2><p>Flink 有两种状态：托管状态（Managed State）和原始状态（Raw State）。一般情况使用托管状态，只有在托管状态无法实现特殊需求，才会使用原始转态，一般情况不使用。</p><p>托管状态分类：算子状态（Operator State）和按键分区状态（Keyed State）</p><p>1 按键分区状态</p><p><img src="/2022/03/27/flink-state-program/4.JPG" alt></p><p>2  算子状态</p><p><img src="/2022/03/27/flink-state-program/3.JPG" alt></p><p>3  广播状态 Broadcast State</p><p>   特殊的算子状态</p><h2 id="3-状态持久化"><a href="#3-状态持久化" class="headerlink" title="3 状态持久化"></a>3 状态持久化</h2><p><img src="/2022/03/27/flink-state-program/5.jpg" alt></p><p>对状态进行持久化（ persistence）保存，这样就可以在发生故障后进行重启恢复。</p><p>flink状态持久化方式：写入一个“检查点”（ checkpoint）或者保存点 savepoint<br>保存到外部存储系统中。具体的存储介质，一般是分布式文件系统（ distributed file system）。</p><h2 id="4-状态后端-State-Backends"><a href="#4-状态后端-State-Backends" class="headerlink" title="4 状态后端  State Backends"></a>4 状态后端  State Backends</h2><p>在Flink中，状态的存储、访问以及维护，都是由一个可插拔的组件决定的，这个组件就<br>叫作状态后端（ state backend）。状态后端主要负责两件事：一是本地的状态管理，二是将检查<br>点（ checkpoint）写入远程的 持久化存储。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 状态编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink cep</title>
      <link href="/2022/03/27/flink-cep/"/>
      <url>/2022/03/27/flink-cep/</url>
      
        <content type="html"><![CDATA[<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0 简介"></a>0 简介</h2><p>类似的多个事件的组合，我们把它叫作“复杂事件”。对于复杂时间的处理，由于涉及到事件的严格顺序，有时还有时间约束，我们很难直接用 SQL或DataStream API来完成。于是只好放大招 派底层的处理函数（ process function）上阵了。处理函数确实可以搞定这些需求，不过对于非常复杂的组合事件，我们可能需要设置很多状态、定时器，并在代码中定义各种条件分支（ if else）逻辑来处理，复杂度会非常高，很可能会使代码失去可读性。怎 样处理这类复杂事件呢？ Flink为我们提供了专门用于处理复杂事件的库 CEP，可以让我们更加轻松地解决这类棘手的问题。这在企业的实时风险控制中有非常重要的作用。</p><p>Complex Event Processing，flink 专门用来处理复杂事件的库</p><h2 id="1-原理"><a href="#1-原理" class="headerlink" title="1 原理"></a>1 原理</h2><p>cep底层是状态机</p><p>复杂事件可以通过设计状态机来处理，用户自己写容易出错，cep帮我们封装好，用户写顶层逻辑就可以了</p><h2 id="2-核心步骤"><a href="#2-核心步骤" class="headerlink" title="2 核心步骤"></a>2 核心步骤</h2><p><img src="/2022/03/27/flink-cep/1.jpg" alt></p><p>总结起来，复杂事件处理（CEP）的流程可以分成三个步骤<br>（1）定义一个<strong>匹配规则</strong><br>（2）将匹配规则应用到事件流上，<strong>检测</strong>满足规则的复杂事件<br>（3）对检测到的 复杂事件进行<strong>处理</strong>，得到结果进行输出</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink cep </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多流转换</title>
      <link href="/2022/03/25/multi-stream-flink/"/>
      <url>/2022/03/25/multi-stream-flink/</url>
      
        <content type="html"><![CDATA[<p>简单划分的话，多流转换可以分为“分流”和“合流”两大类。在 Flink中，分流操作可以通过处理函数的侧输出流（ side output）很容易地实现<br>而合流则提供不同层级的各种 API</p><p>任务：</p><p>stream数量 ，每个stream可以有多个子任务（并行度）</p><p>keyby只能算分组，不算分流</p><p>资源：</p><p>task manager数量，slot数量</p><h2 id="1-分流"><a href="#1-分流" class="headerlink" title="1 分流"></a>1 分流</h2><p>所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个<br>DataStream，得到完全平等的多个子 DataStream，如图 8-1所示。一般来说，我们会定义一些<br>筛选条件，将符合条件的数据拣选出来放到对应的流里。</p><p><img src="/2022/03/25/multi-stream-flink/1.JPG" alt></p><p>在Flink 1.13版本中，已经弃用了 .split()方法，取而代之的是直接用处理函数（ process function）的侧输出流 （side output）。</p><h2 id="2-合流"><a href="#2-合流" class="headerlink" title="2 合流"></a>2 合流</h2><p>既然一条流可以分开，自然多条流就可以合并。在实际应用中，我们经常会遇到来源不同<br>的多条流，需要将它们的数据进行联合处理。所以 Flink中合流的操作会更加普遍，对应的<br>API也更加丰富。</p><h3 id="2-1-基本合流"><a href="#2-1-基本合流" class="headerlink" title="2.1 基本合流"></a>2.1 基本合流</h3><p>1 联合（ Union）</p><p><img src="/2022/03/25/multi-stream-flink/2.JPG" alt></p><p>可以多条（大于2）合并，数据类型必须一致</p><p>2 连接（ Connect）</p><p>必须两条，数据类型可以不同</p><h3 id="2-2-双流联结-join"><a href="#2-2-双流联结-join" class="headerlink" title="2.2 双流联结 join"></a>2.2 双流联结 join</h3><p>对于两条流的合并，很多情况我们并不是简单地将所有数据放在一起，而是希望根据某个<br>字段的值将它们联结起来，“配对”去做处理。</p><p>1 窗口联结（ Window Join</p><p>2 间隔联结（ Interval Join</p><p>3 窗口同组联结（ Window CoGroup</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多流转换 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>处理函数(process funtion)</title>
      <link href="/2022/03/22/flink-processfunction/"/>
      <url>/2022/03/22/flink-processfunction/</url>
      
        <content type="html"><![CDATA[<p>处理函数位于底层，操作麻烦，但是使用更加灵活，是flink的“核武器”，轻易不用，但是一定行。</p><p>在处理函数中，我们直面的就是数据流中最基本的元素：数据事件（event）、状态 state<br>以及时间（ time）。</p><p><a href="https://blog.51cto.com/u_15349018/3698518">https://blog.51cto.com/u_15349018/3698518</a></p><h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1 分类"></a>1 分类</h2><p>8种不同的处理函数</p><p>每个处理函数使得的时候注意两个关键函数</p><p>1 processElement</p><p>必须</p><p>元素基本处理</p><p>2 onTimer()</p><p>非必须</p><p>就是设置定时器，然后触发操作</p><h2 id="2-侧输出流（-Side-Output）"><a href="#2-侧输出流（-Side-Output）" class="headerlink" title="2 侧输出流（ Side Output）"></a>2 侧输出流（ Side Output）</h2><p>1 主流</p><p>collect </p><p>2 分流</p><p>处理函数的processElement或者onTimer中使用.output （outputTag，数据）</p><p>获取侧输出流</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Stream.getSideOutput(outputTag)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 处理函数(process funtion) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>watermark(水位线)</title>
      <link href="/2022/03/21/flink-watermark/"/>
      <url>/2022/03/21/flink-watermark/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/lmalds/article/details/52704170">https://blog.csdn.net/lmalds/article/details/52704170</a></p><p><a href="https://blog.csdn.net/lightupworld/article/details/116697831">https://blog.csdn.net/lightupworld/article/details/116697831</a></p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>在实际应用中，一般会采用事件时间语义</p><p>水位线可以看作在数据流中加入一个时钟标记，记录当前的事件时间；这个标记可以直接广播到下游，当下游任务收到这个标记，就可以更新自己的时钟了。</p><p><img src="/2022/03/21/flink-watermark/4.JPG" alt></p><h2 id="2-分类"><a href="#2-分类" class="headerlink" title="2 分类"></a>2 分类</h2><p>1 有序流的水位线</p><p><img src="/2022/03/21/flink-watermark/2.JPG" alt></p><p>方框是事件时间，虚线是水位线，箭头表示数据到达的顺序，比如事件时间为2的数据第一个到，事件时间为5的数据第二个到</p><p>2 乱序流的水位线</p><p><img src="/2022/03/21/flink-watermark/4.png" alt></p><p>比水位线小的数据在后面出现，也就是说本应该在水位线之前出现的数据晚到了，就是迟到数据，迟到数据是丢弃的，比如w（9）后面的8，9</p><p><img src="/2022/03/21/flink-watermark/3.JPG" alt></p><h2 id="3-如何生成水位线"><a href="#3-如何生成水位线" class="headerlink" title="3 如何生成水位线"></a>3 如何生成水位线</h2><p><strong>1 原则</strong></p><p>我们知道，完美的水位线是“绝对正确”的，也就是一个水位线一旦出现，就表示这个时间之前的数据已经全部到齐、之后再也不会出现了。而完美的东西总是可望不可即，我们只能尽量去保证水位线的正确。</p><p>“等” 或者说“延迟”</p><p>为了均衡实时性（少等，会引入大量迟到数据）和准确性（减少迟到数据，多等）</p><p><strong>2 怎么写</strong></p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/event-time/generating_watermarks/">https://nightlies.apache.org/flink/flink-docs-release-1.14/zh/docs/dev/datastream/event-time/generating_watermarks/</a></p><p>1  assignTimestampsAndWatermarks</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Datastream &lt;event&gt;    withTimestampsAndWatermarks = stream.assignTimestampsAndWatermarks(WatermarkStrategy)</span><br></pre></td></tr></table></figure><p>WatermarkStrategy</p><p>内置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">WatermarkStrategy.forMonotonousTimestamps</span><br><span class="line">WatermarkStrategy.forBoundedOutOfOrderness</span><br></pre></td></tr></table></figure><p>自定义</p><p>先实现接口WatermarkGenerator，然后改写一些东西</p><p>2 自定义数据源中发送水位线</p><p>注意：自定义数据源也可以用assignTimestampsAndWatermarks，这里是指在自定义数据源中实现自己的水位线发送，不用assignTimestampsAndWatermarks</p><h2 id="4-水位线传递"><a href="#4-水位线传递" class="headerlink" title="4 水位线传递"></a>4 水位线传递</h2><p><img src="/2022/03/21/flink-watermark/1.JPG" alt></p><p>上游并行度为4，下游并行度为3</p><p>策略：木桶原理，取得是最小的</p><p>为什么木桶原则，以准为第一要义，举个例子图2，如果不是3，是4，那么3不就迟到了吗</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> watermark(水位线) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间语义(Notions of Time)</title>
      <link href="/2022/03/21/flink-time/"/>
      <url>/2022/03/21/flink-time/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/lomodays207/article/details/109642581">https://blog.csdn.net/lomodays207/article/details/109642581</a></p><p><img src="/2022/03/21/flink-time/1.JPG" alt></p><p>在Flink中，由于处理时间比较简单，早期版本默认的时间语义是处理时间；而考虑到事件时间在实际应用中更为广泛，从 1.12版本 开始 Flink已经将 事件时间作为了默认的时间语义。</p><h2 id="1-处理时间（-Processing-Time"><a href="#1-处理时间（-Processing-Time" class="headerlink" title="1.处理时间（ Processing Time)"></a>1.处理时间（ Processing Time)</h2><p>处理时间的概念非常简单，就是指执行处理操作的机器的系统时间。</p><h2 id="2-事件时间（-Event-Time"><a href="#2-事件时间（-Event-Time" class="headerlink" title="2.事件时间（ Event Time)"></a>2.事件时间（ Event Time)</h2><p>事件时间，是指每个事件在对应的设备上发生的时间，也就是数据生成的时间。</p><h2 id="3-摄入时间（-Ingestion-Time）"><a href="#3-摄入时间（-Ingestion-Time）" class="headerlink" title="3  摄入时间（ Ingestion Time）"></a>3  摄入时间（ Ingestion Time）</h2><p>它是指数据进入 Flink数据流的时间，也就是 Source算子读入数据的时间。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink时间语义 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink分层api</title>
      <link href="/2022/03/20/flink-api/"/>
      <url>/2022/03/20/flink-api/</url>
      
        <content type="html"><![CDATA[<p>Flink 根据抽象程度分层，提供了三种不同的 API。每一种 API 在简洁性和表达力上有着不同的侧重，并且针对不同的应用场景。</p><p><img src="/2022/03/20/flink-api/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink分层api </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>处理无界和有界数据</title>
      <link href="/2022/03/20/unboundstream-boundstream/"/>
      <url>/2022/03/20/unboundstream-boundstream/</url>
      
        <content type="html"><![CDATA[<p><a href="https://flink.apache.org/zh/flink-architecture.html">https://flink.apache.org/zh/flink-architecture.html</a></p><p>任何类型的数据都可以形成一种事件流。信用卡交易、传感器测量、机器日志、网站或移动应用程序上的用户交互记录，所有这些数据都形成一种流。</p><p>数据可以被作为无界或者有界流来处理。</p><ol><li><strong>无界流</strong> 有定义流的开始，但没有定义流的结束。它们会无休止地产生数据。无界流的数据必须持续处理，即数据被摄取后需要立刻处理。我们不能等到所有数据都到达再处理，因为输入是无限的，在任何时候输入都不会完成。处理无界数据通常要求以特定顺序摄取事件，例如事件发生的顺序，以便能够推断结果的完整性。</li><li><strong>有界流</strong> 有定义流的开始，也有定义流的结束。有界流可以在摄取所有数据后再进行计算。有界流所有数据可以被排序，所以并不需要有序摄取。有界流处理通常被称为批处理</li></ol><p><img src alt><img src="/2022/03/20/unboundstream-boundstream/1.png" alt="1"></p><p>Apache Flink 擅长处理无界和有界数据集精确的时间控制和状态化使得 Flink 的运行时(runtime)能够运行任何处理无界流的应用。有界流则由一些专为固定大小数据集特殊设计的算法和数据结构进行内部处理，产生了出色的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 处理无界和有界数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink程序构成部分</title>
      <link href="/2022/03/20/flink-program-struct/"/>
      <url>/2022/03/20/flink-program-struct/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/03/20/flink-program-struct/1.JPG" alt></p><p>⚫ 获取执行环境（ execution environment）<br>⚫ 读取数据源（ source）<br>⚫ 定义基于数据的转换操作（ transformations）<br>⚫ 定义计算结果的输出位置（ sink）</p><h4 id><a href="#" class="headerlink" title=" "></a> </h4><h2 id="1-source"><a href="#1-source" class="headerlink" title="1 source"></a>1 source</h2><h4 id="1-从集合中读取数据"><a href="#1-从集合中读取数据" class="headerlink" title="1 从集合中读取数据"></a>1 从集合中读取数据</h4><p>最简单的读取数据的方式，就是在代码中直接创建一个Java集合，然后调用执行环境fromCollection方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。</p><h4 id="2-从文件读取数据"><a href="#2-从文件读取数据" class="headerlink" title="2 从文件读取数据"></a>2 从文件读取数据</h4><p>真正的实际应用中，自然不会直接将数据写在代码中。通常情况下，我们会从存储介质中获取数据，一个比较常见的方式就是读取日志文件。这也是批处理中最常见的读取方式。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; stream = env.readTextFile(&quot;clicks .csv“);</span><br></pre></td></tr></table></figure><h4 id="3-socket"><a href="#3-socket" class="headerlink" title="3 socket"></a>3 socket</h4><p><a href="https://www.jianshu.com/p/cb26a0f6c622">https://www.jianshu.com/p/cb26a0f6c622</a></p><p>socket文本流的读取需要配置两个参数：发送端主机名和端口</p><p>文本流数据的发送，可以通过 Linux系统自带的 netcat工具进行模拟。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -lk 7777</span><br></pre></td></tr></table></figure><h4 id="4-kafka"><a href="#4-kafka" class="headerlink" title="4 kafka"></a>4 kafka</h4><h4 id="5-自定义-Source"><a href="#5-自定义-Source" class="headerlink" title="5 自定义 Source"></a>5 自定义 Source</h4><h2 id="3-sink"><a href="#3-sink" class="headerlink" title="3 sink"></a>3 sink</h2><h4 id="1-输出到文件"><a href="#1-输出到文件" class="headerlink" title="1 输出到文件"></a>1 输出到文件</h4><h4 id="2-输出到-Kafka"><a href="#2-输出到-Kafka" class="headerlink" title="2 输出到 Kafka"></a>2 输出到 Kafka</h4><h4 id="3-输出到-Redis"><a href="#3-输出到-Redis" class="headerlink" title="3 输出到 Redis"></a>3 输出到 Redis</h4><h4 id="4-输出到-Elasticsearch"><a href="#4-输出到-Elasticsearch" class="headerlink" title="4 输出到 Elasticsearch"></a>4 输出到 Elasticsearch</h4><h4 id="5-输出到-MySQL-JDBC"><a href="#5-输出到-MySQL-JDBC" class="headerlink" title="5 输出到 MySQL (JDBC)"></a>5 输出到 MySQL (JDBC)</h4><h4 id="6-自定义-Sink输出"><a href="#6-自定义-Sink输出" class="headerlink" title="6 自定义 Sink输出"></a>6 自定义 Sink输出</h4>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink程序构成部分 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark交互工具</title>
      <link href="/2022/03/19/spark-intercation-tool/"/>
      <url>/2022/03/19/spark-intercation-tool/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/u010886217/article/details/82916401">https://blog.csdn.net/u010886217/article/details/82916401</a></p><p>spark-shell、spark-sql，thriftserver</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark交互工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>jdbc</title>
      <link href="/2022/03/19/jdbc/"/>
      <url>/2022/03/19/jdbc/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/140885502">https://zhuanlan.zhihu.com/p/140885502</a></p><p>JDBC的全称是Java数据库连接(Java Database connect)，它是一套用于执行SQL语句的Java API。应用程序可通过这套API连接到关系数据库，并使用SQL语句来完成对数据库中数据的查询、更新和删除等操作。应用程序使用JDBC访问数据库的方式如下图所示。</p><p><img src="/2022/03/19/jdbc/1.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 框架 </category>
          
          <category> jdbc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jdbc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql,hql区别</title>
      <link href="/2022/03/18/hql-sql/"/>
      <url>/2022/03/18/hql-sql/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.geeksforgeeks.org/difference-between-sql-and-hiveql/">https://www.geeksforgeeks.org/difference-between-sql-and-hiveql/</a></p><p><img src="/2022/03/18/hql-sql/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql,hql区别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python在Ubuntu系统下的调试工具pdb</title>
      <link href="/2022/03/18/python-pdb/"/>
      <url>/2022/03/18/python-pdb/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/lemonaha/article/details/71305344">https://blog.csdn.net/lemonaha/article/details/71305344</a></p><p>两种方式：</p><p>1.侵入式</p><p>不改代码</p><p>python -m pdb XX.py</p><p>2.非倾入式</p><p>加代码</p><p>import pdb</p><p>pdb.set_trace()</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python在Ubuntu系统下的调试工具pdb </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive数据导入导出</title>
      <link href="/2022/03/17/hive-data/"/>
      <url>/2022/03/17/hive-data/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/xing901022/p/5801061.html">https://www.cnblogs.com/xing901022/p/5801061.html</a></p><p><img src="/2022/03/17/hive-data/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive数据导入导出 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker容器与虚拟机有什么区别？</title>
      <link href="/2022/03/16/docker-vm/"/>
      <url>/2022/03/16/docker-vm/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/48174633">https://www.zhihu.com/question/48174633</a></p><p><strong>一、物理机是这样的</strong></p><p><img src="/2022/03/16/docker-vm/1.jpg" alt></p><p><strong>二、虚拟机是这样的</strong></p><p><img src="/2022/03/16/docker-vm/2.jpg" alt></p><p><strong>三、容器是这样的</strong></p><p><img src="/2022/03/16/docker-vm/3.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 部署 </category>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker容器与虚拟机有什么区别？ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark资源参数调优</title>
      <link href="/2022/03/16/spark_para_optimization-resoucrece/"/>
      <url>/2022/03/16/spark_para_optimization-resoucrece/</url>
      
        <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>并发：调度器切换CPU给不同进程使用，实际上CPU在同一时刻只在运行一个进程</p><p>并行：</p><p>1、一台物理机的物理CPU的个数</p><p>2、一个CPU上的核数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 查看CPU信息（型号）</span><br><span class="line">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br><span class="line">        Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz</span><br><span class="line"> </span><br><span class="line"># 查看物理CPU个数</span><br><span class="line"> cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l</span><br><span class="line">2</span><br><span class="line"> </span><br><span class="line"># 查看每个物理CPU中core的个数(即核数)</span><br><span class="line"> cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq</span><br><span class="line">cpu cores    : 6</span><br><span class="line"> </span><br><span class="line"># 查看逻辑CPU的个数</span><br><span class="line">cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l</span><br><span class="line">24</span><br></pre></td></tr></table></figure><p>进程（要拿cpu为单位）：单CPU中进程只能是并发，多CPU计算机中进程可以并行。</p><p>线程（核作为单位）：单CPU单核中线程只能并发，单CPU多核中线程可以并行</p><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p><strong>worker:</strong></p><p>几个执行节点 ，一个worker 对应几个 executor </p><p><strong>executor：</strong>对应进程</p><p>num-executors</p><ul><li>参数说明：该参数用于设置Spark作业<strong>总共要用多少个Executor</strong>进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li></ul><p>executor-cores</p><ul><li>参数说明：该参数用于设置<strong>每个Executor进程的CPU core数量</strong>。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同学的作业运行。</li></ul><p><strong>task</strong>:线程，应该等于partition数量</p><p>job数量取决于action数量，stage数量取决于rdd依赖关系的划分</p><p>spark.default.parallelism</p><ul><li>参数说明：该参数用于设置<strong>每个stage的默认task数量</strong>。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li><li>参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><p>注意：executor-cores为可用的计算资源，task为拆分的任务数量</p><p><strong>内存</strong>：</p><p>driver-memory</p><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li></ul><p>executor-memory</p><ul><li>参数说明：该参数用于设置<strong>每个Executor进程的内存</strong>。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的<strong>代码</strong>时使用，默认是占Executor总内存的20%；第二块是让task通过<strong>shuffle</strong>过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD<strong>持久化</strong>时使用，默认占Executor总内存的60%。</p><p>spark.storage.memoryFraction</p><ul><li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>spark.shuffle.memoryFraction</p><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism=1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/gtscool/p/13072051.html">https://www.cnblogs.com/gtscool/p/13072051.html</a></p><p><a href="https://blog.csdn.net/l1394049664/article/details/81811642">https://blog.csdn.net/l1394049664/article/details/81811642</a></p><p><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark资源参数调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink提交任务</title>
      <link href="/2022/03/14/flink-task/"/>
      <url>/2022/03/14/flink-task/</url>
      
        <content type="html"><![CDATA[<p><a href="https://codeantenna.com/a/Y6wpSYwfRL">https://codeantenna.com/a/Y6wpSYwfRL</a></p><p>1.web ui</p><p><a href="https://blog.csdn.net/godelgnis/article/details/106051751">https://blog.csdn.net/godelgnis/article/details/106051751</a></p><p>2.命令行</p><p>可以指定部署方式</p><p><a href="https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/cli/">https://nightlies.apache.org/flink/flink-docs-release-1.13/zh/docs/deployment/cli/</a></p><p><a href="https://blog.csdn.net/weixin_42993799/article/details/106566037">https://blog.csdn.net/weixin_42993799/article/details/106566037</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">1 参数必选 ： </span><br><span class="line">     -n,--container &lt;arg&gt;   分配多少个yarn容器 (=taskmanager的数量)  </span><br><span class="line">2 参数可选 ： </span><br><span class="line">     -D &lt;arg&gt;                        动态属性  </span><br><span class="line">     -d,--detached                   独立运行  </span><br><span class="line">     -jm,--jobManagerMemory &lt;arg&gt;    JobManager的内存 [in MB]  </span><br><span class="line">     -nm,--name                      在YARN上为一个自定义的应用设置一个名字  </span><br><span class="line">     -q,--query                      显示yarn中可用的资源 (内存, cpu核数)  </span><br><span class="line">     -qu,--queue &lt;arg&gt;               指定YARN队列.  </span><br><span class="line">     -s,--slots &lt;arg&gt;                每个TaskManager使用的slots数量  </span><br><span class="line">     -tm,--taskManagerMemory &lt;arg&gt;   每个TaskManager的内存 [in MB]  </span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;   针对HA模式在zookeeper上创建NameSpace </span><br><span class="line">     -id,--applicationId &lt;yarnAppId&gt; YARN集群上的任务id，附着到一个后台运行的yarn session中</span><br><span class="line"> </span><br><span class="line">3 run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;  </span><br><span class="line"> </span><br><span class="line">    run操作参数:  </span><br><span class="line">    -c,--class &lt;classname&gt;  如果没有在jar包中指定入口类，则需要在这里通过这个参数指定  </span><br><span class="line">    -m,--jobmanager &lt;host:port&gt;  指定需要连接的jobmanager(主节点)地址，使用这个参数可以指定一个不同于配置文件中的jobmanager  </span><br><span class="line">    -p,--parallelism &lt;parallelism&gt;   指定程序的并行度。可以覆盖配置文件中的默认值。</span><br><span class="line"> </span><br><span class="line">4 启动一个新的yarn-session,它们都有一个y或者yarn的前缀</span><br><span class="line"> </span><br><span class="line">    例如：./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar </span><br><span class="line">    </span><br><span class="line">    连接指定host和port的jobmanager：</span><br><span class="line">    ./bin/flink run -m SparkMaster:1234 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1</span><br><span class="line"> </span><br><span class="line">    启动一个新的yarn-session：</span><br><span class="line">    ./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar -input hdfs://hostname:port/hello.txt -output hdfs://hostname:port/result1</span><br><span class="line"> </span><br><span class="line">5 注意：命令行的选项也可以使用./bin/flink 工具获得。</span><br><span class="line"> </span><br><span class="line">6 Action &quot;run&quot; compiles and runs a program.</span><br><span class="line">    </span><br><span class="line">      Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line">      &quot;run&quot; action options:</span><br><span class="line">         -c,--class &lt;classname&gt;               Class with the program entry point</span><br><span class="line">                                              (&quot;main&quot; method or &quot;getPlan()&quot; method.</span><br><span class="line">                                              Only needed if the JAR file does not</span><br><span class="line">                                              specify the class in its manifest.</span><br><span class="line">         -C,--classpath &lt;url&gt;                 Adds a URL to each user code</span><br><span class="line">                                              classloader  on all nodes in the</span><br><span class="line">                                              cluster. The paths must specify a</span><br><span class="line">                                              protocol (e.g. file://) and be</span><br><span class="line">                                              accessible on all nodes (e.g. by means</span><br><span class="line">                                              of a NFS share). You can use this</span><br><span class="line">                                              option multiple times for specifying</span><br><span class="line">                                              more than one URL. The protocol must</span><br><span class="line">                                              be supported by the &#123;@link</span><br><span class="line">                                              java.net.URLClassLoader&#125;.</span><br><span class="line">         -d,--detached                        If present, runs the job in detached</span><br><span class="line">                                              mode</span><br><span class="line">         -n,--allowNonRestoredState           Allow to skip savepoint state that</span><br><span class="line">                                              cannot be restored. You need to allow</span><br><span class="line">                                              this if you removed an operator from</span><br><span class="line">                                              your program that was part of the</span><br><span class="line">                                              program when the savepoint was</span><br><span class="line">                                              triggered.</span><br><span class="line">         -p,--parallelism &lt;parallelism&gt;       The parallelism with which to run the</span><br><span class="line">                                              program. Optional flag to override the</span><br><span class="line">                                              default value specified in the</span><br><span class="line">                                              configuration.</span><br><span class="line">         -q,--sysoutLogging                   If present, suppress logging output to</span><br><span class="line">                                              standard out.</span><br><span class="line">         -s,--fromSavepoint &lt;savepointPath&gt;   Path to a savepoint to restore the job</span><br><span class="line">                                              from (for example</span><br><span class="line">                                              hdfs:///flink/savepoint-1537).</span><br><span class="line"> </span><br><span class="line">7  Options for yarn-cluster mode:</span><br><span class="line">         -d,--detached                        If present, runs the job in detached</span><br><span class="line">                                              mode</span><br><span class="line">         -m,--jobmanager &lt;arg&gt;                Address of the JobManager (master) to</span><br><span class="line">                                              which to connect. Use this flag to</span><br><span class="line">                                              connect to a different JobManager than</span><br><span class="line">                                              the one specified in the</span><br><span class="line">                                              configuration.</span><br><span class="line">         -yD &lt;property=value&gt;                 use value for given property</span><br><span class="line">         -yd,--yarndetached                   If present, runs the job in detached</span><br><span class="line">                                              mode (deprecated; use non-YARN</span><br><span class="line">                                              specific option instead)</span><br><span class="line">         -yh,--yarnhelp                       Help for the Yarn session CLI.</span><br><span class="line">         -yid,--yarnapplicationId &lt;arg&gt;       Attach to running YARN session</span><br><span class="line">         -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">         -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container with</span><br><span class="line">                                              optional unit (default: MB)</span><br><span class="line">         -yn,--yarncontainer &lt;arg&gt;            Number of YARN container to allocate</span><br><span class="line">                                              (=Number of Task Managers)</span><br><span class="line">         -ynl,--yarnnodeLabel &lt;arg&gt;           Specify YARN node label for the YARN</span><br><span class="line">                                              application</span><br><span class="line">         -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class="line">                                              on YARN</span><br><span class="line">         -yq,--yarnquery                      Display available YARN resources</span><br><span class="line">                                              (memory, cores)</span><br><span class="line">         -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">         -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">         -yst,--yarnstreaming                 Start Flink in streaming mode</span><br><span class="line">         -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class="line">                                              (t for transfer)</span><br><span class="line">         -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container with</span><br><span class="line">                                              optional unit (default: MB)</span><br><span class="line">         -yz,--yarnzookeeperNamespace &lt;arg&gt;   Namespace to create the Zookeeper</span><br><span class="line">                                              sub-paths for high availability mode</span><br><span class="line">         -z,--zookeeperNamespace &lt;arg&gt;        Namespace to create the Zookeeper</span><br><span class="line">                                              sub-paths for high availability mode</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink提交任务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pom</title>
      <link href="/2022/03/14/pom/"/>
      <url>/2022/03/14/pom/</url>
      
        <content type="html"><![CDATA[<p>POM( Project Object Model，项目对象模型 ) 是 Maven 工程的基本工作单元，是一个XML文件，包含了项目的基本信息，用于描述项目如何构建，声明项目依赖，等等。</p><p>执行任务或目标时，Maven 会在当前目录中查找 POM。它读取 POM，获取所需的配置信息，然后执行目标。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 框架 </category>
          
          <category> maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pom </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink部署</title>
      <link href="/2022/03/12/flink-deploy/"/>
      <url>/2022/03/12/flink-deploy/</url>
      
        <content type="html"><![CDATA[<p>Flink的部署方式是灵活的，跟Spark一样，支持Local，Standalone，Yarn，Mesos，Kubernetes</p><p>代码中好像不能指定部署方式，和spark不同</p><p><a href="https://blog.csdn.net/qq_33689414/article/details/90671685">https://blog.csdn.net/qq_33689414/article/details/90671685</a></p><h2 id="1-Local"><a href="#1-Local" class="headerlink" title="1 Local"></a>1 Local</h2><p>最简单的启动方式，其实是不搭建集群，直接本地启动。本地部署非常简单，直接解压安装包就可以使用，不用进行任何配置；一般用来做一些简单的测试。</p><h2 id="2-Standalone"><a href="#2-Standalone" class="headerlink" title="2  Standalone"></a>2  Standalone</h2><p><img src="/2022/03/12/flink-deploy/1.png" alt></p><p>会话模式，应用模式</p><p>区别在于jobmaster的启动时间点，会话预先启动，应用在作业提交启动</p><h2 id="3-Yarn"><a href="#3-Yarn" class="headerlink" title="3 Yarn"></a>3 Yarn</h2><p>1 会话 session</p><p>在会话模式下，我们需要先启动一个YARN session，这个会话会创建一个 Flink集群。</p><p><img src="/2022/03/12/flink-deploy/2.JPG" alt></p><p>2  单作业 per-job</p><p>flink不会预先启动，在提交作业，才启动新的jobmanager</p><p><img src="/2022/03/12/flink-deploy/3.JPG" alt></p><p>3 应用application</p><p>与单作业很相似</p><p>区别在于提交给yarn资源管理器的不是具体作业，而是整个应用（包含了多个作业）</p><h2 id="4-Mesos"><a href="#4-Mesos" class="headerlink" title="4 Mesos"></a>4 Mesos</h2><h2 id="5-Kubernetes"><a href="#5-Kubernetes" class="headerlink" title="5 Kubernetes"></a>5 Kubernetes</h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flink架构原理</title>
      <link href="/2022/03/12/flink-srtuct/"/>
      <url>/2022/03/12/flink-srtuct/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Flink组件"><a href="#1-Flink组件" class="headerlink" title="1.Flink组件"></a>1.Flink组件</h2><p><img src="/2022/03/12/flink-srtuct/13.png" alt></p><h4 id="1-JobManager"><a href="#1-JobManager" class="headerlink" title="1 JobManager"></a>1 JobManager</h4><p><img src="/2022/03/12/flink-srtuct/11.JPG" alt></p><h4 id="2-任务管理器（-TaskManager）"><a href="#2-任务管理器（-TaskManager）" class="headerlink" title="2 任务管理器（ TaskManager）"></a>2 任务管理器（ TaskManager）</h4><p><img src="/2022/03/12/flink-srtuct/3.JPG" alt></p><h2 id="2-任务调度流程"><a href="#2-任务调度流程" class="headerlink" title="2.任务调度流程"></a>2.任务调度流程</h2><p><img src="/2022/03/12/flink-srtuct/12.png" alt></p><p><img src="/2022/03/12/flink-srtuct/9.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flink架构原理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink vs spark</title>
      <link href="/2022/03/12/flink-spark/"/>
      <url>/2022/03/12/flink-spark/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/03/12/flink-spark/1.JPG" alt></p><p><strong>数据模型</strong><br>1 spark 采用 RDD 模型， spark streaming 的 DStream 实际上也就是一组 组小批<br>数据 RDD 的集合<br>2 flink 基本数据模型是数据流，以及事件（ Event ）序列<br><strong>运行时架构</strong><br>1 spark 是批计算，将 DAG 划分为不同的 stage ，一个完成后才可以计算下一个<br>2 flink 是标准的流执行模式，一个事件在一个节点处理完后可以直接发往下一个节<br>点进行处理</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flink </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink vs spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka常见问题</title>
      <link href="/2022/03/12/kafka-problem/"/>
      <url>/2022/03/12/kafka-problem/</url>
      
        <content type="html"><![CDATA[<h2 id="1-kafka启动后一段时间自动退出的解决方案"><a href="#1-kafka启动后一段时间自动退出的解决方案" class="headerlink" title="1 kafka启动后一段时间自动退出的解决方案"></a>1 kafka启动后一段时间自动退出的解决方案</h2><p><a href="https://blog.csdn.net/weixin_46303867/article/details/115256466">https://blog.csdn.net/weixin_46303867/article/details/115256466</a></p><h2 id="2-ERROR-Shutdown-broker-because-all-log-dirs-in-…-have-failed"><a href="#2-ERROR-Shutdown-broker-because-all-log-dirs-in-…-have-failed" class="headerlink" title="2 ERROR Shutdown broker because all log dirs in … have failed"></a>2 ERROR Shutdown broker because all log dirs in … have failed</h2><p><a href="https://blog.csdn.net/szxiaohe/article/details/103639127">https://blog.csdn.net/szxiaohe/article/details/103639127</a></p><h2 id="3-连接zookeeper超时"><a href="#3-连接zookeeper超时" class="headerlink" title="3 连接zookeeper超时"></a>3 连接zookeeper超时</h2><p><a href="https://www.jianshu.com/p/ce215e6ef203">https://www.jianshu.com/p/ce215e6ef203</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka常见计算</title>
      <link href="/2022/03/12/kafka-cal/"/>
      <url>/2022/03/12/kafka-cal/</url>
      
        <content type="html"><![CDATA[<h3 id="Kafka机器数量计算"><a href="#Kafka机器数量计算" class="headerlink" title="Kafka机器数量计算"></a>Kafka机器数量计算</h3><p>经验公式：Kafka机器数量= 2 <em>（峰值生产速度</em> 副本数 / 100）+ 1</p><p>1）峰值生产速度</p><p>峰值生产速度可以压测得到。</p><p>2）副本数</p><p>副本数默认是1个，在企业里面2-3个都有，2个居多。</p><p>副本多可以提高可靠性，但是会降低网络传输效率。</p><p>例子：</p><p>先拿到峰值生产速度，再根据设定的副本数，就能预估出需要部署Kafka的数量。</p><p>比如我们的峰值生产速度是50M/s。副本数为2。</p><p>Kafka机器数量 = 2 <em>（50</em> 2 / 100）+ 1 = 3台</p><h3 id="Kafka分区数计算"><a href="#Kafka分区数计算" class="headerlink" title="Kafka分区数计算"></a>Kafka分区数计算</h3><p>（1）创建一个只有1个分区的topic</p><p>（2）测试这个topic的producer吞吐量和consumer吞吐量。</p><p>（3）假设他们的值分别是Tp和Tc，单位可以是MB/s。</p><p>（4）然后假设总的目标吞吐量是Tt，那么分区数 = Tt / min（Tp，Tc）</p><p>例如：producer吞吐量 = 20m/s；consumer吞吐量 = 50m/s，期望吞吐量100m/s；分区数 = 100 / 20 = 5分区</p><p><a href="https://blog.csdn.net/weixin_42641909/article/details/89294698">https://blog.csdn.net/weixin_42641909/article/details/89294698</a></p><p>分区数一般设置为：3-10个</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka常见计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka与Zookeeper的关系</title>
      <link href="/2022/03/12/kafka-zoo/"/>
      <url>/2022/03/12/kafka-zoo/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.lilinchao.com/archives/1548.html">https://www.lilinchao.com/archives/1548.html</a></p><p><a href="https://developer.51cto.com/article/658581.html">https://developer.51cto.com/article/658581.html</a></p><p><strong>过去</strong></p><p>Apache Kafka的一个关键依赖是Apache Zookeeper，它是一个分布式配置和同步服务。 Zookeeper是Kafka代理和消费者之间的协调接口。 Kafka服务器通过Zookeeper集群共享信息。 Kafka在Zookeeper中存储基本元数据，例如关于主题，代理，消费者偏移(队列读取器)等的信息。</p><p>由于所有关键信息存储在Zookeeper中，并且它通常在其整体上复制此数据，因此Kafka代理/ Zookeeper的故障不会影响Kafka集群的状态。 Kafka将恢复状态，一旦Zookeeper重新启动。 这为Kafka带来了零停机时间。 Kafka代理之间的领导者选举也通过使用Zookeeper在领导者失败的情况下完成。</p><p><strong>未来</strong></p><p>Kafka 2.8.0，移除了对Zookeeper的依赖，通过KRaft进行自己的集群管理</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka与Zookeeper的关系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark常见错误</title>
      <link href="/2022/03/09/spark-error/"/>
      <url>/2022/03/09/spark-error/</url>
      
        <content type="html"><![CDATA[<h4 id="Python-in-worker-has-different-version-2-7-than-that-in-driver-3-7-PySpark-cannot-run-with-different-minor-versions"><a href="#Python-in-worker-has-different-version-2-7-than-that-in-driver-3-7-PySpark-cannot-run-with-different-minor-versions" class="headerlink" title="Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions"></a>Python in worker has different version 2.7 than that in driver 3.7, PySpark cannot run with different minor versions</h4><p>核心思路：分别指定driver和excutor的python版本，使其统一</p><p>方法一：修改环境变量</p><p>1./在环境变量文件 /etc/profile 中添加指定的pyspark，python的版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PYSPARK_PYTHON=指定的python路径</span><br><span class="line">export PYSPARK_DRIVER_PYTHON=指定的python路径</span><br></pre></td></tr></table></figure><p>保存后source一下 /etc/profile ,使之生效</p><p>2.代码内指定</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;]=&quot;&quot; ##driver </span><br><span class="line">os.environ[&quot;PYSPARK_PYTHON&quot;]=&quot;&quot; ### worker  ,excutor</span><br></pre></td></tr></table></figure><p>方法二：spark-submit工具指定</p><p>在spark-submit时增加参数 <code>--conf spark.pyspark.python</code>和 <code>--conf spark.pyspark.driver.python</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--driver-memory 5g --num-executors 5 --executor-cores 1 --executor-memory 1G  </span><br><span class="line">--conf spark.pyspark.python=./.../bin/python --conf spark.pyspark.driver.python=./.../bin/python xx.py </span><br></pre></td></tr></table></figure><h2 id="spark-sql-不能查询到hive的数据库，只查询到default数据库"><a href="#spark-sql-不能查询到hive的数据库，只查询到default数据库" class="headerlink" title="spark.sql 不能查询到hive的数据库，只查询到default数据库"></a>spark.sql 不能查询到hive的数据库，只查询到default数据库</h2><p>说明spark没有连接到hive</p><p><a href="https://www.cnblogs.com/yjt1993/p/13963144.html">https://www.cnblogs.com/yjt1993/p/13963144.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark常见错误 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java常见问题</title>
      <link href="/2022/03/08/java-problem/"/>
      <url>/2022/03/08/java-problem/</url>
      
        <content type="html"><![CDATA[<h3 id="Could-not-find-or-load-main-class"><a href="#Could-not-find-or-load-main-class" class="headerlink" title="Could not find or load main class"></a>Could not find or load main class</h3><p><a href="https://blog.csdn.net/gao_zhennan/article/details/112749742">https://blog.csdn.net/gao_zhennan/article/details/112749742</a></p><p><a href="https://www.html.cn/softprog/java/142271.html">https://www.html.cn/softprog/java/142271.html</a></p><p><a href="https://blog.csdn.net/qq_43189115/article/details/99856659">https://blog.csdn.net/qq_43189115/article/details/99856659</a></p><p><a href="https://blog.csdn.net/zdash21/article/details/101310736">https://blog.csdn.net/zdash21/article/details/101310736</a></p><p><a href="https://www.jianshu.com/p/bd5d07982699">https://www.jianshu.com/p/bd5d07982699</a></p><h2 id="java-lang-NoClassDefFoundError"><a href="#java-lang-NoClassDefFoundError" class="headerlink" title="java.lang.NoClassDefFoundError"></a>java.lang.NoClassDefFoundError</h2><p><a href="https://www.jianshu.com/p/8dcdb02f97f7">https://www.jianshu.com/p/8dcdb02f97f7</a></p><p><a href="https://blog.csdn.net/jamesjxin/article/details/46606307">https://blog.csdn.net/jamesjxin/article/details/46606307</a></p><h2 id="Warning-Class-‘com-xxx-xxx‘-not-found-in-module-‘xxxx‘"><a href="#Warning-Class-‘com-xxx-xxx‘-not-found-in-module-‘xxxx‘" class="headerlink" title="Warning: Class ‘com.xxx.xxx‘ not found in module ‘xxxx‘"></a>Warning: Class ‘com.xxx.xxx‘ not found in module ‘xxxx‘</h2><p><a href="https://blog.csdn.net/diligent_jianhao/article/details/111515183">https://blog.csdn.net/diligent_jianhao/article/details/111515183</a></p><h2 id="java-Compilation-failed-internal-java-compiler-error"><a href="#java-Compilation-failed-internal-java-compiler-error" class="headerlink" title="java: Compilation failed: internal java compiler error"></a>java: Compilation failed: internal java compiler error</h2><p><a href="https://blog.csdn.net/ximaiyao1984/article/details/114782006">https://blog.csdn.net/ximaiyao1984/article/details/114782006</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IDEA</title>
      <link href="/2022/03/08/idea/"/>
      <url>/2022/03/08/idea/</url>
      
        <content type="html"><![CDATA[<h2 id="激活"><a href="#激活" class="headerlink" title="激活"></a>激活</h2><p><a href="https://www.heiz123.com/2022/02/242/#di_liu_bu_da_kai_IDEA_tian_ru_zhi_ding_ji_huo_ma_wan_cheng_ji_huo">https://www.heiz123.com/2022/02/242/#di_liu_bu_da_kai_IDEA_tian_ru_zhi_ding_ji_huo_ma_wan_cheng_ji_huo</a></p><h2 id="IDEA修改变量的值"><a href="#IDEA修改变量的值" class="headerlink" title="IDEA修改变量的值"></a>IDEA修改变量的值</h2><p><a href="https://blog.csdn.net/qq_36925114/article/details/102484525">https://blog.csdn.net/qq_36925114/article/details/102484525</a></p><h2 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h2><p><a href="https://www.cnblogs.com/chiangchou/p/idea-debug.html">https://www.cnblogs.com/chiangchou/p/idea-debug.html</a></p><h2 id="language-level"><a href="#language-level" class="headerlink" title="language level"></a>language level</h2><p><a href="https://blog.csdn.net/glpghz/article/details/107509987">https://blog.csdn.net/glpghz/article/details/107509987</a></p><p>当我们项目使用的是 JDK 8，但是代码却没有使用 JDK 8 的新特性，只需使用 JDK 7 的时候我们可以选择 <code>7 - Diamonds，ARM，multi-catch etc</code></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> ide </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pyspark环境</title>
      <link href="/2022/03/07/pyspark-dependency/"/>
      <url>/2022/03/07/pyspark-dependency/</url>
      
        <content type="html"><![CDATA[<h2 id="1-依赖"><a href="#1-依赖" class="headerlink" title="1 依赖"></a>1 依赖</h2><p>PYSPARK_PYTHON=</p><p>PYSPARK_DRIVER_PYTHON=</p><p>JAVA_HOME = /usr/local/jdk1.8.0_11</p><p>HADOOP_CONF_DIR=/cloud/dahua/spark-2.4.4-binhadoop2.7/conf</p><p>SPARK_HOME=/usr/local/spark-2.4.4-bin-hadoop2.7</p><p>SCALA_HOME=/usr/local/scala-2.11.8</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyspark环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>maven</title>
      <link href="/2022/03/06/maven/"/>
      <url>/2022/03/06/maven/</url>
      
        <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h2><p>Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。</p><h2 id="2-安装配置"><a href="#2-安装配置" class="headerlink" title="2 安装配置"></a>2 安装配置</h2><p><a href="https://blog.csdn.net/qq_19734597/article/details/120996418">https://blog.csdn.net/qq_19734597/article/details/120996418</a></p><p><a href="https://blog.csdn.net/weixin_34234829/article/details/89686175">https://blog.csdn.net/weixin_34234829/article/details/89686175</a></p><p><a href="https://blog.csdn.net/idomyway/article/details/81974677">https://blog.csdn.net/idomyway/article/details/81974677</a></p><p>1 更换镜像</p><p>conf/settings.xml</p><p><a href="https://www.cnblogs.com/digdeep/p/5026066.html">https://www.cnblogs.com/digdeep/p/5026066.html</a></p><p><a href="https://blog.csdn.net/qq_42931492/article/details/107283590">https://blog.csdn.net/qq_42931492/article/details/107283590</a></p><p>2 网络配置</p><p>conf/settings.xml</p><p><a href="https://blog.csdn.net/zongf0504/article/details/88797831">https://blog.csdn.net/zongf0504/article/details/88797831</a></p><p>3 问题</p><p>(org.apache.maven.wagon.providers.http.httpclient.NoHttpResponseException) caught when processing request to {}-&gt;<a href="http://xxxxxx-&gt;http//maven.aliyun.com:80">http://XXXXXX-&gt;http://maven.aliyun.com:80</a>: The target server failed to respond</p><h2 id="3-管理包"><a href="#3-管理包" class="headerlink" title="3 管理包"></a>3 管理包</h2><p><a href="https://blog.51cto.com/u_15119353/3303815">https://blog.51cto.com/u_15119353/3303815</a></p><p>1 设置setting.xml</p><p>2 编写pom</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-java&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-connector-kafka-0.11_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.bahir&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-connector-redis_2.11&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.0&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-connector-elasticsearch6_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;mysql&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;5.1.44&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-statebackend-rocksdb_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-table-planner_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-table-planner-blink_2.12&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;flink-csv&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.10.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;/dependencies&gt;</span><br></pre></td></tr></table></figure><p>3 生成本地包的repository</p><p>默认位置在/user/.m2/repository，在setting可以修改</p><p>4 环境使用本地repository</p><p><a href="https://blog.csdn.net/weixin_42476601/article/details/87884514">https://blog.csdn.net/weixin_42476601/article/details/87884514</a></p><p><img src="/2022/03/06/maven/1.png" alt></p><h2 id="4-问题"><a href="#4-问题" class="headerlink" title="4 问题"></a>4 问题</h2><p>1.com/atguigu/gmall/hive/udtf/ExplodeJSONArray has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0</p><p>解决：两边jdk版本对齐就好</p><p>打包时候注意要：maven clean,maven compile</p><p>class file versions对应jdk版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">49 = Java 5</span><br><span class="line">50 = Java 6</span><br><span class="line">51 = Java 7</span><br><span class="line">52 = Java 8</span><br><span class="line">53 = Java 9</span><br><span class="line">54 = Java 10</span><br><span class="line">55 = Java 11</span><br><span class="line">56 = Java 12</span><br><span class="line">57 = Java 13</span><br><span class="line">58 = Java 14</span><br></pre></td></tr></table></figure><p>2.Unknown host maven.aliyun.com</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 框架 </category>
          
          <category> maven </category>
          
      </categories>
      
      
        <tags>
            
            <tag> maven </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java框架</title>
      <link href="/2022/03/06/java-framework/"/>
      <url>/2022/03/06/java-framework/</url>
      
        <content type="html"><![CDATA[<p><a href="https://segmentfault.com/a/1190000016917114">https://segmentfault.com/a/1190000016917114</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 框架 </category>
          
          <category> 框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java应用场景</title>
      <link href="/2022/03/06/java-application/"/>
      <url>/2022/03/06/java-application/</url>
      
        <content type="html"><![CDATA[<p><strong>前端</strong></p><p><strong>后端</strong></p><p><strong>大数据</strong></p><p><a href="https://www.cnblogs.com/zlt9/p/7206238.html">https://www.cnblogs.com/zlt9/p/7206238.html</a></p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java应用场景 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>it的技术场景</title>
      <link href="/2022/03/06/application/"/>
      <url>/2022/03/06/application/</url>
      
        <content type="html"><![CDATA[<p><strong>前后端</strong></p><p>指的web前后端</p><p><strong>移动端</strong></p><p>ios和安卓</p><p><strong>服务端 ，客户端</strong></p><p>客户端（Client）是指与服务器相对应并为客户提供本地服务的程序。 除了仅在本地运行的某些应用程序外，它们通常安装在普通客户端上，并且需要与服务器一起使用。</p><p>服务端：顾名思义是服务的，客户端发送的请求交给服务器端处理，是以response对象存在，服务端处理完毕后反馈给客户端。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> it的技术场景 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Use reduceByKey instead of groupByKey</title>
      <link href="/2022/03/04/reduceByKey/"/>
      <url>/2022/03/04/reduceByKey/</url>
      
        <content type="html"><![CDATA[<p>groupByKey creates a lot of shuffling which hampers the performance, while reduceByKey does not shuffle the data as much</p><p><a href="https://blog.csdn.net/qq_17685725/article/details/123033552">https://blog.csdn.net/qq_17685725/article/details/123033552</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Use reduceByKey instead of groupByKey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>持久化</title>
      <link href="/2022/03/04/persist/"/>
      <url>/2022/03/04/persist/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cloud.tencent.com/developer/article/1760389">https://cloud.tencent.com/developer/article/1760389</a></p><p><a href="https://blog.csdn.net/dudadudadd/article/details/114102341">https://blog.csdn.net/dudadudadd/article/details/114102341</a></p><p><a href="https://yiqingqing.blog.csdn.net/article/details/121772325">https://yiqingqing.blog.csdn.net/article/details/121772325</a></p><p><a href="https://blog.csdn.net/feizuiku0116/article/details/122839247">https://blog.csdn.net/feizuiku0116/article/details/122839247</a></p><p><a href="https://blog.csdn.net/CyAurora/article/details/119654676">https://blog.csdn.net/CyAurora/article/details/119654676</a></p><p><a href="https://www.cnblogs.com/Transkai/p/11347224.html">https://www.cnblogs.com/Transkai/p/11347224.html</a></p><p><a href="https://blog.csdn.net/CyAurora/article/details/119654676">https://blog.csdn.net/CyAurora/article/details/119654676</a></p><p><a href="https://blog.csdn.net/dudadudadd/article/details/114102341">https://blog.csdn.net/dudadudadd/article/details/114102341</a></p><h2 id="1-缓存"><a href="#1-缓存" class="headerlink" title="1 缓存"></a>1 缓存</h2><p>懒执行</p><p>空间换时间</p><p><img src="/2022/03/04/persist/1.JPG" alt></p><p>rdd3如果不消失，那么绿色链路就不用执行两次</p><p>持久化的目标就是将rdd3保存到内存或者磁盘</p><p>但是有丢失风险，比如硬盘损坏，内存被清理等，所以为了规避风险，会保留rdd的血缘（依赖）关系</p><p>如何保存：</p><p><img src="/2022/03/04/persist/2.JPG" alt></p><p><img src="/2022/03/04/persist/3.JPG" alt></p><h4 id="1-persist"><a href="#1-persist" class="headerlink" title="1 persist"></a>1 persist</h4><p><img src="/2022/03/04/persist/3.png" alt></p><h4 id="2-cache"><a href="#2-cache" class="headerlink" title="2 cache"></a>2 cache</h4><p><a href="https://blog.csdn.net/donger__chen/article/details/86366339">https://blog.csdn.net/donger__chen/article/details/86366339</a></p><p>底层调用persist，persist的特殊情况，persist(MEMORY_ONLY)</p><h2 id="2-checkpoint"><a href="#2-checkpoint" class="headerlink" title="2 checkpoint"></a>2 checkpoint</h2><p>特殊的持久化</p><p>仅支持硬盘</p><p>设计上认为安全没有风险，所以不需要保留血缘关系</p><p>如何保存：</p><p><img src="/2022/03/04/persist/4.JPG" alt></p><h2 id="3-对比"><a href="#3-对比" class="headerlink" title="3 对比"></a>3 对比</h2><p><img src="/2022/03/04/persist/5.JPG" alt></p><p><img src="/2022/03/04/persist/6.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 持久化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pyspark</title>
      <link href="/2022/03/02/pyspark/"/>
      <url>/2022/03/02/pyspark/</url>
      
        <content type="html"><![CDATA[<p>PySpark宗旨是在不破坏Spark已有的运行时架构，在Spark架构外层包装一层Python API，借助Py4j实现Python和Java的交互，进而实现通过Python编写Spark应用程序，其运行时架构如下图所示。</p><p><img src="/2022/03/02/pyspark/1.JPG" alt></p><p><img src="/2022/03/02/pyspark/2.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparksql运行流程</title>
      <link href="/2022/03/02/sparksql-run/"/>
      <url>/2022/03/02/sparksql-run/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/03/02/sparksql-run/1.JPG" alt></p><p>转成rdd来做</p><p><a href="https://blog.csdn.net/qq_25002995/article/details/104748504">https://blog.csdn.net/qq_25002995/article/details/104748504</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sparksql运行流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sparkcore运行流程</title>
      <link href="/2022/03/02/sparkcore-run/"/>
      <url>/2022/03/02/sparkcore-run/</url>
      
        <content type="html"><![CDATA[<h2 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h2><p><img src="/2022/03/02/sparkcore-run/5.JPG" alt></p><p>1 注册并申请资源，分配资源</p><p>2 job-&gt;stage-&gt;task</p><p>3 4 5 driver executor 交互执行任务</p><p>executor 向driver申请任务， 还是driver给executor 分配任务？</p><h2 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h2><p><img src="/2022/03/02/sparkcore-run/6.JPG" alt></p><p><img src="/2022/03/02/sparkcore-run/7.JPG" alt></p><p><img src="/2022/03/02/sparkcore-run/8.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sparkcore运行流程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>key/value</title>
      <link href="/2022/03/02/key-value/"/>
      <url>/2022/03/02/key-value/</url>
      
        <content type="html"><![CDATA[<p>某个字段为key，某个字段为value</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> key/value </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive与传统数据库对比</title>
      <link href="/2022/03/01/hive-vs-database/"/>
      <url>/2022/03/01/hive-vs-database/</url>
      
        <content type="html"><![CDATA[<div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">Hive</th><th>传统数据库</th></tr></thead><tbody><tr><td style="text-align:left">查询语言</td><td style="text-align:left">HQL</td><td>SQL</td></tr><tr><td style="text-align:left">数据存储</td><td style="text-align:left">HDFS</td><td>Raw Device或者 Local FS</td></tr><tr><td style="text-align:left">数据格式</td><td style="text-align:left">用户自定义</td><td>系统决定</td></tr><tr><td style="text-align:left">数据更新</td><td style="text-align:left">不支持</td><td>支持</td></tr><tr><td style="text-align:left">执行</td><td style="text-align:left">MapReduce</td><td>Excutor</td></tr><tr><td style="text-align:left">执行延迟</td><td style="text-align:left">高</td><td>低</td></tr><tr><td style="text-align:left">处理数据规模</td><td style="text-align:left">大</td><td>小</td></tr><tr><td style="text-align:left">索引</td><td style="text-align:left">0.8版本后加入位图索引</td><td>有复杂的索引</td></tr><tr><td style="text-align:left">可扩展性</td><td style="text-align:left">高</td><td>低</td></tr></tbody></table></div><p><a href="https://cloud.tencent.com/developer/article/1785857">https://cloud.tencent.com/developer/article/1785857</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive与传统数据库对比 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IaaS、PaaS和SaaS</title>
      <link href="/2022/03/01/saas/"/>
      <url>/2022/03/01/saas/</url>
      
        <content type="html"><![CDATA[<p>IaaS： Infrastructure-as-a-Service（基础设施即服务）</p><p>PaaS： Platform-as-a-Service（平台即服务）</p><p>SaaS： Software-as-a-Service（软件即服务）</p><p><a href="https://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html">https://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IaaS、PaaS和SaaS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ip和端口</title>
      <link href="/2022/03/01/netport/"/>
      <url>/2022/03/01/netport/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_33950757/article/details/90441617">https://blog.csdn.net/weixin_33950757/article/details/90441617</a></p><p>IP地址（家庭地址）：</p><p>例如：218.18.170.149；理解为：广东省.深圳市.龙岗区.电信（桥头东路二道巷149号）；</p><p>端口后（门牌号）：</p><p>例如：218.18.170.149:1011，端口为（1011）号；意思就是我家有很多房间，其中的一个房间为1011号；</p><p>禁止端口：禁止任何人来打开我的1011号房间；</p><p>端口登陆：1011号房间的门是加密的防盗门，你必须输入用户名和密码你才能进入1011号房间</p>]]></content>
      
      
      <categories>
          
          <category> 计算机网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ip和端口 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>各组件web ui的地址</title>
      <link href="/2022/03/01/web-ui/"/>
      <url>/2022/03/01/web-ui/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_41851454/article/details/79938811">https://blog.csdn.net/qq_41851454/article/details/79938811</a></p><p>node ip+port</p><h2 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h2><p>resource maneger +8088</p><h2 id="hdfs"><a href="#hdfs" class="headerlink" title="hdfs"></a>hdfs</h2><p>namenode +50070/9870/9871</p><h2 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h2><p>4040: 是一个运行的Application在运行的过程中临时绑定的端口,用以查看当前任务的状态.4040被占用会顺延到4041.4042等.4040是一个临时端口,当前程序运行完成后, 4040就会被注销哦。当使用spark交互工具，如spark-sql,spark-shell</p><p>8080: 默认是StandAlone下, Master角色(进程)的WEB端口,用以查看当前Master(集群)的状态</p><p>18080: 默认是历史服务器的端口, 由于每个程序运行完成后,4040端口就被注销了. 在以后想回看某个程序的运行状态就可以通过历史服务器查看,历史服务器长期稳定运行,可供随时查看被记录的程序的运行过程.</p><p>配置历史服务器</p><p><a href="https://blog.csdn.net/Heitao5200/article/details/79674684">https://blog.csdn.net/Heitao5200/article/details/79674684</a></p><p><a href="https://blog.csdn.net/yu0_zhang0/article/details/80396080">https://blog.csdn.net/yu0_zhang0/article/details/80396080</a></p><p>注意端口号和hadoop一致，9000-&gt;8020</p><h2 id="flink"><a href="#flink" class="headerlink" title="flink"></a>flink</h2><p>Apache Flink runs the dashboard on port 8081. Since this is a common port there might be conflict with some other services running on the same machines</p><p>port和端口可以在flink/conf/flink-conf.yaml 中查看</p><h2 id="hive-metastore"><a href="#hive-metastore" class="headerlink" title="hive metastore"></a>hive metastore</h2><p>端口9083</p><h2 id="hbase"><a href="#hbase" class="headerlink" title="hbase"></a>hbase</h2><p>16010</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> 基础组件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 各组件web ui的地址 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dataframe</title>
      <link href="/2022/02/27/sparksql/"/>
      <url>/2022/02/27/sparksql/</url>
      
        <content type="html"><![CDATA[<h2 id="0-两种风格"><a href="#0-两种风格" class="headerlink" title="0.两种风格"></a>0.两种风格</h2><p>DataFrame支持两种风格进行编程，分别是：</p><h3 id="1-DSL风格"><a href="#1-DSL风格" class="headerlink" title="1 DSL风格"></a>1 DSL风格</h3><p>DSL称之为：领域特定语言。<br>其实就是指DataFrame的特有API<br>DSL风格意思就是以调用API的方式来处理Data<br>比如：df.where().limit()</p><h3 id="2-SQL风格"><a href="#2-SQL风格" class="headerlink" title="2 SQL风格"></a>2 SQL风格</h3><p>SQL语法风格<br>SQL风格就是使用SQL语句处理DataFrame的数据<br>比如：spark.sql(“SELECT * FROM xxx)</p><h2 id="1-用户自定义函数"><a href="#1-用户自定义函数" class="headerlink" title="1 用户自定义函数"></a>1 用户自定义函数</h2><p><img src="/2022/02/27/sparksql/1.png" alt></p><p><img src="/2022/02/27/sparksql/2.png" alt></p><p>步骤：</p><p><a href="https://blog.csdn.net/qq_43665254/article/details/112379113">https://blog.csdn.net/qq_43665254/article/details/112379113</a></p><p><a href="https://blog.csdn.net/sunflower_sara/article/details/104044412">https://blog.csdn.net/sunflower_sara/article/details/104044412</a></p><p>1、定义函数</p><p>2、注册函数</p><p>3、使用函数</p><h2 id="2-withColumn"><a href="#2-withColumn" class="headerlink" title="2 withColumn"></a>2 withColumn</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from pyspark.sql.functions import col, lit</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">df.withColumn(&#x27;age2&#x27;, df.age + 2).collect()</span><br><span class="line">### 结合udf</span><br><span class="line">def fun(A,B):</span><br><span class="line">XXXX</span><br><span class="line">return XX</span><br><span class="line">fun1 = udf(fun, StringType())</span><br><span class="line">df.withColumn(&#x27;age2&#x27;, fun1(col_name1,col_name2))###</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dataframe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rdd</title>
      <link href="/2022/02/27/sparkcore/"/>
      <url>/2022/02/27/sparkcore/</url>
      
        <content type="html"><![CDATA[<h2 id="0-分类"><a href="#0-分类" class="headerlink" title="0.分类"></a>0.分类</h2><p>算子就是分布式集合对象的api</p><p>rdd算子分为两类：1.transformation 2.action</p><p><a href="https://blog.csdn.net/weixin_45271668/article/details/106441457">https://blog.csdn.net/weixin_45271668/article/details/106441457</a></p><h2 id="1-共享变量"><a href="#1-共享变量" class="headerlink" title="1 共享变量"></a>1 共享变量</h2><p><a href="https://blog.csdn.net/Android_xue/article/details/79780463">https://blog.csdn.net/Android_xue/article/details/79780463</a></p><p><a href="https://chowdera.com/2022/02/202202091419262471.html">https://chowdera.com/2022/02/202202091419262471.html</a></p><p>两种共享变量：广播变量（broadcast variable）与累加器（accumulator）</p><p>广播变量解决了什么问题?<br>分布式集合RDD和本地集合进行关联使用的时候, 降低内存占用以及减少网络IO传输, 提高性能.</p><p>累加器解决了什么问题?<br>分布式代码执行中, 进行全局累加</p><h3 id="1-广播变量"><a href="#1-广播变量" class="headerlink" title="1.广播变量"></a>1.广播变量</h3><p><img src="/2022/02/27/sparkcore/1.JPG" alt></p><p><img src="/2022/02/27/sparkcore/2.JPG" alt></p><p><img src="/2022/02/27/sparkcore/3.JPG" alt></p><p><img src="/2022/02/27/sparkcore/4.JPG" alt></p><p><img src="/2022/02/27/sparkcore/5.JPG" alt></p><h3 id="2-累加器"><a href="#2-累加器" class="headerlink" title="2.累加器"></a>2.累加器</h3><p><img src="/2022/02/27/sparkcore/6.JPG" alt></p><p><img src="/2022/02/27/sparkcore/7.JPG" alt></p><p><img src="/2022/02/27/sparkcore/8.JPG" alt></p><p><img src="/2022/02/27/sparkcore/9.JPG" alt></p><p><img src="/2022/02/27/sparkcore/10.JPG" alt></p><p><img src="/2022/02/27/sparkcore/11.JPG" alt></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd1=sc.parallelize([1,2,3,4,5,6,7,8,9,10],2)</span><br><span class="line">count=sc.accumulator(0)</span><br><span class="line"></span><br><span class="line">def map_func(data):</span><br><span class="line">global count</span><br><span class="line">count+=data</span><br><span class="line"></span><br><span class="line"># count = sc.accumulator(0)</span><br><span class="line"># rdd3=rdd2.map(lambda x:x)</span><br><span class="line"># rdd3.collect()</span><br><span class="line"># print(count)</span><br><span class="line">start_time = time.time()</span><br><span class="line">result=rdd1.reduce(lambda a, b: a + b)</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(result)</span><br><span class="line">print(end_time - start_time)</span><br><span class="line"># print(count)</span><br><span class="line">start_time = time.time()</span><br><span class="line">rdd2 = rdd1.map(map_func)</span><br><span class="line">rdd2.collect()</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(count)</span><br><span class="line">print(end_time - start_time)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">55</span><br><span class="line">1.092909574508667</span><br><span class="line">55</span><br><span class="line">0.09459614753723145</span><br></pre></td></tr></table></figure><p>累加器和reduce都可以得到聚合结果，效率？？？谁先执行 谁短，怎么衡量</p><h2 id="2-ByKey-操作"><a href="#2-ByKey-操作" class="headerlink" title="2 *ByKey 操作"></a>2 *ByKey 操作</h2><p><a href="https://blog.csdn.net/weixin_43810802/article/details/120772452">https://blog.csdn.net/weixin_43810802/article/details/120772452</a></p><p><a href="https://blog.csdn.net/zhuzuwei/article/details/104446388">https://blog.csdn.net/zhuzuwei/article/details/104446388</a></p><p><a href="https://blog.csdn.net/weixin_40161254/article/details/103472056">https://blog.csdn.net/weixin_40161254/article/details/103472056</a></p><p>Use reduceByKey instead of groupByKey</p><p>groupByKey creates a lot of shuffling which hampers the performance, while reduceByKey does not shuffle the data as much</p><h2 id="3-reduce"><a href="#3-reduce" class="headerlink" title="3 reduce"></a>3 reduce</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">in：</span><br><span class="line">table=pd.DataFrame(&#123;&quot;num&quot;:[1,1,2]&#125;)</span><br><span class="line">table = spark.createDataFrame(table)</span><br><span class="line">from pyspark.sql import Row</span><br><span class="line">table.rdd.reduce(lambda a,b:Row(num=a[0]+b[0]))</span><br><span class="line">out：</span><br><span class="line">Row(num=4)</span><br></pre></td></tr></table></figure><p>聚合的作用</p><p>注意点就是  a,b 操作后的数据类型和a，b保持一致，举个例子 a+b 和a ，b类型一致，否则（a+b）+c会报错</p><h2 id="4-collect、-take、top、first，foreach"><a href="#4-collect、-take、top、first，foreach" class="headerlink" title="4 collect、 take、top、first，foreach"></a>4 collect、 take、top、first，foreach</h2><p>1 collect</p><p><strong><code>collect</code></strong>() </p><p>返回包含此RDD中的所有元素的列表</p><p>注意：因为所有数据都会加载到driver，所有只适用于数据量不大的情况</p><p>2 first</p><p><code>first</code>()</p><p>返回RDD中的第一个元素</p><p>3 take</p><p><code>take</code>(<em>num</em>)</p><p>取RDD的前num个元素</p><p>4 top</p><p><code>top</code>(<em>num</em>, <em>key=None</em>)</p><p>排序</p><p>Get the top N elements from an RDD</p><p>5 foreach</p><p><code>foreach</code>(<em>f</em>)</p><p>Applies a function to all elements of this RDD  分区输出</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def f(x):</span><br><span class="line">    print(x)</span><br><span class="line">    </span><br><span class="line">sc.parallelize([1,2,3,4,5]).foreach(f)</span><br><span class="line"></span><br><span class="line">output：</span><br><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">5</span><br><span class="line">会变</span><br><span class="line">3</span><br><span class="line">5</span><br><span class="line">2</span><br><span class="line">4</span><br><span class="line">1</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> rdd </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sparksql对比hive sql</title>
      <link href="/2022/02/27/sparksql-vs-hql/"/>
      <url>/2022/02/27/sparksql-vs-hql/</url>
      
        <content type="html"><![CDATA[<p>Hive和Spark 均是“分布式SQL计算引擎”，mysql等不是，mysql跑单机上</p><p>均是构建大规模结构化数据计算的绝佳利器，同时SparkSQL拥有更好的性能。目前，企业中使用Hive仍旧居多，但SparkSQL将会在很近的未来替代Hive成为分布式SQL计算市场的顶级</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sparksql对比hive sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql常见操作</title>
      <link href="/2022/02/27/sql-common-operation/"/>
      <url>/2022/02/27/sql-common-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="1-拼接"><a href="#1-拼接" class="headerlink" title="1 拼接"></a>1 拼接</h2><p>1 union ，union all</p><p><a href="https://www.w3school.com.cn/sql/sql_union.asp">https://www.w3school.com.cn/sql/sql_union.asp</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select &#x27;mobile&#x27; as platform union</span><br><span class="line">select &#x27;desktop&#x27; as platform union</span><br><span class="line">select &#x27;both&#x27; as platform </span><br></pre></td></tr></table></figure><p>2 join</p><p>left join 、right join、 inner join，FULL OUTER JOIN，默认join 为 inner join</p><p><a href="https://www.cnblogs.com/ingstyle/p/4368064.html">https://www.cnblogs.com/ingstyle/p/4368064.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#多个left join</span><br><span class="line">select a.*,b.*,c.* </span><br><span class="line">from a left join b on a.id=b.id </span><br><span class="line">left join c on b.id=c.id</span><br><span class="line"></span><br><span class="line">select *</span><br><span class="line">from Trips T left join  Users  U1 on T.client_id =U1.users_id  </span><br><span class="line">left join  Users U2 on T.driver_id  =U2.users_id  </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from Trips T1</span><br><span class="line">left join Users U1 on (T1.client_id =U1.users_id  and U1.banned =&quot;Yes&quot; )    不可</span><br><span class="line"></span><br><span class="line">from Trips T1</span><br><span class="line">join Users U1 on (T1.client_id =U1.users_id  and U1.banned =&quot;Yes&quot; ) 可</span><br></pre></td></tr></table></figure><p>3.from student A,student B,student C</p><p>将三个 student 表相互连接成一个</p><p><a href="https://blog.csdn.net/zhangyj_315/article/details/2249209">https://blog.csdn.net/zhangyj_315/article/details/2249209</a></p><h2 id="2-分组"><a href="#2-分组" class="headerlink" title="2 分组"></a>2 分组</h2><p>1 group by</p><p>分组, 分完每组就一行，取每组第一行数据</p><p>group by columns1,columns2</p><h6 id><a href="#" class="headerlink" title="#"></a>#</h6><p>有时候不能取第一行，怎么解决？<a href="https://leetcode-cn.com/problems/product-sales-analysis-iii/">1070. 产品销售分析 III</a></p><p>a 可以先排序，把目标行变成第一行 可以参考<a href="https://blog.csdn.net/shiyong1949/article/details/78482737">https://blog.csdn.net/shiyong1949/article/details/78482737</a>  好像不行</p><p>b 使用开窗函数解决，可以</p><h6 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h6><p>分组+条件判断</p><p>Having</p><p>having 子句的作用是筛选满足条件的组，不是在组内选行</p><p>在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。</p><h6 id="-2"><a href="#-2" class="headerlink" title="#"></a>#</h6><h2 id="4-子查询"><a href="#4-子查询" class="headerlink" title="4 子查询"></a>4 子查询</h2><p>嵌套</p><p><a href="http://c.biancheng.net/sql/sub-query.html">http://c.biancheng.net/sql/sub-query.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select a.Score as Score,</span><br><span class="line">        (select count(DISTINCT b.Score) from Scores  b where b.Score &gt;= a.Score) as &#x27;Rank&#x27;</span><br><span class="line">from Scores a</span><br><span class="line">order by a.Score DESC</span><br></pre></td></tr></table></figure><h2 id="5-别名"><a href="#5-别名" class="headerlink" title="5 别名"></a>5 别名</h2><p>as后加别名，也可不要as</p><h2 id="6-匹配"><a href="#6-匹配" class="headerlink" title="6 匹配"></a>6 匹配</h2><p>分为完整匹配和模糊匹配</p><p><a href="https://www.cnblogs.com/Leophen/p/11397621.html">https://www.cnblogs.com/Leophen/p/11397621.html</a></p><p>关键字like</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from table where name like &quot;%三%&quot;</span><br></pre></td></tr></table></figure><p>通配符：%，_，[<em>charlist</em>] ，[!<em>charlist</em>]</p><h2 id="7-条件"><a href="#7-条件" class="headerlink" title="7 条件"></a>7 条件</h2><p>1.IF</p><p>表达式：IF( expr1 , expr2 , expr3 )</p><p>expr1条件，条件为true，则值是expr2 ，false，值就是expr3</p><p>2 case</p><p>input几行output几行</p><p>一行一行来</p><p><a href="https://zhuanlan.zhihu.com/p/240717732">https://zhuanlan.zhihu.com/p/240717732</a></p><p>两种形式：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">--type1</span><br><span class="line">CASE  &lt;表达式&gt;</span><br><span class="line">   WHEN &lt;值1&gt; THEN &lt;操作&gt;</span><br><span class="line">   WHEN &lt;值2&gt; THEN &lt;操作&gt;</span><br><span class="line">   ...</span><br><span class="line">   ELSE &lt;操作&gt;</span><br><span class="line">END</span><br><span class="line">--type2</span><br><span class="line">CASE</span><br><span class="line">    WHEN &lt;条件1&gt; THEN &lt;命令&gt;</span><br><span class="line">    WHEN &lt;条件2&gt; THEN &lt;命令&gt;</span><br><span class="line">    ...</span><br><span class="line">    ELSE commands</span><br><span class="line">END</span><br></pre></td></tr></table></figure><p>then后面多个值</p><p>then 1 可</p><p>then (1,1) 不可</p><p>where xx in （1,1）可</p><p>3 where</p><h4 id="1126-查询活跃业务"><a href="#1126-查询活跃业务" class="headerlink" title="1126. 查询活跃业务"></a><a href="https://leetcode-cn.com/problems/active-businesses/">1126. 查询活跃业务</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Write your MySQL query statement below</span><br><span class="line">select t.business_id </span><br><span class="line">from </span><br><span class="line">(select *, avg(E1.occurences  ) over(partition by E1.event_type ) as ave_occu</span><br><span class="line">from Events E1 ) t</span><br><span class="line">where t.occurences &gt; t.ave_occu</span><br><span class="line">group by t.business_id</span><br><span class="line">having    count(t.business_id)&gt;=2</span><br></pre></td></tr></table></figure><p>4  条件判断</p><p>(E1.id , E1.month) in ((1,8))</p><p> BETWEEN ‘2019-06-28’ AND ‘2019-07-27’</p><p>EXISTS ：用于判断查询子句是否有记录，如果有一条或多条记录存在返回 True，否则返回 False。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT column_name(s)</span><br><span class="line">FROM table_name</span><br><span class="line">WHERE EXISTS</span><br><span class="line">(SELECT column_name FROM table_name WHERE condition);</span><br></pre></td></tr></table></figure><h2 id="8-NULL"><a href="#8-NULL" class="headerlink" title="8 NULL"></a>8 NULL</h2><p>1.空字符和null的区别</p><p><a href="https://blog.csdn.net/weixin_42214393/article/details/80463912">https://blog.csdn.net/weixin_42214393/article/details/80463912</a></p><p>2.判断NULL</p><p>is not NULL</p><p>!=NULL 有问题</p><p>ifnull(sum(quantity), 0)</p><p>3  in、not in</p><p><a href="https://blog.csdn.net/qq_22592457/article/details/108024521">https://blog.csdn.net/qq_22592457/article/details/108024521</a></p><ol><li>使用in时，in后面的括号中忽略null值</li><li>使用not in时，如果 not in后面的括号中没有null，正常判断，会查询条件列中符合要求的数据</li><li>使用not in时，如果 not in后面的括号中有null，直接返回false，查询结果为空。</li></ol><h2 id="9-日期"><a href="#9-日期" class="headerlink" title="9 日期"></a>9 日期</h2><p>1 大小判断</p><p>available_from &lt; ‘2019-05-23’</p><p>datediff(date1,date2)</p><p>2 格式转化</p><p>DATE_FORMAT()</p><p><a href="https://www.w3school.com.cn/sql/func_date_format.asp">https://www.w3school.com.cn/sql/func_date_format.asp</a></p><h2 id="10-去重"><a href="#10-去重" class="headerlink" title="10 去重"></a>10 去重</h2><p>1 distinct</p><p><a href="https://blog.csdn.net/zhangzehai2234/article/details/88361586">https://blog.csdn.net/zhangzehai2234/article/details/88361586</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select distinct expression[,expression...] from tables [where conditions];</span><br></pre></td></tr></table></figure><p>在使用distinct的过程中主要注意一下几点：</p><ul><li>在对字段进行去重的时候，要保证distinct在所有字段的最前面</li><li>如果distinct关键字后面有多个字段时，则会对多个字段进行组合去重，只有多个字段组合起来的值是相等的才会被去重</li></ul><p>distinct , count 一起用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">##建表</span><br><span class="line">Create table If Not Exists Spending (user_id int, spend_date date, platform ENUM(&#x27;desktop&#x27;, &#x27;mobile&#x27;), amount int);</span><br><span class="line">Truncate table Spending;</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;1&#x27;, &#x27;2019-07-01&#x27;, &#x27;mobile&#x27;, &#x27;100&#x27;);</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;1&#x27;, &#x27;2019-07-01&#x27;, &#x27;desktop&#x27;, &#x27;100&#x27;);</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;2&#x27;, &#x27;2019-07-01&#x27;, &#x27;mobile&#x27;, &#x27;100&#x27;);</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;2&#x27;, &#x27;2019-07-02&#x27;, &#x27;mobile&#x27;, &#x27;100&#x27;);</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;3&#x27;, &#x27;2019-07-01&#x27;, &#x27;desktop&#x27;, &#x27;100&#x27;);</span><br><span class="line">insert into Spending (user_id, spend_date, platform, amount) values (&#x27;3&#x27;, &#x27;2019-07-02&#x27;, &#x27;desktop&#x27;, &#x27;100&#x27;);</span><br><span class="line"></span><br><span class="line">###查询</span><br><span class="line">select distinct spend_date ,count(user_id ) </span><br><span class="line">from Spending</span><br><span class="line"></span><br><span class="line">##result</span><br><span class="line">##返回结果就一行，distinct后多行，count一行，多行一行还是一行；count结果还是distinct前的数量</span><br><span class="line">spend_datecount(user_id )</span><br><span class="line">2019-07-016</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="11-show-status"><a href="#11-show-status" class="headerlink" title="11 show status"></a>11 show status</h2><p><a href="https://blog.csdn.net/qq_29168493/article/details/79149132">https://blog.csdn.net/qq_29168493/article/details/79149132</a></p><p>查看当前数据库状态，可以统计当前数据库不同语句的执行频次</p><h2 id="12-explain"><a href="#12-explain" class="headerlink" title="12 explain"></a>12 explain</h2><p>获取sql执行计划，结果明细参考</p><p><a href="https://cloud.tencent.com/developer/article/1093229">https://cloud.tencent.com/developer/article/1093229</a></p><h2 id="14-事务"><a href="#14-事务" class="headerlink" title="14 事务"></a>14 事务</h2><p><a href="http://m.biancheng.net/sql/transaction.html">http://m.biancheng.net/sql/transaction.html</a></p><h2 id="15-递归查询"><a href="#15-递归查询" class="headerlink" title="15 递归查询"></a>15 递归查询</h2><p><a href="https://zhuanlan.zhihu.com/p/372330656">https://zhuanlan.zhihu.com/p/372330656</a></p><p><a href="https://medium.com/swlh/recursion-in-sql-explained-graphically-679f6a0f143b">https://medium.com/swlh/recursion-in-sql-explained-graphically-679f6a0f143b</a></p><p><a href="https://www.sqlservertutorial.net/sql-server-basics/sql-server-recursive-cte/">https://www.sqlservertutorial.net/sql-server-basics/sql-server-recursive-cte/</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">WITH expression_name (column_list)</span><br><span class="line">AS</span><br><span class="line">(</span><br><span class="line">    -- Anchor member</span><br><span class="line">    initial_query  </span><br><span class="line">    UNION ALL</span><br><span class="line">    -- Recursive member that references expression_name.</span><br><span class="line">    recursive_query  </span><br><span class="line">)</span><br><span class="line">-- references expression name</span><br><span class="line">SELECT *</span><br><span class="line">FROM   expression_name</span><br></pre></td></tr></table></figure><p><img src="/2022/02/27/sql-common-operation/1.png" alt></p><p>分析  <a href="https://zhuanlan.zhihu.com/p/372330656">https://zhuanlan.zhihu.com/p/372330656</a></p><p>R0 ：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> UserID,ManagerID,Name,Name <span class="keyword">AS</span> ManagerName</span><br><span class="line"> <span class="keyword">FROM</span> dbo.Employee</span><br><span class="line"> <span class="keyword">WHERE</span> ManagerID<span class="operator">=</span><span class="number">-1</span></span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>userid</th><th>managerid</th><th>name</th><th>managername</th></tr></thead><tbody><tr><td>1</td><td>-1</td><td>boss</td><td>boss</td></tr></tbody></table></div><p>R1：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c.UserID,c.ManagerID,c.Name,p.Name <span class="keyword">AS</span> ManagerName</span><br><span class="line"><span class="keyword">FROM</span> CTE P</span><br><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> dbo.Employee c <span class="keyword">ON</span> p.UserID<span class="operator">=</span>c.ManagerID</span><br></pre></td></tr></table></figure><p>此时Employee为完整的，cte为</p><div class="table-container"><table><thead><tr><th>userid</th><th>managerid</th><th>name</th><th>managername</th></tr></thead><tbody><tr><td>1</td><td>-1</td><td>boss</td><td>boss</td></tr></tbody></table></div><p>得到结果为</p><div class="table-container"><table><thead><tr><th>c.userid</th><th>c.managerid</th><th>c.name</th><th>c.managername</th><th>p.userid</th><th>p.managerid</th><th>p.name</th><th>p.managername</th></tr></thead><tbody><tr><td>11</td><td>1</td><td>A1</td><td>A1</td><td>1</td><td>-1</td><td>boss</td><td>boss</td></tr><tr><td>12</td><td>1</td><td>A2</td><td>A2</td><td>1</td><td>-1</td><td>boss</td><td>boss</td></tr><tr><td>13</td><td>1</td><td>A3</td><td>A3</td><td>1</td><td>-1</td><td>boss</td><td>boss</td></tr></tbody></table></div><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> c.UserID,c.ManagerID,c.Name,p.Name <span class="keyword">AS</span> ManagerName</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>userid</th><th>managerid</th><th>name</th><th>name</th></tr></thead><tbody><tr><td>11</td><td>1</td><td>A1</td><td>boss</td></tr><tr><td>12</td><td>1</td><td>A2</td><td>boss</td></tr><tr><td>13</td><td>1</td><td>A3</td><td>boss</td></tr></tbody></table></div><p>R2,R3…</p><p>最后union all R0，R1，R2，。。。</p><h2 id="16-select-常数"><a href="#16-select-常数" class="headerlink" title="16 select 常数"></a>16 select 常数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SELECT 1,-1,N&#x27;Boss&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 11,1,N&#x27;A1&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 12,1,N&#x27;A2&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 13,1,N&#x27;A3&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 111,11,N&#x27;B1&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 112,11,N&#x27;B2&#x27;</span><br><span class="line">UNION  ALL</span><br><span class="line">SELECT 121,12,N&#x27;C1&#x27;</span><br></pre></td></tr></table></figure><p>1    -1    Boss  —字段</p><p>1    -1    Boss<br>11    1    A1<br>12    1    A2<br>13    1    A3<br>111    11    B1<br>112    11    B2<br>121    12    C1</p><h2 id="17-CTE"><a href="#17-CTE" class="headerlink" title="17 CTE"></a>17 CTE</h2><p>Common Table Expression</p><p>with …as… </p><h2 id="18-触发器"><a href="#18-触发器" class="headerlink" title="18 触发器"></a>18 触发器</h2><p>就是做了某些操作，自动触发的行为</p><p>触发器是自动执行的，当用户对表中数据作了某些操作之后立即被触发。</p><p><a href="https://blog.csdn.net/weixin_48033173/article/details/111713117">https://blog.csdn.net/weixin_48033173/article/details/111713117</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql常见操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql优化</title>
      <link href="/2022/02/27/sql-optimization/"/>
      <url>/2022/02/27/sql-optimization/</url>
      
        <content type="html"><![CDATA[<h2 id="1-汇总"><a href="#1-汇总" class="headerlink" title="1 汇总"></a>1 汇总</h2><p><a href="https://blog.csdn.net/hguisu/article/details/5731629">https://blog.csdn.net/hguisu/article/details/5731629</a></p><p><a href="https://www.analyticsvidhya.com/blog/2021/10/a-detailed-guide-on-sql-query-optimization/">https://www.analyticsvidhya.com/blog/2021/10/a-detailed-guide-on-sql-query-optimization/</a></p><p><a href="https://blog.devart.com/how-to-optimize-sql-query.html#sql-query-optimization-basics">https://blog.devart.com/how-to-optimize-sql-query.html#sql-query-optimization-basics</a></p><h2 id="2-减少全表扫描"><a href="#2-减少全表扫描" class="headerlink" title="2 减少全表扫描"></a>2 减少全表扫描</h2><p><a href="https://www.cnblogs.com/feiling/p/3393356.html">https://www.cnblogs.com/feiling/p/3393356.html</a></p><p><a href="https://www.jianshu.com/p/03968ac9d8ad">https://www.jianshu.com/p/03968ac9d8ad</a></p><h2 id="3-创建索引"><a href="#3-创建索引" class="headerlink" title="3 创建索引"></a>3 创建索引</h2><p><a href="https://blog.csdn.net/happyheng/article/details/53143345">https://blog.csdn.net/happyheng/article/details/53143345</a></p><p><a href="https://www.runoob.com/mysql/mysql-index.html">https://www.runoob.com/mysql/mysql-index.html</a></p><p><a href="https://blog.csdn.net/wangfeijiu/article/details/113409719">https://blog.csdn.net/wangfeijiu/article/details/113409719</a></p><h4 id="0-作用"><a href="#0-作用" class="headerlink" title="0 作用"></a>0 作用</h4><p>可以提高查询效率</p><p>和主键的区别，主键是特殊的索引，索引不一定是主键</p><p><a href="https://blog.csdn.net/krismile__qh/article/details/98477484">https://blog.csdn.net/krismile__qh/article/details/98477484</a></p><p><a href="https://blog.csdn.net/weixin_33375360/article/details/113371197">https://blog.csdn.net/weixin_33375360/article/details/113371197</a></p><p>既然有主键为啥还要索引，关键在于这是两个东西，一个是为了唯一表示，一个是为了提高查询效率，底层也不同</p><p><a href="https://cache.one/read/17347789">https://cache.one/read/17347789</a></p><h4 id="1-索引分类"><a href="#1-索引分类" class="headerlink" title="1 索引分类"></a>1 索引分类</h4><p>聚集索引与非聚集索引</p><h4 id="2-常见操作"><a href="#2-常见操作" class="headerlink" title="2 常见操作"></a>2 常见操作</h4><p>1、创建索引</p><p>创建表时指定索引</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">drop TABLE if EXISTS s1;</span><br><span class="line">create table s1(</span><br><span class="line">    id int ,</span><br><span class="line"></span><br><span class="line">    age int,</span><br><span class="line">    email varchar(30),</span><br><span class="line">index(id)</span><br><span class="line"></span><br><span class="line">    );</span><br></pre></td></tr></table></figure><p>创建表后</p><p>CREATE INDEX 索引名 ON 表名(列的列表);/ALTER TABLE 表名 ADD INDEX 索引名 (列名1，列名2,…);</p><p>2、删除索引</p><p>DROP INDEX index_name ON talbe_name<br>ALTER TABLE table_name DROP INDEX index_name</p><p>3、查看索引</p><p> SHOW INDEX FROM table_name;</p><p>4、使用索引</p><p>建立了索引，底层查询效率变高了</p><p>查询语句编写上和原来一样，没有区别</p><p><a href="https://blog.csdn.net/lenux2017/article/details/80086265">https://blog.csdn.net/lenux2017/article/details/80086265</a></p><h4 id="3-索引失效"><a href="#3-索引失效" class="headerlink" title="3 索引失效"></a>3 索引失效</h4><p><a href="https://www.cnblogs.com/technologykai/articles/14172224.html">https://www.cnblogs.com/technologykai/articles/14172224.html</a></p><h4 id="4-索引设计"><a href="#4-索引设计" class="headerlink" title="4 索引设计"></a>4 索引设计</h4><p><a href="http://c.biancheng.net/view/7366.html">http://c.biancheng.net/view/7366.html</a></p><h2 id="4-视图"><a href="#4-视图" class="headerlink" title="4 视图"></a>4 视图</h2><p><a href="https://blog.csdn.net/talentluke/article/details/6420197">https://blog.csdn.net/talentluke/article/details/6420197</a></p><p><a href="http://m.biancheng.net/sql/create-view.html">http://m.biancheng.net/sql/create-view.html</a></p><p>视图为虚拟的表，包含的不是数据而是sql查询</p><p>视图和表的主要区别在于：</p><ul><li>表占用物理存储空间，也包含真正的数据；</li><li>视图不需要物理存储空间（除非您为视图添加索引），也不包含真正的数据，它只是从表中引用数据。</li></ul><p>作用</p><ul><li>简化数据访问，让复杂的 SQL 语句简单化。用户只需要对视图写简单的代码就能返回需要的数据，一些复杂的逻辑放在视图中完成。</li><li>防止敏感的字段被选中，同时仍然提供对其它重要数据的访问。</li><li>可以对视图添加一些额外的索引，来提高查询的效率。</li></ul><p>使用视图的时候跟表一样 </p><p><strong>和cte的区别</strong></p><p><a href="https://blog.csdn.net/happyboy53/article/details/2731420">https://blog.csdn.net/happyboy53/article/details/2731420</a></p><p>子查询包含的是数据，将数据存在内存，而视图包含的不是数据而是sql查询</p><h2 id="5-存储过程"><a href="#5-存储过程" class="headerlink" title="5 存储过程"></a>5 存储过程</h2><p> SQL 语言层面的代码封装与重用</p><p><a href="https://www.runoob.com/w3cnote/mysql-stored-procedure.html">https://www.runoob.com/w3cnote/mysql-stored-procedure.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql函数</title>
      <link href="/2022/02/27/sql-func/"/>
      <url>/2022/02/27/sql-func/</url>
      
        <content type="html"><![CDATA[<h2 id="1-单行函数和多行函数"><a href="#1-单行函数和多行函数" class="headerlink" title="1 单行函数和多行函数"></a>1 单行函数和多行函数</h2><p>单行函数：单入单出</p><p>多行函数：多入单出，最常见的就是聚合函数</p><p>应该不存在单入多出，多入多出可以简化为单入单出的多次</p><p><a href="https://blog.csdn.net/lailai186/article/details/12570899">https://blog.csdn.net/lailai186/article/details/12570899</a></p><p>注意：</p><p>多行，单行结果组合返回一行</p><p>例子：select column1 ,count(column2) ，只返回一行</p><p>问题来了，多多不一样呢，比如3和2，应该不存在</p><h2 id="2-用户自定义函数"><a href="#2-用户自定义函数" class="headerlink" title="2 用户自定义函数"></a>2 用户自定义函数</h2><p><a href="https://blog.csdn.net/qq_23833037/article/details/53170789">https://blog.csdn.net/qq_23833037/article/details/53170789</a></p><h2 id="3-聚合函数"><a href="#3-聚合函数" class="headerlink" title="3 聚合函数"></a>3 聚合函数</h2><p>顾名思义，将数据聚集返回单一的值</p><p><a href="https://blog.csdn.net/qq_40456829/article/details/83657396">https://blog.csdn.net/qq_40456829/article/details/83657396</a></p><h2 id="4-开窗函数（Window-Function）"><a href="#4-开窗函数（Window-Function）" class="headerlink" title="4 开窗函数（Window Function）"></a>4 开窗函数（Window Function）</h2><p><a href="https://segmentfault.com/a/1190000040088969">https://segmentfault.com/a/1190000040088969</a></p><p><a href="https://www.51cto.com/article/639541.html">https://www.51cto.com/article/639541.html</a></p><p><a href="https://blog.csdn.net/weixin_43412569/article/details/107992998">https://blog.csdn.net/weixin_43412569/article/details/107992998</a></p><p><strong>作用</strong></p><p>行数保持不变</p><p>输入多行（一个窗口）、返回一个值</p><p><strong>计算过程</strong></p><p><img src="/2022/02/27/sql-func/1.png" alt></p><p>当前行-》分区-》排序-》范围-》计算-》结果填入当前行</p><p><strong>语法</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window_function ([expression]) OVER (</span><br><span class="line">   [ PARTITION BY part_list ]</span><br><span class="line">   [ ORDER BY order_list ]</span><br><span class="line">   [ &#123; ROWS | RANGE &#125; BETWEEN frame_start AND frame_end ] )</span><br></pre></td></tr></table></figure><ul><li><p>expression</p></li><li><p>PARTITION BY  </p><p>表示将数据按 part_list 进行分区， 不加partition by 默认用全部（一个分区）</p><p>partition by  columns1,columns2</p></li><li><p>ORDER BY </p><p>表示将各个分区内的数据按 order_list进行排序</p></li><li><p>ROWS / RANGE 决定数据范围</p><p><a href="https://blog.csdn.net/qq_42374697/article/details/115109386">https://blog.csdn.net/qq_42374697/article/details/115109386</a></p></li></ul><p><strong>分类</strong></p><p><a href="https://www.cnblogs.com/52xf/p/4209211.html">https://www.cnblogs.com/52xf/p/4209211.html</a></p><p>可以分为以下 3 类：</p><ul><li>聚合（Aggregate）：<code>AVG()</code>, <code>COUNT()</code>, <code>MIN()</code>, <code>MAX()</code>, <code>SUM()</code>…    </li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sum(frequency) over(partiton by   num )  --分组累加</span><br><span class="line">sum(frequency) over()         --total_frequency  全部累加</span><br><span class="line">sum(frequency) over(order by num desc)  --desc_frequency, 逆序累加</span><br><span class="line">sum(frequency) over(order by num asc)   --asc_frequency,正序累加</span><br><span class="line">SUM(Salary) OVER (PARTITION BY Id ORDER BY Month asc range 2 PRECEDING) --range 2 PRECEDING 当前以及前面2行</span><br><span class="line"></span><br><span class="line">avg(frequency) over()  --total_frequency ，全部平均</span><br><span class="line">avg(frequency) over(order by num desc)  --desc_frequency, 逆序平均</span><br><span class="line">avg(frequency) over(order by num asc)   --asc_frequency,  正序平均</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​    <a href="https://blog.csdn.net/qq_54494937/article/details/119537881">https://blog.csdn.net/qq_54494937/article/details/119537881</a></p><p>​    <a href="https://leetcode-cn.com/problems/find-median-given-frequency-of-numbers/">https://leetcode-cn.com/problems/find-median-given-frequency-of-numbers/</a></p><p>​    <a href="https://blog.csdn.net/qq_54494937/article/details/119537881">https://blog.csdn.net/qq_54494937/article/details/119537881</a></p><ul><li><p>取值（Value）：<code>FIRST_VALUE()</code>, <code>LAST_VALUE()</code>, <code>LEAD()</code>, <code>LAG()</code>…</p></li><li><p>排序（Ranking）：<code>RANK()</code>, <code>DENSE_RANK()</code>, <code>ROW_NUMBER()</code>, <code>NTILE()</code>…</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rank() OVER (PARTITION BY Id ORDER BY Month DESC) </span><br></pre></td></tr></table></figure><p><a href="https://leetcode-cn.com/problems/find-cumulative-salary-of-an-employee/">https://leetcode-cn.com/problems/find-cumulative-salary-of-an-employee/</a></p></li></ul><h2 id="5-数学运算函数"><a href="#5-数学运算函数" class="headerlink" title="5 数学运算函数"></a>5 数学运算函数</h2><p><a href="https://blog.csdn.net/a_lllll/article/details/87880389">https://blog.csdn.net/a_lllll/article/details/87880389</a></p><h4 id="abs"><a href="#abs" class="headerlink" title="abs"></a>abs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select  distinct C1.seat_id as seat_id</span><br><span class="line">from Cinema C1 join Cinema C2</span><br><span class="line">on C1.free=1 and C2.free=1 and abs(C1.seat_id-C2.seat_id)=1</span><br><span class="line">order by seat_id</span><br></pre></td></tr></table></figure><h4 id="power"><a href="#power" class="headerlink" title="power"></a>power</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Write your MySQL query statement below</span><br><span class="line">select  round(min(Power(Power(P1.x-P2.x,2)+Power(P1.y-P2.y,2),0.5)),2) as shortest </span><br><span class="line">from Point2D   P1 join Point2D  P2 </span><br><span class="line">on P1.x!=P2.x or P1.y!=P2.y</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据划分,rdd分区</title>
      <link href="/2022/02/26/spark-job_partition/"/>
      <url>/2022/02/26/spark-job_partition/</url>
      
        <content type="html"><![CDATA[<h2 id="1-application，job，stage，task"><a href="#1-application，job，stage，task" class="headerlink" title="1 application，job，stage，task"></a>1 application，job，stage，task</h2><p><img src="/2022/02/26/spark-job_partition/1.JPG" alt></p><p>0 application</p><p>任务</p><p>1 Job</p><p>一个action 一个job</p><p>2 Stage</p><p>一个job包含一个或者多个stage，根据rdd的依赖关系构建dag，根据dag划分stage</p><p><img src="/2022/02/26/spark-job_partition/3.png" alt></p><p>3 Task</p><p>1个stage包含1个或者多个task</p><p>Task的类型分为2种：ShuffleMapTask和ResultTask</p><p>ShuffleMapTask要进行Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask。如果要按照map端和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为Spark中的Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色。</p><h2 id="2-rdd分区"><a href="#2-rdd分区" class="headerlink" title="2 rdd分区"></a>2 rdd分区</h2><p><img src="/2022/02/26/spark-job_partition/1.webp" alt></p><p>record就是记录</p><p>2 为什么分区？</p><p>分区的主要作用是用来实现并行计算，提高效率</p><p>3 分区方式</p><p>Spark包含两种数据分区方式：HashPartitioner（哈希分区）和RangePartitioner（范围分区）。</p><p>4 分区数设置</p><p><a href="https://justdodt.github.io/2018/04/23/Spark-RDD-%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F%E7%A1%AE%E5%AE%9A/">https://justdodt.github.io/2018/04/23/Spark-RDD-%E7%9A%84%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F%E7%A1%AE%E5%AE%9A/</a></p><h2 id="3-关系"><a href="#3-关系" class="headerlink" title="3 关系"></a>3 关系</h2><p><img src="/2022/02/26/spark-job_partition/2.png" alt></p><p>task数量和Partition数量一样</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/3aa52ee3a802">https://www.jianshu.com/p/3aa52ee3a802</a></p><p><a href="https://blog.csdn.net/hjw199089/article/details/77938688">https://blog.csdn.net/hjw199089/article/details/77938688</a></p><p><a href="https://blog.csdn.net/mys_35088/article/details/80864092">https://blog.csdn.net/mys_35088/article/details/80864092</a></p><p><a href="https://blog.csdn.net/dmy1115143060/article/details/82620715">https://blog.csdn.net/dmy1115143060/article/details/82620715</a></p><p><a href="https://blog.csdn.net/xxd1992/article/details/85254666">https://blog.csdn.net/xxd1992/article/details/85254666</a></p><p><a href="https://blog.csdn.net/m0_46657040/article/details/108737350">https://blog.csdn.net/m0_46657040/article/details/108737350</a></p><p><a href="https://blog.csdn.net/heiren_a/article/details/111954523">https://blog.csdn.net/heiren_a/article/details/111954523</a></p><p><a href="https://blog.csdn.net/u011564172/article/details/53611109">https://blog.csdn.net/u011564172/article/details/53611109</a></p><p><a href="https://blog.csdn.net/qq_22473611/article/details/107822168">https://blog.csdn.net/qq_22473611/article/details/107822168</a></p><p><a href="https://www.jianshu.com/p/3aa52ee3a802">https://www.jianshu.com/p/3aa52ee3a802</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据划分,rdd分区 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark模块</title>
      <link href="/2022/02/26/spark-module/"/>
      <url>/2022/02/26/spark-module/</url>
      
        <content type="html"><![CDATA[<p><img src="/2022/02/26/spark-module/1.png" alt></p><p>整个Spark 框架模块包含：Spark Core、Spark SQL、Spark Streaming、Spark GraphX、Spark MLlib，而后四项的能力都是建立在核心引擎之上</p><p><strong>Spark Core</strong>：Spark的核心，Spark核心功能均由Spark Core模块提供，是Spark运行的基础。Spark Core以RDD为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。<br><strong>SparkSQL</strong>：基于SparkCore之上，提供结构化数据的处理模块。SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了StructuredStreaming模块，可以以SparkSQL为基础，进行数据的流式计算。</p><p>数据抽象:dataset(Java、Scala)   dataframe(Java、Scala、Python、R)<br><strong>SparkStreaming</strong>：以SparkCore为基础，提供数据的流式计算功能。<br><strong>MLlib</strong>：以SparkCore为基础，进行机器学习计算，内置了大量的机器学习库和API算法等。方便用户以分布式计算的模式进行机器学习计算。<br><strong>GraphX</strong>：以SparkCore为基础，进行图计算，提供了大量的图计算API，方便用于以分布式计算模式进行图计算。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark模块 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark vs MapReduce</title>
      <link href="/2022/02/25/mapreduce-spark/"/>
      <url>/2022/02/25/mapreduce-spark/</url>
      
        <content type="html"><![CDATA[<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p><a href="https://www.educba.com/mapreduce-vs-spark/">https://www.educba.com/mapreduce-vs-spark/</a></p><div class="table-container"><table><thead><tr><th></th><th>MapReduce</th><th>Spark</th></tr></thead><tbody><tr><td><strong>Product’s Category</strong></td><td>From the introduction, we understood that MapReduce enables the processing of data and hence is majorly a data processing engine.</td><td>Spark, on the other hand, is a framework that drives complete analytical solutions or applications and hence making it an obvious choice for data scientists to use this as a data analytics engine.</td></tr><tr><td><strong>Framework’s Performance and Data Processing</strong></td><td>In the case of MapReduce, reading and writing operations are performed from and to a disk thus leading to slowness in the processing speed.</td><td>In Spark, the number of read/write cycles is minimized along with storing data in memory allowing it to be 10 times faster. But spark may suffer a major degradation if data doesn’t fit in memory.</td></tr><tr><td><strong>Latency</strong></td><td>As a result of lesser performance than Spark, MapReduce has a higher latency in computing.</td><td>Since Spark is faster, it enables developers with low latency computing.</td></tr><tr><td><strong>Manageability of framework</strong></td><td>MapReduce being only a batch engine, other components must be handled separately yet synchronously thus making it difficult to manage.</td><td>Spark is a complete data analytics engine, has the capability to perform batch, interactive streaming, and similar component all under the same cluster umbrella and thus easier to manage!</td></tr><tr><td><strong>Real-time Analysis</strong></td><td>MapReduce was built mainly for batch processing and hence fails when used for real-time analytics use cases.</td><td>Data coming from real-time live streams like Facebook, Twitter, etc. can be efficiently managed and processed in Spark.</td></tr><tr><td><strong>Interactive Mode</strong></td><td>MapReduce doesn’t provide the gamut of having interactive mode.</td><td>In spark it is possible to process the data interactively</td></tr><tr><td><strong>Security</strong></td><td>MapReduce has accessibility to all features of Hadoop security and as a result of this, it is can be easily integrated with other projects of Hadoop Security. MapReduce also supports ASLs.</td><td>In Spark, the security is by default set to OFF which might lead to a major security fallback. In the case of authentication, only the shared secret password method is possible in Spark.</td></tr><tr><td><strong>Tolerance to Failure</strong></td><td>In case of crash of MapReduce process, the process is capable of starting from the place where it was left off earlier as it relies on Hard Drives rather than RAMs</td><td>In case of crash of Spark process, the processing should start from the beginning and hence becomes less fault-tolerant than MapReduce as it relies of RAM usage.</td></tr></tbody></table></div><p><img src="/2022/02/25/mapreduce-spark/2.png" alt></p><h2 id="spark为什么比MapReduce快"><a href="#spark为什么比MapReduce快" class="headerlink" title="spark为什么比MapReduce快"></a>spark为什么比MapReduce快</h2><p><a href="https://blog.csdn.net/JENREY/article/details/84873874">https://blog.csdn.net/JENREY/article/details/84873874</a></p><p>1 spark基于内存 ，mapreduce基于磁盘</p><p>指的是中间结果</p><p>MapReduce：通常需要将计算的中间结果写入磁盘，然后还要读取磁盘，从而导致了频繁的磁盘IO</p><p>Spark：不需要每次将计算的中间结果写入磁盘</p><p>2 spark粗粒度资源申请，MapReduce细粒度资源申请</p><p>spark 执行task不需要自己申请资源，提交任务的时候统一申请了</p><p>MapReduce 执行task任务的时候，task自己申请</p><p>3 spark基于多线程，mapreduce基于多进程</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark vs MapReduce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark参数</title>
      <link href="/2022/02/23/context/"/>
      <url>/2022/02/23/context/</url>
      
        <content type="html"><![CDATA[<h2 id="SparkSession、SparkContext、HiveContext、SQLContext"><a href="#SparkSession、SparkContext、HiveContext、SQLContext" class="headerlink" title="SparkSession、SparkContext、HiveContext、SQLContext"></a>SparkSession、SparkContext、HiveContext、SQLContext</h2><p><a href="https://blog.csdn.net/weixin_43648241/article/details/108917865">https://blog.csdn.net/weixin_43648241/article/details/108917865</a></p><p>SparkSession包含SparkContext</p><p>SparkContext包含HiveContext</p><p>HiveContext包含SQLContext</p><p>SparkSession &gt;  SparkContext &gt;  HiveContext &gt; SQLContext</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">SparkSession.builder.\</span><br><span class="line">config(&quot;hive.metastore.uris&quot;, &quot;thrift://xxx.xx.x.xx:xxxx&quot;).\</span><br><span class="line">config(&quot;spark.pyspark.python&quot;, &quot;/opt/dm_python3/bin/python&quot;).\</span><br><span class="line">config(&#x27;spark.default.parallelism &#x27;, 10 ).\</span><br><span class="line">config(&#x27;spark.sql.shuffle.partitions&#x27;, 200 ).\</span><br><span class="line">config(&quot;spark.driver.maxResultSize&quot;, &quot;16g&quot;).\</span><br><span class="line">config(&quot;spark.port.maxRetries&quot;, &quot;100&quot;).\</span><br><span class="line">config(&quot;spark.driver.memory&quot;,&quot;16g&quot;).\</span><br><span class="line">config(&quot;spark.yarn.queue&quot;, &quot;dcp&quot; ).\</span><br><span class="line">config(&quot;spark.executor.memory&quot;, &quot;16g&quot; ).\</span><br><span class="line">config( &quot;spark.executor.cores&quot;, 20).\</span><br><span class="line">config(&quot;spark.files&quot;, addfile).\</span><br><span class="line">config( &quot;spark.executor.instances&quot;, 6 ).\</span><br><span class="line">config(&quot;spark.speculation&quot;, False).\</span><br><span class="line">config( &quot;spark.submit.pyFiles&quot;, zipfile).\</span><br><span class="line">appName(&quot;testing&quot;).\</span><br><span class="line">master(&quot;yarn&quot;).\</span><br><span class="line">enableHiveSupport().\</span><br><span class="line">getOrCreate()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark参数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark支持的存储介质</title>
      <link href="/2022/02/21/spark-store/"/>
      <url>/2022/02/21/spark-store/</url>
      
        <content type="html"><![CDATA[<p>Spark 支持多种的存储介质，在存储层 Spark 支持从 HDFS、HBase、Hive、ES、MongoDB、MySQL、PostgreSQL、AWS、Ali Cloud 等不同的存储系统、大数据库、关系型数据库中读入和写出数据，在实时流计算中可以从 Flume、Kafka 等多种数据源获取数据并执行流式计算。</p><p><a href="https://cloud.tencent.com/developer/article/1942980">https://cloud.tencent.com/developer/article/1942980</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark支持的存储介质 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>提交Spark任务</title>
      <link href="/2022/02/17/spark-task/"/>
      <url>/2022/02/17/spark-task/</url>
      
        <content type="html"><![CDATA[<h2 id="1-spark-submit"><a href="#1-spark-submit" class="headerlink" title="1.spark-submit"></a>1.spark-submit</h2><p><a href="https://spark.apache.org/docs/latest/submitting-applications.html">https://spark.apache.org/docs/latest/submitting-applications.html</a></p><p>The <code>spark-submit</code> script in Spark’s <code>bin</code> directory is used to launch applications on a cluster. It can use all of Spark’s supported <a href="https://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types">cluster managers</a> through a uniform interface so you don’t have to configure your application especially for each one.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><ul><li><code>--class</code>: The entry point for your application (e.g. <code>org.apache.spark.examples.SparkPi</code>)</li><li><code>--master</code>: The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster (e.g. <code>spark://23.195.26.187:7077</code>)</li><li><code>--deploy-mode</code>: Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>) (default: <code>client</code>) <strong>†</strong></li><li><code>--conf</code>: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. <code>--conf = --conf =</code>)</li><li><code>application-jar</code>: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an <code>hdfs://</code> path or a <code>file://</code> path that is present on all nodes.</li><li><code>application-arguments</code>: Arguments passed to the main method of your main class, if any</li></ul><p>当前为客户端，driver在哪取决于deploy mode</p><h2 id="2-run-file-py"><a href="#2-run-file-py" class="headerlink" title="2.run file.py"></a>2.run file.py</h2><p>当前为客户机，而且这种方式默认driver在客户机，也就是client模式</p><p>此时若是代码指定cluster会报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config(&quot;spark.submit.deployMode&quot;, &quot;cluster&quot;)</span><br></pre></td></tr></table></figure><p>Exception in thread “main” org.apache.spark.SparkException: Cluster deploy mode is not applicable to Spark shells.</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 提交Spark任务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell命令执行hive脚本</title>
      <link href="/2022/02/15/shell-hive/"/>
      <url>/2022/02/15/shell-hive/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/longshenlmj/article/details/50542683">https://blog.csdn.net/longshenlmj/article/details/50542683</a></p>]]></content>
      
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell命令执行hive脚本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>表字段的数据类型</title>
      <link href="/2022/02/15/column_type/"/>
      <url>/2022/02/15/column_type/</url>
      
        <content type="html"><![CDATA[<h2 id><a href="#" class="headerlink" title=" "></a> </h2><p><a href="https://www.html.cn/qa/other/20219.html">https://www.html.cn/qa/other/20219.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 表字段的数据类型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库分类</title>
      <link href="/2022/02/15/database-con/"/>
      <url>/2022/02/15/database-con/</url>
      
        <content type="html"><![CDATA[<p><strong>常见关系型数据库</strong>：</p><ol><li>Oracle </li><li>MySql </li><li>Microsoft SQL Server</li><li>SQLite </li><li>PostgreSQL</li><li>IBM DB2</li></ol><p><strong>常见的非关系型数据库</strong>：</p><ol><li>键值数据库：Redis、Memcached、Riak </li><li>列族数据库：Bigtable、HBase、Cassandra </li><li>文档数据库：MongoDB、CouchDB、MarkLogic </li><li>图形数据库：Neo4j、InfoGrid</li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>主外键</title>
      <link href="/2022/02/15/key-id/"/>
      <url>/2022/02/15/key-id/</url>
      
        <content type="html"><![CDATA[<p><strong>主键、外键</strong></p><p><a href="https://blog.csdn.net/weixin_31642161/article/details/113113942">https://blog.csdn.net/weixin_31642161/article/details/113113942</a></p><p>有可能没有主键</p><p><strong>联合主键，复合主键</strong></p><p>联合主键：数据库表的主键由两个及以上的字段组成。 </p><p>复合主键：有争议</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 主外键 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据采集</title>
      <link href="/2022/02/13/data-collect/"/>
      <url>/2022/02/13/data-collect/</url>
      
        <content type="html"><![CDATA[<h2 id="1-离线"><a href="#1-离线" class="headerlink" title="1 离线"></a>1 离线</h2><p><img src="/2022/02/13/data-collect/1.JPG" alt></p><h3 id="1-用户行为数据"><a href="#1-用户行为数据" class="headerlink" title="1.用户行为数据"></a>1.用户行为数据</h3><p>jar-》log-》flume-》kafka-》flume-》hdfs</p><p>用户行为数据存储在日志服务器，以.log文件存在，log-》flume-》kafka-》flume-》hdfs</p><h3 id="2-业务数据"><a href="#2-业务数据" class="headerlink" title="2 业务数据"></a>2 业务数据</h3><p>jar-》mysql-》sqoop-》hdfs</p><p>业务数据存储在mysql，使用sqoop导入hdfs</p><h2 id="2-实时"><a href="#2-实时" class="headerlink" title="2 实时"></a>2 实时</h2><p><img src="/2022/02/13/data-collect/2.JPG" alt></p><h3 id="1-用户行为数据-1"><a href="#1-用户行为数据-1" class="headerlink" title="1.用户行为数据"></a>1.用户行为数据</h3><p>前端-》Nginx-》日志服务器-》）log，Kafka（ods</p><p>1 前端埋点数据</p><p>通过jar包模拟</p><p>2 Nginx</p><p><a href="https://blog.csdn.net/qq_40036754/article/details/102463099">https://blog.csdn.net/qq_40036754/article/details/102463099</a></p><p>负载均衡</p><p>3 日志服务器</p><p>spring boot搭建</p><p>首先，Spring 就是一个java框架，spring boot在 Spring 的基础上演进</p><p>4  落盘，整合 Kafka</p><p>落盘指的是存在日志服务器</p><p>生产者-》kafka-》消费者</p><p>​           生产     消费</p><h3 id="2-业务数据-1"><a href="#2-业务数据-1" class="headerlink" title="2 业务数据"></a>2 业务数据</h3><p>jar-》mysql-》flinkcdc-》kafka（ods）</p><p>不能使用sqoop，因为sqoop底层为mapreduce，太慢了，改用canal，maxwell或者flinkcdc</p><p>数据从mysql读到kafka，不是hdfs</p><p><strong>flink-cdc</strong></p><p><a href="https://cloud.tencent.com/developer/article/1801766">https://cloud.tencent.com/developer/article/1801766</a></p><p>Change Data Capture(变更数据获取）</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数据集成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据采集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据字典</title>
      <link href="/2022/02/10/data-dict/"/>
      <url>/2022/02/10/data-dict/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/arxive/p/9673830.html">https://www.cnblogs.com/arxive/p/9673830.html</a></p><p><a href="https://blog.panoply.io/how-to-create-a-data-dictionary">https://blog.panoply.io/how-to-create-a-data-dictionary</a></p><p><a href="https://www.secoda.co/blog/how-to-create-a-data-dictionary-a-step-by-step-guide">https://www.secoda.co/blog/how-to-create-a-data-dictionary-a-step-by-step-guide</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据字典 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库建模</title>
      <link href="/2022/02/10/database-design/"/>
      <url>/2022/02/10/database-design/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/L_zhai/article/details/118650439">https://blog.csdn.net/L_zhai/article/details/118650439</a></p><p><a href="https://cloud.tencent.com/developer/article/1644918">https://cloud.tencent.com/developer/article/1644918</a></p><p><img src="/2022/02/10/database-design/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据库建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OLAP和OLTP的区别</title>
      <link href="/2022/02/09/olap-oltp/"/>
      <url>/2022/02/09/olap-oltp/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/schoolbag/p/9759214.html">https://www.cnblogs.com/schoolbag/p/9759214.html</a></p><p><img src="/2022/02/09/olap-oltp/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OLAP和OLTP的区别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ETL</title>
      <link href="/2022/02/09/etl/"/>
      <url>/2022/02/09/etl/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/yjd_hycf_space/p/7772722.html">https://www.cnblogs.com/yjd_hycf_space/p/7772722.html</a></p><p><a href="https://blog.csdn.net/qq_33269009/article/details/90522087">https://blog.csdn.net/qq_33269009/article/details/90522087</a></p><p><a href="https://blog.csdn.net/Stubborn_Cow/article/details/48420997">https://blog.csdn.net/Stubborn_Cow/article/details/48420997</a></p><p>注意：很多人理解的ETL是在经过前两个部分之后，加载到数据仓库的数据库中就完事了。ETL不仅仅是在源数据—&gt;ODS这一步，ODS—&gt;DW, DW—&gt;DM包含更为重要和复杂的ETL过程。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> ETL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据同步</title>
      <link href="/2022/02/09/data-synchronization/"/>
      <url>/2022/02/09/data-synchronization/</url>
      
        <content type="html"><![CDATA[<h2 id="1-数据同步策略"><a href="#1-数据同步策略" class="headerlink" title="1.数据同步策略"></a>1.数据同步策略</h2><p><img src="/2022/02/09/data-synchronization/1.JPG" alt></p><h3 id="1-全量"><a href="#1-全量" class="headerlink" title="1 全量"></a>1 全量</h3><p>存储完整的数据。</p><h3 id="2-增量"><a href="#2-增量" class="headerlink" title="2 增量"></a>2 增量</h3><p>存储新增加的数据。</p><h3 id="3-新增及变化"><a href="#3-新增及变化" class="headerlink" title="3 新增及变化"></a>3 新增及变化</h3><p>存储新增加的数据和变化的数据。</p><h3 id="4-特殊"><a href="#4-特殊" class="headerlink" title="4 特殊"></a>4 特殊</h3><p>某些特殊的表，可不必遵循上述同步策略。</p><p>1.例如某些不会发生变化的表</p><p>地区表，省份表，民族表等，可以只存一份固定值。</p><p>2.拉链表</p><p>在第一天同步拉链表的时候，需要同步全量数据，并且设置endtime = 9999；首日过后，每日同步数据到拉链表中就是新增及变化的数据，可以采用分区策略，以999为一个分区表示有效的数据，加上以过期时间为分区；</p><h2 id="2-首日，每日"><a href="#2-首日，每日" class="headerlink" title="2.首日，每日"></a>2.首日，每日</h2><p>首日同步，视情况，不一定全量</p><p>每日同步，视情况</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数据集成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据同步 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive架构</title>
      <link href="/2022/02/09/hive-framework/"/>
      <url>/2022/02/09/hive-framework/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/hive/design#Design-HiveArchitecture">https://cwiki.apache.org/confluence/display/hive/design#Design-HiveArchitecture</a></p><p><a href="https://zhuanlan.zhihu.com/p/87545980">https://zhuanlan.zhihu.com/p/87545980</a></p><p><a href="https://blog.csdn.net/oTengYue/article/details/91129850">https://blog.csdn.net/oTengYue/article/details/91129850</a></p><p><a href="https://jiamaoxiang.top/2020/06/27/Hive%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/">https://jiamaoxiang.top/2020/06/27/Hive%E7%9A%84%E6%9E%B6%E6%9E%84%E5%89%96%E6%9E%90/</a></p><p><a href="https://www.javatpoint.com/hive-architecture">https://www.javatpoint.com/hive-architecture</a></p><p><img src="/2022/02/09/hive-framework/1.jpg" alt></p><h2 id="Hive-Client"><a href="#Hive-Client" class="headerlink" title="Hive Client"></a>Hive Client</h2><p>Hive allows writing applications in various languages, including Java, Python, and C++. It supports different types of clients such as:-</p><ul><li>Thrift Server - It is a cross-language service provider platform that serves the request from all those programming languages that supports Thrift.</li><li>JDBC Driver - It is used to establish a connection between hive and Java applications. The JDBC Driver is present in the class org.apache.hadoop.hive.jdbc.HiveDriver.</li><li>ODBC Driver - It allows the applications that support the ODBC protocol to connect to Hive.</li></ul><h2 id="Hive-Services"><a href="#Hive-Services" class="headerlink" title="Hive Services"></a>Hive Services</h2><p>The following are the services provided by Hive:-</p><ul><li>Hive CLI - The Hive CLI (Command Line Interface) is a shell where we can execute Hive queries and commands.</li><li>Hive Web User Interface - The Hive Web UI is just an alternative of Hive CLI. It provides a web-based GUI for executing Hive queries and commands.</li><li>Hive MetaStore - It is a central repository that stores all the structure information of various tables and partitions in the warehouse. It also includes metadata of column and its type information, the serializers and deserializers which is used to read and write data and the corresponding HDFS files where the data is stored.</li><li>Hive Server - It is referred to as Apache Thrift Server. It accepts the request from different clients and provides it to Hive Driver.</li><li>Hive Driver - It receives queries from different sources like web UI, CLI, Thrift, and JDBC/ODBC driver. It transfers the queries to the compiler.</li><li>Hive Compiler - The purpose of the compiler is to parse the query and perform semantic analysis on the different query blocks and expressions. It converts HiveQL statements into MapReduce jobs.</li><li>Hive Execution Engine - Optimizer generates the logical plan in the form of DAG of map-reduce tasks and HDFS tasks. In the end, the execution engine executes the incoming tasks in the order of their dependencies.</li></ul><h2 id="计算引擎"><a href="#计算引擎" class="headerlink" title="计算引擎"></a>计算引擎</h2><p>Hive支持MapReduce、Tez、Spark</p><p><a href="https://cloud.tencent.com/developer/article/1893808">https://cloud.tencent.com/developer/article/1893808</a></p><p><a href="https://blog.csdn.net/kwu_ganymede/article/details/52223133">https://blog.csdn.net/kwu_ganymede/article/details/52223133</a></p><h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p><a href="https://cloud.tencent.com/developer/article/1411821">https://cloud.tencent.com/developer/article/1411821</a></p><p>Hive是基于hdfs的，它的数据存储在Hadoop分布式文件系统中。Hive本身是没有专门的数据存储格式，也没有为数据建立索引，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。</p><p>default数据库中的表的存储位置 /user/hive/warehouse<br>其他数据库的表自己指定</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据质量</title>
      <link href="/2022/02/09/data-quality/"/>
      <url>/2022/02/09/data-quality/</url>
      
        <content type="html"><![CDATA[<p>The Challenges of Data Quality and Data Quality Assessment in the Big Data Era</p><p><a href="https://pdfs.semanticscholar.org/0fb3/7330a4170ec63d60eec7dbb2b86e6829a3de.pdf">https://pdfs.semanticscholar.org/0fb3/7330a4170ec63d60eec7dbb2b86e6829a3de.pdf</a></p><p>A Data Quality in Use model for Big Data</p><p>Automating LargeScale Data Quality Verification</p><p>Data Sets and Data Quality in Software Engineering</p><p>Discovering Data Quality Rules </p><p>Context-aware Data Quality Assessment for Big Data</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数据质量 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据质量 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可视化报表</title>
      <link href="/2022/02/09/visrion-report/"/>
      <url>/2022/02/09/visrion-report/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/410170345">https://zhuanlan.zhihu.com/p/410170345</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 可视化报表 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 可视化报表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sql</title>
      <link href="/2022/02/07/sql/"/>
      <url>/2022/02/07/sql/</url>
      
        <content type="html"><![CDATA[<h2 id="1-SQL语句类别"><a href="#1-SQL语句类别" class="headerlink" title="1.SQL语句类别"></a>1.SQL语句类别</h2><p>SQL 语句主要可以划分为以下 3 个类别。</p><p>DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。</p><p>DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查）</p><p>DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。</p><h2 id="2-sql语句执行顺序"><a href="#2-sql语句执行顺序" class="headerlink" title="2.sql语句执行顺序"></a>2.sql语句执行顺序</h2><p><a href="https://www.cnblogs.com/Qian123/p/5669259.html">https://www.cnblogs.com/Qian123/p/5669259.html</a></p><p><a href="https://www.cnblogs.com/Qian123/p/5669259.html">https://www.cnblogs.com/Qian123/p/5669259.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1600323">https://cloud.tencent.com/developer/article/1600323</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、from子句组装来自不同数据源的数据；</span><br><span class="line">2、where子句基于指定的条件对记录行进行筛选； </span><br><span class="line">3、group by子句将数据划分为多个分组； </span><br><span class="line">4、聚合函数进行计算； </span><br><span class="line">5、having子句筛选分组； </span><br><span class="line">6、计算所有的表达式； </span><br><span class="line">7、select字段； </span><br><span class="line">8、order by对结果集进行排序。</span><br></pre></td></tr></table></figure><p>感觉好像先select后having</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> sql </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Solr</title>
      <link href="/2022/02/07/solr/"/>
      <url>/2022/02/07/solr/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/71629409?fileGuid=It0Qkg2AiecFMx62">https://zhuanlan.zhihu.com/p/71629409?fileGuid=It0Qkg2AiecFMx62</a></p><p>Solr是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Solr </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Solr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Atlas</title>
      <link href="/2022/02/07/Atlas/"/>
      <url>/2022/02/07/Atlas/</url>
      
        <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h2><p>Apache Atlas为组织提供开放式元数据管理和治理功能，用以构建其数据资产目录，对这些资产进行分类和管理，并为数据分析师和数据治理+团队，提供围绕这些数据资产的协作功能。</p><h2 id="2-Atlas的具体功能"><a href="#2-Atlas的具体功能" class="headerlink" title="2.Atlas的具体功能"></a>2.Atlas的具体功能</h2><div class="table-container"><table><thead><tr><th>元数据分类</th><th>支持对元数据进行分类管理，例如个人信息，敏感信息等</th></tr></thead><tbody><tr><td><strong>元数据检索</strong></td><td><strong>可按照元数据类型、元数据分类进行检索，支持全文检索</strong></td></tr><tr><td><strong>血缘依赖</strong></td><td><strong>支持表到表和字段到字段之间的血缘依赖，便于进行问题回溯和影响分析等</strong></td></tr></tbody></table></div><p>1）表与表之间的血缘依赖</p><p><img src="/2022/02/07/Atlas/1.jpg" alt></p><p>2）字段与字段之间的血缘依赖</p><p><img src="/2022/02/07/Atlas/2.jpg" alt></p><h2 id="3-Atlas架构原理"><a href="#3-Atlas架构原理" class="headerlink" title="3.Atlas架构原理"></a>3.Atlas架构原理</h2><p><img src="/2022/02/07/Atlas/3.jpg" alt></p><h2 id="4-使用"><a href="#4-使用" class="headerlink" title="4.使用"></a>4.使用</h2><h3 id="4-1-Hive元数据初次导入"><a href="#4-1-Hive元数据初次导入" class="headerlink" title="4.1 Hive元数据初次导入"></a>4.1 Hive元数据初次导入</h3><p><strong>操作：</strong></p><p>Atlas提供了一个Hive元数据导入的脚本，直接执行该脚本，即可完成Hive元数据的初次全量导入。</p><p>/opt/module/atlas/hook-bin/import-hive.sh</p><p><strong>问题：</strong></p><p>Failed to import Hive Meta Data!!!</p><p>注意：hive —service metastore &amp;</p><h3 id="4-2-Hive元数据增量同步"><a href="#4-2-Hive元数据增量同步" class="headerlink" title="4.2 Hive元数据增量同步"></a>4.2 Hive元数据增量同步</h3><p>Hive元数据的增量同步，无需人为干预，只要Hive中的元数据发生变化（执行DDL语句），Hive Hook就会将元数据的变动通知Atlas。除此之外，Atlas还会根据DML语句获取数据之间的血缘关系。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Atlas </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Atlas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>元数据管理</title>
      <link href="/2022/02/07/meta-data-management/"/>
      <url>/2022/02/07/meta-data-management/</url>
      
        <content type="html"><![CDATA[<h2 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h2><p><a href="https://www.ruanyifeng.com/blog/2007/03/metadata.html#">https://www.ruanyifeng.com/blog/2007/03/metadata.html#</a></p><p><a href="https://dataedo.com/kb/data-glossary/what-is-metadata">https://dataedo.com/kb/data-glossary/what-is-metadata</a></p><p><a href="https://www.cnblogs.com/alight/p/3982086.html">https://www.cnblogs.com/alight/p/3982086.html</a></p><p>Metadata is simply data about data. It means it is a description and context of the data. It helps to organize, find and understand data. Here are a few real world examples of metadata:</p><p>Those are some typical metadata elements:</p><ol><li>Title and description</li><li>Tags and categories</li><li>Who created and when</li><li>Who last modified and when</li><li>Who can access or update</li></ol><h2 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h2><p><a href="https://zhuanlan.zhihu.com/p/336504407">https://zhuanlan.zhihu.com/p/336504407</a></p><h2 id="元数据管理工具"><a href="#元数据管理工具" class="headerlink" title="元数据管理工具"></a>元数据管理工具</h2><p>atlas</p><p>hive</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 元数据管理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 元数据管理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ranger</title>
      <link href="/2022/02/07/ranger/"/>
      <url>/2022/02/07/ranger/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="/2022/02/07/ranger/1.JPG" alt></p><p>Ranager的核心是Web应用程序，也称为RangerAdmin模块，此模块由管理策略，审计日志和报告等三部分组成。</p><p>管理员角色的用户可以通过RangerAdmin提供的web界面或REST APIS来定制安全策略。这些策略会由Ranger提供的轻量级的针对不同Hadoop体系中组件的插件来执行。插件会在Hadoop的不同组件的核心进程启动后，启动对应的插件进程来进行安全管理！</p><h2 id="Ranger对Hive进行权限管理"><a href="#Ranger对Hive进行权限管理" class="headerlink" title="Ranger对Hive进行权限管理"></a>Ranger对Hive进行权限管理</h2><p><a href="https://www.jianshu.com/p/d9941b8687b7">https://www.jianshu.com/p/d9941b8687b7</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Ranger </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ranger </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见操作</title>
      <link href="/2022/02/07/linun-common-command/"/>
      <url>/2022/02/07/linun-common-command/</url>
      
        <content type="html"><![CDATA[<h2 id="1-软连接"><a href="#1-软连接" class="headerlink" title="1 软连接"></a>1 软连接</h2><p><a href="https://www.cnblogs.com/sueyyyy/p/10985443.html#">https://www.cnblogs.com/sueyyyy/p/10985443.html#</a></p><p>当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在其它的 目录下用ln命令链接（link）就可以，不必重复的占用磁盘空间。</p><p>ln -s source_path  target_path</p><p>例子：ln  -s  /home/atguigu/bin/    ~/bin</p><h2 id="2-SSH无密登录配置"><a href="#2-SSH无密登录配置" class="headerlink" title="2 SSH无密登录配置"></a>2 SSH无密登录配置</h2><p><a href="http://www.yaowenming.com/A/gAJG0mvg5Z/">http://www.yaowenming.com/A/gAJG0mvg5Z/</a></p><p>（1）hadoop102上生成公钥和私钥：</p><p>[atguigu@hadoop102 .ssh]$ ssh-keygen -t rsa</p><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>（2）将hadoop102公钥拷贝到要免密登录的目标机器上</p><p>[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop102</p><p>[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop103</p><p>[atguigu@hadoop102 .ssh]$ ssh-copy-id hadoop104</p><p>（3）hadoop103上生成公钥和私钥：</p><p>[atguigu@hadoop103 .ssh]$ ssh-keygen -t rsa</p><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）</p><p>（4）将hadoop103公钥拷贝到要免密登录的目标机器上</p><p>[atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop102</p><p>[atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop103</p><p>[atguigu@hadoop103 .ssh]$ ssh-copy-id hadoop104</p><h2 id="3-将text内容加入到file文件的第1行之后"><a href="#3-将text内容加入到file文件的第1行之后" class="headerlink" title="3 将text内容加入到file文件的第1行之后"></a>3 将text内容加入到file文件的第1行之后</h2><p>sed -i ‘1 a text’  file</p><p>例子：sed -i ‘1 a kinit -kt /etc/security/keytab/hive.keytab hive’   hdfs_to_ods_log.sh</p><h2 id="4-修改脚本执行权限"><a href="#4-修改脚本执行权限" class="headerlink" title="4 修改脚本执行权限"></a>4 修改脚本执行权限</h2><p>chmod </p><p>chmod u+x lg .sh</p><p><a href="https://blog.csdn.net/u013197629/article/details/73608613">https://blog.csdn.net/u013197629/article/details/73608613</a></p><h2 id="5-集群分发脚本"><a href="#5-集群分发脚本" class="headerlink" title="5 集群分发脚本"></a>5 集群分发脚本</h2><p>基于rsync命令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rsync  -av     /opt/module   root@hadoop103:/opt/</span><br></pre></td></tr></table></figure><p>1.编写脚本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#1. 判断参数个数</span><br><span class="line">if [ $# -lt 1 ]</span><br><span class="line">then</span><br><span class="line">  echo Not Enough Arguement!</span><br><span class="line">  exit;</span><br><span class="line">fi</span><br><span class="line">#2. 遍历集群所有机器</span><br><span class="line">for host in hadoop102 hadoop103 hadoop104</span><br><span class="line">do</span><br><span class="line">  echo ====================  $host  ====================</span><br><span class="line">  #3. 遍历所有目录，挨个发送</span><br><span class="line">  for file in $@</span><br><span class="line">  do</span><br><span class="line">    #4 判断文件是否存在</span><br><span class="line">    if [ -e $file ]</span><br><span class="line">    then</span><br><span class="line">      #5. 获取父目录</span><br><span class="line">      pdir=$(cd -P $(dirname $file); pwd)</span><br><span class="line">      #6. 获取当前文件的名称</span><br><span class="line">      fname=$(basename $file)</span><br><span class="line">      ssh $host &quot;mkdir -p $pdir&quot;</span><br><span class="line">      rsync -av $pdir/$fname $host:$pdir</span><br><span class="line">    else</span><br><span class="line">      echo $file does not exists!</span><br><span class="line">    fi</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>2.修改脚本xsync具有执行权限</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x xsync</span><br></pre></td></tr></table></figure><h2 id="6-环境变量"><a href="#6-环境变量" class="headerlink" title="6 环境变量"></a>6 环境变量</h2><p><a href="https://blog.csdn.net/white_idiot/article/details/78253004">https://blog.csdn.net/white_idiot/article/details/78253004</a></p><h2 id="7-linux根目录下各个文件夹的作用"><a href="#7-linux根目录下各个文件夹的作用" class="headerlink" title="7 linux根目录下各个文件夹的作用"></a>7 linux根目录下各个文件夹的作用</h2><p><a href="https://blog.51cto.com/u_14233078/2443062">https://blog.51cto.com/u_14233078/2443062</a></p><h2 id="8-如何让你的脚本可以在任意地方都可执行？"><a href="#8-如何让你的脚本可以在任意地方都可执行？" class="headerlink" title="8 如何让你的脚本可以在任意地方都可执行？"></a>8 如何让你的脚本可以在任意地方都可执行？</h2><p><a href="https://www.cnblogs.com/yychuyu/p/12918957.html">https://www.cnblogs.com/yychuyu/p/12918957.html</a></p><h2 id="9-Linux服务器jps报process-information-unavailable"><a href="#9-Linux服务器jps报process-information-unavailable" class="headerlink" title="9 Linux服务器jps报process information unavailable"></a>9 Linux服务器jps报process information unavailable</h2><p><a href="https://blog.csdn.net/weixin_44803002/article/details/103332889">https://blog.csdn.net/weixin_44803002/article/details/103332889</a></p><p>cd /tmp</p><p>rm -rf /tmp/hsperfdata_* </p><h2 id="10-sudo命令"><a href="#10-sudo命令" class="headerlink" title="10 sudo命令"></a>10 sudo命令</h2><p><img src="/2022/02/07/linun-common-command/1.JPG" alt></p><p>使用hive用户启动hiveserver2</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop102 ~]# sudo -i -u hive hiveserver2</span><br></pre></td></tr></table></figure><h2 id="11-jps"><a href="#11-jps" class="headerlink" title="11 jps"></a>11 jps</h2><p><a href="https://blog.csdn.net/wangzhongshun/article/details/112546027">https://blog.csdn.net/wangzhongshun/article/details/112546027</a></p><p>不过jps有个缺点是只能显示当前用户的进程id，要显示其他用户的还只能用linux的ps命令</p><h2 id="12-scp"><a href="#12-scp" class="headerlink" title="12 scp"></a>12 scp</h2><p> scp -r root@hosts :  addr  ./</p><h2 id="13-tar"><a href="#13-tar" class="headerlink" title="13 tar"></a>13 tar</h2><p><a href="https://www.cnblogs.com/w54255787/p/10175202.html">https://www.cnblogs.com/w54255787/p/10175202.html</a></p><p>tar -cvf models.tar models</p><h2 id="14-kill-9-sparksubmit-无效"><a href="#14-kill-9-sparksubmit-无效" class="headerlink" title="14 kill -9 sparksubmit 无效"></a>14 kill -9 sparksubmit 无效</h2><p><a href="https://www.codetd.com/article/4229439">https://www.codetd.com/article/4229439</a></p><p>重启</p><h2 id="15-创建用户和用户组"><a href="#15-创建用户和用户组" class="headerlink" title="15 创建用户和用户组"></a>15 创建用户和用户组</h2><p><a href="https://www.jianshu.com/p/1e3fcfc8e3ef">https://www.jianshu.com/p/1e3fcfc8e3ef</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常见操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop in Secure Mode</title>
      <link href="/2022/02/06/hadoop-security/"/>
      <url>/2022/02/06/hadoop-security/</url>
      
        <content type="html"><![CDATA[<p><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#:~:text=When%20Hadoop%20is%20configured%20to,or%20%2Fetc%2Fhosts%20files">https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SecureMode.html#:~:text=When%20Hadoop%20is%20configured%20to,or%20%2Fetc%2Fhosts%20files</a>.</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>This document describes how to configure authentication for Hadoop in secure mode.</p><p>By default Hadoop runs in <strong>non-secure mode in which no actual authentication </strong>is required.By configuring Hadoop runs in <strong>secure mode, each user and service needs to be authenticated by Kerberos</strong> in order to use Hadoop services.</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop in Secure Mode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop SafeMode</title>
      <link href="/2022/02/06/hadoop-safe-mode/"/>
      <url>/2022/02/06/hadoop-safe-mode/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/bingduanlbd/article/details/51900512">https://blog.csdn.net/bingduanlbd/article/details/51900512</a></p><p><a href="https://developer.aliyun.com/article/566059#">https://developer.aliyun.com/article/566059#</a></p><p><a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode">https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Safemode</a></p><p>During start up the NameNode loads the file system state from the fsimage and the edits log file. It then waits for DataNodes to report their blocks so that it does not prematurely start replicating the blocks though enough replicas already exist in the cluster. During this time NameNode stays in Safemode. Safemode for the NameNode is essentially a read-only mode for the HDFS cluster, where it does not allow any modifications to file system or blocks. Normally the NameNode leaves Safemode automatically after the DataNodes have reported that most file system blocks are available. If required, HDFS could be placed in Safemode explicitly using <code>bin/hadoop dfsadmin -safemode</code> command. NameNode front page shows whether Safemode is on or off. A more detailed description and configuration is maintained as JavaDoc for <code>setSafeMode()</code>.</p><p>安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，当整个系统达到安全标准时，HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求）</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop安全模式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据安全</title>
      <link href="/2022/02/06/data-safety/"/>
      <url>/2022/02/06/data-safety/</url>
      
        <content type="html"><![CDATA[<p>分为两步：</p><p>​    1.用户认证 </p><p>​        Kerberos</p><p>​    2.权限管理    </p><p>​        ranger</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数据安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据安全 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kerberos</title>
      <link href="/2022/02/06/Kerberos/"/>
      <url>/2022/02/06/Kerberos/</url>
      
        <content type="html"><![CDATA[<h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><p>Kerberos是一种计算机网络认证协议，用来在非安全网络中，对个人通信以安全的手段进行<strong>身份认证</strong>。这个词又指麻省理工学院为这个协议开发的一套计算机软件。软件设计上采用客户端/服务器结构，并且能够进行相互认证，即客户端和服务器端均可对对方进行身份认证。可以用于防止窃听、防止重放攻击、保护数据完整性等场合，是一种应用对称密钥体制进行密钥管理的系统。</p><p>1）KDC（Key Distribute Center）：密钥分发中心，负责存储用户信息，管理发放票据。</p><p>2）Realm：Kerberos所管理的一个领域或范围。</p><p>3）Rrincipal：可以理解为Kerberos中保存的一个账号，其格式通常如下：primary<strong>/</strong>instance@realm</p><p>4）keytab：密钥文件。</p><p>有个疑问 ，对谁认证？是对不同用户吗（root，user1，user2）？</p><h2 id="2-认证原理"><a href="#2-认证原理" class="headerlink" title="2.认证原理"></a>2.认证原理</h2><p><img src="/2022/02/06/Kerberos/1.JPG" alt></p><p><a href="https://cloud.tencent.com/developer/article/1496451">https://cloud.tencent.com/developer/article/1496451</a></p><p><a href="https://blog.csdn.net/jewes/article/details/20792021">https://blog.csdn.net/jewes/article/details/20792021</a></p><h2 id="3-基本操作"><a href="#3-基本操作" class="headerlink" title="3.基本操作"></a>3.基本操作</h2><p><a href="https://blog.csdn.net/Happy_Sunshine_Boy/article/details/102801386">https://blog.csdn.net/Happy_Sunshine_Boy/article/details/102801386</a></p><p>1 创建管理员用户</p><p><img src="/2022/02/06/Kerberos/5.JPG" alt></p><p>2  注册</p><p><img src="/2022/02/06/Kerberos/6.JPG" alt></p><p>3 认证</p><p><img src="/2022/02/06/Kerberos/2.JPG" alt></p><p><img src="/2022/02/06/Kerberos/3.JPG" alt></p><p><img src="/2022/02/06/Kerberos/4.JPG" alt></p><h2 id="4-HADOOP"><a href="#4-HADOOP" class="headerlink" title="4.HADOOP"></a>4.HADOOP</h2><p><strong>配置</strong></p><p><a href="https://www.cnblogs.com/yjt1993/p/11769515.html">https://www.cnblogs.com/yjt1993/p/11769515.html</a></p><p><a href="https://makeling.github.io/bigdata/39395030.html">https://makeling.github.io/bigdata/39395030.html</a></p><p><strong>访问HDFS集群文件</strong></p><ol><li><p>Shell命令</p><p>kinit admin/admin</p><p>klist</p></li><li><p>web页面</p><p>1.安装Kerberos客户端</p><p>2.配置火狐浏览器</p><p>3.认证</p></li></ol><h2 id="5-HIVE"><a href="#5-HIVE" class="headerlink" title="5.HIVE"></a>5.HIVE</h2><p><strong>配置</strong></p><p><a href="https://zhuanlan.zhihu.com/p/137424234">https://zhuanlan.zhihu.com/p/137424234</a></p><p><strong>客服端访问</strong></p><ol><li><p>beeline</p><p>0.首先需使用hive用户启动hiveserver2</p><p>[root@hadoop102 ~]# sudo -i -u hive hiveserver2</p><p>1.认证，执行以下命令，并按照提示输入密码</p><p>[atguigu@hadoop102 ~]$ kinit atguigu</p><p>2.使用beeline客户端连接hiveserver2</p><p>[atguigu@hadoop102 ~]$ beeline</p><p>3.使用如下url进行连接</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!connect jdbc:hive2://hadoop102:10000/;principal=hive/hadoop102@EXAMPLE.COM</span><br></pre></td></tr></table></figure></li><li><p>DataGrip客户端</p><p><a href="https://blog.csdn.net/github_39319229/article/details/112692897">https://blog.csdn.net/github_39319229/article/details/112692897</a></p><p>经常连接不稳定，连接失败可以尝试重启DataGrip</p></li></ol><h2 id="6-数仓"><a href="#6-数仓" class="headerlink" title="6.数仓"></a>6.数仓</h2><p>此处统一将数仓的全部数据资源的所有者设为hive用户，全流程的每步操作均认证为hive用户。</p><h2 id="7-即席查询"><a href="#7-即席查询" class="headerlink" title="7.即席查询"></a>7.即席查询</h2><h2 id="8-spark"><a href="#8-spark" class="headerlink" title="8.spark"></a>8.spark</h2><p><a href="https://www.cnblogs.com/bainianminguo/p/12639887.html">https://www.cnblogs.com/bainianminguo/p/12639887.html</a></p><h2 id="9-hbase"><a href="#9-hbase" class="headerlink" title="9 hbase"></a>9 hbase</h2><p>1 hbase shell kerberos认证错误</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root:kinit atguigu</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/jewes/article/details/20792021">https://blog.csdn.net/jewes/article/details/20792021</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kerberos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kerberos </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集群监控</title>
      <link href="/2022/02/05/monitor-pcs/"/>
      <url>/2022/02/05/monitor-pcs/</url>
      
        <content type="html"><![CDATA[<p>常用运维监控工具详细对比</p><p><a href="https://www.leoheng.com/2021/08/10/%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94/">https://www.leoheng.com/2021/08/10/%E5%B8%B8%E7%94%A8%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7%E5%B7%A5%E5%85%B7%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94/</a></p><p>Zabbix+Grafana</p><p><a href="https://www.cnblogs.com/wushuaishuai/p/10852355.html">https://www.cnblogs.com/wushuaishuai/p/10852355.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 集群监控 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集群监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Grafana</title>
      <link href="/2022/02/05/Grafana/"/>
      <url>/2022/02/05/Grafana/</url>
      
        <content type="html"><![CDATA[<p>Operational dashboards for your data here, there, or anywhere</p><p><em>Grafana</em>是一款用Go语言开发的开源数据可视化工具，可以做数据监控和数据统计，带有告警功能。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Grafana </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Grafana </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zabbix</title>
      <link href="/2022/02/05/Zabbix/"/>
      <url>/2022/02/05/Zabbix/</url>
      
        <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Zabbix是一款能够监控各种网络参数以及服务器健康性和完整性的软件。Zabbix使用灵活的通知机制，允许用户为几乎任何事件配置基于邮件的告警。这样可以快速反馈服务器的问题。基于已存储的数据，Zabbix提供了出色的报告和数据可视化功能。</p><h2 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h2><p><img src="/2022/02/05/Zabbix/1.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Zabbix </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zabbix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase</title>
      <link href="/2022/02/05/hbase/"/>
      <url>/2022/02/05/hbase/</url>
      
        <content type="html"><![CDATA[<h2 id="0-简介"><a href="#0-简介" class="headerlink" title="0 简介"></a>0 简介</h2><p>Apache HBase is the Hadoop database, a distributed, scalable, big data store. HBase is a type of “NoSQL” database.</p><p>HBase是一种构建在<strong>HDFS</strong>之上的<strong>分布式、面向列</strong>的存储系统。</p><p><strong>Hadoop已经有了HDFS和MapReduce，为什么需要HBase</strong></p><p>1 Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于HadoopMapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求。</p><p>2 传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题（分库分表也不能很好解决）。传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间。</p><p><strong>HBase与传统的关系数据库的区别</strong></p><p>1、数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，<strong>它把数据存储为未经解释的字符串。</strong></p><p>2、数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。<strong>HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等</strong>，因为HBase在设计上就避免了复杂的表和表之间的关系。</p><p>3、存储模式：关系数据库是基于行模式存储的。HBase是<strong>基于列存储的</strong>，每个列族都由几个文件保存，不同列族的文件是分离的。</p><p>4、数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。<strong>HBase只有一个索引——行键</strong>，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来。</p><p>5、数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。<strong>而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留。</strong></p><p>6、可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的<strong>水平扩展</strong>而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩。</p><h2 id="1-配置"><a href="#1-配置" class="headerlink" title="1 配置"></a>1 配置</h2><p><a href="https://www.cnblogs.com/frankdeng/p/9310191.html">https://www.cnblogs.com/frankdeng/p/9310191.html</a></p><p>启动成功jps后可以看到hmaster ，hregionservice</p><h2 id="2-hbase-shell"><a href="#2-hbase-shell" class="headerlink" title="2 hbase shell"></a>2 hbase shell</h2><p>不支持sql，对表操作需要使用hbase shell命令或者hbase api</p><h2 id="3-Phoenix"><a href="#3-Phoenix" class="headerlink" title="3 Phoenix"></a>3 Phoenix</h2><p>在hbase上构建SQL层，使得hbase 能够使用标准SQL管理数据，Phoenix中的sql语句还是有些不同的</p><h2 id="4-问题"><a href="#4-问题" class="headerlink" title="4 问题"></a>4 问题</h2><p>1 org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet</p><p><a href="https://cloud.tencent.com/developer/article/1812290">https://cloud.tencent.com/developer/article/1812290</a></p><p>2 stop-hbase.sh关闭不了，一直处于等待状态</p><p><a href="https://blog.csdn.net/weixin_45462732/article/details/106909501">https://blog.csdn.net/weixin_45462732/article/details/106909501</a></p><p>3 hregionservice启动就挂了</p><p>看日志</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/wendyw/p/12691971.html#_label3">https://www.cnblogs.com/wendyw/p/12691971.html#_label3</a></p><p><a href="https://juejin.cn/post/6844903777347043336">https://juejin.cn/post/6844903777347043336</a></p><p><a href="https://www.jianshu.com/p/53864dc3f7b4">https://www.jianshu.com/p/53864dc3f7b4</a></p><p><a href="https://www.cnblogs.com/frankdeng/p/9310191.html">https://www.cnblogs.com/frankdeng/p/9310191.html</a></p><p><a href="https://blog.csdn.net/weixin_45462732/article/details/106909501">https://blog.csdn.net/weixin_45462732/article/details/106909501</a></p><p>中文文档 <a href="http://hbase.org.cn/docs/166.html#regionserver.arch">http://hbase.org.cn/docs/166.html#regionserver.arch</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 非关系型 </category>
          
          <category> HBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>即席查询(Ad hoc)</title>
      <link href="/2022/02/05/Ad-hoc/"/>
      <url>/2022/02/05/Ad-hoc/</url>
      
        <content type="html"><![CDATA[<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>即席查询（Ad Hoc）是用户根据自己的需求，灵活的选择查询条件，系统能够根据用户的选择生成相应的统计报表。即席查询与普通应用查询最大的不同是普通的应用查询是定制开发的，而即席查询是由用户自定义查询条件的。</p><p>举例说明</p><p>以电商的数仓分析项目为例，有一些应用侧/业务侧的分析指标：每日活跃用户数（日活），每日留存用户数（留存），新注册用户有多少下了单（转换率），因为计算方法固定，变化的是每天的数据，因此这些指标的查询/计算SQL是提前写好的，到店被调度（Azkaban）执行即可；</p><p>但有一些指标或者临时增加的指标、临时增加的一些分析需求，是无法预知其计算逻辑的，所以要现写查询SQL，并且希望能很快拿到查询/计算结果，这就是即席查询</p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>Kylin、druid、presto、impala</p><p><a href="https://zhuanlan.zhihu.com/p/266695601">https://zhuanlan.zhihu.com/p/266695601</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 即席查询 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 即席查询 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kylin</title>
      <link href="/2022/02/05/Kylin/"/>
      <url>/2022/02/05/Kylin/</url>
      
        <content type="html"><![CDATA[<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="/2022/02/05/Kylin/1.JPG" alt></p><p>Apache Kylin是一个开源的分布式分析引擎，提供Hadoop/Spark之上的SQL查询接口及多维分析（OLAP）能力以支持超大规模数据，最初由eBay Inc开发并贡献至开源社区。它能在亚秒内查询巨大的Hive表。</p><p>1）REST Server</p><p>REST Server是一套面向应用程序开发的入口点，旨在实现针对Kylin平台的应用开发工作。 此类应用程序可以提供查询、获取结果、触发cube构建任务、获取元数据以及获取用户权限等等。另外可以通过Restful接口实现SQL查询。</p><p>2）查询引擎（Query Engine）</p><p>当cube准备就绪后，查询引擎就能够获取并解析用户查询。它随后会与系统中的其它组件进行交互，从而向用户返回对应的结果。 </p><p>3）路由器（Routing）</p><p>在最初设计时曾考虑过将Kylin不能执行的查询引导去Hive中继续执行，但在实践后发现Hive与Kylin的速度差异过大，导致用户无法对查询的速度有一致的期望，很可能大多数查询几秒内就返回结果了，而有些查询则要等几分钟到几十分钟，因此体验非常糟糕。最后这个路由功能在发行版中默认关闭。</p><p>4）元数据管理工具（Metadata）</p><p>Kylin是一款元数据驱动型应用程序。元数据管理工具是一大关键性组件，用于对保存在Kylin当中的所有元数据进行管理，其中包括最为重要的cube元数据。其它全部组件的正常运作都需以元数据管理工具为基础。 Kylin的元数据存储在hbase中。 </p><p>5）任务引擎（Cube Build Engine）</p><p>这套引擎的设计目的在于处理所有离线任务，其中包括shell脚本、Java API以及Map Reduce任务等等。任务引擎对Kylin当中的全部任务加以管理与协调，从而确保每一项任务都能得到切实执行并解决其间出现的故障。</p><h2 id="Kylin-Cube构建原理，构建优化"><a href="#Kylin-Cube构建原理，构建优化" class="headerlink" title="Kylin Cube构建原理，构建优化"></a>Kylin Cube构建原理，构建优化</h2><p><a href="https://jishuin.proginn.com/p/763bfbd2bb9c">https://jishuin.proginn.com/p/763bfbd2bb9c</a></p><h2 id="BI工具集成"><a href="#BI工具集成" class="headerlink" title="BI工具集成"></a>BI工具集成</h2><p>可以与Kylin结合使用的可视化工具很多，例如：</p><p>ODBC：与Tableau、Excel、PowerBI等工具集成</p><p>JDBC：与Saiku、BIRT等Java工具集成</p><p>RestAPI：与JavaScript、Web网页集成</p><p>Kylin开发团队还贡献了Zepplin的插件，也可以使用Zepplin来访问Kylin服务。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kylin </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presto</title>
      <link href="/2022/02/05/Presto/"/>
      <url>/2022/02/05/Presto/</url>
      
        <content type="html"><![CDATA[<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><img src="/2022/02/05/Presto/1.JPG" alt></p><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p><img src="/2022/02/05/Presto/2.JPG" alt></p><h2 id="Presto优缺点"><a href="#Presto优缺点" class="headerlink" title="Presto优缺点"></a>Presto优缺点</h2><p><img src="/2022/02/05/Presto/3.JPG" alt></p><h2 id="Presto、Impala性能比较"><a href="#Presto、Impala性能比较" class="headerlink" title="Presto、Impala性能比较"></a>Presto、Impala性能比较</h2><p><a href="https://blog.csdn.net/u012551524/article/details/79124532">https://blog.csdn.net/u012551524/article/details/79124532</a></p><p>测试结论：Impala性能稍领先于Presto，但是Presto在数据源支持上非常丰富，包括Hive、图数据库、传统关系型数据库、Redis等。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kylin </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Presto </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>superset</title>
      <link href="/2022/02/05/superset/"/>
      <url>/2022/02/05/superset/</url>
      
        <content type="html"><![CDATA[<h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>Apache Superset是一个开源的、现代的、轻量级BI（Business Intelligence）分析工具，能够对接多种数据源、拥有丰富的图表展示形式、支持自定义仪表盘，且拥有友好的用户界面，十分易用。</p><p>由于Superset能够对接常用的大数据分析工具，如Hive、Kylin、Druid等，且支持自定义仪表盘，故可作为数仓的可视化工具。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> superset </category>
          
      </categories>
      
      
        <tags>
            
            <tag> superset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Azkaban</title>
      <link href="/2022/02/04/Azkaban/"/>
      <url>/2022/02/04/Azkaban/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/wtzhm/article/details/89220508">https://blog.csdn.net/wtzhm/article/details/89220508</a></p><h2 id="1-为什么需要工作流调度系统"><a href="#1-为什么需要工作流调度系统" class="headerlink" title="1 为什么需要工作流调度系统"></a>1 为什么需要工作流调度系统</h2><p>1）一个完整的数据分析系统通常都是由大量任务单元组成：Shell脚本程序，Java程序，MapReduce程序、Hive脚本等</p><p>2）各任务单元之间存在时间先后及前后依赖关系</p><p>3）为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行</p><p><img src="/2022/02/04/Azkaban/1.JPG" alt></p><h2 id="2-常见工作流调度系统"><a href="#2-常见工作流调度系统" class="headerlink" title="2 常见工作流调度系统"></a>2 常见工作流调度系统</h2><p>1）简单的任务调度</p><p>直接使用Linux的Crontab来定义；</p><p>2）复杂的任务调度</p><p>开发调度平台或使用现成的开源调度系统，比如Ooize、Azkaban、 Airflow、DolphinScheduler等。</p><p>3）Azkaban</p><p>Azkaban is a batch workflow job scheduler created at LinkedIn to run Hadoop jobs. Azkaban resolves the ordering through job dependencies and provides an easy to use web user interface to maintain and track your workflows.</p><p>Azkaban是一个开源的任务调度系统，用于负责任务的调度运行（如数据仓库调度），用以替代linux中的crontab。</p><p>和Oozie对比</p><p>总体来说，Ooize相比Azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器Azkaban是很不错的候选对象。</p><h2 id="3-使用"><a href="#3-使用" class="headerlink" title="3 使用"></a>3 使用</h2><p><img src="/2022/02/04/Azkaban/2.JPG" alt></p><h3 id="1-使用流程"><a href="#1-使用流程" class="headerlink" title="1 使用流程"></a>1 使用流程</h3><p>1.数据准备</p><p>2.编写Azkaban工作流程配置文件</p><p>​    a.编写azkaban.project</p><p>​    b.编写gmall.flow文件</p><h3 id="2-多Executor模式下注意事项"><a href="#2-多Executor模式下注意事项" class="headerlink" title="2 多Executor模式下注意事项"></a>2 多Executor模式下注意事项</h3><p>方案一：指定特定的Executor（hadoop102）去执行任务。</p><p>​    a.在MySQL中azkaban数据库executors表中，查询hadoop102上的Executor的id。</p><p>​    b.在执行工作流程时加入useExecutor属性</p><p>方案二：在Executor所在所有节点部署任务所需脚本和应用。</p><p>推荐使用方案二</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Azkaban </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Azkaban </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离线数仓搭建例子(电商为例)</title>
      <link href="/2022/02/03/build-datawarehouse/"/>
      <url>/2022/02/03/build-datawarehouse/</url>
      
        <content type="html"><![CDATA[<h2 id="0-架构"><a href="#0-架构" class="headerlink" title="0 架构"></a>0 架构</h2><p><img src="/2022/02/03/build-datawarehouse/121.JPG" alt></p><h2 id="1-数据来源"><a href="#1-数据来源" class="headerlink" title="1.数据来源"></a>1.数据来源</h2><h3 id="1-1-用户行为数据"><a href="#1-1-用户行为数据" class="headerlink" title="1.1 用户行为数据"></a>1.1 用户行为数据</h3><p>用户在使用产品过程中，通过埋点收集与客户端产品交互过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。用户行为数据通常存储在日志文件中。</p><p>我们的日志结构大致可分为两类，<strong>一是普通页面埋点日志，二是启动日志。</strong></p><p>普通页面日志结构如下，每条日志包含了，当前页面的页面信息，所有事件（动作）、所有曝光信息以及错误信息。除此之外，还包含了一系列公共信息，包括设备信息，地理位置，应用信息等，即下边的common字段。</p><p>启动日志结构相对简单，主要包含公共信息，启动信息和错误信息。</p><h3 id="1-2-业务数据"><a href="#1-2-业务数据" class="headerlink" title="1.2 业务数据"></a>1.2 业务数据</h3><p>就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据<strong>。</strong>业务数据通常存储在MySQL、Oracle等数据库中。</p><p>总共47张表，选了27张业务表</p><h2 id="2-数据采集"><a href="#2-数据采集" class="headerlink" title="2 数据采集"></a>2 数据采集</h2><p>1 用户行为数据</p><p>jar-》log（日志服务器）-》flume-》kafka-》flume-》hdfs</p><p>2 业务数据</p><p>jar-》mysql-》sqoop-》hdfs</p><h2 id="3-ODS"><a href="#3-ODS" class="headerlink" title="3.ODS"></a>3.ODS</h2><p>hdfs（ori）-》hdfs（ods）</p><h4 id="1-用户行为数据"><a href="#1-用户行为数据" class="headerlink" title="1 用户行为数据"></a>1 用户行为数据</h4><p>1张表，topic_log -&gt; ods_log</p><p>a.建表</p><p>就一个字段”line“</p><p>b 分区</p><p>首日，每日都是全量</p><p>c .数据装载</p><h4 id="2-业务数据"><a href="#2-业务数据" class="headerlink" title="2 业务数据"></a>2 业务数据</h4><p>27张表</p><p>0 整体</p><p>​    b  同步</p><p><img src="/2022/02/03/build-datawarehouse/13.JPG" alt></p><p>​    c .数据装载</p><p><img src="/2022/02/03/build-datawarehouse/14.JPG" alt></p><p>1.活动信息表</p><p>​    activity_info -&gt; ods_activity_info</p><p>​    a  建表</p><p>​        少了个“activity_desc”，多了”dt”</p><p><img src="/2022/02/03/build-datawarehouse/7.JPG" alt></p><p><img src="/2022/02/03/build-datawarehouse/6.JPG" alt></p><p>​    b  同步</p><p>​        首日，每日都是全量</p><p>​    c .数据装载</p><h2 id="4-DIM"><a href="#4-DIM" class="headerlink" title="4.DIM"></a>4.DIM</h2><p>hdfs（ods）-》hdfs（dim）</p><p>构建6张维度表</p><p>1 商品维度表</p><p>​    a 建表</p><p>​    b 分区</p><p>​        首日，每日都是全量</p><p>​    c 数据装载</p><p>2 优惠券维度表</p><p>3 活动维度表</p><p>4 地区维度表</p><p>​    b 分区</p><p>​        地区维度表数据相对稳定，变化概率较低，故无需每日装载，首日全量</p><p>5 时间维度表</p><p>​    b 分区</p><p>​        通常情况下，时间维度表的数据并不是来自于业务系统，而是手动写入，并且时间维度表数据具有可预见性，无须每日导入，一般首日可一次性导入一年的数据</p><p>​    c 数据装载</p><p>​            1）创建临时表tmp_dim_date_info</p><p>​            2）将数据文件上传到HFDS上临时表指定路径/warehouse/gmall/tmp/tmp_dim_date_info/</p><p>​            3）执行以下语句将其导入时间维度表</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table dim_date_info select * from tmp_dim_date_info;</span><br></pre></td></tr></table></figure><p>6 用户维度表</p><p>​    b 分区</p><p>​        拉链表</p><p><a href="https://cloud.tencent.com/developer/article/1752848#">https://cloud.tencent.com/developer/article/1752848#</a></p><p><img src="/2022/02/03/build-datawarehouse/11.JPG" alt></p><p>​    c 数据装载</p><p><img src="/2022/02/03/build-datawarehouse/12.JPG" alt></p><h2 id="5-DWD"><a href="#5-DWD" class="headerlink" title="5.DWD"></a>5.DWD</h2><p>hdfs（ods）-》hdfs（dwd）</p><h4 id="1-用户行为日志"><a href="#1-用户行为日志" class="headerlink" title="1 用户行为日志"></a>1 用户行为日志</h4><p>0 日志数据拆解</p><p>ods_log由两部分构成，分别为页面日志和启动日志，拆解成5张表</p><p><img src="/2022/02/03/build-datawarehouse/1.JPG" alt></p><p>1 启动日志表</p><p>​    b 分区</p><p>​        首日，每日全量</p><p>​    c 数据装载</p><p><img src="/2022/02/03/build-datawarehouse/2.JPG" alt></p><p>2 页面日志表</p><p>3 动作日志表</p><p>4 曝光日志表</p><p>5 错误日志表</p><h4 id="2-业务数据-1"><a href="#2-业务数据-1" class="headerlink" title="2  业务数据"></a>2  业务数据</h4><p>1 评价事实表（事务型事实表）</p><p>​    b 分区</p><p><img src="/2022/02/03/build-datawarehouse/15.JPG" alt></p><p>​    c 数据装载</p><p><img src="/2022/02/03/build-datawarehouse/16.JPG" alt></p><p>​        首日，动态分区；每日，静态分区</p><p>2 订单明细事实表（事务型事实表）</p><p>3 退单事实表（事务型事实表）</p><p>4 加购事实表（周期型快照事实表）</p><p>​    b 分区</p><p><img src="/2022/02/03/build-datawarehouse/17.JPG" alt></p><p>​    c 数据加载</p><p><img src="/2022/02/03/build-datawarehouse/18.JPG" alt></p><p>5 收藏事实表（周期型快照事实表）</p><p>6 优惠券领用事实表（累积型快照事实表）</p><p>​    b 分区</p><p><img src="/2022/02/03/build-datawarehouse/19.JPG" alt></p><p>​    c 数据加载</p><p><img src="/2022/02/03/build-datawarehouse/20.JPG" alt></p><p>​        （1）首日</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table dwd_coupon_use partition(dt)</span><br><span class="line">select</span><br><span class="line">    id,</span><br><span class="line">    coupon_id,</span><br><span class="line">    user_id,</span><br><span class="line">    order_id,</span><br><span class="line">    coupon_status,</span><br><span class="line">    get_time,</span><br><span class="line">    using_time,</span><br><span class="line">    used_time,</span><br><span class="line">    expire_time,</span><br><span class="line">    coalesce(date_format(used_time,&#x27;yyyy-MM-dd&#x27;),date_format(expire_time,&#x27;yyyy-MM-dd&#x27;),&#x27;9999-99-99&#x27;)</span><br><span class="line">from ods_coupon_use</span><br><span class="line">where dt=&#x27;2020-06-14&#x27;;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​        （2）每日</p><p><img src="/2022/02/03/build-datawarehouse/21.JPG" alt></p><p>7 支付事实表（累积型快照事实表）</p><p>8 退款事实表（累积型快照事实表）</p><p>9 订单事实表（累积型快照事实表）</p><h2 id="6-DWS"><a href="#6-DWS" class="headerlink" title="6.DWS"></a>6.DWS</h2><p>hdfs（dwd）-》hdfs（dws）</p><p>0 整体</p><p>​    b 分区</p><p>​        <img src="/2022/02/03/build-datawarehouse/22.JPG" alt></p><p>​    c 数据装载</p><p>​    <img src="/2022/02/03/build-datawarehouse/23.JPG" alt></p><p>1 访客主题</p><p>2 用户主题</p><p>3 商品主题</p><p>4 优惠券主题</p><p>5 活动主题</p><p>6 地区主题</p><h2 id="7-DWT"><a href="#7-DWT" class="headerlink" title="7.DWT"></a>7.DWT</h2><p>hdfs（dws）-》hdfs（DWT）</p><p>0 整体</p><p>c 数据装载</p><p><img src="/2022/02/03/build-datawarehouse/24.JPG" alt></p><p>只保留当天和前一天的分区，过时的需要清理掉i</p><p>1 访客主题</p><p>2 用户主题</p><p>3 商品主题</p><p>4 优惠券主题</p><p>5 活动主题</p><p>6 地区主题</p><h2 id="8-ADS"><a href="#8-ADS" class="headerlink" title="8.ADS"></a>8.ADS</h2><p>hdfs（DWT）-》hdfs（ADS）-》mysql</p><h4 id="1-访客主题"><a href="#1-访客主题" class="headerlink" title="1 访客主题"></a>1 访客主题</h4><p>1 访客统计</p><p> a 建表</p><div class="table-container"><table><thead><tr><th>指标</th><th>说明</th><th>对应字段</th></tr></thead><tbody><tr><td>访客数</td><td>统计访问人数</td><td>uv_count</td></tr><tr><td>页面停留时长</td><td>统计所有页面访问记录总时长，以秒为单位</td><td>duration_sec</td></tr><tr><td>平均页面停留时长</td><td>统计每个会话平均停留时长，以秒为单位</td><td>avg_duration_sec</td></tr><tr><td>页面浏览总数</td><td>统计所有页面访问记录总数</td><td>page_count</td></tr><tr><td>平均页面浏览数</td><td>统计每个会话平均浏览页面数</td><td>avg_page_count</td></tr><tr><td>会话总数</td><td>统计会话总数</td><td>sv_count</td></tr><tr><td>跳出数</td><td>统计只浏览一个页面的会话个数</td><td>bounce_count</td></tr><tr><td>跳出率</td><td>只有一个页面的会话的比例</td><td>bounce_rate</td></tr></tbody></table></div><p>b 分区</p><p>c 数据装载</p><p>第一步：对所有页面访问记录进行会话的划分。</p><p>第二步：统计每个会话的浏览时长和浏览页面数。</p><p>第三步：统计上述各指标。</p><p>2  路径分析</p><p>用户访问路径的可视化通常使用桑基图</p><p><img src="/2022/02/03/build-datawarehouse/4.jpg" alt></p><h4 id="2-用户主题"><a href="#2-用户主题" class="headerlink" title="2 用户主题"></a>2 用户主题</h4><h4 id="3-商品主题"><a href="#3-商品主题" class="headerlink" title="3 商品主题"></a>3 商品主题</h4><h4 id="4-订单主题"><a href="#4-订单主题" class="headerlink" title="4 订单主题"></a>4 订单主题</h4><h4 id="5-优惠券主题"><a href="#5-优惠券主题" class="headerlink" title="5 优惠券主题"></a>5 优惠券主题</h4><h4 id="6-活动主题"><a href="#6-活动主题" class="headerlink" title="6 活动主题"></a>6 活动主题</h4><h2 id="9-Azkaban全流程调度"><a href="#9-Azkaban全流程调度" class="headerlink" title="9.Azkaban全流程调度"></a>9.Azkaban全流程调度</h2><p><img src="/2022/02/03/build-datawarehouse/5.jpg" alt></p><p>就是将原来写的脚本文件串起来</p><p><img src="/2022/02/03/build-datawarehouse/8.JPG" alt></p><p><img src="/2022/02/03/build-datawarehouse/9.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 离线数仓搭建例子 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据工具</title>
      <link href="/2022/02/01/datawarehouse-develop-tool/"/>
      <url>/2022/02/01/datawarehouse-develop-tool/</url>
      
        <content type="html"><![CDATA[<h2 id="1-数据库管理工具"><a href="#1-数据库管理工具" class="headerlink" title="1 数据库管理工具"></a>1 数据库管理工具</h2><p><strong>Navicat</strong></p><p>破解</p><p><a href="https://cdmana.com/2022/114/202204240555321952.html">https://cdmana.com/2022/114/202204240555321952.html</a></p><p><strong>DataGrip</strong></p><p>需要用到JDBC协议连接到Hive，故需要启动HiveServer2。</p><p><em>DataGrip</em> 版是由JetBrains 公司（就是那个出品Intellij IDEA 的公司）推出的数据库管理软件。</p><h2 id="2-数据库设计工具"><a href="#2-数据库设计工具" class="headerlink" title="2 数据库设计工具"></a>2 数据库设计工具</h2><p>EZDML:来辅助我们梳理复杂的业务表关系，效果如下</p><p><img src="/2022/02/01/datawarehouse-develop-tool/11.jpg" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark on Hive &amp; Hive on Spark</title>
      <link href="/2022/02/01/sparkonhive/"/>
      <url>/2022/02/01/sparkonhive/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cloud.tencent.com/developer/article/1624245">https://cloud.tencent.com/developer/article/1624245</a></p><p><a href="https://blog.csdn.net/weixin_41290471/article/details/106203419">https://blog.csdn.net/weixin_41290471/article/details/106203419</a></p><p><a href="https://www.cnblogs.com/qingyunzong/p/8992664.html">https://www.cnblogs.com/qingyunzong/p/8992664.html</a></p><h2 id="1-Hive-on-Spark"><a href="#1-Hive-on-Spark" class="headerlink" title="1 Hive on Spark"></a>1 Hive on Spark</h2><p>就是将spark作为hive的计算引擎</p><p>Hive既作为存储元数据又负责SQL的解析优化，语法是HQL语法，执行引擎变成了Spark，Spark负责采用RDD执行。</p><h2 id="2-Spark-on-Hive"><a href="#2-Spark-on-Hive" class="headerlink" title="2 Spark on Hive"></a>2 Spark on Hive</h2><p>就是因为Spark自身没有元数据管理功能，所以使用Hive的Metastore服务作为元数据管理服务。计算由Spark执行。</p><p>Hive只作为存储元数据，Spark负责SQL解析优化，语法是Spark SQL语法，Spark负责采用RDD执行。</p><p>SparkSQL 的元数据的状态有两种：</p><p>1、in_memory,用完了元数据也就丢了</p><p>2、hive , 通过hive去保存的，也就是说，hive的元数据存在哪儿，它的元数据也就存在哪儿。换句话说，SparkSQL的数据仓库在建立在Hive之上实现的。我们要用SparkSQL去构建数据仓库的时候，必须依赖于Hive。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark on Hive &amp; Hive on Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive</title>
      <link href="/2022/01/31/hive/"/>
      <url>/2022/01/31/hive/</url>
      
        <content type="html"><![CDATA[<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>1.FAILED: SemanticException Failed to get a spark session: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to create Spark client for Spark session</p><p><a href="https://blog.csdn.net/qq_41504585/article/details/108064512">https://blog.csdn.net/qq_41504585/article/details/108064512</a></p><h2 id="启动metastore"><a href="#启动metastore" class="headerlink" title="启动metastore"></a>启动metastore</h2><p><a href="https://blog.csdn.net/u010670689/article/details/41576647">https://blog.csdn.net/u010670689/article/details/41576647</a></p><p>hive —service metastore 2&gt;&amp;1 &gt;&gt; /var/log.log &amp;</p><h2 id="启动hiveserver2"><a href="#启动hiveserver2" class="headerlink" title="启动hiveserver2"></a>启动hiveserver2</h2><p>hiveserver2</p><h2 id="连接方式"><a href="#连接方式" class="headerlink" title="连接方式"></a>连接方式</h2><p><a href="https://blog.csdn.net/qq_41851454/article/details/79833306">https://blog.csdn.net/qq_41851454/article/details/79833306</a></p><p><a href="https://www.shuzhiduo.com/A/RnJW4Z2r5q/">https://www.shuzhiduo.com/A/RnJW4Z2r5q/</a></p><p>1.cli</p><p>hive </p><p>2.beeline</p><p><a href="https://www.jianshu.com/p/97bbe79d88d2">https://www.jianshu.com/p/97bbe79d88d2</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hive </category>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sqoop</title>
      <link href="/2022/01/31/sqoop/"/>
      <url>/2022/01/31/sqoop/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.yangshuaibin.com/detail/388673">https://www.yangshuaibin.com/detail/388673</a></p><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>关系型数据库 &lt;&lt;——&gt;&gt; hdfs ，双向，用的mapreduce</p><p>sqoop的作用就是将关系型数据库中的某张表数据抽取到Hadoop的hdfs文件系统当中，底层运用的还是Map Reduce 。 它利用MapReduce加快数据传输速度，批处理方式进行数据传输。 也可以将HDFS上的文件数据导出到关系型数据库中的某张表。</p><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><p><a href="https://blog.csdn.net/qq_44665283/article/details/120709047">https://blog.csdn.net/qq_44665283/article/details/120709047</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> sqoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>各种表</title>
      <link href="/2022/01/30/table/"/>
      <url>/2022/01/30/table/</url>
      
        <content type="html"><![CDATA[<h2 id="1-分区表"><a href="#1-分区表" class="headerlink" title="1 分区表"></a>1 分区表</h2><p><a href="https://www.jianshu.com/p/1cdd3e3c5b3c">https://www.jianshu.com/p/1cdd3e3c5b3c</a></p><p><a href="https://www.jianshu.com/p/163f8375c0d6">https://www.jianshu.com/p/163f8375c0d6</a></p><p>1 mysql分区</p><p>RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区。</p><p>LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择。</p><p>HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL 中有效的、产生非负整数值的任何表达式。</p><p>KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL 服务器提供其自身的哈希函数。必须有一列或多列包含整数值。</p><p>2 Hive中分区表</p><p>分两类：静态分区、动态分区；</p><p>Hive中没有复杂的分区类型（List,Range,Hash）</p><h2 id="2-内部表-amp-外部表"><a href="#2-内部表-amp-外部表" class="headerlink" title="2 内部表&amp;外部表"></a>2 内部表&amp;外部表</h2><p><a href="https://www.cnblogs.com/qiaoyihang/p/6225151.html">https://www.cnblogs.com/qiaoyihang/p/6225151.html</a></p><p><a href="https://blog.csdn.net/qq_36743482/article/details/78393678">https://blog.csdn.net/qq_36743482/article/details/78393678</a></p><p>1 未被external修饰的是内部表（managed table），被external修饰的为外部表（external table）；</p><p>2 内部表数据由Hive自身管理，外部表数据由HDFS管理；</p><p>3 内部表数据存储的位置是hive.metastore.warehouse.dir（默认：/user/hive/warehouse），外部表数据的存储位置由自己制定（如果没有LOCATION，Hive将在HDFS上的/user/hive/warehouse文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）；<br>4 删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除；<br>5 对内部表的修改会将修改直接同步给元数据，而对外部表的表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;）</p><h2 id="3-临时表"><a href="#3-临时表" class="headerlink" title="3 临时表"></a>3 临时表</h2><p><a href="https://www.cnblogs.com/duanxz/p/3724120.html">https://www.cnblogs.com/duanxz/p/3724120.html</a></p><h2 id="4-字典表"><a href="#4-字典表" class="headerlink" title="4 字典表"></a>4 字典表</h2><p><a href="https://www.cnblogs.com/jpfss/p/10418873.html">https://www.cnblogs.com/jpfss/p/10418873.html</a></p><h2 id="5-全量表，增量表、快照表、流水表"><a href="#5-全量表，增量表、快照表、流水表" class="headerlink" title="5 全量表，增量表、快照表、流水表"></a>5 全量表，增量表、快照表、流水表</h2><p>全量表：所有数据的最新状态</p><p>增量表：新增数据</p><p>快照表：</p><p><img src="/2022/01/30/table/12.JPG" alt></p><p>流水表： 对于表的每一个修改都会记录，可以用于反映实际记录的变更。</p><h2 id="6-切片表"><a href="#6-切片表" class="headerlink" title="6 切片表"></a>6 切片表</h2><p>切片表：切片表根据基础表，往往只反映某一个维度的相应数据。其表结构与基础表结构相同，但数据往往只有某一维度，或者某一个事实条件的数据</p><h2 id="7-拉链表"><a href="#7-拉链表" class="headerlink" title="7 拉链表"></a>7 拉链表</h2><p>1 定义</p><p><img src="/2022/01/30/table/1.JPG" alt></p><p>2 应用场景</p><p><img src="/2022/01/30/table/2.JPG" alt></p><p>3 分区</p><p><img src="/2022/01/30/table/4.JPG" alt></p><p>4 数据加载</p><p><img src="/2022/01/30/table/5.JPG" alt></p><p>1）首日装载</p><p>拉链表首日装载，需要进行初始化操作，具体工作为将截止到初始化当日的全部历史用户导入一次性导入到拉链表中。目前的ods_user_info 表的第一个分区，即2020-06-14 分区中就是全部的历史用户，故将该分区数据进行一定处理后导入拉链表的9999-99-99 分区即可。</p><p>2）每日装载</p><p><img src="/2022/01/30/table/6.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据库 </category>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 各种表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flume</title>
      <link href="/2022/01/30/flume/"/>
      <url>/2022/01/30/flume/</url>
      
        <content type="html"><![CDATA[<h2 id="1-作用"><a href="#1-作用" class="headerlink" title="1.作用"></a>1.作用</h2><p>日志采集</p><p>Flume是流式日志采集工具，FLume提供对数据进行简单处理并且写到各种数据接收方（可定制）的能力，Flume提供从本地文件（spooling directory source）、实时日志（taildir、exec）、REST消息、Thift、Avro、Syslog、Kafka等数据源上收集数据的能力。</p><h2 id="2-Flume架构"><a href="#2-Flume架构" class="headerlink" title="2.Flume架构"></a>2.Flume架构</h2><p><a href="https://jiandansuifeng.blog.csdn.net/article/details/118926483">https://jiandansuifeng.blog.csdn.net/article/details/118926483</a></p><p><a href="https://www.jianshu.com/p/9a5c682b0551">https://www.jianshu.com/p/9a5c682b0551</a></p><p>三大核心组件：</p><ul><li>Source:数据源</li><li>Channel:临时存储数据的管道</li><li>Sink: 目的地</li></ul><h2 id="3-拦截器-interceptor"><a href="#3-拦截器-interceptor" class="headerlink" title="3.拦截器(interceptor)"></a>3.拦截器(interceptor)</h2><p><a href="https://blog.51cto.com/u_15241496/2869403">https://blog.51cto.com/u_15241496/2869403</a></p><p>拦截器是简单的插件式组件，设置在source和channel之间。source接收到的事件event，在写入channel之前，拦截器都可以进行转换或者删除这些事件。每个拦截器只处理同一个source接收到的事件。可以自定义拦截器。</p><p>1 jar包</p><p>2 将打包的jar放到XXX/flume/lib</p><p>3 配置文件</p><p>/opt/module/flume/conf</p><p>vim file-flume-kafka.conf</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type =</span><br><span class="line">com.atguigu.flume.interceptor.ETLInterceptor$Builder</span><br></pre></td></tr></table></figure><h2 id="4-为什么要集成Flume和Kafka"><a href="#4-为什么要集成Flume和Kafka" class="headerlink" title="4.为什么要集成Flume和Kafka"></a>4.为什么要集成Flume和Kafka</h2><p><a href="https://blog.csdn.net/qq_37466640/article/details/103425555">https://blog.csdn.net/qq_37466640/article/details/103425555</a></p><h2 id="5-例子"><a href="#5-例子" class="headerlink" title="5.例子"></a>5.例子</h2><p><img src="/2022/01/30/flume/33.JPG" alt></p><h3 id="4-1-采集日志"><a href="#4-1-采集日志" class="headerlink" title="4.1 采集日志"></a>4.1 采集日志</h3><p><img src="/2022/01/30/flume/11.JPG" alt></p><p><strong>1）Source</strong></p><p>（1）Taildir Source相比Exec Source、Spooling Directory Source的优势</p><p>TailDir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。不会丢数据，但是有可能会导致数据重复。</p><p>Exec Source可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</p><p>Spooling Directory Source监控目录，支持断点续传。</p><p>（2）batchSize大小如何设置？</p><p>答：Event 1K左右时，500-1000合适（默认为100）</p><p><strong>2）Channel</strong></p><p>采用Kafka Channel，省去了Sink，提高了效率。KafkaChannel数据存储在Kafka里面，所以数据是存储在磁盘中。</p><p>注意在Flume1.7以前，Kafka Channel很少有人使用，因为发现parseAsFlumeEvent这个配置起不了作用。也就是无论parseAsFlumeEvent配置为true还是false，都会转为Flume Event。这样的话，造成的结果是，会始终都把Flume的headers中的信息混合着内容一起写入Kafka的消息中，这显然不是我所需要的，我只是需要把内容写入即可。</p><p><strong>3）Sink</strong></p><p>省去了Sink，提高了效率</p><p><strong>4）Interceptor</strong></p><p>ETLInterceptor</p><h3 id="4-2-消费数据"><a href="#4-2-消费数据" class="headerlink" title="4.2  消费数据"></a>4.2  消费数据</h3><p><img src="/2022/01/30/flume/22.JPG" alt></p><p><strong>1）Source</strong></p><p><strong>2）Channel</strong></p><p>MemoryChannel传输数据速度更快，但因为数据保存在JVM的堆内存中，Agent进程挂掉会导致数据丢失，适用于对数据质量要求不高的需求。FileChannel传输速度相对于Memory慢，但数据安全保障高，Agent进程挂掉也可以从失败中恢复数据。</p><p>金融类公司、对钱要求非常准确的公司通常会选择FileChannel</p><p>传输的是普通日志信息（京东内部一天丢100万-200万条，这是非常正常的），通常选择MemoryChannel。</p><p><strong>3）Sink</strong></p><p>HDFS Sink</p><p><strong>4）Interceptor</strong></p><p>TimeStampInterceptor</p><p>flume时间拦截器</p><p>获取日志中的实际时间</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> flume </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka原理结构</title>
      <link href="/2022/01/30/kafka/"/>
      <url>/2022/01/30/kafka/</url>
      
        <content type="html"><![CDATA[<p>Apache Kafka® is an event streaming platform. What does that mean?</p><p>Kafka combines three key capabilities so you can implement  your use cases for event streaming end-to-end with a single battle-tested solution:</p><ol><li>To <strong>publish</strong> (write) and <strong>subscribe to</strong> (read) streams of events, including continuous import/export of your data from other systems.</li><li>To <strong>store</strong> streams of events durably and reliably for as long as you want.</li><li>To <strong>process</strong> streams of events as they occur or retrospectively.</li></ol><p>And all this functionality is provided in a distributed, highly scalable, elastic, fault-tolerant, and secure manner. Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.</p><p>详细原理见：<a href="https://blog.csdn.net/weixin_45366499/article/details/106943229">https://blog.csdn.net/weixin_45366499/article/details/106943229</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> Kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka原理结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper</title>
      <link href="/2022/01/30/zookeeper/"/>
      <url>/2022/01/30/zookeeper/</url>
      
        <content type="html"><![CDATA[<p>是一个分布式的，开放源码的<strong>分布式应用程序协调服务</strong>，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。</p><h2 id="简单操作"><a href="#简单操作" class="headerlink" title="简单操作"></a>简单操作</h2><p>1 查看状态</p><p>bin/zkServer.sh status</p><p>有leader follower </p><p>shell脚本编写：start，stop，status</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1 某个节点起不来</p><p><a href="https://blog.csdn.net/qq_48268603/article/details/117687875">https://blog.csdn.net/qq_48268603/article/details/117687875</a></p><p><a href="https://blog.csdn.net/u012453843/article/details/70878117">https://blog.csdn.net/u012453843/article/details/70878117</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://geek-docs.com/zookeeper/zookeeper-tutorial/zookeeper-profile.html#">https://geek-docs.com/zookeeper/zookeeper-tutorial/zookeeper-profile.html#</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> ZooKeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ZooKeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>克隆虚拟机修改静态IP不成功解决办法</title>
      <link href="/2022/01/29/ifconfig/"/>
      <url>/2022/01/29/ifconfig/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/nullnullago/article/details/122343454">https://blog.csdn.net/nullnullago/article/details/122343454</a></p><p><a href="https://blog.csdn.net/xiaozizai2015/article/details/88855915">https://blog.csdn.net/xiaozizai2015/article/details/88855915</a></p><p><a href="https://blog.csdn.net/Panda_813/article/details/104606990?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=2">https://blog.csdn.net/Panda_813/article/details/104606990?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1.pc_relevant_default&amp;utm_relevant_index=2</a></p><p><a href="https://blog.csdn.net/qq_38616559/article/details/104949693">https://blog.csdn.net/qq_38616559/article/details/104949693</a></p><p>注意一个坑：修改ip的时候，vim ens33文件产生冲突，以为修改成功，其实没有</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 克隆虚拟机修改静态IP不成功解决办法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop trick</title>
      <link href="/2022/01/28/hadoop-trick/"/>
      <url>/2022/01/28/hadoop-trick/</url>
      
        <content type="html"><![CDATA[<h2 id="1-集群数据均衡"><a href="#1-集群数据均衡" class="headerlink" title="1 集群数据均衡"></a>1 集群数据均衡</h2><p>1 节点间数据均衡</p><p>（1）开启数据均衡命令</p><p>start-balancer.sh -threshold 10</p><p>（2）停止数据均衡命令</p><p>stop-balancer.sh</p><p>2 磁盘间数据均衡</p><p>（1）生成均衡计划（我们只有一块磁盘，不会生成计划）</p><p>hdfs diskbalancer -plan hadoop103</p><p>（2）执行均衡计划</p><p>hdfs diskbalancer -execute hadoop103.plan.json</p><p>（3）查看当前均衡任务的执行情况</p><p>hdfs diskbalancer -query hadoop103</p><p>（4）取消均衡任务</p><p>hdfs diskbalancer -cancel hadoop103.plan.json</p><h2 id="2-数据压缩"><a href="#2-数据压缩" class="headerlink" title="2 数据压缩"></a>2 数据压缩</h2><p><a href="https://cloud.tencent.com/developer/article/1417401">https://cloud.tencent.com/developer/article/1417401</a></p><p>1 LZO压缩</p><p>LZO压缩文件的可切片特性依赖于其索引，故我们需要手动为LZO压缩文件创建索引。若无索引，则LZO文件的切片只有一个。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 bin]$ hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_log/dt=2020-06-14</span><br></pre></td></tr></table></figure><h2 id="3-Hadoop参数调优"><a href="#3-Hadoop参数调优" class="headerlink" title="3 Hadoop参数调优"></a>3 Hadoop参数调优</h2><p><a href="https://developer.aliyun.com/article/566013">https://developer.aliyun.com/article/566013</a></p><p>1）HDFS参数调优</p><p>2）YARN参数调优</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop trick </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop架构</title>
      <link href="/2022/01/28/hadoop-framewowk/"/>
      <url>/2022/01/28/hadoop-framewowk/</url>
      
        <content type="html"><![CDATA[<h2 id="1-总览"><a href="#1-总览" class="headerlink" title="1 总览"></a>1 总览</h2><p><a href="https://blog.csdn.net/wangxudongx/article/details/104079998">https://blog.csdn.net/wangxudongx/article/details/104079998</a></p><h2 id="2-MapReduce"><a href="#2-MapReduce" class="headerlink" title="2 MapReduce"></a>2 MapReduce</h2><p><a href="https://zhuanlan.zhihu.com/p/377204048">https://zhuanlan.zhihu.com/p/377204048</a></p><h2 id="3-hdfs"><a href="#3-hdfs" class="headerlink" title="3 hdfs"></a>3 hdfs</h2><h4 id="0-读写原理"><a href="#0-读写原理" class="headerlink" title="0 读写原理"></a>0 读写原理</h4><p><a href="https://blog.csdn.net/whdxjbw/article/details/81072207">https://blog.csdn.net/whdxjbw/article/details/81072207</a></p><h4 id="1-文件格式"><a href="#1-文件格式" class="headerlink" title="1 文件格式"></a>1 文件格式</h4><p><a href="https://www.cnblogs.com/wqbin/p/14635480.html">https://www.cnblogs.com/wqbin/p/14635480.html</a></p><p>Hadoop中的文件格式大致上分为面向行和面向列两类</p><p>面向行：同一行的数据存储在一起，即连续存储。SequenceFile,MapFile,Avro Datafile。采用这种方式，如果只需要访问行的一小部分数据，亦需要将整行读入内存，推迟序列化一定程度上可以缓解这个问题，但是从磁盘读取整行数据的开销却无法避免。面向行的存储适合于整行数据需要同时处理的情况。</p><p>面向列：整个文件被切割为若干列数据，每一列数据一起存储。Parquet , RCFile,ORCFile。面向列的格式使得读取数据时，可以跳过不需要的列，适合于只处于行的一小部分字段的情况。但是这种格式的读写需要更多的内存空间，因为需要缓存行在内存中（为了获取多行中的某一列）。同时不适合流式写入，因为一旦写入失败，当前文件无法恢复，而面向行的数据在写入失败时可以重新同步到最后一个同步点，所以Flume采用的是面向行的存储格式。</p><h4 id="2-上传文件"><a href="#2-上传文件" class="headerlink" title="2 上传文件"></a>2 上传文件</h4><p>1.页面</p><p>2.命令行</p><p><a href="https://blog.csdn.net/tandelin/article/details/89514784">https://blog.csdn.net/tandelin/article/details/89514784</a></p><h2 id="4-RPC"><a href="#4-RPC" class="headerlink" title="4 RPC"></a>4 RPC</h2><p>Remote Procdure Call，中文名：远程过程调用</p><p>它允许一台计算机程序远程调用另外一台计算机的子程序，而不用去关心底层的网络通信细节</p><p>Hadoop的进程间交互都是通过RPC来进行的，比如Namenode与Datanode</p><p><a href="https://www.cnblogs.com/SmallBird-Nest/p/11430330.html">https://www.cnblogs.com/SmallBird-Nest/p/11430330.html</a></p><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux</title>
      <link href="/2022/01/27/linux-comm/"/>
      <url>/2022/01/27/linux-comm/</url>
      
        <content type="html"><![CDATA[<p>root用户也是用户，就是权限高于普通用户</p><p>cd ~ ：回到当前用户目录</p><p>cd /：回到根目录</p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop部署方式</title>
      <link href="/2022/01/26/hadoop-persudo/"/>
      <url>/2022/01/26/hadoop-persudo/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/aohun0743/article/details/101702331">https://blog.csdn.net/aohun0743/article/details/101702331</a></p><p><a href="https://cloud.tencent.com/developer/article/1924241?from=article.detail.1336692">https://cloud.tencent.com/developer/article/1924241?from=article.detail.1336692</a></p><p><strong>本地模式</strong></p><p>单机运行，只是用来演示一下官方案例。生产环境不用。</p><p><strong>伪分布式模式</strong></p><p>也是单机运行，但是具备Hadoop集群的所有功能，一台服务器模拟一个分布式的环境。个别缺钱的公司用来测试，生产环境不用。</p><p><strong>完全分布式模式</strong></p><p>多台服务器组成分布式环境。生产环境使用。</p><p>搭建：</p><p><a href="https://blog.csdn.net/a1786742005/article/details/104104983">https://blog.csdn.net/a1786742005/article/details/104104983</a></p><p><strong>高可用完全分布式模式</strong></p><p>HA高可用是Hadoop2.x才开始引入的机制，是为了解决Hadoop的单点故障问题。主要有两种部署方式，一种是NFS（Network File System）方式，另外一种是QJM（Quorum Journal Manager）方式。用得较多的是QJM方式，稳定性更好。实际操作中，生产环境的Hadoop集群搭建一般都会做HA部署。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop部署方式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD，DataFrame，Dataset</title>
      <link href="/2022/01/25/spark-rdd/"/>
      <url>/2022/01/25/spark-rdd/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.knoldus.com/spark-rdd-vs-dataframes/">https://blog.knoldus.com/spark-rdd-vs-dataframes/</a></p><p><a href="https://blog.csdn.net/hellozhxy/article/details/82660610">https://blog.csdn.net/hellozhxy/article/details/82660610</a></p><p><a href="https://www.cnblogs.com/lestatzhang/p/10611320.html#Spark_16">https://www.cnblogs.com/lestatzhang/p/10611320.html#Spark_16</a></p><p><a href="https://www.jianshu.com/p/77811ae29fdd">https://www.jianshu.com/p/77811ae29fdd</a></p><p><a href="https://zhuanlan.zhihu.com/p/379578271">https://zhuanlan.zhihu.com/p/379578271</a></p><p><a href="https://spark.apache.org/docs/3.2.0/sql-programming-guide.html#content">https://spark.apache.org/docs/3.2.0/sql-programming-guide.html#content</a></p><p><img src="/2022/01/25/spark-rdd/1.png" alt></p><h2 id="1-DataFrame-和-RDDs-应该如何选择？"><a href="#1-DataFrame-和-RDDs-应该如何选择？" class="headerlink" title="1 DataFrame 和 RDDs 应该如何选择？"></a>1 DataFrame 和 RDDs 应该如何选择？</h2><p>DataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据</p><ul><li>如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。</li><li>如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs，</li></ul><h2 id="2-为什么出现Dataset？"><a href="#2-为什么出现Dataset？" class="headerlink" title="2 为什么出现Dataset？"></a>2 为什么出现Dataset？</h2><p>1.相比DataFrame，Dataset提供了编译时类型检查，对于分布式程序来讲，提交一次作业太费劲了（要编译、打包、上传、运行），到提交到集群运行时才发现错误，实在是想骂人，这也是引入Dataset的一个重要原因。</p><p>2.RDD转换DataFrame后不可逆，但RDD转换Dataset是可逆的（这也是Dataset产生的原因）</p><p>注意：</p><p>The Dataset API is available in <a href="https://spark.apache.org/docs/3.2.0/api/scala/org/apache/spark/sql/Dataset.html">Scala</a> and <a href="https://spark.apache.org/docs/3.2.0/api/java/index.html?org/apache/spark/sql/Dataset.html">Java</a>. Python does not have the support for the Dataset API. But due to Python’s dynamic nature, many of the benefits of the Dataset API are already available (i.e. you can access the field of a row by name naturally <code>row.columnName</code>).</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDD、DataFrame、DataSet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓建模</title>
      <link href="/2022/01/25/dimention-modeling/"/>
      <url>/2022/01/25/dimention-modeling/</url>
      
        <content type="html"><![CDATA[<p>关系建模和维度建模是两种数据仓库的建模技术。关系建模由Bill Inmon所倡导，维度建模由Ralph Kimball所倡导。目前主流为维度建模。</p><p><a href="https://zhuanlan.zhihu.com/p/362991213">https://zhuanlan.zhihu.com/p/362991213</a></p><h2 id="1-关系建模（范式建模）"><a href="#1-关系建模（范式建模）" class="headerlink" title="1.关系建模（范式建模）"></a>1.关系建模（范式建模）</h2><h3 id="1-1-范式"><a href="#1-1-范式" class="headerlink" title="1.1 范式"></a>1.1 范式</h3><p>1 目的</p><p>降低数据的冗余性</p><p>2 目前业界范式</p><p>第一范式(1NF)、第二范式(2NF)、第三范式(3NF)、巴斯-科德范式(BCNF)、第四范式(4NF)、第五范式(5NF)。逐个遵循，一般要求遵循第一，第二，第三范式，也就是三范式。</p><p><a href="https://blog.csdn.net/Dream_angel_Z/article/details/45175621">https://blog.csdn.net/Dream_angel_Z/article/details/45175621</a></p><h3 id="1-2-建模"><a href="#1-2-建模" class="headerlink" title="1.2 建模"></a>1.2 建模</h3><p><img src="/2022/01/25/dimention-modeling/7.JPG" alt></p><p>1 建模</p><p>关系建模将复杂的数据抽象为两个概念——<strong>实体和关系（实体表，关系表），并使用规范化（三范式）的方式表示</strong>出来</p><p>2 特点</p><p>关系模型严格遵循第三范式（3NF），数据冗余程度低，数据的一致性容易得到保证。</p><p>由于数据分布于众多的表中，查询会相对复杂，在大数据的场景下，查询效率相对较低。</p><h2 id="2-维度建模"><a href="#2-维度建模" class="headerlink" title="2.维度建模"></a>2.维度建模</h2><p><a href="https://www.jianshu.com/p/daab50a23c56">https://www.jianshu.com/p/daab50a23c56</a></p><p><a href="https://cloud.tencent.com/developer/article/1772027">https://cloud.tencent.com/developer/article/1772027</a></p><h3 id="2-1-事实表和维度表"><a href="#2-1-事实表和维度表" class="headerlink" title="2.1 事实表和维度表"></a>2.1 事实表和维度表</h3><p>1 事实表</p><p>存储业务事实，事实表中的每行数据代表一个业务事件（下单、支付、退款、评价等）。</p><p>事实表的特征：</p><p>​    内容相对的窄：列数较少（主要是外键id和度量值）</p><p>​    非常的大</p><p>​    经常发生变化，每天会新增加很多。</p><p>分类：事务型事实表，周期型快照事实表，累积型快照事实表</p><p><img src="/2022/01/25/dimention-modeling/1.png" alt></p><p>2 维度表</p><p>维度表：一般是对事实的描述信息。每一张维表对应现实世界中的一个对象或者概念。  例如：用户、商品、日期、地区等。</p><p>维表的特征：</p><p>​    维表的范围很宽（具有多个属性、列比较多）</p><p>​    跟事实表相比，行数相对较小：通常&lt; 10万条</p><p>​    内容相对固定：编码表</p><h3 id="2-2-维度模型分类"><a href="#2-2-维度模型分类" class="headerlink" title="2.2 维度模型分类"></a>2.2 维度模型分类</h3><p>在维度建模的基础上又分为三种模型：星型模型、雪花模型、星座模型。</p><p><img src="/2022/01/25/dimention-modeling/4.JPG" alt></p><p><img src="/2022/01/25/dimention-modeling/5.JPG" alt></p><p>星座模型是多个星型模型交织</p><h3 id="2-3-建模"><a href="#2-3-建模" class="headerlink" title="2.3 建模"></a>2.3 建模</h3><p><img src="/2022/01/25/dimention-modeling/6.JPG" alt></p><p>1 建模</p><p>维度模型面向业务，将业务用<strong>事实表和维度表</strong>呈现出来。</p><p>步骤：</p><p><a href="https://www.cnblogs.com/suheng01/p/13522677.html">https://www.cnblogs.com/suheng01/p/13522677.html</a></p><p>选择业务过程→声明粒度→确认维度→确认事实</p><p>2 特点</p><p>维度模型以数据分析作为出发点，不遵循三范式，故数据存在一定的冗余。</p><p>表结构简单，故查询简单，查询效率较高。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pretrain</title>
      <link href="/2022/01/24/pretrain/"/>
      <url>/2022/01/24/pretrain/</url>
      
        <content type="html"><![CDATA[<p><a href="https://huggingface.co/docs/transformers/task_summary">https://huggingface.co/docs/transformers/task_summary</a>   Language Modeling</p><p><a href="https://huggingface.co/blog/how-to-train">https://huggingface.co/blog/how-to-train</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pretrain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark 数据倾斜</title>
      <link href="/2022/01/20/spark-data-im/"/>
      <url>/2022/01/20/spark-data-im/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/kaede1209/article/details/81145560">https://blog.csdn.net/kaede1209/article/details/81145560</a></p><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p><p>发生在两个过程：</p><ol><li>数据源数据不均匀</li><li>shuffle过程中key的分布不均<ol><li>单个rdd中进行聚合的时候key分布不均</li><li>多个rdd进行join过程中key的不均匀</li></ol></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark 数据倾斜 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓架构</title>
      <link href="/2022/01/20/datawarehouse-struct/"/>
      <url>/2022/01/20/datawarehouse-struct/</url>
      
        <content type="html"><![CDATA[<p>数仓架构</p><p><a href="https://notomato.blog.csdn.net/article/details/110790403">https://notomato.blog.csdn.net/article/details/110790403</a></p><p>离线数据仓库到实时数据仓库</p><p><a href="https://blog.csdn.net/fuyipingwml1976124/article/details/105571193">https://blog.csdn.net/fuyipingwml1976124/article/details/105571193</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 实时数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数仓架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nvidia-smi 查看GPU使用率很高但是看不到进程</title>
      <link href="/2022/01/19/nvidia/"/>
      <url>/2022/01/19/nvidia/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/gostman/article/details/107456597">https://blog.csdn.net/gostman/article/details/107456597</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> 深度学习框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nvidia-smi 查看GPU使用率很高但是看不到进程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextGCN Graph Convolutional Networks for Text Classification</title>
      <link href="/2022/01/19/textgcn/"/>
      <url>/2022/01/19/textgcn/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1809.05679">https://arxiv.org/abs/1809.05679</a></p><p>1.build a single text graph for a <strong>corpus</strong> based on word co-occurrence and document word relations,</p><p>2.then learn a Text Graph Convolutional Network (Text GCN) for the corpus. Our Text GCN is initialized with one-hot representation for word and document, it then jointly learns the embeddings for both words and documents, as supervised by the known class labels for documents. </p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> NLP </category>
          
          <category> GCN </category>
          
          <category> 文本分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TextGCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RDD依赖关系</title>
      <link href="/2022/01/19/spark-wide-narrow-dependancy/"/>
      <url>/2022/01/19/spark-wide-narrow-dependancy/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么要提出宽窄依赖"><a href="#为什么要提出宽窄依赖" class="headerlink" title="为什么要提出宽窄依赖"></a>为什么要提出宽窄依赖</h2><p>根据rdd的依赖关系构建dag，根据dag划分stage</p><h2 id="如何区别宽窄依赖"><a href="#如何区别宽窄依赖" class="headerlink" title="如何区别宽窄依赖"></a>如何区别宽窄依赖</h2><p><img src="/2022/01/19/spark-wide-narrow-dependancy/1.jpg" alt></p><p>父rdd  -&gt; 子rdd， 蓝色框是rdd分区</p><p>union：2个父rdd  -&gt;  1个子rdd</p><p>区分宽窄依赖主要就是看父RDD数据流向，要是流向一个的话就是窄依赖，流向多个的话就是宽依赖。</p><h2 id="划分stage"><a href="#划分stage" class="headerlink" title="划分stage"></a>划分stage</h2><p><a href="https://blog.csdn.net/weixin_40271036/article/details/79996516">https://blog.csdn.net/weixin_40271036/article/details/79996516</a></p><p><img src="/2022/01/19/spark-wide-narrow-dependancy/2.jpg" alt></p><p>整体思路是：从后往前推，遇到宽依赖就断开，划分为一个stage；遇到窄依赖就将这个RDD加入该stage中</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.jianshu.com/p/5c2301dfa360">https://www.jianshu.com/p/5c2301dfa360</a></p><p><a href="https://zhuanlan.zhihu.com/p/67068559">https://zhuanlan.zhihu.com/p/67068559</a></p><p><a href="https://blog.csdn.net/m0_49834705/article/details/113111596">https://blog.csdn.net/m0_49834705/article/details/113111596</a></p><p><a href="https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502">https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDD依赖关系 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shuffle</title>
      <link href="/2022/01/19/spark-shuffle/"/>
      <url>/2022/01/19/spark-shuffle/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/arachis/p/Spark_Shuffle.html">https://www.cnblogs.com/arachis/p/Spark_Shuffle.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/70331869">https://zhuanlan.zhihu.com/p/70331869</a></p><p><a href="https://www.educba.com/spark-shuffle/">https://www.educba.com/spark-shuffle/</a></p><p><a href="https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502">https://lmrzero.blog.csdn.net/article/details/106015264?spm=1001.2014.3001.5502</a></p><p><a href="https://blog.csdn.net/zp17834994071/article/details/107887292">https://blog.csdn.net/zp17834994071/article/details/107887292</a></p><p><a href="https://zhuanlan.zhihu.com/p/431015932">https://zhuanlan.zhihu.com/p/431015932</a></p><h2 id="0-shuffle是什么，什么时候shuffle"><a href="#0-shuffle是什么，什么时候shuffle" class="headerlink" title="0 shuffle是什么，什么时候shuffle"></a>0 shuffle是什么，什么时候shuffle</h2><p><img src="/2022/01/19/spark-shuffle/1.png" alt></p><p>what：多个partition的数据流向一个partition</p><p>when：宽依赖会有shuffle</p><p>shuffle分为两个阶段：shuffle read  ， shuffle write</p><p>map端-》shuffle read-》 shuffle write-》reduce端</p><h2 id="1-mapreduce-shuffle"><a href="#1-mapreduce-shuffle" class="headerlink" title="1 mapreduce shuffle"></a>1 mapreduce shuffle</h2><p><img src="/2022/01/19/spark-shuffle/1.JPG" alt></p><h2 id="2-spark-shuffle"><a href="#2-spark-shuffle" class="headerlink" title="2 spark shuffle"></a>2 spark shuffle</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p><img src="/2022/01/19/spark-shuffle/2.JPG" alt></p><p><img src="/2022/01/19/spark-shuffle/3.JPG" alt=" "></p><h3 id="2-分类"><a href="#2-分类" class="headerlink" title="2 分类"></a>2 分类</h3><p><a href="https://www.51cto.com/article/703950.html#">https://www.51cto.com/article/703950.html#</a></p><p>过去hash shuffle ，现在sort shuffle</p><h4 id="1-Hash-Shuffle"><a href="#1-Hash-Shuffle" class="headerlink" title="1.Hash Shuffle"></a>1.Hash Shuffle</h4><p><img src="/2022/01/19/spark-shuffle/4.JPG" alt></p><p><img src="/2022/01/19/spark-shuffle/5.JPG" alt></p><p><img src="/2022/01/19/spark-shuffle/6.JPG" alt></p><h4 id="2-Sort-Shuffle"><a href="#2-Sort-Shuffle" class="headerlink" title="2.Sort Shuffle"></a>2.Sort Shuffle</h4><p>1 普通机制的SortShuffleManager</p><p><img src="/2022/01/19/spark-shuffle/12.JPG" alt></p><p><img src="/2022/01/19/spark-shuffle/8.JPG" alt></p><p>2 bypass</p><p><img src="/2022/01/19/spark-shuffle/9.JPG" alt></p><p> 此时task会为每个reduce端的task都创建一个临时磁盘文件，并将数据按key进行hash，然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。<br> 该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>3 总结</p><p>bypass与普通SortShuffleManager运行机制的不同在于：<br>第一，磁盘写机制不同;<br>第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p><h2 id="3-对比"><a href="#3-对比" class="headerlink" title="3 对比"></a>3 对比</h2><p><a href="https://www.zhihu.com/question/27643595">https://www.zhihu.com/question/27643595</a></p><h2 id="4-优化"><a href="#4-优化" class="headerlink" title="4 优化"></a>4 优化</h2><p>因此在我们的开发过程中，能避免则尽可能避免使用会进行shuffle的算子，尽量使用非shuffle算子</p><p>1 shuffle算子：</p><p><a href="https://blog.csdn.net/py_tamir/article/details/95457813">https://blog.csdn.net/py_tamir/article/details/95457813</a></p><p>reduceByKey、join、distinct、repartition</p><p>2 非shuffle算子</p><p>map，flatMap</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shuffle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据组件</title>
      <link href="/2022/01/18/bigdata-component/"/>
      <url>/2022/01/18/bigdata-component/</url>
      
        <content type="html"><![CDATA[<p><strong>大数据组件分类</strong></p><p><img src="/2022/01/18/bigdata-component/1.JPG" alt></p><p><strong>发行版本</strong></p><p>免费：Apache </p><p>收费：（CDH，HDP，二合一），阿里云，亚马逊云，华为云等</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> 基础组件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据组件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark部署方式</title>
      <link href="/2022/01/18/cluster-clinet/"/>
      <url>/2022/01/18/cluster-clinet/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_37163925/article/details/106260434">https://blog.csdn.net/qq_37163925/article/details/106260434</a></p><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html">https://spark.apache.org/docs/latest/cluster-overview.html</a></p><p><a href="https://book.itheima.net/course/1269935677353533441/1270998166728089602/1270999667882074115">https://book.itheima.net/course/1269935677353533441/1270998166728089602/1270999667882074115</a></p><p>通过设置mater来选择部署方式。该属性没有默认值。这是Spark程序需要连接的集群管理器所在的URL地址。如果这个属性在提交应用程序的时候没设置，程序将会通过System.getenv(“MASTER”)来获取MASTER环境变量；但是如果MASTER环境变量没有设定，那么程序将会把master的值设定为local[*]，之后程序将在本地启动。</p><p>local为单机</p><p>standalone是Spark自身实现资源调度</p><p>yarn为使用hadoop yarn来实现资源调度</p><h2 id="1-local"><a href="#1-local" class="headerlink" title="1 local"></a>1 local</h2><p>本地模式就是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时环境</p><p>local【N】：N为线程数量，通常N为cpu的core的数量</p><p>local【*】：cpu的core数量</p><p>跑local可以不依赖hadoop</p><p><a href="https://blog.csdn.net/wangmuming/article/details/37695619">https://blog.csdn.net/wangmuming/article/details/37695619</a></p><p><a href="https://blog.csdn.net/bettesu/article/details/68512570">https://blog.csdn.net/bettesu/article/details/68512570</a></p><h2 id="2-Standalone"><a href="#2-Standalone" class="headerlink" title="2 Standalone"></a>2 Standalone</h2><p><a href="https://sfzsjx.github.io/2019/08/26/spark-standalone-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86">https://sfzsjx.github.io/2019/08/26/spark-standalone-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86</a></p><h4 id="2-1-client"><a href="#2-1-client" class="headerlink" title="2.1 client"></a>2.1 client</h4><p><img src="/2022/01/18/cluster-clinet/1.png" alt></p><p>执行流程</p><ol><li>client 模式提交任务后，会在客户端启动Driver进程。</li><li>Driver 会向Master申请启动Application启动资源。</li><li>资源申请成功后，Driver端会将task发送到worker端执行。</li><li>worker端执行成功后将执行结果返回给Driver端</li></ol><h4 id="2-2-cluster"><a href="#2-2-cluster" class="headerlink" title="2.2 cluster"></a>2.2 cluster</h4><p><img src="/2022/01/18/cluster-clinet/2.png" alt></p><p>执行流程：</p><ol><li>客户端使用命令spark-submit –deploy-mode cluster 后会启动spark-submit进程</li><li>此进程为Driver向Master 申请资源。</li><li>Master会随机在一台Worker节点来启动Driver进程。</li><li>Driver启动成功后，spark-submit关闭，然后Driver向Master申请资源。</li><li>Master接收到请求后，会在资源充足的Worker节点上启动Executor进程。</li><li>Driver分发Task到Executor中执行。</li></ol><h4 id="2-3-高可用HA"><a href="#2-3-高可用HA" class="headerlink" title="2.3 高可用HA"></a>2.3 高可用HA</h4><p><img src="/2022/01/18/cluster-clinet/14.JPG" alt></p><p><img src="/2022/01/18/cluster-clinet/15.JPG" alt></p><h2 id="3-Mesos"><a href="#3-Mesos" class="headerlink" title="3 Mesos"></a>3 Mesos</h2><p>a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)</p><h2 id="4-YARN"><a href="#4-YARN" class="headerlink" title="4 YARN"></a>4 YARN</h2><p>为什么要YARN？</p><p><img src="/2022/01/18/cluster-clinet/16.JPG" alt></p><p><img src="/2022/01/18/cluster-clinet/6.JPG" alt></p><p>Spark On YARN是有两种运行模式的,一种是Cluster模式一种是Client模式.这两种模式的区别就是Driver运行的位置.</p><p>Cluster模式即:Driver运行在YARN容器内部, 和ApplicationMaster在同一个容器内</p><p>Client模式即:Driver运行在客户端进程中, 比如Driver运行在spark-submit程序的进程中</p><p><img src="/2022/01/18/cluster-clinet/9.JPG" alt></p><h3 id="4-1-cluster"><a href="#4-1-cluster" class="headerlink" title="4.1 cluster"></a>4.1 cluster</h3><p><img src="/2022/01/18/cluster-clinet/11.JPG" alt></p><p>具体流程步骤如下：<br>1）、任务提交后会和ResourceManager通讯申请启动ApplicationMaster;<br>2）、随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster就是Driver；<br>3）、Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster的资源申请后会分配Container,然后在合适的NodeManager上启动Executor进程;<br>4）、Executor进程启动后会向Driver反向注册;<br>5）、Executor全部注册完成后Driver开始执行main函数，之后执行到Action算子时，触发一个job，并根据宽依赖开始划分stage，每个stage生成对应的taskSet，之后将task分发到各个Executor上执行;</p><h3 id="4-2-client"><a href="#4-2-client" class="headerlink" title="4.2 client"></a>4.2 client</h3><p><img src="/2022/01/18/cluster-clinet/12.JPG" alt></p><p>具体流程步骤如下：<br>1）、Driver在任务提交的本地机器上运行，Driver启动后会和ResourceManager通讯申请启动ApplicationMaster；<br>2）、随后ResourceManager分配Container，在合适的NodeManager上启动ApplicationMaster，此时的ApplicationMaster的功能相当于一个ExecutorLaucher，只负责向ResourceManager申请Executor内存；<br>3）、ResourceManager接到ApplicationMaster的资源申请后会分配Container，然后ApplicationMaster在资源分配指定的NodeManager上启动Executor进程；<br>4）、Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main函数；<br>5）、之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分Stage，每个Stage生成对应的TaskSet，之后将Task分发到各个Executor上执行。</p><h2 id="5-Kubernetes"><a href="#5-Kubernetes" class="headerlink" title="5 Kubernetes"></a>5 Kubernetes</h2><p>an open-source system for automating deployment, scaling, and management of containerized applications.</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark部署方式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch VS TensorFlow</title>
      <link href="/2022/01/18/tf-torch/"/>
      <url>/2022/01/18/tf-torch/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/37102973">https://zhuanlan.zhihu.com/p/37102973</a></p><p>随着TF2.0出现，TF也有动态图了</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> 深度学习框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch VS TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt-learning小帮手-openprompt</title>
      <link href="/2022/01/17/prompt-assitaant/"/>
      <url>/2022/01/17/prompt-assitaant/</url>
      
        <content type="html"><![CDATA[<p>清华NLP实验室推出OpenPrompt开源工具包</p><h2 id="1-结构"><a href="#1-结构" class="headerlink" title="1 结构"></a>1 结构</h2><p><img src="/2022/01/17/prompt-assitaant/1.png" alt></p><h2 id="2-教程"><a href="#2-教程" class="headerlink" title="2 教程"></a>2 教程</h2><p>可以参考官方<a href="https://hub.fastgit.xyz/thunlp/OpenPrompt">https://hub.fastgit.xyz/thunlp/OpenPrompt</a></p><p>有详细的步骤和case</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://hub.fastgit.xyz/thunlp/OpenPrompt">https://hub.fastgit.xyz/thunlp/OpenPrompt</a></p><p><a href="https://zhuanlan.zhihu.com/p/420335724">https://zhuanlan.zhihu.com/p/420335724</a></p><p><a href="https://github.com/thunlp/OpenPrompt">https://github.com/thunlp/OpenPrompt</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prompt-learning小帮手-openprompt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GNN核心构成</title>
      <link href="/2022/01/17/GNN-component/"/>
      <url>/2022/01/17/GNN-component/</url>
      
        <content type="html"><![CDATA[<p><strong>GNN种类很多，包括GCN，GAEs，RecGNNs等，他们的差异在于图结构，消息传递</strong></p><h3 id="1-图结构"><a href="#1-图结构" class="headerlink" title="1.图结构"></a>1.图结构</h3><p>同构图，异构图，结点和边的设计等</p><p>同构图：只有一种类型的节点和边</p><p>异构图：可以有不同类型的节点和边</p><h3 id="2-消息传递"><a href="#2-消息传递" class="headerlink" title="2.消息传递"></a>2.消息传递</h3><p>消息传递是实现GNN的一种通用框架和编程范式。包含以下两个过程：</p><p>1 Message Propagation</p><p>聚合邻居节点的特征，形成一个消息向量</p><p>2 Representation Updating</p><p>更新当前时刻的节点表示</p><p><img src="/2022/01/17/GNN-component/1.JPG" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://docs.dgl.ai/guide/message.html#">https://docs.dgl.ai/guide/message.html#</a></p><p><a href="https://zhuanlan.zhihu.com/p/352510643">https://zhuanlan.zhihu.com/p/352510643</a></p><p><a href="https://aclanthology.org/2020.acl-main.547.pdf">https://aclanthology.org/2020.acl-main.547.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/350900048">https://zhuanlan.zhihu.com/p/350900048</a></p><p><a href="https://docs.dgl.ai/guide_cn/graph-heterogeneous.html#guide-cn-graph-heterogeneous">https://docs.dgl.ai/guide_cn/graph-heterogeneous.html#guide-cn-graph-heterogeneous</a></p><p><a href="https://zhuanlan.zhihu.com/p/376062090">https://zhuanlan.zhihu.com/p/376062090</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN核心构成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pointwise vs pairwise</title>
      <link href="/2022/01/17/pointwise-pairwise/"/>
      <url>/2022/01/17/pointwise-pairwise/</url>
      
        <content type="html"><![CDATA[<p>pairwise算法聚焦于精确的预测每个文档之间的相关度，pairwise算法主要关心两个文档之间的顺序，相比pointwise的算法更加接近于排序的概念。</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> pairwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pointwise vs pairwise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BertGCN Transductive Text Classification by Combining GCN and BERT</title>
      <link href="/2022/01/17/bert-gcn/"/>
      <url>/2022/01/17/bert-gcn/</url>
      
        <content type="html"><![CDATA[<p>origin paper： <a href="https://arxiv.org/abs/2105.05727">https://arxiv.org/abs/2105.05727</a></p><p>ori code git： <a href="https://github.com/ZeroRin/BertGCN">https://github.com/ZeroRin/BertGCN</a></p><p>官方知乎： <a href="https://zhuanlan.zhihu.com/p/378798855">https://zhuanlan.zhihu.com/p/378798855</a></p><p>TextGCN： <a href="https://arxiv.org/abs/1809.05679">https://arxiv.org/abs/1809.05679</a></p><p><strong>图结构</strong></p><p>we construct a heterogeneous graph containing both word nodes and document nodes following TextGCN.  如下图</p><p><img src="/2022/01/17/bert-gcn/3.JPG" alt></p><p>node ：word nodes and document nodes</p><p>edge ： We build edges among nodes based on word occurrence in documents (document-word edges) and word co-occurrence in the whole corpus (word-word edges)</p><p><strong>edge weight</strong></p><p>也和TextGCN一样</p><p><img src="/2022/01/17/bert-gcn/4.JPG" alt></p><p><strong>node data</strong></p><p>不同</p><p><img src="/2022/01/17/bert-gcn/2.JPG" alt></p><p><img src="/2022/01/17/bert-gcn/1.JPG" alt></p><p><strong>重点是解决了TextGCN和BERT一起联调的收敛问题</strong></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> GCN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BertGCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch GPU训练</title>
      <link href="/2022/01/17/torch-multi-GPU/"/>
      <url>/2022/01/17/torch-multi-GPU/</url>
      
        <content type="html"><![CDATA[<p>模型，数据需要指定设备</p><h2 id="单机单卡"><a href="#单机单卡" class="headerlink" title="单机单卡"></a>单机单卡</h2><h2 id="单机多卡"><a href="#单机多卡" class="headerlink" title="单机多卡"></a>单机多卡</h2><p><a href="https://blog.csdn.net/weixin_38208912/article/details/105122668">https://blog.csdn.net/weixin_38208912/article/details/105122668</a></p><p><a href="https://blog.csdn.net/leviopku/article/details/109318226">https://blog.csdn.net/leviopku/article/details/109318226</a></p><p><a href="https://blog.csdn.net/weixin_43750248/article/details/115698356">https://blog.csdn.net/weixin_43750248/article/details/115698356</a></p><p><a href="https://zhuanlan.zhihu.com/p/74792767">https://zhuanlan.zhihu.com/p/74792767</a></p><h2 id="多机多卡"><a href="#多机多卡" class="headerlink" title="多机多卡"></a>多机多卡</h2>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch GPU训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>torch可以把string变Tensor吗？</title>
      <link href="/2022/01/13/torch-notice/"/>
      <url>/2022/01/13/torch-notice/</url>
      
        <content type="html"><![CDATA[<p>NO. there is no string tensor so you cannot directly convert string to tensor</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> torch可以把string变Tensor吗？ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>魔法方法</title>
      <link href="/2022/01/13/python_magic/"/>
      <url>/2022/01/13/python_magic/</url>
      
        <content type="html"><![CDATA[<h2 id="getitem"><a href="#getitem" class="headerlink" title="getitem"></a>getitem</h2><p><a href="https://zhuanlan.zhihu.com/p/27661382">https://zhuanlan.zhihu.com/p/27661382</a></p><h2 id="all"><a href="#all" class="headerlink" title="all"></a>all</h2><p><a href="https://orangleliu.blog.csdn.net/article/details/49848413">https://orangleliu.blog.csdn.net/article/details/49848413</a></p><h2 id="new"><a href="#new" class="headerlink" title="new"></a>new</h2><p>类实例化的时候先调用new 然后调用init</p><p><a href="https://blog.csdn.net/qq_23183809/article/details/120819491">https://blog.csdn.net/qq_23183809/article/details/120819491</a></p><p><a href="https://blog.csdn.net/Lq_520/article/details/81712905">https://blog.csdn.net/Lq_520/article/details/81712905</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 魔法方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓分层</title>
      <link href="/2022/01/13/datawarehouse-multi-layer/"/>
      <url>/2022/01/13/datawarehouse-multi-layer/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/BeiisBei/article/details/105723188#_19">https://blog.csdn.net/BeiisBei/article/details/105723188#_19</a></p><p><a href="https://www.saoniuhuo.com/article/detail-72.html">https://www.saoniuhuo.com/article/detail-72.html</a></p><p><a href="https://www.dianjilingqu.com/20890.html">https://www.dianjilingqu.com/20890.html</a></p><p><a href="https://www.i4k.xyz/article/wjt199866/115184169#ODS__100">https://www.i4k.xyz/article/wjt199866/115184169#ODS__100</a></p><h2 id="1-为什么要分层"><a href="#1-为什么要分层" class="headerlink" title="1.为什么要分层"></a>1.为什么要分层</h2><p>1.清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。<br>2.数据血缘追踪：简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的张业务表，但是它的来源有很多，如果有一张来 源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。<br>3.减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。<br>4.把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。<br>5.屏蔽原始数据的异常：屏蔽业务的影响，不必改一次业务就需要重新接入数据。</p><h2 id="2-分层结构"><a href="#2-分层结构" class="headerlink" title="2.分层结构"></a>2.分层结构</h2><p><img src="/2022/01/13/datawarehouse-multi-layer/11.png" alt></p><h3 id="2-1-ODS层"><a href="#2-1-ODS层" class="headerlink" title="2.1 ODS层"></a>2.1 ODS层</h3><p><strong>作用</strong></p><p><a href="https://blog.csdn.net/xuebo_911/article/details/8156016#">https://blog.csdn.net/xuebo_911/article/details/8156016#</a></p><p>起到备份数据的作用</p><p><strong>构建</strong></p><p>1 直接加载原始日志、数据，保持原貌不做处理</p><h3 id="2-2-DIM层"><a href="#2-2-DIM层" class="headerlink" title="2.2 DIM层"></a>2.2 DIM层</h3><p>dim存放维度表，dwd存放事实表</p><p><strong>作用</strong></p><p>维度表可以看作是用户来分析数据的窗口，维度表中包含事实数据表中事实记录的特性，有些特性提供描述性信息，有些特性指定如何汇总事实数据表数据，以便为分析者提供有用的信息，维度表包含帮助汇总数据的特性的层次结构。</p><p><strong>构建</strong></p><p><a href="https://help.aliyun.com/document_detail/137615.html">https://help.aliyun.com/document_detail/137615.html</a></p><p>1 构建维度表，主要是对业务事实的描述信息，例如何人，何时，何地等</p><h3 id="2-3-DWD层"><a href="#2-3-DWD层" class="headerlink" title="2.3 DWD层"></a>2.3 DWD层</h3><p><strong>作用</strong></p><p>保存业务事实明细，一行信息代表一次业务行为，例如一次下单。</p><p><strong>构建</strong></p><p>1 对ODS层数据进行清洗（去除空值，脏数据，超过极限范围的数据、脱敏等）</p><p>2 构建事实表</p><p><a href="https://help.aliyun.com/document_detail/114457.html">https://help.aliyun.com/document_detail/114457.html</a></p><h3 id="2-4-DWS"><a href="#2-4-DWS" class="headerlink" title="2.4 DWS"></a>2.4 DWS</h3><p><strong>作用</strong></p><p>避免重复计算</p><p>1）问题引出</p><p>两个需求，统计每个省份订单的个数、统计每个省份订单的总金额，都是将省份表和订单表进行join，group by省份，然后计算。同样数据被计算了两次，实际上类似的场景还会更多。</p><p>2） 那怎么设计能避免重复计算呢？</p><p>针对上述场景，可以设计一张地区宽表，其主键为地区ID，字段包含为：下单次数、下单金额、支付次数、支付金额等。上述所有指标都统一进行计算，并将结果保存在该宽表中，这样就能有效避免数据的重复计算。</p><p><strong>构建</strong></p><p><a href="https://help.aliyun.com/document_detail/126913.html">https://help.aliyun.com/document_detail/126913.html</a></p><p>0 分主题</p><p>1 构建宽表</p><p>2 以DWD为基础，按天进行轻度汇总。DWS层存放的所有主题对象当天的汇总行为，例如每个地区当天的下单次数，下单金额等</p><h3 id="2-5-DWT"><a href="#2-5-DWT" class="headerlink" title="2.5 DWT"></a>2.5 DWT</h3><p><strong>和DWS区别</strong></p><p>DWS按天进行轻度汇总，DWT累积汇总</p><p><strong>作用</strong></p><p>避免重复计算</p><p><strong>构建</strong></p><p>0 分主题</p><p>1 构建宽表</p><p>2 以DWS为基础，对数据进行累积汇总。DWT层存放的是所有主题对象的累积行为，例如每个地区最近７天（１５天、３０天、６０天）的下单次数、下单金额等</p><h3 id="2-6-ADS层"><a href="#2-6-ADS层" class="headerlink" title="2.6 ADS层"></a>2.6 ADS层</h3><p><strong>作用</strong></p><p>为各种统计报表提供数据</p><p><strong>构建</strong></p><p>1 由于这层的数据量不大，所有没有分区，列式存储，压缩 </p><h2 id="3-构建过程"><a href="#3-构建过程" class="headerlink" title="3.构建过程"></a>3.构建过程</h2><p>借助hive，主要就是写SQL，核心步骤就是：</p><p>1.建表</p><p>2.分区规划</p><p>​    按什么分区，比如说按天，按月</p><p>​    注意数据同步策略，全量，增量等</p><p>3.数据装载</p><p>​    注意首日，每日</p><p>联系业务和不同层的要求</p><h2 id="4-全流程调度"><a href="#4-全流程调度" class="headerlink" title="4.全流程调度"></a>4.全流程调度</h2><p>Azkaban</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 构建数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DGL notice</title>
      <link href="/2022/01/12/dgl-notice/"/>
      <url>/2022/01/12/dgl-notice/</url>
      
        <content type="html"><![CDATA[<p>any graph transformation will result in the loss of batch information. for a workaround  , you may should use set_batch_num_nodes and set_batch_num_edges</p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DGL notice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用DGL构造图</title>
      <link href="/2022/01/11/construct-graph-dgl/"/>
      <url>/2022/01/11/construct-graph-dgl/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/liyinggang/p/13360917.html">https://www.cnblogs.com/liyinggang/p/13360917.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/406833147">https://zhuanlan.zhihu.com/p/406833147</a></p><p><a href="https://blog.csdn.net/wufeil7/article/details/107106299">https://blog.csdn.net/wufeil7/article/details/107106299</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 使用DGL构造图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库书单</title>
      <link href="/2022/01/05/data-warehouse/"/>
      <url>/2022/01/05/data-warehouse/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.douban.com/doulist/1202941/?start=0&amp;sort=seq&amp;playable=0&amp;sub_type=">https://www.douban.com/doulist/1202941/?start=0&amp;sort=seq&amp;playable=0&amp;sub_type=</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 数据仓库 </category>
          
          <category> 数仓 </category>
          
          <category> 离线数仓 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库书单 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DAG</title>
      <link href="/2022/01/05/dag/"/>
      <url>/2022/01/05/dag/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_16669583/article/details/106026722">https://blog.csdn.net/qq_16669583/article/details/106026722</a></p><p><a href="https://blog.csdn.net/u011564172/article/details/70172060">https://blog.csdn.net/u011564172/article/details/70172060</a></p><h2 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h2><p>根据rdd的依赖关系构建dag，根据dag划分stage</p><h2 id="啥样"><a href="#啥样" class="headerlink" title="啥样"></a>啥样</h2><p><img src="/2022/01/05/dag/1.JPG" alt></p><p>1个action=1个job=1个dag</p><h2 id="调度流程"><a href="#调度流程" class="headerlink" title="调度流程"></a>调度流程</h2><p><img src="/2022/01/05/dag/2.png" alt></p><p><img src="/2022/01/05/dag/1.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>spark优化</title>
      <link href="/2022/01/05/spark-optimaization/"/>
      <url>/2022/01/05/spark-optimaization/</url>
      
        <content type="html"><![CDATA[<p>8 Performance Optimization Techniques Using Spark</p><p><a href="https://www.syntelli.com/eight-performance-optimization-techniques-using-spark#">https://www.syntelli.com/eight-performance-optimization-techniques-using-spark#</a></p><p>Spark性能优化指南（美团）</p><p><a href="https://tech.meituan.com/2016/04/29/spark-tuning-basic.html">https://tech.meituan.com/2016/04/29/spark-tuning-basic.html</a></p><p><a href="https://tech.meituan.com/2016/05/12/spark-tuning-pro.html">https://tech.meituan.com/2016/05/12/spark-tuning-pro.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 优化 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark架构</title>
      <link href="/2022/01/05/spark-struct/"/>
      <url>/2022/01/05/spark-struct/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/70424613">https://zhuanlan.zhihu.com/p/70424613</a></p><p><a href="https://zhuanlan.zhihu.com/p/348024116">https://zhuanlan.zhihu.com/p/348024116</a></p><p><a href="https://spark.apache.org/docs/latest/cluster-overview.html">https://spark.apache.org/docs/latest/cluster-overview.html</a></p><p><a href="https://www.zhihu.com/question/437293024">https://www.zhihu.com/question/437293024</a></p><p><img src="/2022/01/05/spark-struct/1.png" alt></p><p>There are several useful things to note about this architecture:</p><ol><li>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.</li><li>Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN/Kubernetes).</li><li>The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see <a href="https://spark.apache.org/docs/latest/configuration.html#networking">spark.driver.port in the network config section</a>). As such, the driver program must be network addressable from the worker nodes.</li><li>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.</li></ol><p>上图的4个框应该不是指四台机器，仅仅用来示意spark的架构组成，在机器上的分布取决于部署方式</p><p>客户端/客户机指的是提交任务的机器</p><p>主从架构（master/slave）：集群由一个主服务器和若干个从服务器（机器）组成</p><p>一个worker可以有多个excutor，默认情况下，只会启动一个Executor</p><p><a href="https://www.cnblogs.com/ExMan/p/14358363.html">https://www.cnblogs.com/ExMan/p/14358363.html</a></p><p>一个executor可以执行多个task，取决于cpu的核数</p><p><a href="https://blog.csdn.net/mzqadl/article/details/104217828">https://blog.csdn.net/mzqadl/article/details/104217828</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 基础组件 </category>
          
          <category> spark </category>
          
          <category> 原理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark架构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java</title>
      <link href="/2022/01/05/java/"/>
      <url>/2022/01/05/java/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Java平台"><a href="#1-Java平台" class="headerlink" title="1.Java平台"></a>1.Java平台</h2><p><a href="https://www.cnblogs.com/HeavenZhi/p/14075331.html#jrejava-runtime-environment">https://www.cnblogs.com/HeavenZhi/p/14075331.html#jrejava-runtime-environment</a></p><p><a href="https://blog.csdn.net/ZGL_cyy/article/details/104081834">https://blog.csdn.net/ZGL_cyy/article/details/104081834</a></p><p>java是语言，这个是平台。Java 平台由 Java 虚拟机（Java Virtual Machine，JVM）和 Java 应用编程接口（Application Programming Interface，API））构成。</p><p><img src="/2022/01/05/java/1.jpg" alt></p><p><strong>java SE</strong></p><p><a href="https://baike.baidu.com/item/JAVA SE/4662159">Java SE（Java Platform Standard Edition，Java 平台标准版）</a>以前称为 J2SE，它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 包含了支持 Java Web 服务开发的类，并为 Java EE 提供基础，如 Java 语言基础、<a href="https://baike.baidu.com/item/Java数据库连接/1173389?fr=aladdin">JDBC</a> 操作、<a href="https://baike.baidu.com/item/IO/5918?fr=aladdin">I/O</a> 操作、<a href="https://baike.baidu.com/item/网络通信/9636548?fr=aladdin">网络通信</a>以及<a href="https://baike.baidu.com/item/多线程">多线程</a>等技术。</p><p><strong>Java EE</strong></p><p><a href="https://baike.baidu.com/item/JavaEE/3066623?fr=aladdin">Java EE（Java Platform Enterprise Edition，Java 平台企业版）</a>以前称为 J2EE。企业版本帮助开发和部署可移植、健壮、可伸缩且安全的服务器端 Java 应用程序。Java EE 是在 Java SE 基础上构建的，它提供 Web 服务、组件模型、管理和通信 API，可以用来实现企业级的面向服务体系结构（Service Oriented Architecture，SOA）和 <a href="https://baike.baidu.com/item/web2.0/97695?fr=aladdin">Web 2.0</a> 应用程序。</p><p><strong>Java ME</strong></p><p><a href="https://baike.baidu.com/item/Java ME?fromtitle=Javame&amp;fromid=2106282">Java ME（Java Platform Micro Edition，Java 平台微型版）</a>以前称为 J2ME，也叫 K-JAVA。 Java ME 为在移动设备和嵌入式设备（比如手机、PDA、电视机顶盒和打印机）上运行的应用程序提供一个健壮且灵活的环境。</p><p>Java ME 包括灵活的用户界面、健壮的安全模型、丰富的内置网络协议以及对可以动态下载的联网和离线应用程序。基于 Java ME 规范的应用程序 只需编写一次就可以用于许多设备，而且可以利用每个设备的本机功能。</p><h2 id="2-run-command"><a href="#2-run-command" class="headerlink" title="2 run command"></a>2 run command</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//he_test</span><br><span class="line">//lib</span><br><span class="line">//src</span><br><span class="line">//HelloWorld.java</span><br><span class="line">//bin</span><br><span class="line">package com.hlw.test;</span><br><span class="line">public class HelloWorld &#123;</span><br><span class="line">    /* 第一个Java程序</span><br><span class="line"> *      * 它将输出字符串 Hello World</span><br><span class="line"> *           */</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        System.out.println(&quot;Hello World&quot;); // 输出 Hello World</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>he_test下</p><p>1.编译代码</p><p>javac  -d bin/   ./src/HelloWorld.java</p><p>bin/下生成com/hlw/test/HelloWorld.class</p><p>2.运行程序</p><p>java -classpath/-cp ./bin/ com/hlw/test/HelloWorld</p><h2 id="3-Java-源文件的命名规则"><a href="#3-Java-源文件的命名规则" class="headerlink" title="3 Java 源文件的命名规则"></a>3 Java 源文件的命名规则</h2><ul><li>通常情况下，Java 程序源文件的主文件名可以任意。</li><li>但如果其中定义了一个 public 类，则该源文件的文件名必须与该 public 类的类名相同。</li><li>一个Java 源文件可包含多个类定义，但最多只能包含一个public类定义。</li></ul><h2 id="4-jar包"><a href="#4-jar包" class="headerlink" title="4 jar包"></a>4 jar包</h2><h4 id="0-下载jar包"><a href="#0-下载jar包" class="headerlink" title="0.下载jar包"></a>0.下载jar包</h4><p><a href="https://www.cnblogs.com/Marydon20170307/p/9149256.html">https://www.cnblogs.com/Marydon20170307/p/9149256.html</a></p><h4 id="1-环境导入jar包"><a href="#1-环境导入jar包" class="headerlink" title="1.环境导入jar包"></a>1.环境导入jar包</h4><p><strong>ide</strong></p><p><a href="https://www.jianshu.com/p/9762b7098b76">https://www.jianshu.com/p/9762b7098b76</a></p><p><a href="https://www.cnblogs.com/mracale/p/10493823.html">https://www.cnblogs.com/mracale/p/10493823.html</a></p><p><strong>ubuntu</strong></p><p><a href="https://blog.csdn.net/weixin_30681615/article/details/99745369">https://blog.csdn.net/weixin_30681615/article/details/99745369</a></p><p><a href="https://blog.csdn.net/chencaw/article/details/78884107">https://blog.csdn.net/chencaw/article/details/78884107</a></p><p><a href="https://blog.csdn.net/j_bean/article/details/75095337">https://blog.csdn.net/j_bean/article/details/75095337</a></p><p>例子:</p><h6 id="目录结构"><a href="#目录结构" class="headerlink" title="####目录结构"></a>####目录结构</h6><p>test</p><p>​    lib</p><p>​        spark-assembly_2.10-1.6.0-cdh5.16.1.jar</p><p>​    src</p><p>​        demo.java</p><p>​    bin</p><h6 id><a href="#" class="headerlink" title="#"></a>#</h6><p>在test路径下</p><p>1.编译</p><p>javac -cp ./lib/spark-assembly_2.10-1.6.0-cdh5.16.1.jar  -d ./bin/  ./src/demo.java</p><p>2.运行</p><p>java -cp ./lib/spark-assembly_2.10-1.6.0-cdh5.16.1.jar:./bin/  com/yzy/spark/demo</p><p>注意：1 -cp 需要列出所有jar包，ide也是全部列出了 2 window上，用;分隔；linux上是:分隔</p><h4 id="2-代码引用jar包"><a href="#2-代码引用jar包" class="headerlink" title="2.代码引用jar包"></a>2.代码引用jar包</h4><p>import</p><h4 id="3-怎么生成jar"><a href="#3-怎么生成jar" class="headerlink" title="3.怎么生成jar"></a>3.怎么生成jar</h4><p><a href="https://www.cnblogs.com/swordfall/p/11359370.html">https://www.cnblogs.com/swordfall/p/11359370.html</a></p><p><a href="https://blog.csdn.net/smgsn01/article/details/108038046">https://blog.csdn.net/smgsn01/article/details/108038046</a></p><p>maven，ide自带，jar命令</p><h4 id="4-执行"><a href="#4-执行" class="headerlink" title="4.执行"></a>4.执行</h4><p>java -jar XXX.jar</p><h2 id="5-package"><a href="#5-package" class="headerlink" title="5 package"></a>5 package</h2><p><a href="https://blog.csdn.net/qq_41297896/article/details/90056534">https://blog.csdn.net/qq_41297896/article/details/90056534</a></p><p><a href="https://www.runoob.com/java/java-package.html">https://www.runoob.com/java/java-package.html</a></p><p><a href="https://www.runoob.com/w3cnote/java-compile-with-package.html">https://www.runoob.com/w3cnote/java-compile-with-package.html</a></p><p>为了更好地组织类，用于区别类名的命名空间</p><p>包的命名规范</p><p><a href="https://blog.csdn.net/shi779276212/article/details/92795085">https://blog.csdn.net/shi779276212/article/details/92795085</a></p><p>使用：</p><p><img src="/2022/01/05/java/3.JPG" alt></p><p><img src="/2022/01/05/java/2.JPG" alt></p><h2 id="6-重写-Override-与重载-Overload"><a href="#6-重写-Override-与重载-Overload" class="headerlink" title="6 重写(Override)与重载(Overload)"></a>6 重写(Override)与重载(Overload)</h2><p><a href="https://www.runoob.com/java/java-override-overload.html">https://www.runoob.com/java/java-override-overload.html</a></p><p>重写：是子类对父类同名函数的二次实现</p><p>重载：一个类内存在多个重名函数，而参数不同</p><h2 id="7-多态"><a href="#7-多态" class="headerlink" title="7 多态"></a>7 多态</h2><p><a href="http://c.biancheng.net/view/1001.html#">http://c.biancheng.net/view/1001.html#</a></p><p><a href="https://www.runoob.com/java/java-polymorphism.html">https://www.runoob.com/java/java-polymorphism.html</a></p><p>多态是同一个行为具有多个不同表现形式或形态的能力。</p><p><img src="/2022/01/05/java/4.jpg" alt></p><p>多态存在的三个必要条件</p><ul><li>继承</li><li>重写</li><li>父类引用指向子类对象：<strong>Parent p = new Child();</strong></li></ul><p>分类：</p><p><a href="https://blog.csdn.net/zhao_miao/article/details/83750898">https://blog.csdn.net/zhao_miao/article/details/83750898</a></p><p>1 向上转型</p><p><code>父类 父类对象 = 子类实例</code></p><p>只能调用父特有，子覆盖，不能子特有</p><p>2 向下转型</p><p>向下转型之前一定要进行向上转型！！</p><p><code>子类 子类对象 = （子类）父类实例</code></p><p>可以父特有，子覆盖，子特有</p><h2 id="8-函数入口"><a href="#8-函数入口" class="headerlink" title="8 函数入口"></a>8 函数入口</h2><p><a href="https://blog.csdn.net/weixin_29740921/article/details/114249667">https://blog.csdn.net/weixin_29740921/article/details/114249667</a></p><p>函数入口为main，就和c++的main函数一样，main方法的写法是固定的</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;&#125;</span><br><span class="line">public static void main(String args[]) &#123;&#125;</span><br></pre></td></tr></table></figure><p>一个java文件可以不只有一个main，不同类都可以有自己的main，然后选哪个main呢，也就是选哪个类呢？</p><p>选公共类（至多包含一个）；若是没有就选用同名类，和文件同名</p><h2 id="9-引用"><a href="#9-引用" class="headerlink" title="9 引用"></a>9 引用</h2><p>java没有指针</p><p>和C++的引用不一样，和C++指针有点像</p><p>java 4种引用</p><p><a href="https://blog.csdn.net/linzhiqiang0316/article/details/88591907">https://blog.csdn.net/linzhiqiang0316/article/details/88591907</a></p><h2 id="10-数据类型和变量类型的区别"><a href="#10-数据类型和变量类型的区别" class="headerlink" title="10 数据类型和变量类型的区别"></a>10 数据类型和变量类型的区别</h2><p><a href="https://blog.csdn.net/qq_61411852/article/details/123130531">https://blog.csdn.net/qq_61411852/article/details/123130531</a></p><p>1.数据类型</p><p>定义一个变量，每一种数据类型需要用到的存储空间都不同，这时需要用不同的数据类型来定义变量；例如：int float double等等</p><p>2.变量类型</p><ul><li>局部变量：类的方法中的变量。</li><li>实例变量：独立于方法之外的变量，不过没有 static 修饰。</li><li>类变量：独立于方法之外的变量，用 static 修饰。</li></ul><h2 id="12-数据类型"><a href="#12-数据类型" class="headerlink" title="12 数据类型"></a>12 数据类型</h2><p>基本类型：boolean， char， int， byte，short，long， float，double（有值域范围）</p><p>为什么存在基本类型还要包装类？</p><p>基本类型不是对象，包装类是一个对象，增加面向对象特性</p><p>包装类：Boolean，Character，Integer，Byte，Short，Long，Float，Double</p><p>自动装箱</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Integer i = 100;</span><br><span class="line"></span><br><span class="line">等价于</span><br><span class="line">Integer i = Integer.valueOf(100);</span><br><span class="line">ArrayList intList = new ArrayList();</span><br><span class="line">intList.add(1); </span><br></pre></td></tr></table></figure><p>自动拆箱</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int t = i; </span><br><span class="line">等价于</span><br><span class="line"></span><br><span class="line">int t = i.intValue(); </span><br></pre></td></tr></table></figure><h2 id="13-数组"><a href="#13-数组" class="headerlink" title="13 数组"></a>13 数组</h2><p>声明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dataType[] arrayRefVar;   // 首选的方法</span><br><span class="line"> </span><br><span class="line">或</span><br><span class="line"> </span><br><span class="line">dataType arrayRefVar[];  // 效果相同，但不是首选方法</span><br></pre></td></tr></table></figure><p>创建</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">arrayRefVar=new dataType[arraySize];</span><br><span class="line">或者</span><br><span class="line">arrayRefVar = &#123;value0, value1, ..., valuek&#125;;</span><br></pre></td></tr></table></figure><p>多维数组</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type[][] typeName = new type[typeLength1][typeLength2];</span><br></pre></td></tr></table></figure><h2 id="14-foreach"><a href="#14-foreach" class="headerlink" title="14 foreach"></a>14 foreach</h2><p>和普通for循环比较，在遍历数组、集合方面，foreach为开发人员提供了极大的方便</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for(元素类型t 元素变量x : 遍历对象obj)&#123; </span><br><span class="line">     引用了x的java语句; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class TestArray &#123;</span><br><span class="line">   public static void main(String[] args) &#123;</span><br><span class="line">      double[] myList = &#123;1.9, 2.9, 3.4, 3.5&#125;;</span><br><span class="line"> </span><br><span class="line">      // 打印所有数组元素</span><br><span class="line">      for (double element: myList) &#123;</span><br><span class="line">         System.out.println(element);</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="15-从控制台读取数据"><a href="#15-从控制台读取数据" class="headerlink" title="15 从控制台读取数据"></a>15 从控制台读取数据</h2><p>1 BufferedReader </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 throws IOException</span><br><span class="line">2 BufferedReader br = new BufferedReader(new InputStreamReader(System.in));</span><br><span class="line">3 c = (char) br.read();</span><br><span class="line">或者str = br.readLine();</span><br></pre></td></tr></table></figure><p>2 Scanner </p><h2 id="16-类型转化"><a href="#16-类型转化" class="headerlink" title="16 类型转化"></a>16 类型转化</h2><p><a href="http://c.biancheng.net/view/796.html">http://c.biancheng.net/view/796.html</a></p><p>1自动类型转换</p><ul><li>两种数据类型彼此兼容</li><li>目标类型的取值范围大于源数据类型（低级类型数据转换成高级类型数据）</li></ul><p>2 强制类型转换</p><p>所以当两种数据类型不兼容，或目标类型的取值范围小于源类型时，自动转换将无法进行，这时就需要进行强制类型转换。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">int a = 3;</span><br><span class="line">double b = 5.0;</span><br><span class="line">a = (int)b;</span><br></pre></td></tr></table></figure><h2 id="17-异常处理"><a href="#17-异常处理" class="headerlink" title="17 异常处理"></a>17 异常处理</h2><p><a href="http://c.biancheng.net/view/6751.html">http://c.biancheng.net/view/6751.html</a></p><p>Exception 是异常的基类</p><p>1 语句抛出异常</p><p><strong>try</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">try&#123;</span><br><span class="line">  // 程序代码</span><br><span class="line">&#125;catch(异常类型1 异常的变量名1)&#123;</span><br><span class="line">  // 程序代码</span><br><span class="line">&#125;catch(异常类型2 异常的变量名2)&#123;</span><br><span class="line">  // 程序代码</span><br><span class="line">&#125;finally&#123;</span><br><span class="line">  // 程序代码</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>throw</strong></p><p>当 throw 语句执行时，它后面的语句将不执行，此时程序转向调用者程序，寻找与之相匹配的 catch 语句，执行相应的异常处理程序。如果没有找到相匹配的 catch 语句，则再转向上一层的调用程序。这样逐层向上，直到最外层的异常处理程序终止程序并打印出调用栈情况。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">throw ExceptionObject;</span><br></pre></td></tr></table></figure><p>2 方法抛出异常</p><p><strong>throws</strong></p><p>函数声明异常，但是本身不处理异常，交给调用者处理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">returnType method_name(paramList) throws Exception 1,Exception2,…&#123;…&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import java.io.FileInputStream;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">public class Test04 &#123;</span><br><span class="line">    public void readFile() throws IOException &#123;</span><br><span class="line">        // 定义方法时声明异常</span><br><span class="line">        FileInputStream file = new FileInputStream(&quot;read.txt&quot;); // 创建 FileInputStream 实例对象</span><br><span class="line">        int f;</span><br><span class="line">        while ((f = file.read()) != -1) &#123;</span><br><span class="line">            System.out.println((char) f);</span><br><span class="line">            f = file.read();</span><br><span class="line">        &#125;</span><br><span class="line">        file.close();</span><br><span class="line">    &#125;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Throws t = new Test04();</span><br><span class="line">        try &#123;</span><br><span class="line">            t.readFile(); // 调用 readFHe()方法</span><br><span class="line">        &#125; catch (IOException e) &#123;</span><br><span class="line">            // 捕获异常</span><br><span class="line">            System.out.println(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) throws Exception</span><br><span class="line">异常谁处理，JVM处理</span><br></pre></td></tr></table></figure><h2 id="20-final，static，static和final一起使用"><a href="#20-final，static，static和final一起使用" class="headerlink" title="20 final，static，static和final一起使用"></a>20 final，static，static和final一起使用</h2><p><a href="https://blog.csdn.net/hust_yfang/article/details/79585696">https://blog.csdn.net/hust_yfang/article/details/79585696</a></p><p>1 final</p><p>类</p><p>变量</p><p>方法</p><p>2 static</p><p><strong>变量</strong></p><p><strong>方法</strong></p><p>老看见函数前面有static，可以没有，二者区别： 加了static，可以不用实例化类就能调用方法；没有加static，必须实例化类才可以用</p><p>3 static和final一起使用</p><p>变量</p><p>方法</p><h2 id="22-泛型"><a href="#22-泛型" class="headerlink" title="22 泛型"></a>22 泛型</h2><p>目的：为了兼容多种数据类型</p><p><strong>泛型标记符</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">E - Element (在集合中使用，因为集合中存放的是元素)；</span><br><span class="line">T - Type（Java 类）；</span><br><span class="line">K - Key（键）；</span><br><span class="line">V - Value（值）；</span><br><span class="line">N - Number（数值类型）；</span><br><span class="line">R - Result （返回结果，多用于函数式编程）；</span><br><span class="line">? - 表示不确定的java类型。</span><br><span class="line">O </span><br></pre></td></tr></table></figure><p>1 泛型方法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public   void printArray( int [] inputArray ) -》public  &lt; E &gt; void printArray( E[] inputArray )</span><br><span class="line">public  &lt;T extends Comparable&lt;T&gt;&gt; T maximum(T x, T y, T z) //Comparable是一个类</span><br></pre></td></tr></table></figure><p>2 泛型类</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">public class Box -》 public class Box&lt;T&gt;</span><br></pre></td></tr></table></figure><h2 id="23-对象作为返回值"><a href="#23-对象作为返回值" class="headerlink" title="23 对象作为返回值"></a>23 对象作为返回值</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> class Phone &#123;</span><br><span class="line"></span><br><span class="line">    String brand;//品牌</span><br><span class="line">    double price;</span><br><span class="line">    String color;</span><br><span class="line"></span><br><span class="line">    public void call(String who)&#123;</span><br><span class="line">        System.out.println(&quot;给&quot;+who+&quot;打电话&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void sendMessage()&#123;</span><br><span class="line">        System.out.println(&quot;群发短信&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public class test &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Phone two=getPhone();</span><br><span class="line">        System.out.println(two.price);</span><br><span class="line">        System.out.println(two.brand);</span><br><span class="line">        System.out.println(two.color);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static Phone getPhone()&#123;</span><br><span class="line">        Phone one=new Phone();</span><br><span class="line">        one.brand=&quot;苹果&quot;;</span><br><span class="line">        one.price=8388.0;</span><br><span class="line">        one.color=&quot;玫瑰金&quot;;</span><br><span class="line">        return one;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="24-正则"><a href="#24-正则" class="headerlink" title="24 正则"></a>24 正则</h2><p>java和python正则很像，但不是完全一样</p><p><a href="https://blog.csdn.net/weixin_39574708/article/details/114958384">https://blog.csdn.net/weixin_39574708/article/details/114958384</a></p><p><a href="https://blog.csdn.net/henu_xiaohei/article/details/84765678">https://blog.csdn.net/henu_xiaohei/article/details/84765678</a></p><p>1 | </p><p>或者</p><p>2 固定匹配位数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\d&#123;7,8&#125; //7为或者8为</span><br><span class="line">[A-Z]&#123;1&#125;[0-9]&#123;8&#125;</span><br></pre></td></tr></table></figure><p>注意matches()，find（）</p><p>match是全部匹配，find是部分匹配</p><p>3 转义字符</p><p>转义字符\ *：不同于原来的字母含义，比如\n表示换行</p><p>java正则为什么要两个斜杆表示转义</p><p><a href="https://blog.csdn.net/qq_37325947/article/details/107819945">https://blog.csdn.net/qq_37325947/article/details/107819945</a></p><h2 id="25-java如何兼容不同类型的输入"><a href="#25-java如何兼容不同类型的输入" class="headerlink" title="25 java如何兼容不同类型的输入"></a>25 java如何兼容不同类型的输入</h2><ol><li>函数重载</li><li>泛型</li></ol><h2 id="26-装饰器"><a href="#26-装饰器" class="headerlink" title="26 装饰器"></a>26 装饰器</h2><p>@Override</p><p>需要你重写</p><h2 id="28-类"><a href="#28-类" class="headerlink" title="28 类"></a>28 类</h2><p>this </p><p>和python self一样，指的是对象</p><h3 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h3><p>类直接调用方法</p><p><a href="https://blog.csdn.net/qq_40136594/article/details/83996659">https://blog.csdn.net/qq_40136594/article/details/83996659</a></p><p>需要是静态方法</p><h4 id="1-创建对象"><a href="#1-创建对象" class="headerlink" title="1 创建对象"></a>1 创建对象</h4><p>new </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">public class Puppy&#123;</span><br><span class="line">   public Puppy(String name)&#123;</span><br><span class="line">      //这个构造器仅有一个参数：name</span><br><span class="line">      System.out.println(&quot;小狗的名字是 : &quot; + name ); </span><br><span class="line">   &#125;</span><br><span class="line">   public static void main(String[] args)&#123;</span><br><span class="line">      // 下面的语句将创建一个Puppy对象</span><br><span class="line">      Puppy myPuppy = new Puppy( &quot;tommy&quot; );</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="2-类继承"><a href="#2-类继承" class="headerlink" title="2 类继承"></a>2 类继承</h4><p>1 extends</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class 子类 extends 父类 &#123;</span><br><span class="line">&#125;</span><br><span class="line">//只可以单继承</span><br></pre></td></tr></table></figure><p><img src="/2022/01/05/java/3.png" alt></p><p>2 implements</p><p>一般用于类继承接口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public interface A &#123;</span><br><span class="line">    public void eat();</span><br><span class="line">    public void sleep();</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">public interface B &#123;</span><br><span class="line">    public void show();</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">public class C implements A,B &#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>super关键字：当前对象，但是用来调用父类成员</p><p>this关键字：当前对象，用来调用子类成员</p><p>关于构造函数：</p><p>1 进入子类对应的构造函数</p><p>2 若是没有显示调用父类的构造函数，则自动调用；若是显示的调用父类的构造函数（super()，必须写第一句，super.SuperClass（） 错误），则显示调用</p><p>3 先调用父类，然后调用子类</p><h4 id="3-匿名类"><a href="#3-匿名类" class="headerlink" title="3 匿名类"></a>3 匿名类</h4><p><a href="https://www.runoob.com/java/java-anonymous-class.html">https://www.runoob.com/java/java-anonymous-class.html</a></p><p><img src="/2022/01/05/java/4.png" alt></p><h4 id="4-抽象类，抽象方法"><a href="#4-抽象类，抽象方法" class="headerlink" title="4 抽象类，抽象方法"></a>4 抽象类，抽象方法</h4><p>作用：抽象类为所有子类提供了一个通用模板，子类可以在这个模板基础上进行扩展，可以避免子类设计的随意性</p><p>1 抽象类</p><p>特殊的类：抽象类不能实例化对象，会报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public abstract class Employee</span><br><span class="line">1 在普通类的基础上加abstract</span><br></pre></td></tr></table></figure><p>怎么使用抽象类？</p><p>通过类继承的方式使用</p><p>2 抽象方法</p><p>普通类不能有抽象方法，只有抽象类，或者接口</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public abstract double computePay();</span><br><span class="line">1. abstract</span><br><span class="line">2. 只有函数声明，没有函数具体实现</span><br><span class="line">3 子类不是抽象类，需要给出抽象类中的抽象方法的具体实现；子类也是抽象类，那么可以不需要</span><br></pre></td></tr></table></figure><h4 id="5-接口"><a href="#5-接口" class="headerlink" title="5 接口"></a>5 接口</h4><p>接口并不是类，是抽象方法的集合</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[可见度] interface 接口名称 [extends 其他的接口名， 其他的接口名， 其他的接口名] &#123;</span><br><span class="line">        // 声明变量,只能是 public static final 类型的</span><br><span class="line">        // 抽象方法</span><br><span class="line">&#125;</span><br><span class="line">//接口只能继承接口，extends，可以是多继承</span><br></pre></td></tr></table></figure><p>怎么使用接口？通过类来实现</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...implements 接口名称[, 其他接口名称, 其他接口名称..., ...] ...</span><br><span class="line"></span><br><span class="line">public class MammalInt implements Animal // Animal是接口</span><br></pre></td></tr></table></figure><p>关系：</p><p>类和类，继承，只能单继承</p><p>类和接口，实现，可以多实现</p><p>接口和接口，继承，可以多继承</p><p>default</p><p><a href="https://blog.csdn.net/qq_35835624/article/details/80196932">https://blog.csdn.net/qq_35835624/article/details/80196932</a></p><p>配合接口使用</p><p>使得接口内的函数可以写方法体，原来全是抽象方法，不能有方法体</p><h2 id="29-Lambda表达式"><a href="#29-Lambda表达式" class="headerlink" title="29 Lambda表达式"></a>29 Lambda表达式</h2><p>声明的时候需要 -&gt;</p><p><a href="https://www.runoob.com/java/java8-lambda-expressions.html">https://www.runoob.com/java/java8-lambda-expressions.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">MathOperation subtraction = (a, b) -&gt; a - b; //声明</span><br><span class="line">System.out.println(&quot;10 - 5 = &quot; + tester.operate(10, 5, subtraction));//调用</span><br><span class="line"></span><br><span class="line">一起</span><br><span class="line"> System.out.println(&quot;10 - 5 = &quot; + tester.operate(10, 5,  (a, b) -&gt; a - b));</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
          <category> 基础语法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ranknet对比listnet</title>
      <link href="/2022/01/04/ranknet-listnet/"/>
      <url>/2022/01/04/ranknet-listnet/</url>
      
        <content type="html"><![CDATA[<p>The ListNet method grows on the bases of RankNet, they both employ the Cross Entropy function as a loss function and Gradient Descendant as algorithm to train a Neural Network Model. While the ListNet uses document list as instances, RankNet uses document pairs.</p><p><img src="/2022/01/04/ranknet-listnet/1.JPG" alt></p><p>We investigated why the listwise method ListNet can outperform the pairwise methods of RankNet, Ranking SVM, and RankBoost.</p><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p><p>1.for the pairwise approach the number of document pairs varies largely from query to query</p><p>2.The pairwise approach actually employs a ‘pairwise’ loss function, which might be too loose as an approximation of the performance measures</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> listwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ranknet对比listnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>消息传递</title>
      <link href="/2021/12/31/make-own-gnn-module/"/>
      <url>/2021/12/31/make-own-gnn-module/</url>
      
        <content type="html"><![CDATA[<h2 id="1-使用内置的消息传递api"><a href="#1-使用内置的消息传递api" class="headerlink" title="1.使用内置的消息传递api"></a>1.使用内置的消息传递api</h2><p>比如GraphConv</p><h2 id="2-实现自己的消息传递策略"><a href="#2-实现自己的消息传递策略" class="headerlink" title="2.实现自己的消息传递策略"></a>2.实现自己的消息传递策略</h2><p><strong>Write your own GNN module</strong></p><p><a href="https://docs.dgl.ai/tutorials/blitz/3_message_passing.html">https://docs.dgl.ai/tutorials/blitz/3_message_passing.html</a></p><p><strong>Message Passing APIs</strong></p><p><a href="https://docs.dgl.ai/guide/message-api.html#guide-message-passing-api">https://docs.dgl.ai/guide/message-api.html#guide-message-passing-api</a></p><p><a href="https://docs.dgl.ai/guide/message-heterograph.html">https://docs.dgl.ai/guide/message-heterograph.html</a></p><p><strong>Apply Message Passing On Part Of The Graph</strong></p><p><a href="https://docs.dgl.ai/guide/message-part.html">https://docs.dgl.ai/guide/message-part.html</a></p><p><strong>message function,Reduce function</strong></p><p><a href="https://docs.dgl.ai/api/python/dgl.function.html">https://docs.dgl.ai/api/python/dgl.function.html</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 消息传递 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow模型导出总结</title>
      <link href="/2021/12/29/tf-export/"/>
      <url>/2021/12/29/tf-export/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/113734249">https://zhuanlan.zhihu.com/p/113734249</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow export </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DGL</title>
      <link href="/2021/12/27/DGL/"/>
      <url>/2021/12/27/DGL/</url>
      
        <content type="html"><![CDATA[<p><a href="https://docs.dgl.ai/api/python/nn.html">https://docs.dgl.ai/api/python/nn.html</a></p><p>实现了常见的图卷积和图池化</p><p><img src="/2021/12/27/DGL/1.JPG" alt></p><p>以graphconv为例，不仅给出了接口和代码还有论文介绍</p><p><img src="/2021/12/27/DGL/2.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConvGNNs</title>
      <link href="/2021/12/27/GCN/"/>
      <url>/2021/12/27/GCN/</url>
      
        <content type="html"><![CDATA[<p>ConvGNNs fall into two categories, <strong>spectral-based</strong> and <strong>spatial-based</strong>. Spectral based approaches <strong>define graph convolutions by introducing filters</strong> from the perspective of graph signal processing [82] where the graph convolutional operation is interpreted as removing noises from graph signals. Spatial-based approaches inherit ideas from RecGNNs to <strong>define graph convolutions by information propagation</strong>. <strong>spatial-based</strong> methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality.</p><p>谱域图卷积是空域图卷积的特例</p><p><img src="/2021/12/27/GCN/11.JPG" alt></p><p><a href="https://zhuanlan.zhihu.com/p/139682302">https://zhuanlan.zhihu.com/p/139682302</a></p><p><a href="https://zhuanlan.zhihu.com/p/122968925">https://zhuanlan.zhihu.com/p/122968925</a></p><p><a href="https://blog.csdn.net/weixin_45901519/article/details/106388964">https://blog.csdn.net/weixin_45901519/article/details/106388964</a></p><p><a href="https://blog.csdn.net/weixin_45901519/article/details/106436591">https://blog.csdn.net/weixin_45901519/article/details/106436591</a></p><p><a href="https://blog.csdn.net/weixin_45901519/article/details/106492963">https://blog.csdn.net/weixin_45901519/article/details/106492963</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> GCN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GCN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bert系列之句向量生成</title>
      <link href="/2021/12/21/bert-emb/"/>
      <url>/2021/12/21/bert-emb/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/444346578">https://zhuanlan.zhihu.com/p/444346578</a></p><h2 id="1-sentence-bert"><a href="#1-sentence-bert" class="headerlink" title="1 sentence-bert"></a>1 sentence-bert</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据，nli有标签</p><p>无监督,有监督loss一样，文中有3种loss，区别在于数据集</p><p>无监督:nli有标签;有监督:sts有标签数据</p><h2 id="2-simcse"><a href="#2-simcse" class="headerlink" title="2 simcse"></a>2 simcse</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据</p><p>无监督，有监督区别在于：样本构造不同</p><p>无监督样本正负来源于sts无标签数据数据增强，有监督样本正负来源于sts有标签数据</p><h2 id="3-consert"><a href="#3-consert" class="headerlink" title="3 consert"></a>3 consert</h2><p>sts任务，数据分为sts无标签数据，sts有标签数据，还有nli数据集（有标签）</p><p><strong>相同</strong></p><p>和simcse相同之处：都是在finetune引入对比</p><p><strong>不同</strong></p><p>1 无监督</p><p>和simces loss一样为NT-Xent，不同在于sts无标签数据数据增强方式不同</p><p>2 有监督</p><p>区别在于loss和数据源</p><p>simcse loss为NT-Xent，数据源为sts有标签数据</p><p>consert  loss为  NT-Xent  +  别的有监督loss（比如cross entropy），数据源为sts无标签数据和nli数据集（有标签），+表示融合 ，论文有3种融合方式</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Learning to Rank From Pairwise Approach to Listwise Approach(listnet)</title>
      <link href="/2021/12/21/listnet/"/>
      <url>/2021/12/21/listnet/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p><p><a href="https://blog.csdn.net/Mr_tyting/article/details/80554849">https://blog.csdn.net/Mr_tyting/article/details/80554849</a></p><p>核心思想为：</p><ol><li>Given two lists of scores（模型和人）</li><li>we can first calculate two permutation probability distributions from them（简化到用top1）</li><li>and then calculate the distance between the two distributions as the listwise loss function.（交叉熵）</li></ol><h2 id="4-Probability-Models"><a href="#4-Probability-Models" class="headerlink" title="4. Probability Models"></a>4. Probability Models</h2><h3 id="4-1-Permutation-Probability"><a href="#4-1-Permutation-Probability" class="headerlink" title="4.1. Permutation Probability"></a>4.1. Permutation Probability</h3><p><img src="/2021/12/21/listnet/1.JPG" alt></p><p><img src="/2021/12/21/listnet/2.JPG" alt></p><p>$\pi=(2,3,1) $指的是对象2排在第一位</p><p>上面是topn的形式</p><p>因为总共有n！次排序组合</p><h3 id="4-2-Top-One-Probability"><a href="#4-2-Top-One-Probability" class="headerlink" title="4.2. Top One Probability"></a>4.2. Top One Probability</h3><p><strong>topk：</strong></p><script type="math/tex; mode=display">P_s(\pi)=\prod \limits_{j=1}^K\frac{\phi(S_{\pi(j)})}{\sum_{k=j}^n\phi(S_{\pi(k)})}</script><p>总共有N ! / ( N − k ) ! 种不同排列，大大减少了计算复杂度</p><p><strong>top1：</strong></p><p><img src="/2021/12/21/listnet/3.JPG" alt></p><p>此时有n种不同排列情况</p><p>概率分布的含义：对于每个j，分别都处于第一的概率是多少</p><h2 id="5-Learning-Method-ListNet"><a href="#5-Learning-Method-ListNet" class="headerlink" title="5.Learning Method: ListNet"></a>5.Learning Method: ListNet</h2><p>We employ a new learning method for optimizing the listwise loss function based on <strong>top one probability</strong>, with Neural Network as model and Gradient Descent as optimization algorithm. We refer to the method as ListNet.</p><p><img src="/2021/12/21/listnet/4.JPG" alt></p><p><img src="/2021/12/21/listnet/5.JPG" alt></p><p>$y^{(i)}$是真实的score list，有个疑问就是$y^{(i)}$怎么得到？关于这个，应该是先有真实的score list（人打），然后基于score list得到排序，参考 <a href="https://zhuanlan.zhihu.com/p/66514492">https://zhuanlan.zhihu.com/p/66514492</a></p><p><img src="/2021/12/21/listnet/6.JPG" alt></p><p><img src="/2021/12/21/listnet/7.JPG" alt></p><h2 id="核心步骤"><a href="#核心步骤" class="headerlink" title="核心步骤"></a>核心步骤</h2><p>1.打标得到真实的score list，模型得到预测的score list</p><p>2.然后用softmax得到真实的和预测的score list的概率分布</p><p>3.然后用交叉熵计算两种概率分布的差距</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> listwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> listnet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>listwise</title>
      <link href="/2021/12/20/listwise/"/>
      <url>/2021/12/20/listwise/</url>
      
        <content type="html"><![CDATA[<p>博客</p><p><a href="https://zhuanlan.zhihu.com/p/66514492">https://zhuanlan.zhihu.com/p/66514492</a></p><p>Listwise Approach to Learning to Rank - Theory and Algorithm（ListMLE）</p><p><a href="http://icml2008.cs.helsinki.fi/papers/167.pdf">http://icml2008.cs.helsinki.fi/papers/167.pdf</a></p><p>Learning to Rank: From Pairwise Approach to Listwise Approach（ListNet） </p><p><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> listwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> listwise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pairwise</title>
      <link href="/2021/12/20/pairwise/"/>
      <url>/2021/12/20/pairwise/</url>
      
        <content type="html"><![CDATA[<p>pairwise learning to rank 的方法可以分为两大类。</p><p>第一类是基于<strong>打分函数</strong>，它们通过一些特殊的设计让模型依靠“样本对”的信息来学习得到每个样本的score。所以得到这类方法最后的全局排序结果很简单，就是用所有样本的score来排序即可。</p><p>另一类方法是基于<strong>优先函数</strong>的方法。这类方法的整个过程分为两个阶段，第一阶段是用机器学习模型来学习两个样本之间的优先关系，例如f(x1, x2)=1表示样本x1优先于x2（x1应该排在x2前面），f(x1, x2)=-1表示样本x2优先于x1（x1应该排在x2后面）。从题主的问题来看，可能问的是“当我们已经训练出了优先函数f之后，如何对所有样本进行排序，并且使该排序在最大程度上与f的结果一致”。这个问题在学界被称为Rank Aggregation（排列聚合）。</p><p>具体参考 <a href="https://www.zhihu.com/question/389068269">https://www.zhihu.com/question/389068269</a></p><p>别的相关参考：</p><p><a href="https://www.jianshu.com/p/235756fbf6b6">https://www.jianshu.com/p/235756fbf6b6</a></p><p><a href="https://zhuanlan.zhihu.com/p/318300682">https://zhuanlan.zhihu.com/p/318300682</a></p><p><a href="https://zhuanlan.zhihu.com/p/65224450">https://zhuanlan.zhihu.com/p/65224450</a></p><p><a href="https://zhuanlan.zhihu.com/p/65756030">https://zhuanlan.zhihu.com/p/65756030</a></p><p><a href="https://www.zhihu.com/question/389068269/answer/1180120736">https://www.zhihu.com/question/389068269/answer/1180120736</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> pairwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pairwise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pointwise</title>
      <link href="/2021/12/20/pointwise/"/>
      <url>/2021/12/20/pointwise/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/113302654#">https://zhuanlan.zhihu.com/p/113302654#</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> pointwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pointwise </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调节学习率</title>
      <link href="/2021/12/18/lr/"/>
      <url>/2021/12/18/lr/</url>
      
        <content type="html"><![CDATA[<p>当学习率过大的时候会导致模型难以收敛，过小的时候会收敛速度过慢，合理的学习率才能让模型收敛到最小点而非局部最优点或鞍点</p><p>经验值： 0.01 ~ 0.001 </p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p>原因：起初距离目标偏离大，可以设置较大，为了快速收敛，后续逐渐靠近目标，需要精细化一点，所以希望值小一点</p><p><strong>分类</strong></p><p>1.轮数衰减</p><p>每经过k个epochs后学习率减半</p><p>2.指数衰减</p><script type="math/tex; mode=display">\alpha_t=decay\_rate^{epoch}*\alpha_{t-1}</script><p>3.分数衰减</p><script type="math/tex; mode=display">\alpha_t=\frac{\alpha_{t-1}}{1+decay\_rate*epoch}</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/LiuPeiP_VIPL/article/details/119581343">https://blog.csdn.net/LiuPeiP_VIPL/article/details/119581343</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 调节学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图神经工具</title>
      <link href="/2021/12/16/gnn-trick/"/>
      <url>/2021/12/16/gnn-trick/</url>
      
        <content type="html"><![CDATA[<p>PyG， DGL对比</p><p><a href="https://www.zhihu.com/question/399802947">https://www.zhihu.com/question/399802947</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyG,DGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>函数参数</title>
      <link href="/2021/12/15/python-paramerter/"/>
      <url>/2021/12/15/python-paramerter/</url>
      
        <content type="html"><![CDATA[<h2 id="1-默认参数"><a href="#1-默认参数" class="headerlink" title="1 默认参数"></a>1 默认参数</h2><p><a href="https://blog.csdn.net/weixin_41972881/article/details/81562731">https://blog.csdn.net/weixin_41972881/article/details/81562731</a></p><p><a href="https://blog.csdn.net/weixin_45775963/article/details/103696945">https://blog.csdn.net/weixin_45775963/article/details/103696945</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def fun(va1,va2=[]):</span><br><span class="line">    print(va2)</span><br><span class="line">    va2.append(va1)</span><br><span class="line">    return va2</span><br><span class="line">te1=fun(10)</span><br><span class="line">te1=fun(20)</span><br></pre></td></tr></table></figure><p>va2如果没有传参，采用默认的，默认的会变化，不是一直是[]</p><p>va2如果是外部的传参，以传参为主，会覆盖</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 函数参数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Enhanced-RCNN An Efficient Method for Learning Sentence Similarity</title>
      <link href="/2021/12/14/enhanced-rcnn/"/>
      <url>/2021/12/14/enhanced-rcnn/</url>
      
        <content type="html"><![CDATA[<p>特点：非预训练，参数量少</p><p><img src="/2021/12/14/enhanced-rcnn/1.JPG" alt></p><h2 id="1-input-encoding"><a href="#1-input-encoding" class="headerlink" title="1 input encoding"></a>1 input encoding</h2><p>得到两个encoding，RNN Encoding，RCNN Encoding</p><p>1 BiGRU</p><p><img src="/2021/12/14/enhanced-rcnn/2.JPG" alt></p><p>$\textbf{a}=\{a_1,a_2,…,a_{l_a}\},\textbf{a}$ 是句子，$l_a$ 是句子1的长度</p><p>得到RNN Encoding，$\overline{\textbf{p}}_i$统一表示$\overline{\textbf{a}}_i,\overline{\textbf{b}}_i$</p><p>2 CNN</p><p>在 BiGRU 编码的基础上，使用 CNN 来进行二次编码</p><p>结构如下，“newtork in network”,k 是卷积核的kernel size，比如k=1,卷积核为$1 \times 1$</p><p><img src="/2021/12/14/enhanced-rcnn/3.JPG" alt></p><p>对于每个 CNN 单元，具体的计算过程如下:</p><p><img src="/2021/12/14/enhanced-rcnn/4.JPG" alt></p><p>得到 RCNN Encoding $\widetilde{\textbf{p}}_i$</p><h2 id="2-Interactive-Sentence-Representation"><a href="#2-Interactive-Sentence-Representation" class="headerlink" title="2 Interactive Sentence Representation"></a>2 Interactive Sentence Representation</h2><p>1  Soft-attention Alignment</p><p>attention：</p><p><img src="/2021/12/14/enhanced-rcnn/5.JPG" alt></p><p>加了attention的rnn encoding：</p><p><img src="/2021/12/14/enhanced-rcnn/6.JPG" alt></p><p>2  Interaction Modeling</p><p><img src="/2021/12/14/enhanced-rcnn/7.JPG" alt></p><p>$\overline{\textbf{p}}$是rnn encoding</p><p>$\hat{}$是加了attention的rnn encoding</p><p>$\widetilde{}$是rcnn encoding</p><p>最终得到Interactive Sentence Representation为$\textbf{o}_a,\textbf{o}_b$</p><h2 id="3-Similarity-Modeling"><a href="#3-Similarity-Modeling" class="headerlink" title="3 Similarity Modeling"></a>3 Similarity Modeling</h2><p>1 Fusion Layer</p><p><img src="/2021/12/14/enhanced-rcnn/8.JPG" alt></p><p><img src="/2021/12/14/enhanced-rcnn/9.JPG" alt></p><p>g是门控函数</p><p><img src="/2021/12/14/enhanced-rcnn/10.JPG" alt></p><p>2  Label Prediction</p><p>全连接层</p><h2 id="4-loss"><a href="#4-loss" class="headerlink" title="4 loss"></a>4 loss</h2><p>交叉熵</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://sci-hub.st/10.1145/3366423.3379998">https://sci-hub.st/10.1145/3366423.3379998</a></p><p><a href="https://zhuanlan.zhihu.com/p/138061003">https://zhuanlan.zhihu.com/p/138061003</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Enhanced-RCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱</title>
      <link href="/2021/12/12/knowledgegraph/"/>
      <url>/2021/12/12/knowledgegraph/</url>
      
        <content type="html"><![CDATA[<p>东南的课程：</p><p><a href="https://github.com/npubird/KnowledgeGraphCourse">https://github.com/npubird/KnowledgeGraphCourse</a></p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-train, Prompt, and Predict A Systematic Survey of Prompting Methods in Natural Language Processing</title>
      <link href="/2021/12/12/Prompt_survey/"/>
      <url>/2021/12/12/Prompt_survey/</url>
      
        <content type="html"><![CDATA[<h2 id="0-和pre-train，finetune区别"><a href="#0-和pre-train，finetune区别" class="headerlink" title="0 和pre-train，finetune区别"></a>0 和pre-train，finetune区别</h2><p><img src="/2021/12/12/Prompt_survey/11.JPG" alt></p><p><img src="/2021/12/12/Prompt_survey/1.JPG" alt></p><p>prompt感觉是一种特殊的finetune方式，还是先pre-train然后prompt tuning</p><p>目的：prompt narrowing the gap between pre-training and fine-tuning</p><h2 id="1-怎么做"><a href="#1-怎么做" class="headerlink" title="1 怎么做"></a>1 怎么做</h2><p>3步</p><h4 id="1-Prompt-Addition"><a href="#1-Prompt-Addition" class="headerlink" title="1 Prompt Addition"></a>1 Prompt Addition</h4><p><img src="/2021/12/12/Prompt_survey/0.JPG" alt></p><p>$x^{‘}=f_{prompt}(x)$ x是input text</p><ol><li>Apply a template, which is a textual string that has two slots: an input slot [X] for input x and an answer slot<br>[Z] for an intermediate generated answer text z that will later be mapped into y.</li><li>Fill slot [X] with the input text x.</li></ol><h4 id="2-Answer-Search"><a href="#2-Answer-Search" class="headerlink" title="2 Answer Search"></a>2 Answer Search</h4><p><img src="/2021/12/12/Prompt_survey/5.JPG" alt></p><p>f：fills in the location [Z] in prompt $x^{‘}$ with the potential answer z</p><p>Z：a set of permissible values for z</p><h4 id="3-Answer-Mapping"><a href="#3-Answer-Mapping" class="headerlink" title="3 Answer Mapping"></a>3 Answer Mapping</h4><p>因为上面的 $\hat{z}$ 还不是  $\hat{y}$，比如情感分析，“excellent”, “fabulous”, “wonderful” -》positive</p><p>go from the highest-scoring answer $\hat{z}$ to the highest-scoring output  $\hat{y}$ </p><h4 id="4-举个例子，文本情感分类的任务"><a href="#4-举个例子，文本情感分类的任务" class="headerlink" title="4 举个例子，文本情感分类的任务"></a>4 举个例子，文本情感分类的任务</h4><p><strong>原来</strong></p><p> “ I love this movie.”  -》 positive</p><p><strong>现在</strong></p><p>1 $x=$ “ I love this movie.”  -》模板为： “ [x] Overall, it was a [z] movie.” -》$x^{‘}$为”I love this movie. Overall ,it was a [z] movie.”</p><p>2 下一步会进行答案搜索，顾名思义就是LM寻找填在[z] 处可以使得分数最高的文本 $\hat{z}$(比如”excellent”, “great”, “wonderful” )</p><p>3 最后是答案映射。有时LM填充的文本并非任务需要的最终形式(最终为positive，上述为”excellent”, “great”, “wonderful”)，因此要将此文本映射到最终的输出$\hat{y}$</p><h2 id="2-Prompt方法分类"><a href="#2-Prompt方法分类" class="headerlink" title="2 Prompt方法分类"></a>2 Prompt方法分类</h2><p><img src="/2021/12/12/Prompt_survey/2.JPG" alt></p><h2 id="3-Prompt-Engineering"><a href="#3-Prompt-Engineering" class="headerlink" title="3 Prompt Engineering"></a>3 Prompt Engineering</h2><p>1 one must first consider the prompt shape,</p><p>2 then decide whether to take a manual or automated approach to create prompts of the desired shape</p><h3 id="1-Prompt-Shape"><a href="#1-Prompt-Shape" class="headerlink" title="1 Prompt Shape"></a>1 Prompt Shape</h3><p>Prompt的形状主要指的是[X]和[Z]的位置和数量。</p><p>如果在句中，一般称这种prompt为<strong>cloze prompt</strong>；如果在句末，一般称这种prompt为<strong>prefix prompt</strong>。</p><p>在实际应用过程中选择哪一种主要取决于任务的形式和模型的类别。cloze prompts和Masked Language Model的训练方式非常类似，因此对于使用MLM的任务来说cloze prompts更加合适；对于生成任务来说，或者使用自回归LM解决的任务，prefix prompts就会更加合适；Full text reconstruction models较为通用，因此两种prompt均适用。另外，对于文本对的分类，prompt模板通常要给输入预留两个空，[x1]和[x2]。</p><h3 id="2-create-prompts"><a href="#2-create-prompts" class="headerlink" title="2 create prompts"></a>2 create prompts</h3><h4 id="1-Manual-Template-Engineering"><a href="#1-Manual-Template-Engineering" class="headerlink" title="1 Manual Template Engineering"></a>1 Manual Template Engineering</h4><h4 id="2-Automated-Template-Learning"><a href="#2-Automated-Template-Learning" class="headerlink" title="2 Automated Template Learning"></a>2 Automated Template Learning</h4><h5 id="1-Discrete-Prompts"><a href="#1-Discrete-Prompts" class="headerlink" title="1 Discrete Prompts"></a>1 Discrete Prompts</h5><p>the prompt 作用在文本上</p><p>D1: Prompt Mining</p><p>D2: Prompt Paraphrasing</p><p>D3: Gradient-based Search</p><p>D4: Prompt Generation</p><p>D5: Prompt Scoring</p><h5 id="2-Continuous-Prompts"><a href="#2-Continuous-Prompts" class="headerlink" title="2 Continuous Prompts"></a>2 Continuous Prompts</h5><p>the prompt 直接作用到模型的embedding空间</p><p>C1: Prefix Tuning</p><p>C2: Tuning Initialized with Discrete Prompts</p><p>C3: Hard-Soft Prompt Hybrid Tuning</p><h2 id="4-Answer-Engineering"><a href="#4-Answer-Engineering" class="headerlink" title="4 Answer Engineering"></a>4 Answer Engineering</h2><p>two dimensions that must be considered when performing answer<br>engineering:1  deciding the answer shape and 2  choosing an answer design method.</p><h4 id="1-Answer-Shape"><a href="#1-Answer-Shape" class="headerlink" title="1 Answer Shape"></a>1 Answer Shape</h4><p>和Prompt Shape啥区别？？？</p><h4 id="2-Answer-Space-Design-Methods"><a href="#2-Answer-Space-Design-Methods" class="headerlink" title="2 Answer Space Design Methods"></a>2 Answer Space Design Methods</h4><h5 id="1-Manual-Design"><a href="#1-Manual-Design" class="headerlink" title="1 Manual Design"></a>1 Manual Design</h5><h5 id="2-automatic-automatic"><a href="#2-automatic-automatic" class="headerlink" title="2 automatic automatic"></a>2 automatic automatic</h5><h6 id="1-Discrete-Answer-Search"><a href="#1-Discrete-Answer-Search" class="headerlink" title="1 Discrete Answer Search"></a>1 Discrete Answer Search</h6><h6 id="2-Continuous-Answer-Search"><a href="#2-Continuous-Answer-Search" class="headerlink" title="2 Continuous Answer Search"></a>2 Continuous Answer Search</h6><h2 id="5-Multi-Prompt-Learning"><a href="#5-Multi-Prompt-Learning" class="headerlink" title="5 Multi-Prompt Learning"></a>5 Multi-Prompt Learning</h2><p>之前在讨论single prompt，现在介绍multiple prompts</p><p><img src="/2021/12/12/Prompt_survey/3.JPG" alt></p><h2 id="6-Training-Strategies-for-Prompting-Methods"><a href="#6-Training-Strategies-for-Prompting-Methods" class="headerlink" title="6 Training Strategies for Prompting Methods"></a>6 Training Strategies for Prompting Methods</h2><p>1 Training Settings</p><p>full-data</p><p>few-shot /zero-shot</p><p>2 Parameter Update Methods</p><p><img src="/2021/12/12/Prompt_survey/4.JPG" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/2107.13586">https://arxiv.org/abs/2107.13586</a></p><p>刘鹏飞博士 <a href="https://zhuanlan.zhihu.com/p/395115779">https://zhuanlan.zhihu.com/p/395115779</a></p><p><a href="https://zhuanlan.zhihu.com/p/399295895">https://zhuanlan.zhihu.com/p/399295895</a></p><p><a href="https://zhuanlan.zhihu.com/p/440169921">https://zhuanlan.zhihu.com/p/440169921</a></p><p><a href="https://zhuanlan.zhihu.com/p/399295895">https://zhuanlan.zhihu.com/p/399295895</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Prompt </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prompt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generalizing from a Few Examples A Survey on Few-Shot Learning</title>
      <link href="/2021/12/12/few-shot/"/>
      <url>/2021/12/12/few-shot/</url>
      
        <content type="html"><![CDATA[<p>paper：  <a href="https://arxiv.org/abs/1904.05046">https://arxiv.org/abs/1904.05046</a></p><p>git:  <a href="https://github.com/tata1661/FSL-Mate/tree/master/FewShotPapers#Applications">https://github.com/tata1661/FSL-Mate/tree/master/FewShotPapers#Applications</a></p><p>原文按应用对FSL做了总结，与NLP相关的有：</p><ol><li><strong>High-risk learning: Acquiring new word vectors from tiny data,</strong> in EMNLP, 2017. <em>A. Herbelot and M. Baroni.</em> <a href="https://www.aclweb.org/anthology/D17-1030.pdf">paper</a></li><li><strong>MetaEXP: Interactive explanation and exploration of large knowledge graphs,</strong> in TheWebConf, 2018. <em>F. Behrens, S. Bischoff, P. Ladenburger, J. Rückin, L. Seidel, F. Stolp, M. Vaichenker, A. Ziegler, D. Mottin, F. Aghaei, E. Müller, M. Preusse, N. Müller, and M. Hunger.</em> <a href="https://meta-exp.github.io/resources/paper.pdf">paper</a> <a href="https://hpi.de/en/mueller/metaex">code</a></li><li><strong>Few-shot representation learning for out-of-vocabulary words,</strong> in ACL, 2019. <em>Z. Hu, T. Chen, K.-W. Chang, and Y. Sun.</em> <a href="https://www.aclweb.org/anthology/P19-1402.pdf">paper</a></li><li><strong>Learning to customize model structures for few-shot dialogue generation tasks,</strong> in ACL, 2020. <em>Y. Song, Z. Liu, W. Bi, R. Yan, and M. Zhang.</em> <a href="https://www.aclweb.org/anthology/2020.acl-main.517.pdf">paper</a></li><li><strong>Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network,</strong> in ACL, 2020. <em>Y. Hou, W. Che, Y. Lai, Z. Zhou, Y. Liu, H. Liu, and T. Liu.</em> <a href="https://www.aclweb.org/anthology/2020.acl-main.128.pdf">paper</a></li><li><strong>Meta-reinforced multi-domain state generator for dialogue systems,</strong> in ACL, 2020. <em>Y. Huang, J. Feng, M. Hu, X. Wu, X. Du, and S. Ma.</em> <a href="https://www.aclweb.org/anthology/2020.acl-main.636.pdf">paper</a></li><li><strong>Few-shot knowledge graph completion,</strong> in AAAI, 2020. <em>C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V. Chawla.</em> <a href="https://aaai.org/ojs/index.php/AAAI/article/view/5698">paper</a></li><li><strong>Universal natural language processing with limited annotations: Try few-shot textual entailment as a start,</strong> in EMNLP, 2020. <em>W. Yin, N. F. Rajani, D. Radev, R. Socher, and C. Xiong.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.660.pdf">paper</a> <a href="https://github.com/salesforce/UniversalFewShotNLP">code</a></li><li><strong>Simple and effective few-shot named entity recognition with structured nearest neighbor learning,</strong> in EMNLP, 2020. <em>Y. Yang, and A. Katiyar.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.516.pdf">paper</a> <a href="https://github.com/asappresearch/structshot">code</a></li><li><strong>Discriminative nearest neighbor few-shot intent detection by transferring natural language inference,</strong> in EMNLP, 2020. <em>J. Zhang, K. Hashimoto, W. Liu, C. Wu, Y. Wan, P. Yu, R. Socher, and C. Xiong.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.411.pdf">paper</a> <a href="https://github.com/salesforce/DNNC-few-shot-intent">code</a></li><li><strong>Few-shot learning for opinion summarization,</strong> in EMNLP, 2020. <em>A. Bražinskas, M. Lapata, and I. Titov.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.337.pdf">paper</a> <a href="https://github.com/abrazinskas/FewSum">code</a></li><li><strong>Adaptive attentional network for few-shot knowledge graph completion,</strong> in EMNLP, 2020. <em>J. Sheng, S. Guo, Z. Chen, J. Yue, L. Wang, T. Liu, and H. Xu.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.131.pdf">paper</a> <a href="https://github.com/JiaweiSheng/FAAN">code</a></li><li><strong>Few-shot complex knowledge base question answering via meta reinforcement learning,</strong> in EMNLP, 2020. <em>Y. Hua, Y. Li, G. Haffari, G. Qi, and T. Wu.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.469.pdf">paper</a> <a href="https://github.com/DevinJake/MRL-CQA">code</a></li><li><strong>Self-supervised meta-learning for few-shot natural language classification tasks,</strong> in EMNLP, 2020. <em>T. Bansal, R. Jha, T. Munkhdalai, and A. McCallum.</em> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.38.pdf">paper</a> <a href="https://github.com/iesl/metanlp">code</a></li><li><strong>Uncertainty-aware self-training for few-shot text classification,</strong> in NeurIPS, 2020. <em>S. Mukherjee, and A. Awadallah.</em> <a href="https://proceedings.neurips.cc/paper/2020/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf">paper</a> <a href="https://github.com/microsoft/UST">code</a></li><li><strong>Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction,</strong> in NeurIPS, 2020:. <em>J. Baek, D. B. Lee, and S. J. Hwang.</em> <a href="https://proceedings.neurips.cc/paper/2020/file/0663a4ddceacb40b095eda264a85f15c-Paper.pdf">paper</a> <a href="https://github.com/JinheonBaek/GEN">code</a></li><li><strong>MetaNER: Named entity recognition with meta-learning,</strong> in TheWebConf, 2020. <em>J. Li, S. Shang, and L. Shao.</em> <a href="https://dl.acm.org/doi/10.1145/3366423.3380127">paper</a></li><li><strong>Conditionally adaptive multi-task learning: Improving transfer learning in NLP using fewer parameters &amp; less data,</strong> in ICLR, 2021. <em>J. Pilault, A. E. hattami, and C. Pal.</em> <a href="https://openreview.net/pdf?id=de11dbHzAMF">paper</a> <a href="https://github.com/CAMTL/CA-MTL">code</a></li><li><strong>Revisiting few-sample BERT fine-tuning,</strong> in ICLR, 2021. <em>T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y. Artzi.</em> <a href="https://openreview.net/pdf?id=cO1IH43yUF">paper</a> <a href="https://pytorch.org/docs/1.4.0/_modules/torch/optim/adamw.html">code</a></li><li><strong>Few-shot conversational dense retrieval,</strong> in SIGIR, 2021. <em>S. Yu, Z. Liu, C. Xiong, T. Feng, and Z. Liu.</em> <a href="https://dl.acm.org/doi/pdf/10.1145/3404835.3462856">paper</a> <a href="https://github.com/thunlp/ConvDR">code</a></li><li><strong>Relational learning with gated and attentive neighbor aggregator for few-shot knowledge graph completion,</strong> in SIGIR, 2021. <em>G. Niu, Y. Li, C. Tang, R. Geng, J. Dai, Q. Liu, H. Wang, J. Sun, F. Huang, and L. Si.</em> <a href="https://dl.acm.org/doi/pdf/10.1145/3404835.3462925">paper</a></li><li><strong>Few-shot language coordination by modeling theory of mind,</strong> in ICML, 2021. <em>H. Zhu, G. Neubig, and Y. Bisk.</em> <a href="http://proceedings.mlr.press/v139/zhu21d/zhu21d.pdf">paper</a> <a href="https://github.com/CLAW-Lab/ToM">code</a></li><li><strong>Graph-evolving meta-learning for low-resource medical dialogue generation,</strong> in AAAI, 2021. <em>S. Lin, P. Zhou, X. Liang, J. Tang, R. Zhao, Z. Chen, and L. Lin.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17577/17384">paper</a></li><li><strong>KEML: A knowledge-enriched meta-learning framework for lexical relation classification,</strong> in AAAI, 2021. <em>C. Wang, M. Qiu, J. Huang, and X. He.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17640/17447">paper</a></li><li><strong>Few-shot learning for multi-label intent detection,</strong> in AAAI, 2021. <em>Y. Hou, Y. Lai, Y. Wu, W. Che, and T. Liu.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17541/17348">paper</a> <a href="https://github.com/AtmaHou/FewShotMultiLabel">code</a></li><li><strong>SALNet: Semi-supervised few-shot text classification with attention-based lexicon construction,</strong> in AAAI, 2021. <em>J.-H. Lee, S.-K. Ko, and Y.-S. Han.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17558/17365">paper</a></li><li><strong>Learning from my friends: Few-shot personalized conversation systems via social networks,</strong> in AAAI, 2021. <em>Z. Tian, W. Bi, Z. Zhang, D. Lee, Y. Song, and N. L. Zhang.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17638/17445">paper</a> <a href="https://github.com/tianzhiliang/FewShotPersonaConvData">code</a></li><li><strong>Relative and absolute location embedding for few-shot node classification on graph,</strong> in AAAI, 2021. <em>Z. Liu, Y. Fang, C. Liu, and S. C.H. Hoi.</em> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16551/16358">paper</a></li><li><strong>Few-shot question answering by pretraining span selection,</strong> in ACL-IJCNLP, 2021. <em>O. Ram, Y. Kirstain, J. Berant, A. Globerson, and O. Levy.</em> <a href="https://aclanthology.org/2021.acl-long.239.pdf">paper</a> <a href="https://github.com/oriram/splinter">code</a></li><li><strong>A closer look at few-shot crosslingual transfer: The choice of shots matters,</strong> in ACL-IJCNLP, 2021. <em>M. Zhao, Y. Zhu, E. Shareghi, I. Vulic, R. Reichart, A. Korhonen, and H. Schütze.</em> <a href="https://aclanthology.org/2021.acl-long.447.pdf">paper</a> <a href="https://github.com/fsxlt">code</a></li><li><strong>Learning from miscellaneous other-classwords for few-shot named entity recognition,</strong> in ACL-IJCNLP, 2021. <em>M. Tong, S. Wang, B. Xu, Y. Cao, M. Liu, L. Hou, and J. Li.</em> <a href="https://aclanthology.org/2021.acl-long.487.pdf">paper</a> <a href="https://github.com/shuaiwa16/OtherClassNER.git">code</a></li><li><strong>Distinct label representations for few-shot text classification,</strong> in ACL-IJCNLP, 2021. <em>S. Ohashi, J. Takayama, T. Kajiwara, and Y. Arase.</em> <a href="https://aclanthology.org/2021.acl-short.105.pdf">paper</a> <a href="https://github.com/21335732529sky/difference_extractor">code</a></li><li><strong>Entity concept-enhanced few-shot relation extraction,</strong> in ACL-IJCNLP, 2021. <em>S. Yang, Y. Zhang, G. Niu, Q. Zhao, and S. Pu.</em> <a href="https://aclanthology.org/2021.acl-short.124.pdf">paper</a> <a href="https://github.com/LittleGuoKe/ConceptFERE">code</a></li><li><strong>On training instance selection for few-shot neural text generation,</strong> in ACL-IJCNLP, 2021. <em>E. Chang, X. Shen, H.-S. Yeh, and V. Demberg.</em> <a href="https://aclanthology.org/2021.acl-short.2.pdf">paper</a> <a href="https://gitlab.com/erniecyc/few-selector">code</a></li><li><strong>Unsupervised neural machine translation for low-resource domains via meta-learning,</strong> in ACL-IJCNLP, 2021. <em>C. Park, Y. Tae, T. Kim, S. Yang, M. A. Khan, L. Park, and J. Choo.</em> <a href="https://aclanthology.org/2021.acl-long.225.pdf">paper</a> <a href="https://github.com/papago-lab/MetaGUMT">code</a></li><li><strong>Meta-learning with variational semantic memory for word sense disambiguation,</strong> in ACL-IJCNLP, 2021. <em>Y. Du, N. Holla, X. Zhen, C. Snoek, and E. Shutova.</em> <a href="https://aclanthology.org/2021.acl-long.409.pdf">paper</a> <a href="https://github.com/YDU-uva/VSM_WSD">code</a></li><li><strong>Multi-label few-shot learning for aspect category detection,</strong> in ACL-IJCNLP, 2021. <em>M. Hu, S. Z. H. Guo, C. Xue, H. Gao, T. Gao, R. Cheng, and Z. Su.</em> <a href="https://aclanthology.org/2021.acl-long.495.pdf">paper</a></li><li><strong>TextSETTR: Few-shot text style extraction and tunable targeted restyling,</strong> in ACL-IJCNLP, 2021. <em>P. Rileya, N. Constantb, M. Guob, G. Kumarc, D. Uthusb, and Z. Parekh.</em> <a href="https://aclanthology.org/2021.acl-long.293.pdf">paper</a></li><li><strong>Few-shot text ranking with meta adapted synthetic weak supervision,</strong> in ACL-IJCNLP, 2021. <em>S. Sun, Y. Qian, Z. Liu, C. Xiong, K. Zhang, J. Bao, Z. Liu, and P. Bennett.</em> <a href="https://aclanthology.org/2021.acl-long.390.pdf">paper</a> <a href="https://github.com/thunlp/MetaAdaptRank">code</a></li><li><strong>PROTAUGMENT: Intent detection meta-learning through unsupervised diverse paraphrasing,</strong> in ACL-IJCNLP, 2021. <em>T. Dopierre, C. Gravier, and W. Logerais.</em> <a href="https://aclanthology.org/2021.acl-long.191.pdf">paper</a> <a href="https://github.com/tdopierre/ProtAugment">code</a></li><li><strong>AUGNLG: Few-shot natural language generation using self-trained data augmentation,</strong> in ACL-IJCNLP, 2021. <em>X. Xu, G. Wang, Y.-B. Kim, and S. Lee.</em> <a href="https://aclanthology.org/2021.acl-long.95.pdf">paper</a> <a href="https://github.com/XinnuoXu/AugNLG">code</a></li><li><strong>Meta self-training for few-shot neural sequence labeling,</strong> in KDD, 2021. <em>Y. Wang, S. Mukherjee, H. Chu, Y. Tu, M. Wu, J. Gao, and A. H. Awadallah.</em> <a href="https://dl.acm.org/doi/pdf/10.1145/3447548.3467235">paper</a> <a href="https://github.com/microsoft/MetaST">code</a></li><li><strong>Knowledge-enhanced domain adaptation in few-shot relation classification,</strong> in KDD, 2021. <em>J. Zhang, J. Zhu, Y. Yang, W. Shi, C. Zhang, and H. Wang.</em> <a href="https://dl.acm.org/doi/pdf/10.1145/3447548.3467438">paper</a> <a href="https://github.com/imJiawen/KEFDA">code</a></li><li><strong>Few-shot text classification with triplet networks, data augmentation, and curriculum learning,</strong> in NAACL-HLT, 2021. <em>J. Wei, C. Huang, S. Vosoughi, Y. Cheng, and S. Xu.</em> <a href="https://aclanthology.org/2021.naacl-main.434.pdf">paper</a> <a href="https://github.com/jasonwei20/triplet-loss">code</a></li><li><strong>Few-shot intent classification and slot filling with retrieved examples,</strong> in NAACL-HLT, 2021. <em>D. Yu, L. He, Y. Zhang, X. Du, P. Pasupat, and Q. Li.</em> <a href="https://aclanthology.org/2021.naacl-main.59.pdf">paper</a></li><li><strong>Non-parametric few-shot learning for word sense disambiguation,</strong> in NAACL-HLT, 2021. <em>H. Chen, M. Xia, and D. Chen.</em> <a href="https://aclanthology.org/2021.naacl-main.142.pdf">paper</a> <a href="https://github.com/princeton-nlp/metric-wsd">code</a></li><li><strong>Towards few-shot fact-checking via perplexity,</strong> in NAACL-HLT, 2021. <em>N. Lee, Y. Bang, A. Madotto, and P. Fung.</em> <a href="https://aclanthology.org/2021.naacl-main.158.pdf">paper</a></li><li><strong>ConVEx: Data-efficient and few-shot slot labeling,</strong> in NAACL-HLT, 2021. <em>M. Henderson, and I. Vulic.</em> <a href="https://aclanthology.org/2021.naacl-main.264.pdf">paper</a></li><li><strong>Few-shot text generation with natural language instructions,</strong> in EMNLP, 2021. <em>T. Schick, and H. Schütze.</em> <a href="https://aclanthology.org/2021.emnlp-main.32.pdf">paper</a></li><li><strong>Towards realistic few-shot relation extraction,</strong> in EMNLP, 2021. <em>S. Brody, S. Wu, and A. Benton.</em> <a href="https://aclanthology.org/2021.emnlp-main.433.pdf">paper</a> <a href="https://github.com/bloomberg/emnlp21_fewrel">code</a></li><li><strong>Few-shot emotion recognition in conversation with sequential prototypical networks,</strong> in EMNLP, 2021. <em>G. Guibon, M. Labeau, H. Flamein, L. Lefeuvre, and C. Clavel.</em> <a href="https://aclanthology.org/2021.emnlp-main.549.pdf">paper</a> <a href="https://github.com/gguibon/protoseq">code</a></li><li><strong>Learning prototype representations across few-shot tasks for event detection,</strong> in EMNLP, 2021. <em>V. Lai, F. Dernoncourt, and T. H. Nguyen.</em> <a href="https://aclanthology.org/2021.emnlp-main.427.pdf">paper</a></li><li><strong>Exploring task difficulty for few-shot relation extraction,</strong> in EMNLP, 2021. <em>J. Han, B. Cheng, and W. Lu.</em> <a href="https://aclanthology.org/2021.emnlp-main.204.pdf">paper</a> <a href="https://github.com/hanjiale/hcrp">code</a></li><li><strong>Honey or poison? Solving the trigger curse in few-shot event detection via causal intervention,</strong> in EMNLP, 2021. <em>J. Chen, H. Lin, X. Han, and L. Sun.</em> <a href="https://aclanthology.org/2021.emnlp-main.637.pdf">paper</a> <a href="https://github.com/chen700564/causalfsed">code</a></li><li><strong>Nearest neighbour few-shot learning for cross-lingual classification,</strong> in EMNLP, 2021. <em>M. S. Bari, B. Haider, and S. Mansour.</em> <a href="https://aclanthology.org/2021.emnlp-main.131.pdf">paper</a></li><li><strong>Knowledge-aware meta-learning for low-resource text classification,</strong> in EMNLP, 2021. <em>H. Yao, Y. Wu, M. Al-Shedivat, and E. P. Xing.</em> <a href="https://aclanthology.org/2021.emnlp-main.136.pdf">paper</a> <a href="https://github.com/huaxiuyao/KGML">code</a></li><li><strong>Few-shot named entity recognition: An empirical baseline study,</strong> in EMNLP, 2021. <em>J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng, J. Gao, and J. Han.</em> <a href="https://aclanthology.org/2021.emnlp-main.813.pdf">paper</a></li><li><strong>MetaTS: Meta teacher-student network for multilingual sequence labeling with minimal supervision,</strong> in EMNLP, 2021. <em>Z. Li, D. Zhang, T. Cao, Y. Wei, Y. Song, and B. Yin.</em> <a href="https://aclanthology.org/2021.emnlp-main.255.pdf">paper</a></li><li><strong>Meta-LMTC: Meta-learning for large-scale multi-label text classification,</strong> in EMNLP, 2021. <em>R. Wang, X. Su, S. Long, X. Dai, S. Huang, and J. Chen.</em> <a href="https://aclanthology.org/2021.emnlp-main.679.pdf">paper</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小样本 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 小样本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AutoTokenizer和BertTokenizer区别</title>
      <link href="/2021/12/09/autotokenizer/"/>
      <url>/2021/12/09/autotokenizer/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/huggingface/transformers/issues/5587">https://github.com/huggingface/transformers/issues/5587</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AutoTokenizer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>继承</title>
      <link href="/2021/12/09/python_succed/"/>
      <url>/2021/12/09/python_succed/</url>
      
        <content type="html"><![CDATA[<h2 id="1-super"><a href="#1-super" class="headerlink" title="1 super"></a>1 super</h2><p>父和子都有init</p><p>子类把父类的 __init__()放到自己的 __init__() 当中  , 这样子类就有了父类的 __init__() 的那些东西</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">class test1:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.a=1</span><br><span class="line"></span><br><span class="line">class test2(test1):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(test2, self).__init__()</span><br><span class="line">        self.b=2</span><br><span class="line"></span><br><span class="line">tt=test2()</span><br><span class="line"># print(tt.a)</span><br><span class="line">print(tt.b)</span><br><span class="line">print(tt.a)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">############################</span><br><span class="line">class test1:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.a=1</span><br><span class="line"></span><br><span class="line">class test2(test1):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # super(test2, self).__init__()</span><br><span class="line">        self.b=2</span><br><span class="line"></span><br><span class="line">tt=test2()</span><br><span class="line"># print(tt.a)</span><br><span class="line">print(tt.b)</span><br><span class="line">print(tt.a)</span><br><span class="line"></span><br><span class="line">2</span><br><span class="line">AttributeError: &#x27;test2&#x27; object has no attribute &#x27;a&#x27;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class pointwise_hybird_contrasive(hybird):</span><br><span class="line">    def __init__(self,config_roberta, path,num):</span><br><span class="line">        super(pointwise_hybird_contrasive, self).__init__(config_roberta, path,num)</span><br><span class="line">super(pointwise_hybird_contrasive, self).\__init\__(config_roberta, path,num)就是对父类hybird的属性进行初始化</span><br></pre></td></tr></table></figure><h2 id="2-继承"><a href="#2-继承" class="headerlink" title="2 继承"></a>2 继承</h2><p>子没有重写，则继承父</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class A:</span><br><span class="line">    x=1</span><br><span class="line">class B(A):</span><br><span class="line">    pass</span><br><span class="line">class C(A):</span><br><span class="line">    pass</span><br><span class="line">B.x=2</span><br><span class="line">print(A.x,B.x,C.x)</span><br><span class="line">A.x=3</span><br><span class="line">print(A.x,B.x,C.x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1 2 1</span><br><span class="line">3 2 3</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/zyh19980527/article/details/107206483">https://blog.csdn.net/zyh19980527/article/details/107206483</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 继承 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch常见操作</title>
      <link href="/2021/12/09/torch-normal/"/>
      <url>/2021/12/09/torch-normal/</url>
      
        <content type="html"><![CDATA[<h2 id="1-pytorch中对tensor操作"><a href="#1-pytorch中对tensor操作" class="headerlink" title="1 pytorch中对tensor操作"></a>1 pytorch中对tensor操作</h2><p><a href="https://blog.csdn.net/HailinPan/article/details/109818774">https://blog.csdn.net/HailinPan/article/details/109818774</a></p><h2 id="2-模型加载"><a href="#2-模型加载" class="headerlink" title="2 模型加载"></a>2 模型加载</h2><p>1 model.load_state_dict(torch.load(path))</p><p>2 model=BertModel.from_pretrained</p><p>后者的底层为前者</p><p>用法不同，前者model为一个对象，然后用load_state_dict加载权重；后者BertModel为一个类，然后用from_pretrained创建对象并加载权重</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch常见操作 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>huggingface</title>
      <link href="/2021/12/07/huggingface/"/>
      <url>/2021/12/07/huggingface/</url>
      
        <content type="html"><![CDATA[<p>NLP小帮手，huggingface的transformer</p><p>git： <a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a></p><p>paper： <a href="https://arxiv.org/abs/1910.03771v5">https://arxiv.org/abs/1910.03771v5</a></p><p>整体结构</p><p><img src="/2021/12/07/huggingface/11.JPG" alt></p><p>简单教程：</p><p><a href="https://blog.csdn.net/weixin_44614687/article/details/106800244">https://blog.csdn.net/weixin_44614687/article/details/106800244</a></p><h2 id="from-pretrained"><a href="#from-pretrained" class="headerlink" title="from_pretrained"></a>from_pretrained</h2><p> 底层为load_state_dict</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Some weights of the model checkpoint at ../../../../test/data/chinese-roberta-wwm-ext were not used when initializing listnet_bert: [&#x27;cls.predictions.transform.dense.weight&#x27;, &#x27;cls.seq_relationship.bias&#x27;, &#x27;cls.predictions.transform.dense.bias&#x27;, &#x27;cls.predictions.decoder.weight&#x27;, &#x27;cls.predictions.transform.LayerNorm.bias&#x27;, &#x27;cls.seq_relationship.weight&#x27;, &#x27;cls.predictions.bias&#x27;, &#x27;cls.predictions.transform.LayerNorm.weight&#x27;]</span><br><span class="line">- This IS expected if you are initializing listnet_bert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).</span><br><span class="line">- This IS NOT expected if you are initializing listnet_bert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</span><br><span class="line">Some weights of listnet_bert were not initialized from the model checkpoint at ../../../../test/data/chinese-roberta-wwm-ext and are newly initialized: [&#x27;Linear2.weight&#x27;, &#x27;Linear1.weight&#x27;, &#x27;Linear1.bias&#x27;, &#x27;Linear2.bias&#x27;]</span><br><span class="line">You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">两部分：1 加载的预训练模型中有参数没有用到  2 自己的模型有参数没有初始化</span><br><span class="line">finetune的时候报这个 很正常</span><br><span class="line">predict的时候应该不会有</span><br></pre></td></tr></table></figure><h2 id="关于model"><a href="#关于model" class="headerlink" title="关于model"></a>关于model</h2><p>BertModel -&gt; our model </p><p>1 加载transformers中的模型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertPreTrainedModel, BertModel,AutoTokenizer,AutoConfig</span><br></pre></td></tr></table></figure><p>2  基于1中的模型搭建自己的结构</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> huggingface </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch中模型的forward方法是如何被自动调用的</title>
      <link href="/2021/12/07/forward/"/>
      <url>/2021/12/07/forward/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/weixin_41912543/article/details/108147378">https://blog.csdn.net/weixin_41912543/article/details/108147378</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> forward </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer-XL  Attentive Language Models Beyond a Fixed-Length Context</title>
      <link href="/2021/12/06/transformer-xl/"/>
      <url>/2021/12/06/transformer-xl/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1901.02860">https://arxiv.org/abs/1901.02860</a></p><p>Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling ( memory and computation受限，长度不可能很大 ).  propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence.</p><h2 id="3-Model"><a href="#3-Model" class="headerlink" title="3 Model"></a>3 Model</h2><h3 id="3-1-Vanilla-Transformer-Language-Models"><a href="#3-1-Vanilla-Transformer-Language-Models" class="headerlink" title="3.1 Vanilla Transformer Language Models"></a>3.1 Vanilla Transformer Language Models</h3><p><img src="/2021/12/06/transformer-xl/1.JPG" alt></p><p>问题：In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.通常做法为vanilla model。 vanilla model就是说把长文本分隔成固定长度的seg来处理，如上图。</p><p>During  training，There are two critical limitations of using a fixed length context. First, the largest possible dependency length is upper bounded by the segment length. Second. simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem</p><p>During evaluation, As shown in Fig. 1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive.</p><h3 id="3-2-Segment-Level-Recurrence-with-State-Reuse"><a href="#3-2-Segment-Level-Recurrence-with-State-Reuse" class="headerlink" title="3.2 Segment-Level Recurrence with State Reuse"></a>3.2 Segment-Level Recurrence with State Reuse</h3><p> introduce a <strong>recurrence mechanism</strong> to the Transformer architecture.</p><p>定义变量</p><p><img src="/2021/12/06/transformer-xl/2.JPG" alt></p><p>转换过程</p><p><img src="/2021/12/06/transformer-xl/3.JPG" alt></p><p>SG(） stands for stop-gradient，$\circ$ 表示矩阵拼接</p><p>具体过程如下图</p><p><img src="/2021/12/06/transformer-xl/4.JPG" alt></p><p>During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a.</p><p>during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model.</p><h3 id="3-3-Relative-Positional-Encodings"><a href="#3-3-Relative-Positional-Encodings" class="headerlink" title="3.3 Relative Positional Encodings"></a>3.3 Relative Positional Encodings</h3><p>how can we keep the positional information coherent when we reuse the states? 如果保留原来的位置编码形式，可以得到如下</p><p><img src="/2021/12/06/transformer-xl/5.JPG" alt></p><p>这种方式存在问题：</p><p><img src="/2021/12/06/transformer-xl/9.JPG" alt></p><p>为了解决这个问题提出了relative positional information。</p><p>standard Transformer</p><p><img src="/2021/12/06/transformer-xl/6.JPG" alt></p><p>we propose</p><p><img src="/2021/12/06/transformer-xl/7.JPG" alt></p><h3 id="3-4-完整算法流程"><a href="#3-4-完整算法流程" class="headerlink" title="3.4 完整算法流程"></a>3.4 完整算法流程</h3><p><img src="/2021/12/06/transformer-xl/8.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 特征提取器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer-XL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling</title>
      <link href="/2021/12/02/hcan/"/>
      <url>/2021/12/02/hcan/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_EMNLP2019.pdf">https://cs.uwaterloo.ca/~jimmylin/publications/Rao_etal_EMNLP2019.pdf</a></p><h2 id="2-HCAN-Hybrid-Co-Attention-Network"><a href="#2-HCAN-Hybrid-Co-Attention-Network" class="headerlink" title="2 HCAN: Hybrid Co-Attention Network"></a>2 HCAN: Hybrid Co-Attention Network</h2><p><img src="/2021/12/02/hcan/11.JPG" alt></p><p>three major components: (1) a hybrid encoder (2) a relevance matching module (3) a semantic matching module</p><h3 id="2-1-Hybrid-Encoders"><a href="#2-1-Hybrid-Encoders" class="headerlink" title="2.1 Hybrid Encoders"></a>2.1 Hybrid Encoders</h3><p>hybrid encoder module that explores three types of encoders: <strong>deep, wide, and contextual</strong></p><p>query and context words :$\{w_1^q,w_2^q,…,w_n^q\},\{w_1^c,w_2^c,…,w_m^c\}$, embedding representations $\textbf{Q}\in \mathbb{R}^{n\times L},\textbf{C}\in \mathbb{R}^{m\times L}$</p><p><strong>Deep Encoder</strong></p><p><img src="/2021/12/02/hcan/2.JPG" alt></p><p>$\textbf{U}$表示$\textbf{Q},\textbf{C}$</p><p><strong>Wide Encoder</strong></p><p>Unlike the deep encoder that stacks multiple convolutional layers hierarchically, the wide encoder organizes convolutional layers in parallel, with each convolutional layer having a different window size k</p><p><strong>Contextual Encoder</strong></p><p><img src="/2021/12/02/hcan/3.JPG" alt></p><h3 id="2-2-Relevance-Matching"><a href="#2-2-Relevance-Matching" class="headerlink" title="2.2 Relevance Matching"></a>2.2 Relevance Matching</h3><p><img src="/2021/12/02/hcan/4.JPG" alt></p><p><img src="/2021/12/02/hcan/5.JPG" alt></p><p><img src="/2021/12/02/hcan/6.JPG" alt></p><h3 id="2-3-Semantic-Matching"><a href="#2-3-Semantic-Matching" class="headerlink" title="2.3 Semantic Matching"></a>2.3 Semantic Matching</h3><p><img src="/2021/12/02/hcan/7.JPG" alt></p><p><img src="/2021/12/02/hcan/8.JPG" alt></p><p><img src="/2021/12/02/hcan/9.JPG" alt></p><h3 id="2-4-Final-Classification"><a href="#2-4-Final-Classification" class="headerlink" title="2.4 Final Classification"></a>2.4 Final Classification</h3><p><img src="/2021/12/02/hcan/12.JPG" alt></p><p><img src="/2021/12/02/hcan/10.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hcan(Hybrid Co-Attention Network) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中文文本分类工具</title>
      <link href="/2021/11/30/text-classifying/"/>
      <url>/2021/11/30/text-classifying/</url>
      
        <content type="html"><![CDATA[<p>感谢大佬开源</p><p><a href="https://github.com/649453932/Chinese-Text-Classification-Pytorch">https://github.com/649453932/Chinese-Text-Classification-Pytorch</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中文文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>刷题指南</title>
      <link href="/2021/11/30/leet-coding/"/>
      <url>/2021/11/30/leet-coding/</url>
      
        <content type="html"><![CDATA[<p><a href="https://github.com/halfrost/LeetCode-Go">https://github.com/halfrost/LeetCode-Go</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 刷题指南 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>堆，优先队列</title>
      <link href="/2021/11/30/heap/"/>
      <url>/2021/11/30/heap/</url>
      
        <content type="html"><![CDATA[<p><strong>堆</strong></p><p><a href="https://blog.csdn.net/weixin_45697774/article/details/104481087">https://blog.csdn.net/weixin_45697774/article/details/104481087</a></p><p><strong>优先队列</strong></p><p><a href="https://www.cnblogs.com/xzxl/p/7266404.html">https://www.cnblogs.com/xzxl/p/7266404.html</a></p><p><a href="https://www.jianshu.com/p/b51ab28ca8dd">https://www.jianshu.com/p/b51ab28ca8dd</a></p><p><strong>堆和优先队列的关系</strong></p><p><a href="https://blog.csdn.net/weixin_44337445/article/details/110508591">https://blog.csdn.net/weixin_44337445/article/details/110508591</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 堆，优先队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NER</title>
      <link href="/2021/11/26/NER/"/>
      <url>/2021/11/26/NER/</url>
      
        <content type="html"><![CDATA[<p>Named Entity Recognition，命名实体识别</p><p>旨在从文本中抽取出命名实体，比如人名、地名、机构名等</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p><img src="/2021/11/26/NER/1.jpg" alt></p><h2 id="文本数据标注"><a href="#文本数据标注" class="headerlink" title="文本数据标注"></a>文本数据标注</h2><p>为什么标注？说白了就是标签</p><p><img src="/2021/11/26/NER/2.jpg" alt></p><p><a href="https://blog.csdn.net/scgaliguodong123_/article/details/121303421">https://blog.csdn.net/scgaliguodong123_/article/details/121303421</a></p><p>举个例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">BIO-三位序列标注法(B-begin，I-inside，O-outside)</span><br><span class="line">B-X代表实体X的开头 x:PER(person) , ORG(orgnization),LOC(location)</span><br><span class="line">I-X代表实体X的中间或结尾</span><br><span class="line">O代表不属于任何类型的</span><br><span class="line">样例：</span><br><span class="line"></span><br><span class="line"> 我 O</span><br><span class="line"> 是 O</span><br><span class="line"> 李 B-PER</span><br><span class="line"> 果 I-PER</span><br><span class="line"> 冻 I-PER</span><br><span class="line"> ， O</span><br><span class="line"> 我 O</span><br><span class="line"> 爱 O</span><br><span class="line"> 中 B-ORG</span><br><span class="line"> 国 I-ORG</span><br><span class="line"> ， O</span><br><span class="line"> 我 O</span><br><span class="line"> 来 O</span><br><span class="line"> 自 O</span><br><span class="line"> 四 B-LOC</span><br><span class="line"> 川 I-LOC</span><br><span class="line"> 。 O</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/huangyc/p/10064853.html">https://www.cnblogs.com/huangyc/p/10064853.html</a></p><p><a href="https://www.cnblogs.com/YoungF/p/13488220.htmlhttps://www.cnblogs.com/YoungF/p/13488220.html">https://www.cnblogs.com/YoungF/p/13488220.htmlhttps://www.cnblogs.com/YoungF/p/13488220.html</a></p><p><a href="https://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html">https://tech.meituan.com/2020/07/23/ner-in-meituan-nlp.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/156914795">https://zhuanlan.zhihu.com/p/156914795</a></p><p><a href="https://blog.csdn.net/scgaliguodong123_/article/details/121303421">https://blog.csdn.net/scgaliguodong123_/article/details/121303421</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 信息抽取 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>answer select</title>
      <link href="/2021/11/26/anser-select/"/>
      <url>/2021/11/26/anser-select/</url>
      
        <content type="html"><![CDATA[<p>1 问题定义</p><p>Given a question and a set of candidate sentences, the task is to identify candidate sentences that contain the correct answer to the question. From the definition, the problem can be formulated as a ranking problem, where the goal is to give better rank to the candidate sentences that are relevant to the question.</p><p>2 博客：</p><p><a href="https://zhuanlan.zhihu.com/p/39920446">https://zhuanlan.zhihu.com/p/39920446</a></p><p>3 A Review on Deep Learning Techniques Applied to Answer Selection</p><p><a href="https://aclanthology.org/C18-1181.pdf">https://aclanthology.org/C18-1181.pdf</a></p><p>4 BAS: An Answer Selection Method Using BERT Language Model</p><p><a href="https://arxiv.org/ftp/arxiv/papers/1911/1911.01528.pdf">https://arxiv.org/ftp/arxiv/papers/1911/1911.01528.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 对话系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> answer select </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>权重初始化</title>
      <link href="/2021/11/26/weiht-init/"/>
      <url>/2021/11/26/weiht-init/</url>
      
        <content type="html"><![CDATA[<p><strong>参数初始权重为什么不全0或者任意相同值</strong></p><script type="math/tex; mode=display">某一层任意一个神经元\\z=W_{1\times m}X_{m\times 1}+b_{1\times1}</script><p>如果我们将神经网络中的权重集初始化为零或者相同，那么同一层的所有神经元将在反向传播期间开始产生相同的输出和相同的梯度。导致同一层每个神经元完全一样，等价于只有一个</p><p><strong>常用的三种权值初始化方法</strong></p><p>随机初始化、Xavier initialization、He initialization</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://mdnice.com/writing/6fe7dfe1954945d180d6b36562658af8">https://mdnice.com/writing/6fe7dfe1954945d180d6b36562658af8</a></p><p><a href="https://m.ofweek.com/ai/2021-06/ART-201700-11000-30502442.html">https://m.ofweek.com/ai/2021-06/ART-201700-11000-30502442.html</a></p><p><a href="https://blog.csdn.net/qq_15505637/article/details/79362970">https://blog.csdn.net/qq_15505637/article/details/79362970</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 权重初始化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>chatbot</title>
      <link href="/2021/11/26/chatbot/"/>
      <url>/2021/11/26/chatbot/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/55201625#">https://zhuanlan.zhihu.com/p/55201625#</a></p><p><a href="https://zhuanlan.zhihu.com/p/88546027">https://zhuanlan.zhihu.com/p/88546027</a></p><p><a href="https://blog.csdn.net/m0_37565948/article/details/81582585">https://blog.csdn.net/m0_37565948/article/details/81582585</a></p>]]></content>
      
      
      <categories>
          
          <category> 对话系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatbot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bm25（best matching）</title>
      <link href="/2021/11/26/bm25/"/>
      <url>/2021/11/26/bm25/</url>
      
        <content type="html"><![CDATA[<p>是TD-IDF的优化版本</p><p><a href="https://zhuanlan.zhihu.com/p/79202151">https://zhuanlan.zhihu.com/p/79202151</a></p><p><a href="https://blog.csdn.net/weixin_42486623/article/details/121498706">https://blog.csdn.net/weixin_42486623/article/details/121498706</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> bm25 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习在NLP应用</title>
      <link href="/2021/11/22/acl-contrasive-nlp/"/>
      <url>/2021/11/22/acl-contrasive-nlp/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/435367182">https://zhuanlan.zhihu.com/p/435367182</a></p><p>1.应用在预训练</p><p>Pre-trained Models for Natural Language Processing A Survey</p><p> <a href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p><p>2.应用在finetune </p><p>simcse，consert</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 对比学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对比学习在NLP应用 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sota</title>
      <link href="/2021/11/22/sota/"/>
      <url>/2021/11/22/sota/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.jiqizhixin.com/sota">https://www.jiqizhixin.com/sota</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sota </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间序列预测总结</title>
      <link href="/2021/11/22/time-series-survey/"/>
      <url>/2021/11/22/time-series-survey/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/67832773">https://zhuanlan.zhihu.com/p/67832773</a></p><p><a href="https://cloud.tencent.com/developer/article/1800614">https://cloud.tencent.com/developer/article/1800614</a></p><p><a href="https://blog.csdn.net/weixin_39653948/article/details/105422337">https://blog.csdn.net/weixin_39653948/article/details/105422337</a></p><p>transformer based</p><p><a href="https://arxiv.org/pdf/2001.08317.pdf">https://arxiv.org/pdf/2001.08317.pdf</a></p><p><a href="https://arxiv.org/pdf/2012.07436.pdf">https://arxiv.org/pdf/2012.07436.pdf</a></p><p><a href="https://arxiv.org/abs/1907.00235">https://arxiv.org/abs/1907.00235</a></p><p><a href="https://arxiv.org/pdf/1912.09363.pdf">https://arxiv.org/pdf/1912.09363.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 时间序列预测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间序列预测总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征向量化</title>
      <link href="/2021/11/22/emb/"/>
      <url>/2021/11/22/emb/</url>
      
        <content type="html"><![CDATA[<p>特征-&gt;one hot-&gt; embedding </p><p>目的：one hot 表示特征太稀疏，不利于训练，而且参数过多，速度慢</p><p>举例子： <a href="https://arxiv.org/pdf/1706.06978.pdf">https://arxiv.org/pdf/1706.06978.pdf</a> DIN论文中的特征表示和embdding层</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 特征工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征向量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>token embedding</title>
      <link href="/2021/11/22/token-emb/"/>
      <url>/2021/11/22/token-emb/</url>
      
        <content type="html"><![CDATA[<div id="flowchart-0" class="flow-chart"></div><p><a href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">start=>start: 开始io=>inputoutput: 输入文本cond=>condition: 条件sub=>subroutine: 子流程end=>end: 结束op1=>operation: 输入文本op2=>operation: tokenizeop3=>operation: 词向量矩阵（预训练的或者随机初始化） op4=>operation: token embbeddingop1->op2->op3->op4</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> token embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semi-supervised Learning和Self-Supervised Learning</title>
      <link href="/2021/11/22/simi-self-learning/"/>
      <url>/2021/11/22/simi-self-learning/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_44015059/article/details/106448533">https://blog.csdn.net/qq_44015059/article/details/106448533</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 半监督和自监督 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>colab</title>
      <link href="/2021/11/21/colab/"/>
      <url>/2021/11/21/colab/</url>
      
        <content type="html"><![CDATA[<p>没有GPU怎么办，薅大户！！！</p><p><a href="https://blog.csdn.net/zhang_li_ke/article/details/89704682">https://blog.csdn.net/zhang_li_ke/article/details/89704682</a></p><p><a href="https://www.jianshu.com/p/a42d69568966">https://www.jianshu.com/p/a42d69568966</a></p><p><a href="https://blog.csdn.net/oldmao_2001/article/details/90737735?">https://blog.csdn.net/oldmao_2001/article/details/90737735?</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Google colab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Informer Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</title>
      <link href="/2021/11/21/informer/"/>
      <url>/2021/11/21/informer/</url>
      
        <content type="html"><![CDATA[<p>paper： <a href="https://arxiv.org/abs/2012.07436">https://arxiv.org/abs/2012.07436</a></p><p>code github ： <a href="https://github.com/zhouhaoyi/Informer2020">https://github.com/zhouhaoyi/Informer2020</a></p><p><a href="https://zhuanlan.zhihu.com/p/363084133">https://zhuanlan.zhihu.com/p/363084133</a></p><p><a href="https://blog.csdn.net/fluentn/article/details/115392229">https://blog.csdn.net/fluentn/article/details/115392229</a></p><p><a href="https://blog.csdn.net/weixin_42838061/article/details/117361871">https://blog.csdn.net/weixin_42838061/article/details/117361871</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>there are several severe issues with Transformer that prevent it from being directly applicable to LSTF（Long sequence time-series forecasting）, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture</p><p>Informer, with three distinctive characteristics：1. ProbSparse self-attention mechanism 2. the self-attention distilling 3.the generative style decoder</p><h2 id="2-Preliminary"><a href="#2-Preliminary" class="headerlink" title="2 Preliminary"></a>2 Preliminary</h2><p><img src="/2021/11/21/informer/0.JPG" alt></p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h2><p><img src="/2021/11/21/informer/3.JPG" alt></p><p><img src="/2021/11/21/informer/4.JPG" alt></p><h3 id="3-1-The-Uniform-Input-Representation"><a href="#3-1-The-Uniform-Input-Representation" class="headerlink" title="3.1 The Uniform Input Representation"></a>3.1 The Uniform Input Representation</h3><p><img src="/2021/11/21/informer/1.JPG" alt></p><p><img src="/2021/11/21/informer/2.JPG" alt></p><h3 id="3-2-Efficient-Self-attention-Mechanism"><a href="#3-2-Efficient-Self-attention-Mechanism" class="headerlink" title="3.2 Efficient Self-attention Mechanism"></a>3.2 Efficient Self-attention Mechanism</h3><p><strong>Query Sparsity Measurement</strong></p><p><img src="/2021/11/21/informer/8.JPG" alt></p><p>Some previous attempts have revealed that the distribution of self-attention probability has potential sparsity. The “sparsity” self-attention score forms a long tail distribution,见上图,接下来就是要把不符合sparsity的query找出来,也就是uniform distribution，然后用KL散度衡量两种分布的距离，得出the i-th query’s sparsity measurement</p><p><img src="/2021/11/21/informer/6.JPG" alt></p><p>但是直接用上面的式子存在几个问题：1. requires calculating each dot-product pairs 2.LogSumExp operation has the potential numerical stability issue 因此提出近似的计算</p><p><img src="/2021/11/21/informer/9.JPG" alt></p><p><strong>ProbSparse Self-attention</strong></p><p><img src="/2021/11/21/informer/11.JPG" alt></p><p><img src="/2021/11/21/informer/10.JPG" alt></p><p>The masked version can be achieved by applying positional mask on step 6 and using cmusum() in mean() of step 7. In the practice, we can use sum() as the simpler implement of mean().</p><h3 id="3-3-Encoder-Allowing-for-Processing-Longer-Sequential-Inputs-under-the-Memory-Usage-Limitation"><a href="#3-3-Encoder-Allowing-for-Processing-Longer-Sequential-Inputs-under-the-Memory-Usage-Limitation" class="headerlink" title="3.3 Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation"></a>3.3 Encoder: Allowing for Processing Longer Sequential Inputs under the Memory Usage Limitation</h3><p><img src="/2021/11/21/informer/12.JPG" alt></p><p>To enhance the robustness of the distilling operation, we build replicas of the main stack with halving inputs, and progressively decrease the number of self-attention distilling layers by dropping one layer at a time, like a pyramid in Fig.(2) 注意:encoder部分有两个stack，一个是主stack，一个是从stack，如图2，从stack的输入为主stack的一半，</p><p><strong>Self-attention Distilling</strong></p><p><img src="/2021/11/21/informer/13.JPG" alt></p><h3 id="3-4-Decoder-Generating-Long-Sequential-Outputs-Through-One-Forward-Procedure‘"><a href="#3-4-Decoder-Generating-Long-Sequential-Outputs-Through-One-Forward-Procedure‘" class="headerlink" title="3.4 Decoder: Generating Long Sequential Outputs Through One Forward Procedure‘"></a>3.4 Decoder: Generating Long Sequential Outputs Through One Forward Procedure‘</h3><p>和原来transformer的区别：主要在于prediction，原来是step by step，现在是one forward procedure，怎么实现的呢？关键在于decoder的输入的构造</p><p><img src="/2021/11/21/informer/14.JPG" alt></p><p><img src="/2021/11/21/informer/11.png" alt></p><p>A fully connected layer acquires the final output, and its outsize dy depends on whether we are performing a univariate forecasting or a multivariate one.</p><p>dy=1 uni，dy&gt;1 multi</p>]]></content>
      
      
      <categories>
          
          <category> 时间序列预测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Informer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow2.x 和tensorflow1.x对比</title>
      <link href="/2021/11/16/tf/"/>
      <url>/2021/11/16/tf/</url>
      
        <content type="html"><![CDATA[<p>tf 1.x : 1. session  run 2. 官方推荐 tf.data.Dataset + tf.estimator.Estimator</p><p>tf 2.x: 官方推荐的是 tf.data.Dataset + tf.keras</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_38978225/article/details/108942427">https://blog.csdn.net/qq_38978225/article/details/108942427</a></p><p><a href="https://blog.csdn.net/keeppractice/article/details/105934521">https://blog.csdn.net/keeppractice/article/details/105934521</a></p><p><a href="https://blog.csdn.net/sxlsxl119/article/details/104835420">https://blog.csdn.net/sxlsxl119/article/details/104835420</a></p><p><a href="https://www.zhihu.com/question/267809209">https://www.zhihu.com/question/267809209</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> tensorflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow2.x 和tensorflow1.x的区别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow中的Seq2Seq全家桶</title>
      <link href="/2021/11/16/tf-seq2seq/"/>
      <url>/2021/11/16/tf-seq2seq/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/47929039">https://zhuanlan.zhihu.com/p/47929039</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tensorflow中的Seq2Seq全家桶 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tf ranking</title>
      <link href="/2021/11/16/tf-ranking/"/>
      <url>/2021/11/16/tf-ranking/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/52447211#">https://zhuanlan.zhihu.com/p/52447211#</a></p><p><a href="https://github.com/tensorflow/ranking">https://github.com/tensorflow/ranking</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tf ranking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间复杂度计算</title>
      <link href="/2021/11/12/time-complex/"/>
      <url>/2021/11/12/time-complex/</url>
      
        <content type="html"><![CDATA[<h2 id="1-普通"><a href="#1-普通" class="headerlink" title="1 普通"></a>1 普通</h2><p><a href="https://blog.csdn.net/firefly_2002/article/details/8008987">https://blog.csdn.net/firefly_2002/article/details/8008987</a></p><h2 id="2-递归"><a href="#2-递归" class="headerlink" title="2 递归"></a>2 递归</h2><p><a href="https://www.jianshu.com/p/d6b94dac001d#">https://www.jianshu.com/p/d6b94dac001d#</a></p><p>1 代换法</p><p>2 递归树方法</p><p>3 主定理<img src="/2021/11/12/time-complex/1.JPG" alt></p><h2 id="3-回溯"><a href="#3-回溯" class="headerlink" title="3 回溯"></a>3 回溯</h2><p><a href="https://blog.csdn.net/u013009552/article/details/107064859">https://blog.csdn.net/u013009552/article/details/107064859</a></p><p><a href="https://zhuanlan.zhihu.com/p/62335966">https://zhuanlan.zhihu.com/p/62335966</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 时间复杂度计算 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gradient Accumulation</title>
      <link href="/2021/11/10/gradient-accumulate-steps/"/>
      <url>/2021/11/10/gradient-accumulate-steps/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/u013546508/article/details/121157559">https://blog.csdn.net/u013546508/article/details/121157559</a></p><p><a href="https://blog.csdn.net/Princeicon/article/details/108058822">https://blog.csdn.net/Princeicon/article/details/108058822</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gradient_accumulate_steps，调节学习率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch搭建神经网络</title>
      <link href="/2021/11/10/torch_buildnet/"/>
      <url>/2021/11/10/torch_buildnet/</url>
      
        <content type="html"><![CDATA[<h2 id="0-准备数据，处理数据"><a href="#0-准备数据，处理数据" class="headerlink" title="0.准备数据，处理数据"></a>0.准备数据，处理数据</h2><h2 id="1-搭建网络结构"><a href="#1-搭建网络结构" class="headerlink" title="1.搭建网络结构"></a>1.搭建网络结构</h2><p><a href="https://www.cnblogs.com/tian777/p/15341522.html">https://www.cnblogs.com/tian777/p/15341522.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class pointwise_hybird_contrasive(hybird):</span><br><span class="line">    def __init__(self,config_roberta, path,num):</span><br><span class="line">        super(pointwise_hybird_contrasive, self).__init__(config_roberta, path,num)</span><br><span class="line">        # self.softmax=torch.nn.Softmax()</span><br><span class="line">        # self.CrossEntropyLoss=torch.nn.CrossEntropyLoss()</span><br><span class="line">        # self.FFN2=</span><br><span class="line">        # self.softmax = nn.Softmax(dim=1)</span><br><span class="line">        return</span><br><span class="line">    def forward(self, input_ids, input_mask, segment_ids, all_en_query, all_en_ans):</span><br><span class="line">        ch_match_embedding = self.ch_matching_model(input_ids, input_mask, segment_ids)</span><br><span class="line">        en_match_embedding = self.en_matching_model(all_en_query, all_en_ans)</span><br><span class="line"></span><br><span class="line">        hybird_represent = torch.cat([ch_match_embedding, en_match_embedding], dim=1)</span><br><span class="line">        output = self.FFN2(self.relu(self.FFN1(self.dropout(hybird_represent))))</span><br><span class="line"></span><br><span class="line">        # y_pred_prob, y_pred = torch.max(self.softmax(output.data), 1)</span><br><span class="line"></span><br><span class="line">        return output</span><br><span class="line">    def loss(self,predict,target):</span><br><span class="line">        # predict=predict.reshape(-1,target.shape[1])</span><br><span class="line">        # predict = torch.squeeze(predict, dim=1)</span><br><span class="line">        # predict=torch.unsqueeze(predict, dim=0)</span><br><span class="line">        # target=torch.argmax(target,dim=1)</span><br><span class="line">        # target= torch.unsqueeze(target, dim=0)</span><br><span class="line">        # self.loss(predict,target)</span><br><span class="line">        CrossEntropyLoss=torch.nn.CrossEntropyLoss()</span><br><span class="line">        return CrossEntropyLoss(predict,target)</span><br><span class="line"></span><br><span class="line">    def predict(self, output):</span><br><span class="line">        softmax = nn.Softmax(dim=1)</span><br><span class="line">        y_pred_prob, y_pred = torch.max(softmax(output.data), 1)</span><br><span class="line">        # y_pred = y_pred.cpu().numpy()</span><br><span class="line">        y_pred_prob = y_pred_prob.cpu().numpy()</span><br><span class="line">        for i in range(len(y_pred_prob)):</span><br><span class="line">            if not y_pred[i]:</span><br><span class="line">                y_pred_prob[i] = 1 - y_pred_prob[i]</span><br><span class="line">        return y_pred_prob</span><br></pre></td></tr></table></figure><p><strong>nn.Module</strong></p><p><a href="https://www.cnblogs.com/tian777/p/15341522.html">https://www.cnblogs.com/tian777/p/15341522.html</a></p><h4 id="1-init"><a href="#1-init" class="headerlink" title="1 init"></a>1 init</h4><h4 id="2-forward"><a href="#2-forward" class="headerlink" title="2  forward"></a>2  forward</h4><h4 id="3-loss"><a href="#3-loss" class="headerlink" title="3 loss"></a>3 loss</h4><p><strong>pytorch各种交叉熵函数的汇总具体使用</strong></p><p><a href="https://blog.csdn.net/comway_Li/article/details/121490170">https://blog.csdn.net/comway_Li/article/details/121490170</a></p><p><strong>L2和L1正则化</strong></p><p><a href="https://blog.csdn.net/guyuealian/article/details/88426648">https://blog.csdn.net/guyuealian/article/details/88426648</a></p><p>优化器固定实现L2正则化,源码注释：weight_decay (:obj:<code>float</code>, <code>optional</code>, defaults to 0):<br>Weight decay (L2 penalty)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">param_optimizer = list(model.named_parameters())</span><br><span class="line">no_decay = [&#x27;bias&#x27;, &#x27;LayerNorm.bias&#x27;, &#x27;LayerNorm.weight&#x27;]</span><br><span class="line">optimizer_grouped_parameters = [</span><br><span class="line">    &#123;&#x27;params&#x27;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], &#x27;weight_decay&#x27;: 0.01&#125;,</span><br><span class="line">    &#123;&#x27;params&#x27;: [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], &#x27;weight_decay&#x27;: 0.0&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="4-predict"><a href="#4-predict" class="headerlink" title="4 predict"></a>4 predict</h4><h2 id="2-构建训练框架"><a href="#2-构建训练框架" class="headerlink" title="2.构建训练框架"></a>2.构建训练框架</h2><h3 id="a-迭代器"><a href="#a-迭代器" class="headerlink" title="a.迭代器"></a>a.迭代器</h3><p>Dataset/TensorDataset  -》 Sampler -》 Dataloader </p><p><a href="https://zhuanlan.zhihu.com/p/337850513#">https://zhuanlan.zhihu.com/p/337850513#</a></p><p><a href="https://blog.csdn.net/ljp1919/article/details/116484330">https://blog.csdn.net/ljp1919/article/details/116484330</a></p><p><a href="https://blog.csdn.net/qq_39507748/article/details/105385709">https://blog.csdn.net/qq_39507748/article/details/105385709</a></p><h3 id="b-优化器"><a href="#b-优化器" class="headerlink" title="b.优化器"></a>b.优化器</h3><p><a href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)</span><br><span class="line">optimizer=optim.SGD([</span><br><span class="line">                &#123;&#x27;params&#x27;: model.base.parameters()&#125;,</span><br><span class="line">                &#123;&#x27;params&#x27;: model.classifier.parameters(), &#x27;lr&#x27;: 1e-3&#125;</span><br><span class="line">            ], lr=1e-2, momentum=0.9)</span><br></pre></td></tr></table></figure><h3 id="c-训练"><a href="#c-训练" class="headerlink" title="c 训练"></a>c 训练</h3><p><strong>optimizer.zero_grad() 梯度归零, loss.backward() 反向传播 , optimizer.step() 参数更新</strong></p><p><a href="https://blog.csdn.net/PanYHHH/article/details/107361827">https://blog.csdn.net/PanYHHH/article/details/107361827</a></p><h3 id="d-验证"><a href="#d-验证" class="headerlink" title="d. 验证"></a>d. 验证</h3><p><strong>with torch.no_grad()</strong></p><p>验证，测试时候用：可显著减少显存占用</p><p><a href="https://wstchhwp.blog.csdn.net/article/details/108405102">https://wstchhwp.blog.csdn.net/article/details/108405102</a></p><p><a href="https://blog.csdn.net/weixin_44134757/article/details/105775027">https://blog.csdn.net/weixin_44134757/article/details/105775027</a></p><h3 id="e-评价指标"><a href="#e-评价指标" class="headerlink" title="e. 评价指标"></a>e. 评价指标</h3><h3 id="f-模型保存"><a href="#f-模型保存" class="headerlink" title="f. 模型保存"></a>f. 模型保存</h3><p><a href="https://blog.csdn.net/m0_37605642/article/details/120325062">https://blog.csdn.net/m0_37605642/article/details/120325062</a></p><p><a href="https://blog.csdn.net/weixin_41278720/article/details/80759933">https://blog.csdn.net/weixin_41278720/article/details/80759933</a></p><h3 id="g-可视化"><a href="#g-可视化" class="headerlink" title="g. 可视化"></a>g. 可视化</h3><p><a href="https://blog.csdn.net/Wenyuanbo/article/details/118937790">https://blog.csdn.net/Wenyuanbo/article/details/118937790</a></p><h2 id="3-预测"><a href="#3-预测" class="headerlink" title="3.预测"></a>3.预测</h2><p>加载模型，输入数据，调用网络结构</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_45847624/article/details/114885655">https://blog.csdn.net/qq_45847624/article/details/114885655</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch搭建神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>loss不下降的解决方法</title>
      <link href="/2021/11/10/adjust-loss/"/>
      <url>/2021/11/10/adjust-loss/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/zongza/article/details/89185852">https://blog.csdn.net/zongza/article/details/89185852</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> loss不下降的解决方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>text edit</title>
      <link href="/2021/11/08/text-edit/"/>
      <url>/2021/11/08/text-edit/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/qq_27590277/article/details/118534238">https://blog.csdn.net/qq_27590277/article/details/118534238</a></p><p><a href="https://thinkwee.top/2021/05/11/text-edit-generation/">https://thinkwee.top/2021/05/11/text-edit-generation/</a></p><p><a href="https://zhuanlan.zhihu.com/p/144995580#">https://zhuanlan.zhihu.com/p/144995580#</a>:</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> text edit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wide&amp;Deep和DeepFM</title>
      <link href="/2021/11/05/DeepFM/"/>
      <url>/2021/11/05/DeepFM/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/66928413">https://zhuanlan.zhihu.com/p/66928413</a></p><p><a href="https://blog.csdn.net/sinat_29819401/article/details/91359217">https://blog.csdn.net/sinat_29819401/article/details/91359217</a></p>]]></content>
      
      
      <categories>
          
          <category> 广告系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Wide&amp;Deep和DeepFM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>代价函数，损失函数，目标函数区别</title>
      <link href="/2021/11/04/object-func/"/>
      <url>/2021/11/04/object-func/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/lyl771857509/article/details/79428475">https://blog.csdn.net/lyl771857509/article/details/79428475</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代价函数，损失函数，目标函数区别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-Training with Whole Word Masking for Chinese BERT</title>
      <link href="/2021/11/04/bert-wwm/"/>
      <url>/2021/11/04/bert-wwm/</url>
      
        <content type="html"><![CDATA[<p>BERT-wwm-ext</p><p>wwm：whole word mask</p><p>ext： we also use extended training data  (mark with ext in the model name)</p><h2 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h2><p>1 改变mask策略</p><p>Whole Word Masking，wwm</p><p><img src="/2021/11/04/bert-wwm/1.JPG" alt></p><p>cws: Chinese Word Segmentation</p><p>对比四种mask策略</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Pre-Training with Whole Word Masking for Chinese BERT</p><p><a href="https://arxiv.org/abs/1906.08101v3">https://arxiv.org/abs/1906.08101v3</a></p><p>Revisiting Pre-trained Models for Chinese Natural Language Processing</p><p><a href="https://arxiv.org/abs/2004.13922">https://arxiv.org/abs/2004.13922</a></p><p>github：<a href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ALBERT A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</title>
      <link href="/2021/11/04/albert/"/>
      <url>/2021/11/04/albert/</url>
      
        <content type="html"><![CDATA[<p>There are three main contributions that ALBERT makes over the design choices of BERT：</p><p><strong>1 Factorized embedding parameterization</strong></p><p>原来embedding层是一个矩阵$M_{emb[V\times H]} $,现在变为两个$M_{emb1[V\times E]}$和$M_{emb2[E\times H]}$,参数量从VH变为VE+EH（This parameter reduction is significant when H &gt;&gt; E.）</p><p><strong>2 Cross-layer parameter sharing</strong></p><p>The default decision for ALBERT is to share all parameters across layers（attention，FFN)）</p><p><strong>3 Inter-sentence coherence loss</strong></p><p>原来的NSP改为现在的sop，正例的构建和NSP是一样的，不过负例则是将两句话反过来。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p> <a href="https://zhuanlan.zhihu.com/p/88099919">https://zhuanlan.zhihu.com/p/88099919</a></p><p><a href="https://blog.csdn.net/weixin_37947156/article/details/101529943">https://blog.csdn.net/weixin_37947156/article/details/101529943</a></p><p><a href="https://openreview.net/pdf?id=H1eA7AEtvS">https://openreview.net/pdf?id=H1eA7AEtvS</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>调参</title>
      <link href="/2021/11/04/adjust-papameter/"/>
      <url>/2021/11/04/adjust-papameter/</url>
      
        <content type="html"><![CDATA[<h2 id="1-调什么参数"><a href="#1-调什么参数" class="headerlink" title="1. 调什么参数"></a>1. 调什么参数</h2><p><strong>1 训练层面</strong></p><p>0 权重初始化</p><p>1 学习率</p><p>2 batch size</p><p>3 epoch</p><p>4 dropout</p><p>5 正则化</p><p>6 优化算法</p><p><strong>2 模型层面</strong></p><p>1 激活函数</p><p>2 网络尺寸</p><h2 id="2-超参数怎么调"><a href="#2-超参数怎么调" class="headerlink" title="2. 超参数怎么调"></a>2. 超参数怎么调</h2><p><strong>1.手动调参</strong></p><p>经验值</p><p><strong>2.自动化调参</strong></p><p>​    a.网格搜索</p><p><img src="/2021/11/04/adjust-papameter/1.png" alt></p><p>超参数排序组合，如果有n个参数，每个参数都有m个候选值，那么网格搜索中就要训练m的n次方个模型。</p><p>​    b.随机搜索</p><p><img src="/2021/11/04/adjust-papameter/2.png" alt></p><p>比起网格搜索：1、搜索次数少，快 2. 因为有偶然性，可能不是最优</p><p>​    c.贝叶斯优化</p><p><a href="https://zhuanlan.zhihu.com/p/146633409">https://zhuanlan.zhihu.com/p/146633409</a></p><p>Bayesian optimization algorithm，简称BOA</p><p>网格搜索和随机搜索，每次都是相互独立的，贝叶斯优化利用之前已搜索点的信息确定下一个搜索点</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/340578370">https://zhuanlan.zhihu.com/p/340578370</a></p><p><a href="https://www.jianshu.com/p/92d8943fb0ba">https://www.jianshu.com/p/92d8943fb0ba</a></p><p><a href="https://zhuanlan.zhihu.com/p/146633409">https://zhuanlan.zhihu.com/p/146633409</a></p><p><a href="https://blog.csdn.net/weixin_45884316/article/details/109828084">https://blog.csdn.net/weixin_45884316/article/details/109828084</a></p><p><a href="https://www.cnblogs.com/zingp/p/11352012.html#_label8">https://www.cnblogs.com/zingp/p/11352012.html#_label8</a></p><p><a href="https://www.jianshu.com/p/71f39c2ea512">https://www.jianshu.com/p/71f39c2ea512</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络进行二分类时，输出层使用两个神经元和只使用一个神经元，模型的性能有何差异，为什么？</title>
      <link href="/2021/11/04/2-classify/"/>
      <url>/2021/11/04/2-classify/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/397625619">https://www.zhihu.com/question/397625619</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类算法之朴素贝叶斯</title>
      <link href="/2021/11/03/naive-bayse/"/>
      <url>/2021/11/03/naive-bayse/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/pinard/p/6069267.html">https://www.cnblogs.com/pinard/p/6069267.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 朴素贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>时间序列预测滞后现象</title>
      <link href="/2021/11/01/decay-time-series/"/>
      <url>/2021/11/01/decay-time-series/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.dazhuanlan.com/bb_principessa/topics/1021683">https://www.dazhuanlan.com/bb_principessa/topics/1021683</a></p>]]></content>
      
      
      <categories>
          
          <category> 时间序列预测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 滞后 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</title>
      <link href="/2021/11/01/convtrans/"/>
      <url>/2021/11/01/convtrans/</url>
      
        <content type="html"><![CDATA[<p>原文：<a href="https://arxiv.org/abs/1907.00235">https://arxiv.org/abs/1907.00235</a></p><p>作者利用transformer做时间序列预测，发现了两个问题，然后提出了改进。一个问题是locality-agnostics，the point-wise dot product self-attention in canonical Transformer architecture is insensitive to local context。另外一个问题是memory bottleneck ：space complexity of canonical Transformer grows quadratically with sequence length L, making directly modeling long time series infeasible.为了解决这两个问题，作者提出了convolutional self-attention和LogSparse Transformer。</p><h2 id="3-背景"><a href="#3-背景" class="headerlink" title="3.背景"></a>3.背景</h2><p><strong>问题定义</strong></p><p><img src="/2021/11/01/convtrans/4.GIF" alt></p><p>其中$\Phi$是参数，$\textbf{X}$是辅助输入，就是除了观测值以外的输入，$\textbf{Z}_{i,t}$表示序列$i$在时刻$t$的值</p><p>为了简化式子，定义了</p><p><img src="/2021/11/01/convtrans/5.GIF" alt></p><p>目标就变成了$\textbf{z}_t \sim f(\textbf{Y}_t)$</p><p><strong>Transformer</strong></p><p><img src="/2021/11/01/convtrans/6.GIF" alt></p><p>$h$表示某个头，$M$表示mask matrix</p><h2 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4.方法论"></a>4.方法论</h2><h3 id="4-1-Enhancing-the-locality-of-Transformer"><a href="#4-1-Enhancing-the-locality-of-Transformer" class="headerlink" title="4.1 Enhancing the locality of Transformer"></a>4.1 Enhancing the locality of Transformer</h3><p><img src="/2021/11/01/convtrans/1.GIF" alt></p><p>改进思想如上图所示，原版的transformer，利用了point-wise之间的相似度，万一存在异常点，就会造成偏差。改进方向就是将点和点之间的相似度变为local context based，也就是先利用卷积得到local的表示，然后基于local做Q和K的相似度。当卷积核的尺寸为1就退化为原版的transformer。</p><h3 id="4-2-Breaking-the-memory-bottleneck-of-Transformer"><a href="#4-2-Breaking-the-memory-bottleneck-of-Transformer" class="headerlink" title="4.2 Breaking the memory bottleneck of Transformer"></a>4.2 Breaking the memory bottleneck of Transformer</h3><p><img src="/2021/11/01/convtrans/2.GIF" alt></p><p>原来的transformer需要$O(L^2)$的空间复杂度，每层每个cell为$O(L)$，每层所有cell为$O(L^2)$，然后堆叠$h$层，$h$为常数，所以为$O(L^2)$，如图(a)。作者提出了LogSparse Transformer，如图（b）,空间复杂度为$O(L(logL)^2)$。首先每层每一个cell需要$logL$，每层所有cell就是$LlogL$，然后堆叠$logL$层，最后为$O(L(logL)^2)$。</p><p>对于LogSparse Transformer，筛选规则为：</p><p><img src="/2021/11/01/convtrans/3.GIF" alt></p><p>图（c）和图（d）是对（b）的改进。</p><h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h2><p>评级指标:p-quantile loss $R_p$ with $p\in(0,1)$</p><p><img src="/2021/11/01/convtrans/7.GIF" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/412800154">https://zhuanlan.zhihu.com/p/412800154</a></p>]]></content>
      
      
      <categories>
          
          <category> 时间序列预测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> convtrans </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类回归模型总结</title>
      <link href="/2021/10/29/basic-regression-classfify/"/>
      <url>/2021/10/29/basic-regression-classfify/</url>
      
        <content type="html"><![CDATA[<p>很多的深度模型都属于表示学习，是为了得到好的特征表示，比如文本表示之类的模型，有了好的特征表示，才能增强分类或者回归的效果。某些端到端的模型其实可以拆解成几个部分，比如前置的环节包括了特征提取，特征表示，然后顶层是分类或者回归层。下文总结的是纯粹的分类和回归的模型。</p><h2 id="1-分类"><a href="#1-分类" class="headerlink" title="1.分类"></a>1.分类</h2><p><a href="https://www.jianshu.com/p/169dc01f0589">https://www.jianshu.com/p/169dc01f0589</a></p><h2 id="2-回归"><a href="#2-回归" class="headerlink" title="2.回归"></a>2.回归</h2><p><a href="https://blog.csdn.net/ChenVast/article/details/82107490">https://blog.csdn.net/ChenVast/article/details/82107490</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类回归模型总结 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer时间序列预测</title>
      <link href="/2021/10/29/transformer-time-seties/"/>
      <url>/2021/10/29/transformer-time-seties/</url>
      
        <content type="html"><![CDATA[<p>1.基本的Transformer</p><p><a href="https://zhuanlan.zhihu.com/p/360829130">https://zhuanlan.zhihu.com/p/360829130</a></p><p>2.改进的Transformer</p><p>Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting</p><p><a href="https://arxiv.org/pdf/1907.00235.pdf">https://arxiv.org/pdf/1907.00235.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/391337035">https://zhuanlan.zhihu.com/p/391337035</a></p>]]></content>
      
      
      <categories>
          
          <category> 时间序列预测 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer时间序列预测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>中文词粒度BERT</title>
      <link href="/2021/10/26/ch-word-bert/"/>
      <url>/2021/10/26/ch-word-bert/</url>
      
        <content type="html"><![CDATA[<p><strong>1 Is Word Segmentation Necessary for Deep Learning of Chinese Representations?</strong></p><p>we find that charbased（字粒度） models consistently <strong>outperform</strong> wordbased （词粒度）models.</p><p>We show that it is because word-based models are more vulnerable to data sparsity and the presence of out-of-vocabulary (OOV) words, and thus more prone to overfitting. </p><p><strong>2 腾讯中文词模型</strong></p><p>词模型在公开数据集的表现逊于字模型</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/1905.05526.pdf">https://arxiv.org/pdf/1905.05526.pdf</a></p><p><a href="https://www.jiqizhixin.com/articles/2019-06-27-17">https://www.jiqizhixin.com/articles/2019-06-27-17</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本匹配</title>
      <link href="/2021/10/26/text-matching/"/>
      <url>/2021/10/26/text-matching/</url>
      
        <content type="html"><![CDATA[<h2 id="1-无监督"><a href="#1-无监督" class="headerlink" title="1.无监督"></a>1.无监督</h2><h3 id="1-1-编辑距离"><a href="#1-1-编辑距离" class="headerlink" title="1.1 编辑距离"></a>1.1 编辑距离</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>编辑距离，英文名字为Levenshtein distance，通过描述一个字符串A需要多少次基本操作可以变成字符串B，来衡量两个字符串的相似度。</p><p>基本操作包括：增、删、改</p><p>增：字符串A为“AS”，字符串B为“ ASD“，字符串A-&gt;字符串B需要增加一个字符“D”</p><p>删：字符串A为“ASD”，字符串B为“ AS“，字符串A-&gt;字符串B需要删除一个字符“D”</p><p>改：字符串A为“ASX”，字符串B为“ ASD“，字符串A-&gt;字符串B需要将字符“X”变成字符“D”</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h4><p>实现过程使用动态规划，递推公式为</p><script type="math/tex; mode=display">lev_{a,b}(i,j)=\begin{equation}f(x)=\left\{\begin{aligned}max(i,j) &  & if\ \min(i,j)=0\\min\left\{\begin{aligned}lev_{a,b}(i-1,j)+1 \\lev_{a,b}(i,j-1)+1 \\lev_{a,b}(i-1,j-1)+1_{(a_i\neq b_j)}\end{aligned}\right.\end{aligned}\right.\end{equation}</script><p>$i$和$j$分别表示字符串$a$和字符串$b$的下标，$lev_{a,b}(i,j)$表示子串$a[:i]$到子串$b[:j]$的编辑距离。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def lev(str_a,str_b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    ED距离，用来衡量单词之间的相似度</span><br><span class="line">    :param str_a:</span><br><span class="line">    :param str_b:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    str_a=str_a.lower()</span><br><span class="line">    str_b=str_b.lower()</span><br><span class="line">    matrix_ed=np.zeros((len(str_a)+1,len(str_b)+1),dtype=np.int)</span><br><span class="line">    matrix_ed[0]=np.arange(len(str_b)+1)</span><br><span class="line">    matrix_ed[:,0] = np.arange(len(str_a) + 1)</span><br><span class="line">    for i in range(1,len(str_a)+1):</span><br><span class="line">        for j in range(1,len(str_b)+1):</span><br><span class="line">            # 表示删除a_i</span><br><span class="line">            dist_1 = matrix_ed[i - 1, j] + 1</span><br><span class="line">            # 表示插入b_i</span><br><span class="line">            dist_2 = matrix_ed[i, j - 1] + 1</span><br><span class="line">            # 表示替换b_i</span><br><span class="line">            dist_3 = matrix_ed[i - 1, j - 1] + (1 if str_a[i - 1] != str_b[j - 1] else 0)</span><br><span class="line">            #取最小距离</span><br><span class="line">            matrix_ed[i,j]=np.min([dist_1, dist_2, dist_3])</span><br><span class="line">    print(matrix_ed)</span><br><span class="line">    return matrix_ed[-1,-1]</span><br></pre></td></tr></table></figure><h3 id="1-2-TF-IDF"><a href="#1-2-TF-IDF" class="headerlink" title="1.2 TF-IDF"></a>1.2 TF-IDF</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>（1）TF</p><p>针对某个文本</p><p>$TF_{word}=\frac{word在文本中出现的次数}{文本中所有词的总数}$</p><p>（2）IDF</p><p>针对语料库</p><p>$IDF_{word}=log(\frac{语料库的文本总数}{包含该word的文本数+1})$</p><p>（3）TF-IDF</p><p>$TF-IDF_{word}=TF_{word}*IDF_{word}$</p><p>（4）TF-IDF VEC</p><p>现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”]</p><p> $VEC_{A}=[TF-IDF_{今天},TF-IDF_{天气}，TF-IDF_{真好},0,0]$</p><p>（5）计算两句话的文本相似度</p><p>假设词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”],现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],句子B:”天气不错呀”，分词后[“天气”,”不错呀”]</p><p>利用（3）得到句子A的TF-IDF VEC $VEC_{A}$，句子B的TF-IDF VEC $VEC_B$，利用余弦相似度计算文本相似度</p><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import jieba</span><br><span class="line">import numpy  as np</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">from scipy.linalg import norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TF_IDF_Model(object):</span><br><span class="line">    def __init__(self, corpus_list):</span><br><span class="line"></span><br><span class="line">        self.documents_list = corpus_list</span><br><span class="line">        self.documents_number = len(corpus_list)</span><br><span class="line">        self.get_idf()</span><br><span class="line"></span><br><span class="line">    def get_idf(self):</span><br><span class="line">        df = &#123;&#125;</span><br><span class="line">        self.idf = &#123;&#125;</span><br><span class="line">        tf = []</span><br><span class="line">        for document in self.documents_list:</span><br><span class="line">            temp = &#123;&#125;</span><br><span class="line">            for word in document:</span><br><span class="line">                temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">            tf.append(temp)</span><br><span class="line">            for key in temp.keys():</span><br><span class="line">                df[key] = df.get(key, 0) + 1</span><br><span class="line">        for key, value in df.items():</span><br><span class="line">            self.idf[key] = np.log10(self.documents_number / (value + 1))</span><br><span class="line"></span><br><span class="line">    def get_tf(self, document):</span><br><span class="line">        document = list(jieba.cut(document))</span><br><span class="line">        # tf = []</span><br><span class="line">        temp = &#123;&#125;</span><br><span class="line">        for word in document:</span><br><span class="line">            temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">        # tf.append(temp)</span><br><span class="line">        return temp</span><br><span class="line"></span><br><span class="line">    def tf_idf_vec(self, text):</span><br><span class="line">        tf = self.get_tf(text)</span><br><span class="line">        word = list(self.idf.keys())</span><br><span class="line">        vec = [0] * len(self.idf)</span><br><span class="line">        text = list(jieba.cut(text))</span><br><span class="line">        for ele in text:</span><br><span class="line">            if ele in word:</span><br><span class="line">                vec[word.index(ele)] = tf[ele] * self.idf[ele]</span><br><span class="line">        return vec</span><br><span class="line"></span><br><span class="line">    def cal_similarty(self, sentence1, sentence2):</span><br><span class="line">        vec1 = self.tf_idf_vec(sentence1)</span><br><span class="line">        vec2 = self.tf_idf_vec(sentence2)</span><br><span class="line">        similarty = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))</span><br><span class="line">        return similarty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_model():</span><br><span class="line">    #####bulid corpus</span><br><span class="line">    corpus = pd.read_csv(corpus_path)</span><br><span class="line">    corpus_list = corpus[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    # corpus_list = corpus1[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    corpus_list = [list(jieba.cut(str(doc))) for doc in corpus_list]</span><br><span class="line">    tf_idf_model = TF_IDF_Model(corpus_list)</span><br><span class="line">    joblib.dump(tf_idf_model, model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_model(path):</span><br><span class="line">    tf_idf_model = joblib.load(path)</span><br><span class="line">    return tf_idf_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    from supercat.data_qualifier.tf_idf import TF_IDF_Model</span><br><span class="line">    ####</span><br><span class="line">    train_model()</span><br><span class="line">    ######</span><br><span class="line">    tf_idf_model = load_model(model_path)</span><br><span class="line">    sentence1=&quot;XXXX&quot;</span><br><span class="line">    sentence2=&quot;XXXX&quot;</span><br><span class="line">    print(tf_idf_model.get_tf(sentence1))</span><br><span class="line">    print(tf_idf_model.idf)</span><br><span class="line">    print(tf_idf_model.tf_idf_vec(sentence1))</span><br><span class="line">    print(tf_idf_model.cal_similarty(sentence1,sentence2))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2-有监督"><a href="#2-有监督" class="headerlink" title="2.有监督"></a>2.有监督</h2><p><strong>基于表示的匹配方法</strong>：使用深度学习模型分别表征Query和Doc，通过计算向量相似度来作为语义匹配分数。微软的DSSM[26]及其扩展模型属于基于表示的语义匹配方法，美团搜索借鉴DSSM的双塔结构思想，左边塔输入Query信息，右边塔输入POI、品类信息，生成Query和Doc的高阶文本相关性、高阶品类相关性特征，应用于排序模型中取得了很好的效果。此外，比较有代表性的表示匹配模型还有百度提出 SimNet[27]，中科院提出的多视角循环神经网络匹配模型（MV-LSTM）[28]等。</p><p><strong>基于交互的匹配方法</strong>：这种方法不直接学习Query和Doc的语义表示向量，而是在神经网络底层就让Query和Doc提前交互，从而获得更好的文本向量表示，最后通过一个MLP网络获得语义匹配分数。代表性模型有华为提出的基于卷积神经网络的匹配模型ARC-II[29]，中科院提出的基于矩阵匹配的的层次化匹配模型MatchPyramid[30]。</p><p>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量，缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</p><p>匹配不同于排序，匹配是1对1的，排序是1对多</p><h3 id="2-1基于表示"><a href="#2-1基于表示" class="headerlink" title="2.1基于表示"></a>2.1基于表示</h3><p><a href="https://zhuanlan.zhihu.com/p/138864580">https://zhuanlan.zhihu.com/p/138864580</a></p><p><a href="https://blog.csdn.net/qq_27590277/article/details/121391770">https://blog.csdn.net/qq_27590277/article/details/121391770</a></p><h3 id="2-2-基于交互"><a href="#2-2-基于交互" class="headerlink" title="2.2.基于交互"></a>2.2.基于交互</h3><p><a href="https://blog.csdn.net/guofei_fly/article/details/107501276">https://blog.csdn.net/guofei_fly/article/details/107501276</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformer综述</title>
      <link href="/2021/10/26/transformer-survey/"/>
      <url>/2021/10/26/transformer-survey/</url>
      
        <content type="html"><![CDATA[<p>Transformer-XL</p><p><a href="https://arxiv.org/abs/1901.02860v3">https://arxiv.org/abs/1901.02860v3</a></p><p>RoFormer</p><p><a href="https://arxiv.org/pdf/2104.09864.pdf">https://arxiv.org/pdf/2104.09864.pdf</a></p><p>google2020出品的transformer的综述</p><p><a href="https://arxiv.org/pdf/2009.06732.pdf">https://arxiv.org/pdf/2009.06732.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 特征提取器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>T5</title>
      <link href="/2021/10/26/T5/"/>
      <url>/2021/10/26/T5/</url>
      
        <content type="html"><![CDATA[<p><strong>T5</strong></p><p>原文 <a href="https://arxiv.org/pdf/1910.10683.pdf">https://arxiv.org/pdf/1910.10683.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/88438851">https://zhuanlan.zhihu.com/p/88438851</a></p><p><strong>mT5</strong></p><p>原文 <a href="https://arxiv.org/pdf/2010.11934.pdf">https://arxiv.org/pdf/2010.11934.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/302380842">https://zhuanlan.zhihu.com/p/302380842</a></p><p><strong>Sentence-T5</strong>（文本表示新SOTA）</p><p>原文 <a href="https://arxiv.org/pdf/2108.08877.pdf">https://arxiv.org/pdf/2108.08877.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/403153114">https://zhuanlan.zhihu.com/p/403153114</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>冷启动</title>
      <link href="/2021/10/25/Cold-start/"/>
      <url>/2021/10/25/Cold-start/</url>
      
        <content type="html"><![CDATA[<p>推荐系统冷启动</p><p>mark</p><p><a href="https://zhuanlan.zhihu.com/p/79950668">https://zhuanlan.zhihu.com/p/79950668</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 冷启动 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类任务的类别数量很大(万以上)怎么处理？</title>
      <link href="/2021/10/25/extreme-num-classify/"/>
      <url>/2021/10/25/extreme-num-classify/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/387899184">https://www.zhihu.com/question/387899184</a></p><p>Extreme Multi Label Classification，XML，可以提供一些启发 <a href="https://zhuanlan.zhihu.com/p/131584886">https://zhuanlan.zhihu.com/p/131584886</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类任务的类别数量很大 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐之召回</title>
      <link href="/2021/10/25/recall-survey/"/>
      <url>/2021/10/25/recall-survey/</url>
      
        <content type="html"><![CDATA[<p>总结</p><p><a href="https://blog.csdn.net/luanfenlian0992/article/details/107416438">https://blog.csdn.net/luanfenlian0992/article/details/107416438</a></p><p><a href="https://zhuanlan.zhihu.com/p/364053939">https://zhuanlan.zhihu.com/p/364053939</a></p><p>好用的工具</p><p><a href="https://hub.fastgit.org/shenweichen/DeepMatch">https://hub.fastgit.org/shenweichen/DeepMatch</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 召回 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 召回 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>youtubednn</title>
      <link href="/2021/10/25/youtubednn/"/>
      <url>/2021/10/25/youtubednn/</url>
      
        <content type="html"><![CDATA[<p>原文： <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45530.pdf</a></p><p>几篇优秀博客：</p><p><a href="https://zhuanlan.zhihu.com/p/52169807">https://zhuanlan.zhihu.com/p/52169807</a></p><p><a href="https://zhuanlan.zhihu.com/p/52504407">https://zhuanlan.zhihu.com/p/52504407</a> </p><p><a href="https://zhuanlan.zhihu.com/p/61827629">https://zhuanlan.zhihu.com/p/61827629</a></p><p><a href="https://zhuanlan.zhihu.com/p/46247835">https://zhuanlan.zhihu.com/p/46247835</a></p><p>下文为本人总结。</p><h2 id="2-SYSTEM-OVERVIEW"><a href="#2-SYSTEM-OVERVIEW" class="headerlink" title="2.SYSTEM OVERVIEW"></a>2.SYSTEM OVERVIEW</h2><p><img src="/2021/10/25/youtubednn/1.GIF" alt></p><h2 id="3-CANDIDATE-GENERATION"><a href="#3-CANDIDATE-GENERATION" class="headerlink" title="3.CANDIDATE GENERATION"></a>3.CANDIDATE GENERATION</h2><h3 id="3-1-Recommendation-as-Classification"><a href="#3-1-Recommendation-as-Classification" class="headerlink" title="3.1 Recommendation as Classification"></a>3.1 Recommendation as Classification</h3><p>把推荐问题转换成多分类问题</p><p><img src="/2021/10/25/youtubednn/2.GIF" alt></p><p>where $u    \in \mathbb{R}^{N}$  represents a high-dimensional embedding”of the user, context pair and the $ v_j \in \mathbb{R}^{N}$ represent embeddings of each candidate video.</p><p><strong>train</strong>：</p><p>to efficiently train such a model with millions of classes</p><p>1.hierarchical softmax，效果不佳</p><p>2.采用candidate sampling，correct for this sampling via importance weighting</p><p><strong>At serving time</strong></p><h3 id="3-2-CANDIDATE-GENERATION"><a href="#3-2-CANDIDATE-GENERATION" class="headerlink" title="3.2 CANDIDATE GENERATION"></a>3.2 CANDIDATE GENERATION</h3><p><img src="/2021/10/25/youtubednn/3.GIF" alt></p><h3 id="3-3-Heterogeneous-Signals"><a href="#3-3-Heterogeneous-Signals" class="headerlink" title="3.3 Heterogeneous Signals"></a>3.3 Heterogeneous Signals</h3><h3 id="3-4-Label-and-Context-Selection"><a href="#3-4-Label-and-Context-Selection" class="headerlink" title="3.4 Label and Context Selection"></a>3.4 Label and Context Selection</h3><p><img src="/2021/10/25/youtubednn/4.GIF" alt></p><h3 id="3-5-Experiments-with-Features-and-Depth"><a href="#3-5-Experiments-with-Features-and-Depth" class="headerlink" title="3.5 Experiments with Features and Depth"></a>3.5 Experiments with Features and Depth</h3><p><img src="/2021/10/25/youtubednn/5.GIF" alt></p><h2 id="4-RANKING"><a href="#4-RANKING" class="headerlink" title="4.RANKING"></a>4.RANKING</h2><p><img src="/2021/10/25/youtubednn/6.GIF" alt></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> youtubednn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征工程</title>
      <link href="/2021/10/21/feature-en/"/>
      <url>/2021/10/21/feature-en/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/111296130">https://zhuanlan.zhihu.com/p/111296130</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 特征工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征工程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>协同过滤</title>
      <link href="/2021/10/21/cf/"/>
      <url>/2021/10/21/cf/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.jianshu.com/p/5463ab162a58">https://www.jianshu.com/p/5463ab162a58</a></p><p><a href="https://www.jianshu.com/p/20041e72e9ec">https://www.jianshu.com/p/20041e72e9ec</a></p><p><a href="https://www.cnblogs.com/pinard/p/6349233.html">https://www.cnblogs.com/pinard/p/6349233.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 召回 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 召回 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统评价指标</title>
      <link href="/2021/10/21/recommend-metrice/"/>
      <url>/2021/10/21/recommend-metrice/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/67287992">https://zhuanlan.zhihu.com/p/67287992</a></p><p><a href="http://sofasofa.io/forum_main_post.php?postid=1000292">http://sofasofa.io/forum_main_post.php?postid=1000292</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SimCSE Simple Contrastive Learning of Sentence Embeddings</title>
      <link href="/2021/10/20/simcse/"/>
      <url>/2021/10/20/simcse/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2104.08821.pdf">https://arxiv.org/pdf/2104.08821.pdf</a></p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h2><p> <strong>1 target</strong></p><p>对于$D=\{(x_i,x_i^{+})\}_{i=1}^{m}$,where $x_i$ and $x_i^{+}$ are semantically related. xi,xj+ are not semantically related</p><p>x-&gt;h</p><p>Contrastive learning aims to learn effective representation by pulling semantically close neighbors together and pushing apart non-neighbors</p><p><img src="/2021/10/20/simcse/1.GIF" alt></p><p>N is mini-batch size，分子是正样本，分母为负样本（有一个正样本,感觉是可以忽略）</p><p>分母会包含分子的项吗？从代码看，会的</p><p>loss</p><p><a href="https://www.jianshu.com/p/d73e499ec859">https://www.jianshu.com/p/d73e499ec859</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def loss(self,y_pred,y_true,lamda=0.05):</span><br><span class="line"></span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    exist a query q1 and  ranked condidat list  [d1,d2,d3,...,dn]</span><br><span class="line">     loss=  -log( exp^sim(q1,d1)/t  /   sum(exp^sim(q1,di)/t) i=2,...,n)</span><br><span class="line"></span><br><span class="line">    [q1,q2]    [[d11,d12,d13],[d21,d22,d23]]</span><br><span class="line">     similarities=[[sim(q1d11),sim(q1d12),sim(q1d13)],[sim(q2d21),sim(q2d22),sim(q2d23)] ] y_true=[y1 ,y2 ]</span><br><span class="line"></span><br><span class="line">        loss = F.cross_entropy(similarities, y_true)</span><br><span class="line">    ref ： https://www.jianshu.com/p/d73e499ec859</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    # idxs = torch.arange(0, y_pred.shape[0])</span><br><span class="line">    # y_true = idxs + 1 - idxs % 2 * 2</span><br><span class="line">    y_pred = y_pred.reshape(-1, y_true.shape[1])</span><br><span class="line"></span><br><span class="line">    # y_true=[0]*y_pred.sha pe[0]</span><br><span class="line">    # similarities = F.cosine_similarity(y_pred.unsqueeze(1), y_pred.unsqueeze(0), dim=2)</span><br><span class="line">    # similarities = similarities - torch.eye(y_pred.shape[0]) * 1e12</span><br><span class="line">    y_pred = y_pred / lamda</span><br><span class="line">    y_true = torch.argmax(y_true, dim=1)</span><br><span class="line">    loss = F.cross_entropy(y_pred, y_true)</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure><p><strong>2 representations评价指标</strong></p><p>Alignment： calculates expected distance between embeddings of the paired instances（paired instances就是正例）</p><p><img src="/2021/10/20/simcse/2.GIF" alt></p><p>uniformity： measures how well the embeddings are uniformly distributed</p><p><img src="/2021/10/20/simcse/3.GIF" alt></p><h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/10/20/simcse/11.GIF" alt></p><h3 id="2-1-Unsupervised"><a href="#2-1-Unsupervised" class="headerlink" title="2.1 Unsupervised"></a>2.1 Unsupervised</h3><p>$x_i-&gt;h_i^{z_i},x_i-&gt;h_i^{z_i^{‘}}$</p><p>z is a random mask for dropout，loss为</p><p><img src="/2021/10/20/simcse/4.GIF" alt></p><h3 id="2-2-Supervised"><a href="#2-2-Supervised" class="headerlink" title="2.2 Supervised"></a>2.2 Supervised</h3><p>引入非目标任务的有标签数据集，比如NLI任务，$(x_i,x_i^{+},x_i^{-})$,where $x_i$ is the premise, $x_i^{+}$and $x_i^{-}$are entailment and contradiction hypotheses.</p><p><img src="/2021/10/20/simcse/5.GIF" alt></p><p>$(h_i,h_j^{+})$为normal negatives，$(h_i,h_j^{-})$为hard negatives</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP子任务的评价指标</title>
      <link href="/2021/10/20/NLP-task/"/>
      <url>/2021/10/20/NLP-task/</url>
      
        <content type="html"><![CDATA[<h2 id="1-文本分类"><a href="#1-文本分类" class="headerlink" title="1.文本分类"></a>1.文本分类</h2><p>采用分类任务的评价指标，比如accuracy，recall，F1等</p><h2 id="2-文本匹配"><a href="#2-文本匹配" class="headerlink" title="2.文本匹配"></a>2.文本匹配</h2><p>重点说下一些paper的sts（Semantic Textual Similarity）任务，为什么采用相关系数（Pearson correlation或者spearman correlation）来衡量，比如 S-bert <a href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a>  ，consert  <a href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a>   。 这是因为S-bert和consert 都是文本表示的方法，最后计算文本相似度是利用余弦相似度计算的，相似度的值域为0-1，但是sts数据集的相似度值域为0-5。值域范围不同，不能直接进行比较，用相关系数来间接评价。</p><h2 id="3-文本生成"><a href="#3-文本生成" class="headerlink" title="3.文本生成"></a>3.文本生成</h2><p><a href="https://zhuanlan.zhihu.com/p/144182853">https://zhuanlan.zhihu.com/p/144182853</a></p><p><a href="https://arxiv.org/pdf/2006.14799.pdf">https://arxiv.org/pdf/2006.14799.pdf</a></p><p>文本改写（算是特殊的生成）</p><p><a href="https://aclanthology.org/2020.findings-emnlp.111.pdf">https://aclanthology.org/2020.findings-emnlp.111.pdf</a></p><p><a href="https://arxiv.org/pdf/1909.01187.pdf">https://arxiv.org/pdf/1909.01187.pdf</a></p><p>Exact score: percentage of exactly correctly predicted fusions</p><p>SARI: the average F1 scores of the added, kept, and deleted n-grams</p><h2 id="4-文本表示"><a href="#4-文本表示" class="headerlink" title="4.文本表示"></a>4.文本表示</h2><p><a href="https://arxiv.org/pdf/1908.10084.pdf">https://arxiv.org/pdf/1908.10084.pdf</a></p><p>SentEval (Conneau and Kiela, 2018) is a popular toolkit to evaluate the quality of sentence embeddings. </p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP子任务的评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习中的五种归一化（BN、LN、IN、GN和SN）方法简介</title>
      <link href="/2021/10/18/Normalization/"/>
      <url>/2021/10/18/Normalization/</url>
      
        <content type="html"><![CDATA[<p>mark</p><p><a href="https://blog.csdn.net/u013289254/article/details/99690730">https://blog.csdn.net/u013289254/article/details/99690730</a></p><p><a href="https://blog.csdn.net/qq_35290785/article/details/95879067">https://blog.csdn.net/qq_35290785/article/details/95879067</a></p><p><a href="https://zhuanlan.zhihu.com/p/34879333">https://zhuanlan.zhihu.com/p/34879333</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 归一化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Match to Rank Model for Personalized Click-Through Rate Prediction</title>
      <link href="/2021/10/15/dMR/"/>
      <url>/2021/10/15/dMR/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/10/15/dMR/11.GIF" alt></p><h2 id="1-输入特征"><a href="#1-输入特征" class="headerlink" title="1 输入特征"></a>1 输入特征</h2><p>由4个部分构成，分别为User Profile, User Behavior, Target Item and Context，每个特征都包含子特征，比如User Profile contains user ID, consumption level and so on。最初的表示为one-hot形式，经过embedding层，转成高纬向量，通过查找表来实现。最后4个特征分别表示为$\textbf{x}_p,\textbf{x}_b,\textbf{x}_t,\textbf{x}_c$，以$\textbf{x}_b$来举例，$\textbf{x}_b=[e_1，e_2，…，e_T]\in \mathbb{R}^{T\times d_e}$</p><h2 id="2-User-to-Item-Network"><a href="#2-User-to-Item-Network" class="headerlink" title="2 User-to-Item Network"></a>2 User-to-Item Network</h2><p>we apply attention mechanism with positional encoding as query to adaptively learn the weight for each behavior，where the position of user behavior is the serial number in the behavior sequence ordered by occurred time</p><p><img src="/2021/10/15/dMR/33.GIF" alt></p><p>其中$\textbf{z}\in \mathbb{R}^{d_h}$是学习的参数，$\textbf{p}_t\in \mathbb{P}^{d_p}$是位置$t$的embedding</p><p><img src="/2021/10/15/dMR/44.GIF" alt></p><p>为什么不用$\textbf{x}_t$,而用$\textbf{v}^{‘}$表示Target Item。作者的意思是对于Target Item，有两个查找表，we call $\textbf{V}$ the input representation and $\textbf{V}^{‘}$  the output representation of Target Item。we apply inner product operation to represent the user-to-item relevance</p><p><img src="/2021/10/15/dMR/5.GIF" alt></p><h2 id="3-Item-to-Item-Network"><a href="#3-Item-to-Item-Network" class="headerlink" title="3  Item-to-Item Network"></a>3  Item-to-Item Network</h2><p><img src="/2021/10/15/dMR/10.GIF" alt></p><p><img src="/2021/10/15/dMR/111.GIF" alt></p><p><img src="/2021/10/15/dMR/12.GIF" alt></p><h2 id="4-final"><a href="#4-final" class="headerlink" title="4  final"></a>4  final</h2><p>And the final input of MLP is represented by $\textbf{c}=[\textbf{x}_p,\textbf{x}_t,\textbf{x}_c,\hat{\textbf{u}},r,\hat{r}]$</p><h2 id="5-loss"><a href="#5-loss" class="headerlink" title="5 loss"></a>5 loss</h2><p><strong>target</strong></p><p>The loss for input feature vector $\textbf{x}=[\textbf{x}_p,\textbf{x}_b,\textbf{x}_t,\textbf{x}_c]$ and click label $ y \in \{0, 1\} $is:</p><p><img src="/2021/10/15/dMR/22.GIF" alt></p><p><strong>auxiliary match network</strong></p><p>主要是提高$r$对于user-to-item relevance的表现能力而引入。</p><p>The probability that user with the first $T −1$ behaviors click item $j$ next can be formulated with the softmax function as:</p><p><img src="/2021/10/15/dMR/6.GIF" alt></p><p>其中$\textbf{v}^{‘}_j$表示第$j$个商品的output representation。With cross-entropy as loss function, we have the loss as follows:</p><p><img src="/2021/10/15/dMR/7.GIF" alt></p><p>However, the cost of computing $p_j$ in Equation (6) is huge,引入负采样，然后loss为</p><p><img src="/2021/10/15/dMR/8.GIF" alt></p><p><strong>final</strong></p><p><img src="/2021/10/15/dMR/9.GIF" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>阿里2020年发表在AAAI上的关于CTR的paper，原文链接 <a href="https://sci-hub.se/10.1609/aaai.v34i01.5346">https://sci-hub.se/10.1609/aaai.v34i01.5346</a></p>]]></content>
      
      
      <categories>
          
          <category> 广告系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DMR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>优化算法</title>
      <link href="/2021/10/14/optimize-algorithm/"/>
      <url>/2021/10/14/optimize-algorithm/</url>
      
        <content type="html"><![CDATA[<p>mark</p><p><a href="https://www.cnblogs.com/zingp/p/11352012.html#_label8">https://www.cnblogs.com/zingp/p/11352012.html#_label8</a></p><p><a href="https://www.jianshu.com/p/71f39c2ea512">https://www.jianshu.com/p/71f39c2ea512</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>大数据</title>
      <link href="/2021/10/14/bigdata/"/>
      <url>/2021/10/14/bigdata/</url>
      
        <content type="html"><![CDATA[<p>mark</p><p><a href="https://github.com.cnpmjs.org/heibaiying/BigData-Notes">https://github.com.cnpmjs.org/heibaiying/BigData-Notes</a></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见问题</title>
      <link href="/2021/10/14/ctr/"/>
      <url>/2021/10/14/ctr/</url>
      
        <content type="html"><![CDATA[<p><strong>CTR模型为什么普遍采用二分类建模而不是回归建模</strong></p><p><a href="https://zhuanlan.zhihu.com/p/372110635">https://zhuanlan.zhihu.com/p/372110635</a></p><p><strong>CTR 预估和推荐系统有什么区别？？？</strong></p><p><a href="https://blog.csdn.net/qiqi123i/article/details/105259351">https://blog.csdn.net/qiqi123i/article/details/105259351</a></p><p><strong>CTR 预估和排序学习有什么区别？？？</strong></p><p>CTR预估模型属于point-wise模型</p><p><a href="https://www.zhihu.com/question/359973444#">https://www.zhihu.com/question/359973444#</a></p>]]></content>
      
      
      <categories>
          
          <category> 广告系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Interest Evolution Network for Click-Through Rate Prediction</title>
      <link href="/2021/10/13/dien/"/>
      <url>/2021/10/13/dien/</url>
      
        <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h2><p>对din的改进</p><p>din：强调用户兴趣是多样的，并使用基于注意力模型来捕获用户的兴趣</p><p>dien：不但要找到用户的兴趣，还要抓住用户兴趣的变化过程</p><h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/10/13/dien/11.GIF" alt></p><h3 id="1-behavior-layer"><a href="#1-behavior-layer" class="headerlink" title="1 behavior layer"></a>1 behavior layer</h3><h4 id="Feature-Representation"><a href="#Feature-Representation" class="headerlink" title="Feature Representation"></a>Feature Representation</h4><p>User Profile, User Behavior, Ad and Context</p><p>one-hot vector</p><h4 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h4><p>transforms the large scale sparse feature into lowdimensional dense feature</p><h3 id="2-Interest-Extractor-Layer"><a href="#2-Interest-Extractor-Layer" class="headerlink" title="2 Interest Extractor Layer"></a>2 Interest Extractor Layer</h3><p>利用GRU作为基本单元</p><h3 id="3-Interest-Evolving-Layer"><a href="#3-Interest-Evolving-Layer" class="headerlink" title="3 Interest Evolving Layer"></a>3 Interest Evolving Layer</h3><p>主要两个部分，一个是attention一个是AUGRU</p><p><strong>attention</strong></p><p>用公式表示为：</p><script type="math/tex; mode=display">\alpha_t=\frac{exp(\textbf{h}_tW_{\textbf{e}_a})}{\sum_{j=1}^Texp(\textbf{h}_jW_{\textbf{e}_a})}</script><p><strong>AUGRU</strong></p><p>结构如上图，用式子表达如下：</p><p><img src="/2021/10/13/dien/22.GIF" alt></p><h2 id="3-loss"><a href="#3-loss" class="headerlink" title="3 loss"></a>3 loss</h2><p>target</p><p><img src="/2021/10/13/dien/1.JPG" alt></p><p>为了提高准确率引入Auxiliary loss</p><script type="math/tex; mode=display">L_{aux}=-\frac{1}{N}\sum_{i=1}^N\sum_t[log\sigma(\textbf{h}_t^i,\textbf{e}_b^i[t+1])+log(1-\sigma(\textbf{h}_t^i,\tilde{\textbf{e}}_b^i[t+1]))]</script><p>其中$\sigma$为sigmoid</p><p>global loss：</p><script type="math/tex; mode=display">L=L_{target}+\alpha L_{aux}</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>原文地址 <a href="https://arxiv.org/pdf/1809.03672.pdf">https://arxiv.org/pdf/1809.03672.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 广告系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DIEN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k-fold</title>
      <link href="/2021/10/13/k-fold/"/>
      <url>/2021/10/13/k-fold/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/10/13/k-fold/11.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 验证 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k-fold </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Interest Network for Click-Through Rate Prediction</title>
      <link href="/2021/10/13/din/"/>
      <url>/2021/10/13/din/</url>
      
        <content type="html"><![CDATA[<h2 id="1-DEEP-INTEREST-NETWORK"><a href="#1-DEEP-INTEREST-NETWORK" class="headerlink" title="1.DEEP INTEREST NETWORK"></a>1.DEEP INTEREST NETWORK</h2><p><img src="/2021/10/13/din/11.GIF" alt></p><h3 id="1-1-特征表示"><a href="#1-1-特征表示" class="headerlink" title="1.1 特征表示"></a>1.1 特征表示</h3><p>特征可以表示为$\textbf{x}=[t_1^T,t_2^T,…,t_M^T]^T$，one hot表示，举个例子如下</p><p><img src="/2021/10/13/din/33.GIF" alt></p><p><img src="/2021/10/13/din/22.GIF" alt></p><h3 id="1-2-embedding层"><a href="#1-2-embedding层" class="headerlink" title="1.2 embedding层"></a>1.2 embedding层</h3><p>对于$t_i \in \mathbb{R}^{K_i}$，$W^i=[w_1^i,…,w_j^i,…,w_{K_i}^i] \in \mathbb{R}^{D\times K_i} $</p><p><img src="/2021/10/13/din/44.GIF" alt></p><h3 id="1-3-Pooling-layer-and-Concat-layer"><a href="#1-3-Pooling-layer-and-Concat-layer" class="headerlink" title="1.3 Pooling layer and Concat layer"></a>1.3 Pooling layer and Concat layer</h3><script type="math/tex; mode=display">\textbf{e}_i=pooling(\textbf{e}_{i_1},\textbf{e}_{i_2},...,\textbf{e}_{i_k})</script><p>Two most commonly used pooling layers are sum pooling and average pooling, which apply element-wise sum/average operations to the list of embedding vectors.</p><h3 id="1-4-Activation-unit"><a href="#1-4-Activation-unit" class="headerlink" title="1.4 Activation unit"></a>1.4 Activation unit</h3><p>DIN就是在base的基础上加入local activation unit，作用是对用户行为特征的不同商品给与不同权重，其余保持不变，式子表示如下</p><script type="math/tex; mode=display">\mathcal{V}_{U}(A)=f(\mathcal{V}_{A},\textbf{e}_1,\textbf{e}_2,...,\textbf{e}_H)=\sum_{j=1}^Ha(\textbf{e}_j,\mathcal{V}_{A})\textbf{e}_j=\sum_{j=1}^H\textbf{w}_j\textbf{e}_j</script><p>其中$a(\cdot)$为上图中activate unit,与attention很像，原文是Local activation unit of Eq.(3) shares similar ideas with attention methods which are developed in NMT task[1].</p><h3 id="1-5-MLP"><a href="#1-5-MLP" class="headerlink" title="1.5 MLP"></a>1.5 MLP</h3><h3 id="1-6-Loss"><a href="#1-6-Loss" class="headerlink" title="1.6 Loss"></a>1.6 Loss</h3><p>交叉熵表示为：</p><script type="math/tex; mode=display">L=-\frac{1}{N}\sum_{(\textbf{x},y) \in \textbf{S}}(ylogp(\textbf{x})+(1-y)log(1-p(\textbf{x})))</script><h2 id="2-训练技巧"><a href="#2-训练技巧" class="headerlink" title="2.训练技巧"></a>2.训练技巧</h2><p>Practically, training industrial deep networks with large scale sparse input features is of great challenge. 引入Mini-batch Aware Regularization和Data Adaptive Activation Function，具体不在此介绍</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>原文 <a href="https://arxiv.org/pdf/1706.06978.pdf">https://arxiv.org/pdf/1706.06978.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 广告系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DIN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本改写和term分析</title>
      <link href="/2021/10/12/text-mod/"/>
      <url>/2021/10/12/text-mod/</url>
      
        <content type="html"><![CDATA[<h2 id="1-文本改写"><a href="#1-文本改写" class="headerlink" title="1.文本改写"></a>1.文本改写</h2><p>改写主要步骤query纠错、query对齐、query扩展</p><h3 id="1-1query纠错"><a href="#1-1query纠错" class="headerlink" title="1.1query纠错"></a>1.1query纠错</h3><p>在搜索过程中由于对先验知识的掌握不足或者在使用输入法的时候误输入导致的，本质为去噪过程。</p><p>常用的query纠错方法有数字、拼音、漏字、重复字、谐音/形近字等方式。</p><h3 id="1-2query对齐"><a href="#1-2query对齐" class="headerlink" title="1.2query对齐"></a>1.2query对齐</h3><p>对于输入query并无错误，但表达上与搜索引擎索引内容不相符而作的一种改写操作。例如“星爷是哪一年生的”，通过实体对齐，可改写为“周星驰的出生时间”。</p><p>方法:1.对齐规则 2.文本改写模型</p><h3 id="1-3-query扩展"><a href="#1-3-query扩展" class="headerlink" title="1.3 query扩展"></a>1.3 query扩展</h3><p>是将与用户输入的query的相似扩展query进行展示，使得用户可以选择更多的搜索内容，帮助用户挖掘潜在需求。</p><h2 id="2-term分析"><a href="#2-term分析" class="headerlink" title="2.term分析"></a>2.term分析</h2><p>一段文本分词后，对于不同的词语，在相同文本中的重要性应该是不同的。</p><p>baseline的无监督方法可以是：tf-idf。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/344631739">https://zhuanlan.zhihu.com/p/344631739</a></p>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> query理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本改写和term分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>attention总结</title>
      <link href="/2021/10/12/attention/"/>
      <url>/2021/10/12/attention/</url>
      
        <content type="html"><![CDATA[<p>1.Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</p><p>提出了两种 attention 模式，即 hard attention 和 soft attention</p><p>2.Effective Approaches to Attention-based Neural Machine Translation</p><p>文章提出了两种 attention 的改进版本，即 global attention 和 local attention。</p><p>3.Attention Is All You Need</p><p>提出self attention</p><p>4.Hierarchical Attention Networks for Document Classification</p><p>提出了Hierarchical Attention用于文档分类</p><p>5.Attention-over-Attention Neural Networks for Reading Comprehension</p><p>提出了Attention Over Attention的Attention机制</p><p>6.Convolutional Sequence to Sequence Learning</p><p>论文中还采用了 Multi-step Attention</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 特征提取器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CRF和HMM</title>
      <link href="/2021/10/12/crf/"/>
      <url>/2021/10/12/crf/</url>
      
        <content type="html"><![CDATA[<p>大佬写的很详细，Mark一下</p><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><p><a href="https://www.cnblogs.com/pinard/p/7048333.html">https://www.cnblogs.com/pinard/p/7048333.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/148813079#">https://zhuanlan.zhihu.com/p/148813079#</a></p><h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><p><a href="https://www.cnblogs.com/pinard/p/6945257.html">https://www.cnblogs.com/pinard/p/6945257.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CRF和HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning Recommendation Model for Personalization and Recommendation Systems</title>
      <link href="/2021/10/11/facebook-recommend-sys/"/>
      <url>/2021/10/11/facebook-recommend-sys/</url>
      
        <content type="html"><![CDATA[<p>Facebook19年出品的推荐系统的paper，原文地址 <a href="https://arxiv.org/pdf/1906.00091.pdf">https://arxiv.org/pdf/1906.00091.pdf</a></p><h3 id="DLRM-结构-deep-learning-recommendation-model"><a href="#DLRM-结构-deep-learning-recommendation-model" class="headerlink" title="DLRM 结构(deep learning recommendation model)"></a>DLRM 结构(deep learning recommendation model)</h3><p><img src="/2021/10/11/facebook-recommend-sys/11.JPG" alt></p><p><img src="/2021/10/11/facebook-recommend-sys/22.JPG" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/82839874">https://zhuanlan.zhihu.com/p/82839874</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> facebook推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xgboost</title>
      <link href="/2021/10/11/xgboost/"/>
      <url>/2021/10/11/xgboost/</url>
      
        <content type="html"><![CDATA[<p>a scalable tree boosting system</p><p>是对gbdt的高效实现</p><p>基学习器：XGBoost的可以使用<strong>cart回归树</strong>作为基学习器，也可以使用<strong>线性分类器</strong>作为基学习器；gbdt的基学习器只能是<strong>cart回归树</strong></p><p>XGBoost算法原理小结 <a href="https://www.cnblogs.com/pinard/p/10979808.html">https://www.cnblogs.com/pinard/p/10979808.html</a></p><p>XGBoost类库使用小结 <a href="https://www.cnblogs.com/pinard/p/11114748.html">https://www.cnblogs.com/pinard/p/11114748.html</a></p><p>paper原文 <a href="https://arxiv.org/pdf/1603.02754.pdf">https://arxiv.org/pdf/1603.02754.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> boosting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> xgboost </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KNN</title>
      <link href="/2021/10/10/knn/"/>
      <url>/2021/10/10/knn/</url>
      
        <content type="html"><![CDATA[<p>KNN可以用来分类和回归，以分类为例。</p><h2 id="一-算法流程"><a href="#一-算法流程" class="headerlink" title="一.算法流程"></a>一.算法流程</h2><p><img src="/2021/10/10/knn/11.png" alt></p><p>KNN分类算法的计算过程：</p><p>1）计算待分类点与已知类别的点之间的距离</p><p>2）按照距离递增次序排序</p><p>3）选取与待分类点距离最小的K个点</p><p>4）确定前K个点所在类别的出现次数</p><p>5）返回前K个点出现次数最高的类别作为待分类点的预测分类</p><p>如上图，举个例子：</p><ul><li>如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li><li>如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。</li></ul><h2 id="二-距离度量选择"><a href="#二-距离度量选择" class="headerlink" title="二.距离度量选择"></a>二.距离度量选择</h2><p>1.闵可夫斯基距离</p><p>2.欧式距离</p><p>3.曼哈顿距离</p><h2 id="三-K值的选择"><a href="#三-K值的选择" class="headerlink" title="三.K值的选择"></a>三.K值的选择</h2><p>选择较小的K值，容易发生过拟合；选择较大的K值，则容易欠拟合。在应用中，通常采用交叉验证法来选择最优K值。</p><h2 id="四-优缺点"><a href="#四-优缺点" class="headerlink" title="四.优缺点"></a>四.优缺点</h2><p><strong>优点</strong>:</p><p>1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。</p><p>2）可用于非线性分类。</p><p><strong>缺点</strong>：</p><p>1）需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。</p><p>2）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树vs逻辑回归</title>
      <link href="/2021/10/10/deci-tree-LR/"/>
      <url>/2021/10/10/deci-tree-LR/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/10/10/deci-tree-LR/11.png" alt></p><p>最大的差异上图就可以看出，左边为逻辑回归的决策面，右边为决策树的决策面</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.zhihu.com/question/319481283">https://www.zhihu.com/question/319481283</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树vs逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>意图识别和槽位填充(Intent Detection and Slot Filling)</title>
      <link href="/2021/10/09/intent-detect-slot-fill/"/>
      <url>/2021/10/09/intent-detect-slot-fill/</url>
      
        <content type="html"><![CDATA[<h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>出自 <a href="https://zhuanlan.zhihu.com/p/75228411#">https://zhuanlan.zhihu.com/p/75228411#</a></p><p>在<strong>对话系统的NLU</strong>中，意图识别（Intent Detection，简写为ID）和槽位填充（Slot Filling，简写为SF）是两个重要的子任务。其中，<strong>意图识别</strong>可以看做是NLP中的一个<strong>分类</strong>任务，而<strong>槽位填充</strong>可以看做是一个<strong>序列标注</strong>任务，在早期的系统中，通常的做法是将两者拆分成两个独立的子任务。但这种做法跟人类的语言理解方式是不一致的，事实上我们在实践中发现，两者很多时候是具有较强相关性的，比如下边的例子：</p><blockquote><p>1.我要听[北京天安门, song] — Intent：播放歌曲<br>2.帮我叫个车，到[北京天安门, location] — Inent：打车<br>3.播放[忘情水, song] — Intent：播放歌曲<br>4.播放[复仇者联盟, movie] — Intent：播放视频</p></blockquote><p>1和2中，可以看到同样是“北京天安门”，由于意图的不同，该实体具备完全不同的槽位类型。3和4中，由于槽位类型的不同，导致了最终意图的不同，这往往意味着，在对话系统中的后继流程中将展现出完全不同的行为——-打开网易音乐播放歌曲 or 打开爱奇艺播放电影。</p><p>随着对话系统的热度逐渐上升，研究的重点也逐渐倾向于将两个任务进行联合，以充分利用意图和槽位中的语义关联。那么，问题来了，我们该如何进行<strong>联合</strong>呢？从目前的趋势来看，大体上有两大类方法：</p><ol><li><strong>多任务学习</strong>：按Multi-Task Learning的套路，在学习时最终的loss等于两个任务的loss的weight sum，两者在模型架构上仍然完全独立，或者仅共享特征编码器。</li><li><strong>交互式模型</strong>：将模型中Slot和Intent的隐层表示进行交互，引入更强的归纳偏置，最近的研究显示，这种方法的联合NLU准确率更高。</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>[1] <a href="https://zhuanlan.zhihu.com/p/165963264">https://zhuanlan.zhihu.com/p/165963264</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/75228411#">https://zhuanlan.zhihu.com/p/75228411#</a></p>]]></content>
      
      
      <categories>
          
          <category> 对话系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 意图识别和槽位填充 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>意图识别</title>
      <link href="/2021/10/09/intent-detect/"/>
      <url>/2021/10/09/intent-detect/</url>
      
        <content type="html"><![CDATA[<p>本质是<strong>分类</strong>任务，多用在搜索引擎和智能问答中。</p><p><strong>解决方法</strong></p><p>1、基于规则模板意图识别</p><p><a href="https://blog.csdn.net/qq_16555103/article/details/100767984">https://blog.csdn.net/qq_16555103/article/details/100767984</a></p><p>2、基于深度学习模型来对用户的意图进行判别</p><p>比如fasttext，LSTM+attention，BERT</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_37228811/article/details/104307144?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&amp;spm=1001.2101.3001.4242">https://blog.csdn.net/qq_37228811/article/details/104307144?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.no_search_link&amp;spm=1001.2101.3001.4242</a></p><p><a href="https://blog.csdn.net/qq_16555103/article/details/100767984">https://blog.csdn.net/qq_16555103/article/details/100767984</a></p>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> query理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 意图识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/2021/10/09/svm/"/>
      <url>/2021/10/09/svm/</url>
      
        <content type="html"><![CDATA[<h2 id="1-回顾线性回归和LR"><a href="#1-回顾线性回归和LR" class="headerlink" title="1.回顾线性回归和LR"></a>1.回顾线性回归和LR</h2><p>线性回归是解决回归任务的线性模型</p><p>LR是二分类模型，在线性模型的基础上加入激活函数sigmoid，适用在线性可分的二分类任务</p><h2 id="2-介绍svm"><a href="#2-介绍svm" class="headerlink" title="2.介绍svm"></a>2.介绍svm</h2><p>简单总结：1.对于完全线性可分，硬间隔 2.不能够完全线性可分，引入松弛变量 ，软间隔 3.线性不可分，引入核函数</p><p>原理可参考： <a href="https://zhuanlan.zhihu.com/p/77750026">https://zhuanlan.zhihu.com/p/77750026</a>  或者  <a href="https://blog.csdn.net/qq_37321378/article/details/108807595">https://blog.csdn.net/qq_37321378/article/details/108807595</a></p><p>核函数  <a href="https://blog.csdn.net/mengjizhiyou/article/details/103437423">https://blog.csdn.net/mengjizhiyou/article/details/103437423</a></p><p>与LR区别： <a href="https://www.jianshu.com/p/1b4d9de7000c">https://www.jianshu.com/p/1b4d9de7000c</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>query理解</title>
      <link href="/2021/10/08/query-understanding/"/>
      <url>/2021/10/08/query-understanding/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/112719984">https://zhuanlan.zhihu.com/p/112719984</a></p><p><a href="https://zhuanlan.zhihu.com/p/383733052">https://zhuanlan.zhihu.com/p/383733052</a></p><p><a href="https://zhuanlan.zhihu.com/p/344631739">https://zhuanlan.zhihu.com/p/344631739</a></p>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> query理解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> query理解 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多路召回</title>
      <link href="/2021/10/08/Multiple-recall/"/>
      <url>/2021/10/08/Multiple-recall/</url>
      
        <content type="html"><![CDATA[<h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><p><img src="/2021/10/08/Multiple-recall/22.jpg" alt></p><p>所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后再把这些候选集混合在一起后供后续排序模型使用的策略。值得注意的是，<strong>每一路召回需要尽可能的保持独立性与互斥性，从而在保证各链路能够并行召回的同时，增加召回的多样性。</strong></p><h2 id="2-多路召回融合策略（可以算是粗排）"><a href="#2-多路召回融合策略（可以算是粗排）" class="headerlink" title="2.多路召回融合策略（可以算是粗排）"></a>2.多路召回融合策略（可以算是粗排）</h2><p><img src="/2021/10/08/Multiple-recall/11.jpg" alt></p><p><strong>平均法</strong>：C的计算方法：(0.7 + 0.5 + 0.3)/3</p><p><strong>加权平均</strong>：假设三种策略的权重指定为0.4、0.3、0.2（人为给定或者算法拟合），则B的权重为（0.4 <em> 0.8 + 0.3 </em> 0.6 + 0.2* 0）/ （0.4+0.3+0.2）</p><p><strong>动态加权法</strong>:计算三种召回策略的CTR，作为每天更新的动态权重。但是只考虑了点击率，并不全面。</p><h2 id="3-例子"><a href="#3-例子" class="headerlink" title="3.例子"></a>3.例子</h2><p><a href="https://tianchi.aliyun.com/notebook-ai/detail?postId=144452">https://tianchi.aliyun.com/notebook-ai/detail?postId=144452</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/388601198">https://zhuanlan.zhihu.com/p/388601198</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 召回 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多路召回 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Factorization Machines</title>
      <link href="/2021/09/30/FM/"/>
      <url>/2021/09/30/FM/</url>
      
        <content type="html"><![CDATA[<p>原文地址 <a href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf">https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/50426292">https://zhuanlan.zhihu.com/p/50426292</a></p><p>a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models</p><h2 id="I-INTRODUCTION"><a href="#I-INTRODUCTION" class="headerlink" title="I. INTRODUCTION"></a>I. INTRODUCTION</h2><p>In total, the advantages of our proposed FM are:<br>1) FMs <strong>allow parameter estimation under very sparse data</strong> where SVMs fail.<br>2) FMs have <strong>linear complexity</strong>, can be optimized in the primal and do not rely on support vectors like SVMs.<br>3) FMs are a <strong>general predictor</strong> that can work with any real valued feature vector. </p><h2 id="II-PREDICTION-UNDER-SPARSITY"><a href="#II-PREDICTION-UNDER-SPARSITY" class="headerlink" title="II. PREDICTION UNDER SPARSITY"></a>II. PREDICTION UNDER SPARSITY</h2><p><img src="/2021/09/30/FM/22.JPG" alt></p><p><img src="/2021/09/30/FM/11.JPG" alt></p><h2 id="III-FACTORIZATION-MACHINES-FM"><a href="#III-FACTORIZATION-MACHINES-FM" class="headerlink" title="III. FACTORIZATION MACHINES (FM)"></a>III. FACTORIZATION MACHINES (FM)</h2><h3 id="A-Factorization-Machine-Model"><a href="#A-Factorization-Machine-Model" class="headerlink" title="A. Factorization Machine Model"></a>A. Factorization Machine Model</h3><p><strong>1) 模型:</strong></p><script type="math/tex; mode=display">\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n \bbox[border: 2px solid red]{w_{i,j}}x_ix_j</script><p>$x_i$表示第$i$个特征，但是针对上式，一个很大的问题，用户交互矩阵往往是比较稀疏的，这样就会导致对$w_{i,j}$的估算存在很大的问题。举个例子，假如想要估计Alice(A)和Star Trek(ST)的交互参数$w_{A,ST}$，由于训练集中没有实例同时满足$x_A$和$x_{ST}$非零，这会造成$w_{A,ST}=0$。因此这里使用了矩阵分解的思想：</p><script type="math/tex; mode=display">\textbf{W} = \textbf{V}\textbf{V}^T,\textbf{V}=\begin{pmatrix} \textbf{v}_1 \\ \textbf{v}_2\\...\\\textbf{v}_n  \end{pmatrix} \in {R}^{n\times k}\\\bbox[border: 2px solid red]{w_{i,j}=<\textbf{v}_i,\textbf{v}_j>=\sum_{f=1}^k v_{i,f}\cdot v_{j,f}}\\\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n \bbox[border: 2px solid red]{<\textbf{v}_i,\textbf{v}_j>}x_ix_j</script><p><strong>2) 提升效率:</strong></p><p>直接计算上面的公式求解$\hat{y}(x)$的时间复杂度为$O ( k n^2 ) $，因为所有的特征交叉都需要计算。但是可以通过公式变换，将时间复杂度减少到$O(kn)$，如下公式推导</p><script type="math/tex; mode=display">\begin{align*}\\\sum_{i=1}^n\sum_{j=i+1}^n <\textbf{v}_i,\textbf{v}_j>x_ix_j&=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n <\textbf{v}_i,\textbf{v}_j>x_ix_j-\frac{1}{2}\sum_{i=1}^n\ <\textbf{v}_i,\textbf{v}_i>x_ix_i\\&=...\\&=\frac{1}{2}\sum_{f=1}^k((\sum_{i=1}^nv_{i,f}x_i)^2-\sum_{i=1}^nv_{i,f}^2x_i^2)\end{align*}</script><h3 id="B-Factorization-Machines-as-Predictors"><a href="#B-Factorization-Machines-as-Predictors" class="headerlink" title="B. Factorization Machines as Predictors"></a>B. Factorization Machines as Predictors</h3><p>FM can be applied to a variety of prediction tasks. Among them are: Regression，Binary classification，Ranking</p><h3 id="C-Learning-Factorization-Machines"><a href="#C-Learning-Factorization-Machines" class="headerlink" title="C. Learning Factorization Machines"></a>C. Learning Factorization Machines</h3><p>the model parameters of FMs can be learned efficiently by gradient descent methods – e.g. stochastic gradient descent (SGD).The gradient of the FM model is:</p><script type="math/tex; mode=display">\\\begin{equation}\frac{\partial \hat{y}(x)}{\partial \theta}=\left\{\begin{array}{rcl}1& & {if \ \theta \ is \  w_0 }\\x_i & & {if \ \theta \ is \  w_i}\\x_i\sum_{j=1}^nv_{j,f}x_j-v_{i,f}x_i^2 & & {if \ \theta \ is \  v_{i,f}}\end{array} \right.\end{equation}</script><h3 id="D-d-way-Factorization-Machine"><a href="#D-d-way-Factorization-Machine" class="headerlink" title="D. d-way Factorization Machine"></a>D. d-way Factorization Machine</h3><p>The 2-way FM described so far can easily be generalized to a d-way FM:</p><script type="math/tex; mode=display">\hat{y}(x):=w_0+\sum_{i=1}^nw_ix_i+\sum_{l=2}^d\sum_{i_l=1}^n ...\sum_{i_l=i_{l-1}+1}^n (\prod \limits_{j=1}^lx_{i_j})(\sum_{f=1}^{k_l}\prod \limits_{j=1}^lv_{i_j,f}^{(l)})</script><p>直接计算上式的时间复杂度为$O(k_dn^d)$，利用类似上面的公式变形也可以将其降低为$O(k_d n )$</p><h3 id="E-Summary"><a href="#E-Summary" class="headerlink" title="E. Summary"></a>E. Summary</h3><p>FMs model all possible interactions between values in the feature vector $x$ using factorized interactions instead of full parametrized ones. This has two main advantages:</p><p>1) The interactions between values can be estimated even under high sparsity. Especially, it is possible to generalize to unobserved interactions.<br>2) The number of parameters as well as the time for prediction and learning is linear. </p><h2 id="IV-FMS-VS-SVMS"><a href="#IV-FMS-VS-SVMS" class="headerlink" title="IV. FMS VS. SVMS"></a>IV. FMS VS. SVMS</h2><h2 id="V-FMS-VS-OTHER-FACTORIZATION-MODELS"><a href="#V-FMS-VS-OTHER-FACTORIZATION-MODELS" class="headerlink" title="V. FMS VS. OTHER FACTORIZATION MODELS"></a>V. FMS VS. OTHER FACTORIZATION MODELS</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_26822029/article/details/103993243">https://blog.csdn.net/qq_26822029/article/details/103993243</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 召回 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征稀疏</title>
      <link href="/2021/09/30/sparsity-feature/"/>
      <url>/2021/09/30/sparsity-feature/</url>
      
        <content type="html"><![CDATA[<h3 id="What-are-sparse-features"><a href="#What-are-sparse-features" class="headerlink" title="What are sparse features?"></a>What are sparse features?</h3><p>Features with sparse data are features that have mostly zero values. This is different from features with missing data.</p><h3 id="Why-is-machine-learning-difficult-with-sparse-features"><a href="#Why-is-machine-learning-difficult-with-sparse-features" class="headerlink" title="Why is machine learning difficult with sparse features?"></a>Why is machine learning difficult with sparse features?</h3><p>Common problems with sparse features include:</p><ol><li>If the model has many sparse features, it will <strong>increase the space and time complexity</strong> of models. Linear regression models will fit more coefficients, and tree-based models will have greater depth to account for all features.</li><li>Model algorithms and diagnostic <strong>measures might behave in unknown ways</strong> if the features have sparse data. <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1421">Kuss [2002]</a> shows that goodness-of-fit tests are flawed when the data is sparse.</li><li>If there are too many features, models fit the noise in the training data. This is called <strong>overfitting</strong>. When models overfit, they are unable to generalize to newer data when they are put in production. This negatively impacts the predictive power of models.</li><li>Some models may <strong>underestimate the importance of sparse features and given preference to denser features</strong> even though the sparse features may have predictive power. Tree-based models are notorious for behaving like this. For example, random forests overpredict the importance of features that have more categories than those features that have fewer categories.</li></ol><h3 id="Methods-for-dealing-with-sparse-features"><a href="#Methods-for-dealing-with-sparse-features" class="headerlink" title="Methods for dealing with sparse features"></a>Methods for dealing with sparse features</h3><ol><li><p>Removing features from the model</p></li><li><p>Make the features dense</p></li><li><p>Using models that are robust to sparse features</p></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html#:~:text= Methods for dealing with sparse features ,that are robust to sparse features More">https://www.kdnuggets.com/2021/01/sparse-features-machine-learning-models.html#:~:text=%20Methods%20for%20dealing%20with%20sparse%20features%20,that%20are%20robust%20to%20sparse%20features%20More%20</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 特征工程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征稀疏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Felix Flexible Text Editing Through Tagging and Insertion</title>
      <link href="/2021/09/30/felix/"/>
      <url>/2021/09/30/felix/</url>
      
        <content type="html"><![CDATA[<p>google继lasertagger之后的又一篇text edit paper</p><p>In contrast to conventional sequence-to-sequence (seq2seq) models, FELIX is efficient in <strong>low-resource settings</strong> and <strong>fast</strong> at inference time, while being <strong>capable</strong> of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: <strong>tagging</strong> to decide on the subset of input tokens and their order in the output text and <strong>insertion</strong> to in-fill the missing tokens in the output not present in the input.</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p><img src="/2021/09/30/felix/11.JPG" alt></p><p>In particular, we have designed FELIX with the following requirements in mind: Sample efficiency, Fast inference time, Flexible text editing</p><h2 id="2-Model-description"><a href="#2-Model-description" class="headerlink" title="2 Model description"></a>2 Model description</h2><p>FELIX decomposes the conditional probability of generating an output sequence $y$ from an input<br>$x$ as follows:</p><script type="math/tex; mode=display">p(\textbf{y}|\textbf{x})=p_{ins}(\textbf{y}|\textbf{y}^m)p_{tag}(\textbf{y}^t,\pi|\textbf{x})</script><h3 id="2-1-Tagging-Model"><a href="#2-1-Tagging-Model" class="headerlink" title="2.1 Tagging Model"></a>2.1 Tagging Model</h3><p>trained to optimize both the tagging and pointing loss:</p><script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{pointing  }+\lambda\mathcal{L}_{tagging   }</script><p><strong>Tagging</strong> :</p><p>tag sequence $\textbf{y}^t$由3种tag组成：$KEEP$，$DELETE$，$INSERT (INS)$</p><p>Tags are predicted by applying a single feedforward layer $f$ to the output of the encoder $\textbf{h}^L$ (the source sentence is first encoded using a 12-layer BERT-base model). $\textbf{y}^t_i=argmax(f(\textbf{h}^L_i))$</p><p><strong>Pointing</strong>:</p><p><img src="/2021/09/30/felix/33.JPG" alt></p><p>Given a sequence $\textbf{x}$ and the predicted tags $\textbf{y}^t$ , the re-ordering model generates a permutation $\pi$ so that from $\pi$and  $\textbf{y}^t$ we can reconstruct the insertion model input $\textbf{y}^m$. Thus we have: </p><script type="math/tex; mode=display">p(\textbf{y}^m|\textbf{x}) \approx \prod \limits_{i}p(\pi(i)|\textbf{x},\textbf{y}^t,i)p(\textbf{y}_i^t|\textbf{x})</script><p>Our implementation is based on a <strong>pointer network</strong>. The output of this model is a series of predicted pointers (source token → next target token)</p><p>The input to the Pointer layer at position $i$:</p><script type="math/tex; mode=display">\textbf{h}^{L+1}_{i}=f([\textbf{h}^{L}_{i};e(\textbf{y}_i^t);e(\textbf{p}_i)])</script><p>其中$e(\textbf{y}_i^t)$is the embedding of the predicted tag，$e(\textbf{p}_i)$ is the positional embedding</p><p>The pointer network attends over all hidden states, as such:</p><script type="math/tex; mode=display">p(\pi(i)|\textbf{h}_i^{L+1})=attention(\textbf{h}_i^{L+1},\textbf{h}_{\pi(i)}^{L+1})</script><p>其中$\textbf{h}_i^{L+1}$ as $Q $, $\textbf{h}_{\pi(i)}^{L+1}$ as $K$</p><p>When realizing the pointers, we use a constrained beam search</p><h3 id="2-2-Insertion-Model"><a href="#2-2-Insertion-Model" class="headerlink" title="2.2 Insertion Model"></a>2.2 Insertion Model</h3><p><img src="/2021/09/30/felix/22.JPG" alt></p><p>To represent masked token spans we consider two options: <strong>masking</strong> and <strong>infilling</strong>. In the former case the tagging model predicts how many tokens need to be inserted by specializing the $INSERT$ tag into $INS_k$, where $k$ translates the span into $ k$  $MASK$ tokens. For the infilling case the tagging model predicts a generic $INS$ tag. </p><p>Note that we preserve the deleted​ span in the input to the insertion model by enclosing it between $[REPL]$ and $[/REPL]$ tags.</p><p>our insertion model is also based on a 12-layer BERT-base and we can directly take advantage of the BERT-style pretrained checkpoints.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://aclanthology.org/2020.findings-emnlp.111.pdf">https://aclanthology.org/2020.findings-emnlp.111.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本改写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Embedding based Product Retrieval in Taobao Search</title>
      <link href="/2021/09/28/tao-emb-search/"/>
      <url>/2021/09/28/tao-emb-search/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2106.09297.pdf">https://arxiv.org/pdf/2106.09297.pdf</a></p><p><a href="http://xtf615.com/2021/10/07/taobao-ebr/">http://xtf615.com/2021/10/07/taobao-ebr/</a></p><h2 id="1-INTRODUCTION"><a href="#1-INTRODUCTION" class="headerlink" title="1.INTRODUCTION"></a>1.INTRODUCTION</h2><p><img src="/2021/09/28/tao-emb-search/11.JPG" alt></p><p><img src="/2021/09/28/tao-emb-search/22.JPG" alt></p><p>框架是搜索系统主流的结构，即匹配/检索，粗排，精排，重排。</p><h2 id="2-RELATED-WORK"><a href="#2-RELATED-WORK" class="headerlink" title="2.RELATED WORK"></a>2.RELATED WORK</h2><h3 id="2-1-Deep-Matching-in-Search"><a href="#2-1-Deep-Matching-in-Search" class="headerlink" title="2.1 Deep Matching in Search"></a>2.1 Deep Matching in Search</h3><p>fall into two categories: <strong>representation-based learning</strong> and <strong>interaction-based learning.</strong></p><p>Other than semantic and relevance matching, more complex factors/trade-offs, e.g., user personalization [2, 3, 10] and retrieval efficiency [5], need to be considered when applying deep models to a large-scale online retrieval system.</p><h3 id="2-2-Deep-Retrieval-in-Industry-Search"><a href="#2-2-Deep-Retrieval-in-Industry-Search" class="headerlink" title="2.2 Deep Retrieval in Industry Search"></a>2.2 Deep Retrieval in Industry Search</h3><p>Representation-based models with an ANN (approximate near neighbor) algorithm have become the mainstream trend to efficiently deploy neural retrieval models in industry.</p><h2 id="3-MODEL"><a href="#3-MODEL" class="headerlink" title="3 MODEL"></a>3 MODEL</h2><p>整体结构入下：</p><p><img src="/2021/09/28/tao-emb-search/33.JPG" alt></p><h3 id="3-1-Problem-Formulation"><a href="#3-1-Problem-Formulation" class="headerlink" title="3.1 Problem Formulation"></a>3.1 Problem Formulation</h3><p>$\mathcal{U}=\{u_1,…,u_u,…u_N\}$表示$N$个用户，$\mathcal{Q}=\{q_1,…,q_u,…q_N\}$表示与用户对应的$N$个query，$\mathcal{I}=\{i_1,…,i_u,…i_M\}$表示$M$个商品。将用户$u$的历史行为根据时间分成3个部分：1.real-time，before<br>the current time step，$\mathcal{R}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ 2.short-term, before $\mathcal{R}$ and within ten days,$\mathcal{S}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ 3.long-term sequences,before $\mathcal{S}$ and within one month,$\mathcal{L}^u=\{i_1^u,…,i_t^u,…i_T^u\}$ ，$T$为时间长度。任务可以定义为：</p><script type="math/tex; mode=display">z=\mathcal{F}(\phi(q_u,\mathcal{R}^u,\mathcal{S}^u,\mathcal{L}^u),\varphi(i))</script><p>其中$\mathcal{F}(\cdot),\phi(\cdot),\varphi(\cdot)$分别表示scoring function, query and behaviors encoder, and item encoder</p><h3 id="3-2-User-Tower"><a href="#3-2-User-Tower" class="headerlink" title="3.2 User Tower"></a>3.2 User Tower</h3><h4 id="3-2-1-Multi-Granular-Semantic-Unit"><a href="#3-2-1-Multi-Granular-Semantic-Unit" class="headerlink" title="3.2.1 Multi-Granular Semantic Unit"></a>3.2.1 Multi-Granular Semantic Unit</h4><p>挖掘query的语义，原始输入包含当前query和历史query</p><p>没有说明为什么这么设计，感觉就是工程试验的结论。有个疑问，直接用BERT等深度语言模型来挖掘query的语义不好吗？</p><p>query表示为$q_u=\{w_1^u,…,w_n^u\}$,例如{红色，连衣裙}，$w_u=\{c_1^u,…,c_m^u\}$,例如{红，色}，history query表示为$q_{his}=\{q_1^u,…,q_k^u\} $,例如{绿色，半身裙，黄色，长裙}，其中$w_n \in \mathbb{R}^{1\times d},c_m \in \mathbb{R}^{1\times d},q_k \in \mathbb{R}^{1\times d}$</p><script type="math/tex; mode=display">q_{1\_gram}=mean\_pooling(c_1,...,c_m)\\q_{2\_gram}=mean\_pooling(c_1c_2,...,c_{m-1}c_m)\\q_{seq}=mean\_pooling(w_1,...,w_n)\\q_{seq\_seq}=mean\_pooling(T_{rm}(w_1,...,w_n))\\q_{his\_seq}=softmax(q_{seg}\cdot(q_{his})^{T})q_{his}\\q_{mix}=q_{1\_gram}+q_{2\_gram}+q_{seq}+q_{seq\_seq}+q_{his\_seq}\\Q_{mgs}=concat(q_{1\_gram},q_{2\_gram},q_{seq},q_{seq\_seq},q_{his\_seq},q_{mix})</script><p>其中𝑇𝑟𝑚,𝑚𝑒𝑎𝑛_𝑝𝑜𝑜𝑙𝑖𝑛𝑔, and 𝑐𝑜𝑛𝑐𝑎𝑡 denote the Transformer ,average, and vertical concatenation operation, respectively</p><h4 id="3-2-2-User-Behaviors-Attention"><a href="#3-2-2-User-Behaviors-Attention" class="headerlink" title="3.2.2 User Behaviors Attention"></a>3.2.2 User Behaviors Attention</h4><script type="math/tex; mode=display">e_i^f=W_f\cdot x_i^{f} \in \mathbb{R}^{1\times d_f} \tag{9}\\i_t^u=concat(\{e_i^f\ | \ f \in \mathcal{F} \})</script><p>其中$W_f$是embedding matrix，$x_i^{f}$是one-hot vector, $\mathcal{F}$是side information  (e.g., leaf category, first-level category, brand and,shop)</p><p><strong>real-time sequences</strong></p><p>User’s click_item</p><script type="math/tex; mode=display">\mathcal{R}_{lstm}^u=LSTM(\mathcal{R}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}\\\mathcal{R}_{self\_att}^u=multihead\_selfattention(\mathcal{R}_{lstm}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}\\\mathcal{R}_{zero\_att}^u=\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \} \  \#  add \  a \ zero \ vector \ at \  the \ first \ position \ of \  \mathcal{R}_{self\_att}^u\\H_{real}=softmax(Q_{mgs}\cdot\mathcal{R}_{zero\_att}^T)\cdot\mathcal{R}_{zero\_att}^T</script><p><strong>short-term sequences</strong></p><p>User’s click_item</p><script type="math/tex; mode=display">\mathcal{S}_{self\_att}^u=multihead\_selfattention(\mathcal{S}^u)=\{h_1^{u},...,h_t^{u},...,h_T^{u} \}\\\mathcal{S}_{zero\_att}^u=\{0,h_1^{u},...,h_t^{u},...,h_T^{u} \} \\H_{short}=softmax(Q_{mgs}\cdot\mathcal{S}_{zero\_att}^T)\cdot\mathcal{S}_{zero\_att}^T</script><p><strong>long-term sequence</strong></p><p>$\mathcal{L}^u$由四个部分构成，分别为$\mathcal{L}^{u}_{item},\mathcal{L}^{u}_{shop},\mathcal{L}^{u}_{leaf},\mathcal{L}^{u}_{brand}$,每个部分包含3个动作，分别为click，buy，collect。</p><script type="math/tex; mode=display">\\ \mathcal{L}_{click\_item},\mathcal{L}_{buy\_item},\mathcal{L}_{collect\_item}  \rightarrow L^{T}_{item}\\H_{a\_item}=softmax(Q_{mgs}\cdot  L^{T}_{item})\cdot L^{T}_{item}\\H_{long}=H_{a\_item}+H_{a\_shop}+H_{a\_leaf}+H_{a\_brand}</script><h4 id="3-2-3-Fusion-of-Semantics-and-Personalization"><a href="#3-2-3-Fusion-of-Semantics-and-Personalization" class="headerlink" title="3.2.3 Fusion of Semantics and Personalization"></a>3.2.3 Fusion of Semantics and Personalization</h4><script type="math/tex; mode=display">H_{qu}=Self\_Att^{first}([[cls],Q_{mgs},H_{real},H_{short},H_{long}]) \in \mathbb{R}^{1\times d}</script><h3 id="3-3-Item-Tower"><a href="#3-3-Item-Tower" class="headerlink" title="3.3 Item Tower"></a>3.3 Item Tower</h3><p>For the item tower, we experimentally use item ID and title to obtain the item representation 𝐻𝑖𝑡𝑒𝑚.Given the representation of item 𝑖’s ID, $e_i \in \mathbb{R}^{1\times d}$ , and its title segmentation result $T_i=\{w_1^{i},…,w_N^{i}\}$</p><script type="math/tex; mode=display">H_{item}=e+tanh(W_t\cdot\frac{\sum_{i=1}^Nw_i}{N})</script><p>where $W_t$ is the transformation matrix. We empirically find that applying LSTM [12] or Transformer [27] to capture the context of the title is not as effective as simple mean-pooling since the title is stacked by keywords and lacks grammatical structure.</p><h3 id="3-4-Loss-Function"><a href="#3-4-Loss-Function" class="headerlink" title="3.4 Loss Function"></a>3.4 Loss Function</h3><p>adapt the softmax cross-entropy loss as the training objective</p><script type="math/tex; mode=display">\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+}))}{\sum_{i^{'}\in I }exp(\mathcal{F}(q_u,i^{'}))}\\L(\nabla )=-\sum_{i\in I}y_ilog(\hat{y_i})</script><p>where $\mathcal{F},I,i^+,q_u$denote the inner product, the full item pool, the item tower’s representation $H_{item}$, and the user tower’s representation $H_{qu}$, respectively.</p><h4 id="3-4-1-Smoothing-Noisy-Training-Data"><a href="#3-4-1-Smoothing-Noisy-Training-Data" class="headerlink" title="3.4.1 Smoothing Noisy Training Data"></a>3.4.1 Smoothing Noisy Training Data</h4><p>the softmax function with the temperature parameter $\tau$ is defined as follows</p><script type="math/tex; mode=display">\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+})/\tau)}{\sum_{i^{'}\in I }exp(\mathcal{F}(q_u,i^{'})/\tau)}</script><p>If 𝜏-&gt;0, the fitted distribution is close to one hot distribution,If 𝜏-&gt;∞, the fitted distribution is close to a uniform distribution</p><h4 id="3-4-2-Generating-Relevance-improving-Hard-Negative-Samples"><a href="#3-4-2-Generating-Relevance-improving-Hard-Negative-Samples" class="headerlink" title="3.4.2 Generating Relevance-improving Hard Negative Samples"></a>3.4.2 Generating Relevance-improving Hard Negative Samples</h4><p>We first select the negative items of $i^-$ that have the top-𝑁 inner product scores with $q_u $ to form the hard sample set $I_{hard}$</p><script type="math/tex; mode=display">I_{mix}=\alpha i^++(1-\alpha)I_{hard}</script><p>其中$\alpha\in \mathbb{R}^{N\times1}$is sampled from the uniform distribution 𝑈 (𝑎, 𝑏) (0 ≤ 𝑎 &lt; 𝑏 ≤ 1).</p><script type="math/tex; mode=display">\hat{y}(i^+|q_u)=\frac{exp(\mathcal{F}(q_u,i^{+})/\tau)}{\sum_{i^{'}\in (I\cup I_{mix}) }exp(\mathcal{F}(q_u,i^{'})/\tau)}</script>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 召回 </category>
          
          <category> 向量召回 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Embedding based Product Retrieval in Taobao Search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>From RankNet to LambdaRank to LambdaMART</title>
      <link href="/2021/09/27/lambdamart/"/>
      <url>/2021/09/27/lambdamart/</url>
      
        <content type="html"><![CDATA[<h2 id="1-RankNet"><a href="#1-RankNet" class="headerlink" title="1.RankNet"></a>1.RankNet</h2><p>RankNet采用pairwise的方法进行模型训练。</p><p><strong>loss推导</strong></p><p>给定特定$query$下的两个文档$U_i$和$U_j$，其特征向量分别为$x_i$和$x_j$，经过RankNet进行前向计算得到对应的分数为$s_i=f(x_i)$和$s_j=f(x_j)$。用$U_i \rhd U_j$表示$U_i$比$U_j$排序更靠前。继而可以用下面的公式来表示$U_i$应该比$U_j$排序更靠前的概率：</p><script type="math/tex; mode=display">P_{ij} \equiv P(U_i \rhd U_j) \equiv \frac{1}{1+e^{-\sigma(s_i-s_j)}}</script><p>定义$S_{ij} \in \{0,\pm1\}$为文档$i$和文档$j$被标记的标签之间的关联，即</p><script type="math/tex; mode=display">S_{ij}=\left\{ \begin{aligned} 1&&     文档i比文档j更相关\\ 0&&    文档i和文档j相关性一致\\ -1&&   文档j比文档i更相关 \end{aligned} \right.</script><p>定义$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$U_i$应该比$U_j$排序更靠前的已知概率，则可以用交叉熵定义优化目标的损失函数：</p><script type="math/tex; mode=display">\begin{align*}C&=-\overline{P}_{ij}log{P_{ij}}-(1-\overline{P}_{ij})log(1-P_{ij})\\&=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})\end{align*}</script><p>注意：$\sigma$是超参数</p><p><strong>ranknet 加速</strong></p><h2 id="2-LambdaRank"><a href="#2-LambdaRank" class="headerlink" title="2.LambdaRank"></a>2.LambdaRank</h2><p>ranket缺陷为只考虑pair的相对位置没有考虑二者在列表的整体位置</p><p>LambdaRank本质为ranknet基础上加入Listwise的指标，因此有人将LambdaRank归为listwise方法，也有归到pairwise方法</p><h3 id="2-1-RankNet的局限"><a href="#2-1-RankNet的局限" class="headerlink" title="2.1 RankNet的局限"></a>2.1 RankNet的局限</h3><p><img src="/2021/09/27/lambdamart/11.GIF" alt></p><h3 id="2-2-LambdaRank定义"><a href="#2-2-LambdaRank定义" class="headerlink" title="2.2 LambdaRank定义"></a>2.2 LambdaRank定义</h3><script type="math/tex; mode=display">\begin{align*}\frac{\partial{C}}{\partial{w_k}}&=\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}\\&=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)\\&=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)\end{align*}\\其中\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=-\frac{\partial{C}}{\partial{s_j}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)</script><p>上述公式可以进一步简化，即只考虑$S_{ij}=1$ （为什么可以？？？？？）</p><p>那么Lambda，$\lambda$，就是梯度</p><script type="math/tex; mode=display">\lambda_{ij}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}</script><p>为了加强排序中前后顺序的重要性，Lambda在原基础上引入评价指标Z（如NDCG），把交换两个文档的位置引起的评价指标的变化$|\Delta Z_{ij}|$作为其中一个因子：</p><script type="math/tex; mode=display">\lambda_{ij}=\frac{\partial{C}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta Z_{ij}|</script><p>反推出 LambdaRank 的损失函数：</p><script type="math/tex; mode=display">C=log(1+e^{\sigma (s_i-s_j)})|\Delta Z_{ij}|</script><h2 id="3-LambdaMART"><a href="#3-LambdaMART" class="headerlink" title="3.LambdaMART"></a>3.LambdaMART</h2><p>属于listwise，也有说pairwise。</p><p>LambdaMART=lambda($\lambda$)+mart(gbdt)</p><p>$\lambda$就是梯度，lambdarank就是一种loss，gbdt就是模型</p><p>lambdamart说白了就是利用gbdt计算lambdarank中s，或者说将lambdarank作为gbdt的loss</p><p>gbdt，lambdamart算法流程差异在于step1</p><p><strong>GBDT</strong>：</p><ol><li>初始化： $f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</li><li>for m=1 to M:<br>(a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \qquad i = 1,2 \cdots N$<br>(b). $\left \{ R_{jm} \right\}_1^J = \mathop{\arg\min}\limits_{\left \{ R_{jm} \right\}_1^J}\sum\limits_{i=1}^N \left [\tilde{y}_i - h_m(x_i\,;\,\left \{R_{jm},b_{jm} \right\}_1^J) \right]^2$<br>(c).  $\gamma_{jm} = \mathop{\arg\min}\limits_\gamma \sum\limits_{x_i \in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$<br>(d).  $f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^J \gamma_{jm}I(x \in R_{jm})$</li><li>输出$f_M(x)$</li></ol><p><strong>LambdaMART:</strong></p><p><img src="/2021/09/27/lambdamart/22.png" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/laolu1573/article/details/87372514">https://blog.csdn.net/laolu1573/article/details/87372514</a></p><p><a href="https://liam.page/uploads/slides/lambdamart.pdf">https://liam.page/uploads/slides/lambdamart.pdf</a></p><p><a href="https://blog.csdn.net/zpalyq110/article/details/79527653">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p><p><a href="https://zhuanlan.zhihu.com/p/86354141">https://zhuanlan.zhihu.com/p/86354141</a></p><p><a href="https://www.cnblogs.com/genyuan/p/9788294.html">https://www.cnblogs.com/genyuan/p/9788294.html</a></p><p><a href="https://blog.csdn.net/huagong_adu/article/details/40710305">https://blog.csdn.net/huagong_adu/article/details/40710305</a></p><p><a href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p><p><a href="https://www.cnblogs.com/bentuwuying/p/6690836.html">https://www.cnblogs.com/bentuwuying/p/6690836.html</a></p><p>paper原文： <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf</a></p><p><a href="https://www.jianshu.com/p/a78b3f52c221">https://www.jianshu.com/p/a78b3f52c221</a></p><p><a href="https://blog.csdn.net/zhoujialin/article/details/46697409">https://blog.csdn.net/zhoujialin/article/details/46697409</a></p><p><a href="https://blog.csdn.net/w28971023/article/details/45849659">https://blog.csdn.net/w28971023/article/details/45849659</a></p><p><a href="https://zhuanlan.zhihu.com/p/270608987">https://zhuanlan.zhihu.com/p/270608987</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> listwise </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LambdaMART </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单调栈</title>
      <link href="/2021/09/24/monotonous-stack/"/>
      <url>/2021/09/24/monotonous-stack/</url>
      
        <content type="html"><![CDATA[<p><strong>分类</strong>：</p><p>单调栈也分为<strong>单调递增栈</strong>和<strong>单调递减栈</strong></p><ul><li>单调递增栈：单调递增栈就是从栈底到栈顶数据是从大到小</li><li>单调递减栈：单调递减栈就是从栈底到栈顶数据是从小到大</li></ul><p><strong>举例：</strong>单调递增栈</p><p>现在有一组数10，3，7，4，12。从左到右依次入栈，则如果栈为空或入栈元素值小于栈顶元素值，则入栈；否则，如果入栈则会破坏栈的单调性，则需要把比入栈元素小的元素全部出栈。</p><p>10入栈时，栈为空，直接入栈，栈内元素为10。</p><p>3入栈时，栈顶元素10比3大，则入栈，栈内元素为10，3。</p><p>7入栈时，栈顶元素3比7小，则栈顶元素出栈，此时栈顶元素为10，比7大，则7入栈，栈内元素为10，7。</p><p>4入栈时，栈顶元素7比4大，则入栈，栈内元素为10，7，4。</p><p>12入栈时，栈顶元素4比12小，4出栈，此时栈顶元素为7，仍比12小，栈顶元素7继续出栈，此时栈顶元素为10，仍比12小，10出栈，此时栈为空，12入栈，栈内元素为12。</p><p><strong>伪代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">stack&lt;int&gt; st;</span><br><span class="line">//单调递增栈</span><br><span class="line">for (遍历这个数组)</span><br><span class="line">&#123;</span><br><span class="line">if (栈空 || 栈顶元素大于等于当前比较元素)</span><br><span class="line">&#123;</span><br><span class="line">入栈;</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">while (栈不为空 &amp;&amp; 栈顶元素小于当前元素)</span><br><span class="line">&#123;</span><br><span class="line">栈顶元素出栈;</span><br><span class="line">更新结果;</span><br><span class="line">&#125;</span><br><span class="line">当前数据入栈;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>应用</strong>：</p><p>主要用于$O(n)$ 解决NGE问题（Next Greater Element）</p><ul><li>比当前元素更大的下一个元素</li><li>比当前元素更大的前一个元素</li><li>比当前元素更小的下一个元素</li><li>比当前元素更小的前一个元素</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/lucky52529/article/details/89155694">https://blog.csdn.net/lucky52529/article/details/89155694</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 单调栈 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GBDT</title>
      <link href="/2021/09/24/gbdt/"/>
      <url>/2021/09/24/gbdt/</url>
      
        <content type="html"><![CDATA[<p>GBDT （Gradient Boosting Decison Tree）=Gradient Boosting+cart回归树</p><p>注意是cart回归树，不是cart分类树</p><p>说白了就是gradient boosting基学习器为cart回归树</p><h2 id="gradient-boosting算法流程"><a href="#gradient-boosting算法流程" class="headerlink" title="gradient boosting算法流程:"></a>gradient boosting算法流程:</h2><p>1.初始化：$f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</p><p>2.for m=1 to M:<br>    (a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \quad i = 1,2 \cdots N$<br>    (b).  通过最小化平方误差，用基学习器$h_m(x)$拟合$\tilde{y_i}$，$w_m = \mathop{\arg\min}\limits_w \sum\limits_{i=1}^{N} \left[\tilde{y}_i - h_m(x_i\,;\,w) \right]^2$<br>    (c).  使用line search确定步长$ρ_m$，使$L$最小，$\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))$<br>    (d).  $f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)$</p><p>3.输出$f_M(x)$</p><h2 id="GBDT算法流程："><a href="#GBDT算法流程：" class="headerlink" title="GBDT算法流程："></a>GBDT算法流程：</h2><ol><li>初始化： $f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</li><li>for m=1 to M:<br>(a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \qquad i = 1,2 \cdots N$<br>(b). $\left \{ R_{jm} \right\}_1^J = \mathop{\arg\min}\limits_{\left \{ R_{jm} \right\}_1^J}\sum\limits_{i=1}^N \left [\tilde{y}_i - h_m(x_i\,;\,\left \{R_{jm},b_{jm} \right\}_1^J) \right]^2$<br>(c).  $\gamma_{jm} = \mathop{\arg\min}\limits_\gamma \sum\limits_{x_i \in R_{jm}}L(y_i,f_{m-1}(x_i)+\gamma)$<br>(d).  $f_m(x) = f_{m-1}(x) + \sum\limits_{j=1}^J \gamma_{jm}I(x \in R_{jm})$</li><li>输出$f_M(x)$</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/zpalyq110/article/details/79527653">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p><p><a href="https://zhuanlan.zhihu.com/p/86354141">https://zhuanlan.zhihu.com/p/86354141</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> boosting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GBDT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gradient boosting</title>
      <link href="/2021/09/23/gradient_boosting/"/>
      <url>/2021/09/23/gradient_boosting/</url>
      
        <content type="html"><![CDATA[<h2 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h2><p>Gradient Boosting为boosting算法的一种，采用和AdaBoost同样的加法模型，在第m次迭代中，前m-1个基学习器都是固定的，即</p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x) + \rho_m h_m(x)  \tag{1}</script><p>核心思想是得到基学习器$h_m(x)$和权重$p_m$</p><p><strong>参数空间的梯度下降</strong>很常见，即</p><script type="math/tex; mode=display">\theta = \theta - \alpha \cdot \frac{\partial}{\partial \theta}L(\theta)</script><p>若将$f(x)$当成参数，则同样可以使用<strong>函数空间的梯度下降法</strong>：</p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x) - \rho_m \cdot \frac{\partial}{\partial f_{m-1}(x)}L(y,f_{m-1}(x)) \tag{2}</script><p>对比（1）（2），我们发现$h_m(x) \approx -\frac{\partial L(y,f_{m-1}(x))}{\partial f_{m-1}(x)}$</p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程:"></a>算法流程:</h2><p>1.初始化：$f_0(x) = \mathop{\arg\min}\limits_\gamma \sum\limits_{i=1}^N L(y_i, \gamma)$</p><p>2.for m=1 to M:<br>    (a).  计算负梯度： $\tilde{y}_i = -\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}, \quad i = 1,2 \cdots N$<br>    (b).  通过最小化平方误差，用基学习器$h_m(x)$拟合$\tilde{y_i}$，$w_m = \mathop{\arg\min}\limits_w \sum\limits_{i=1}^{N} \left[\tilde{y}_i - h_m(x_i\,;\,w) \right]^2$<br>    (c).  使用line search确定步长$ρ_m$，使$L$最小，$\rho_m = \mathop{\arg\min}\limits_{\rho} \sum\limits_{i=1}^{N} L(y_i,f_{m-1}(x_i) + \rho h_m(x_i\,;\,w_m))$<br>    (d).  $f_m(x) = f_{m-1}(x) + \rho_m h_m(x\,;\,w_m)$</p><p>3.输出$f_M(x)$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/zhubinwang/p/5170087.html">https://www.cnblogs.com/zhubinwang/p/5170087.html</a></p><p><a href="https://www.cnblogs.com/massquantity/p/9174746.html">https://www.cnblogs.com/massquantity/p/9174746.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> boosting </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gradient boosting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/2021/09/23/decison-tree/"/>
      <url>/2021/09/23/decison-tree/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/09/23/decison-tree/22.png" alt></p><p><img src="/2021/09/23/decison-tree/33.png" alt></p><p>两幅图意思一样</p><h2 id="1-构建算法"><a href="#1-构建算法" class="headerlink" title="1.构建算法"></a>1.构建算法</h2><p>常见方法的有ID3，C4.5，CART，总结如下</p><p><img src="/2021/09/23/decison-tree/11.png" alt></p><h3 id="1-1-ID3"><a href="#1-1-ID3" class="headerlink" title="1.1 ID3"></a>1.1 ID3</h3><p>假设训练数据集为 $D$,∣$D∣$表示其大小。设有$K$个分类$ C_1,C_2,…,C_K$。设特征集为$\textbf{A}$,假设某个特征$ A$ 有$ n$ 个不同的取值 $\{a_1,a_2,…,a_n\}$,根据特征$A$的取值将 $D$ 划分成$n$个子集。记子集 $D_i$中属于类$ C_k$的样本集合为 $D_{ik}$。</p><p>数据集$D$的经验熵$H(D)$:</p><script type="math/tex; mode=display">H(D) = - \sum_{j=1}^K \frac {|C_j|}{|D|} \log \frac {|C_j|}{|D|}</script><p>特征$A$对数据集$ D$的经验条件熵$H(D|A)$</p><script type="math/tex; mode=display">% <![CDATA[\begin{equation*}\begin{split}H(D|A) &= \sum_{i=1}^n \frac {|D_i|}{|D|} H(D_i) \\& = - \sum_{i=1}^n  \frac {|D_i|}{|D|}\sum_{j=1}^K \frac {|D_{ij}|}{|D_i|} \log \frac {|D_{ij}|}{|D_i|} \\\end{split}\end{equation*} %]]></script><p><strong>信息增益</strong>$g(D,A)$</p><script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script><p><strong>算法流程</strong>:</p><ol><li>若$D$中所有实例都属于同一类 $C_k$,则 $T$ 为单节点树,并将类 $C_k$作为该节点的类标记,返回$T$.</li><li>若$\textbf{A}=\phi$,则$T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$.</li><li>否则,按照信息增益的算法,计算每个特征对$D$的信息增益,取信息增益最大的特征 $A_g$.</li><li>如果$A_g&lt; \varepsilon$,则置 $T$为单节点树,并将$D$中实例最大的类$C_k$作为该节点的类标记,返回$T$.</li><li>否则,对$A_g$的每一可能值 $a_i$,依$A_g=a_i$将$D$分成若干非空子集$D_i$</li><li>以$D_i$为训练集,以$\textbf{A}- A_g $为特征集,递归地调用步骤1到步骤5,得到子树 $T_i$,全部 $T_i$构成$T$,返回$T$.</li></ol><h3 id="1-2-C4-5"><a href="#1-2-C4-5" class="headerlink" title="1.2 C4.5"></a>1.2 C4.5</h3><p> <strong>C4.5算法流程与ID3相类似</strong>，只不过将信息增益改为信息增益比，以解决偏向取值较多的属性的问题，另外它可以处理连续型属性。</p><p>分裂信息 $SplitInformation(D,A)$</p><script type="math/tex; mode=display">SplitInformation(D,A) = -\sum_{i=1}^n \frac {|D_i|}{|D|} \log \frac {|D_i|}{|D|}</script><p><strong>信息增益比</strong> $GainRatio(D, A)$</p><script type="math/tex; mode=display">GainRatio(D, A) = \frac {g(D, A)} {SplitInformation(D, A)}</script><h3 id="1-3-CART"><a href="#1-3-CART" class="headerlink" title="1.3 CART"></a>1.3 CART</h3><h4 id="1-3-1-CART分类树"><a href="#1-3-1-CART分类树" class="headerlink" title="1.3.1 CART分类树"></a>1.3.1 CART分类树</h4><p>CART分类树算法使用基尼系数来代替信息增益（比），基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。</p><p>对于样本$D$，个数为$|D|$，假设$K$个类别，第$k$个类别的数量为$|C_k|$，则样本$D$的基尼系数表达式：</p><script type="math/tex; mode=display">Gini(D)=1-\sum_{k=1}^{K}(\frac{|C_k|}{|D|})^2</script><p>对于样本$D$，个数为$|D|$，根据特征$A$的某个值$a$，把$D$分成$|D_1|$和$|D_2|$，则在特征$A=a$的条件下，样本$D$的<strong>基尼系数</strong>表达式为：</p><script type="math/tex; mode=display">Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)</script><p><strong>算法流程</strong>：算法输入训练集$D$，基尼系数的阈值，样本个数阈值。输出的是决策树$T$。</p><p>(1)、对于当前节点的数据集为$D$，如果样本个数小于阈值或没有特征，则当前节点停止递归，返回决策树。</p><p>(2)、计算样本集$D$的基尼系数，如果基尼系数小于阈值，则当前节点停止递归，返回决策树。</p><p>(3)、计算当前节点所有特征的各个特征值对数据集$D$的基尼系数</p><p>(4)、在计算出来的所有基尼系数中，选择基尼系数最小的特征$A$和对应的特征值$a$，并把数据集划分成两部分$D_1$和$D_2$，同时建立当前节点的左右节点，左节点的数据集$D$为$D_1$，右节点的数据集$D$为$D_2$。</p><p>(5)、对左右的子节点递归的调用1-4步，生成决策树。</p><h4 id="1-3-2-CART回归树"><a href="#1-3-2-CART回归树" class="headerlink" title="1.3.2 CART回归树"></a>1.3.2 CART回归树</h4><p>对回归树用<strong>平方误差</strong>最小化准则</p><p><strong>算法流程</strong>：输入为训练数据$D$，输出为回归树$f(x)$</p><p>(1) 选择最优的切分变量$j$和切分点$s$，遍历$j$，对固定的$j$遍历$s$，求解</p><script type="math/tex; mode=display">\min  \limits_{j,s} \ [\min \limits_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min \limits_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]</script><p>(2) 用选定的$(j,s)$划分区域并决定输出值</p><script type="math/tex; mode=display">R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}> s\}\\\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,m=1,2</script><p>(3) 继续对两个子区域调用步骤（1）（2），直至满足停止条件</p><p>(4) 将输入空间划分成$M$个区域$R_1,R_2,…,R_M$，生成回归树</p><script type="math/tex; mode=display">f(x)=\sum_{m=1}^{M}\hat{c}_mI(x\in R_m)</script><h3 id="1-4-多变量决策树"><a href="#1-4-多变量决策树" class="headerlink" title="1.4 多变量决策树"></a>1.4 多变量决策树</h3><p>无论ID3，C4.5，CART都是选择<strong>一个最优的特征</strong>做分类决策，但大多数，分类决策不是由某一个特征决定，而是一组特征。这样得到的决策树更加准确，这种决策树叫多变量决策树(multi-variate decision tree)。在选择最优特征的时，多变量决策树不是选择某一个最优特征，而是选择<strong>一个最优的特征线性组合</strong>做决策。</p><p>代表算法OC1。</p><p><img src="/2021/09/23/decison-tree/1.png" alt></p><h2 id="2-剪枝"><a href="#2-剪枝" class="headerlink" title="2.剪枝"></a>2.剪枝</h2><p>剪枝(pruning)是解决决策树过拟合的主要手段，通过剪枝可以大大提升决策树的泛化能力。通常，剪枝处理可分为：预剪枝，后剪枝。</p><ul><li>预剪枝：通过启发式方法，在生成决策树过程中对划分进行考察，若当前结点的划分影响决策树的泛化性能，则停止划分，并将其标记为叶节点</li><li>后剪枝：对已有的决策树，自底向上的对非叶结点进行考察，若该结点对应的子树替换为叶结点能提升决策树的泛化能力，则将改子树替换为叶结点</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/89607509">https://zhuanlan.zhihu.com/p/89607509</a></p><p><a href="https://www.cnblogs.com/wxquare/p/5379970.html">https://www.cnblogs.com/wxquare/p/5379970.html</a></p><p><a href="https://blog.csdn.net/qq_43468807/article/details/105969232">https://blog.csdn.net/qq_43468807/article/details/105969232</a></p><p><a href="http://leijun00.github.io/2014/09/decision-tree/">http://leijun00.github.io/2014/09/decision-tree/</a></p><p><a href="https://zhuanlan.zhihu.com/p/89607509">https://zhuanlan.zhihu.com/p/89607509</a></p><p><a href="https://www.cnblogs.com/wxquare/p/5379970.html">https://www.cnblogs.com/wxquare/p/5379970.html</a></p><p><a href="https://blog.csdn.net/qq_43468807/article/details/105969232">https://blog.csdn.net/qq_43468807/article/details/105969232</a></p><p><a href="http://leijun00.github.io/2014/09/decision-tree/">http://leijun00.github.io/2014/09/decision-tree/</a></p><p><a href="https://www.cnblogs.com/wqbin/p/11689709.html">https://www.cnblogs.com/wqbin/p/11689709.html</a></p><p><a href="https://www.cnblogs.com/keye/p/10564914.html">https://www.cnblogs.com/keye/p/10564914.html</a></p><p><a href="https://www.cnblogs.com/keye/p/10601501.html">https://www.cnblogs.com/keye/p/10601501.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1486712">https://cloud.tencent.com/developer/article/1486712</a></p><p><a href="https://blog.csdn.net/weixin_44132485/article/details/106502422">https://blog.csdn.net/weixin_44132485/article/details/106502422</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICATION</title>
      <link href="/2021/09/17/doc-bert/"/>
      <url>/2021/09/17/doc-bert/</url>
      
        <content type="html"><![CDATA[<p>原版BERT的最大输入为512，为了使得BERT能解决超长文本的问题，作者在finetune阶段提出了两种策略来弥补这个问题，即利用BERT+LSTM或者BERT+transformer。</p><p>核心步骤：</p><p>1.split the input sequence into segments of a fixed size with overlap.</p><p>2.For each of these segments, we obtain H or P from BERT model.</p><p><img src="/2021/09/17/doc-bert/11.JPG" alt></p><p>3.We then stack these segment-level representations into a sequence, which serves as input to a small (100-dimensional) LSTM layer.//replacing the LSTM recurrent layer in favor of a small Transformer model</p><p>4.Finally, we use two fully connected layers with ReLU (30-dimensional) and softmax (the same dimensionality as the number of classes) activations to obtain the final predictions.</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 超长文本 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN总结</title>
      <link href="/2021/09/15/rnn/"/>
      <url>/2021/09/15/rnn/</url>
      
        <content type="html"><![CDATA[<h2 id="1-单元"><a href="#1-单元" class="headerlink" title="1.单元"></a>1.单元</h2><h3 id="1-1-普通RNN单元"><a href="#1-1-普通RNN单元" class="headerlink" title="1.1 普通RNN单元"></a>1.1 普通RNN单元</h3><h3 id="1-2-LSTM"><a href="#1-2-LSTM" class="headerlink" title="1.2 LSTM"></a>1.2 LSTM</h3><h3 id="1-3-GRU"><a href="#1-3-GRU" class="headerlink" title="1.3 GRU"></a>1.3 GRU</h3><h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><h3 id="1-1-输入、输出"><a href="#1-1-输入、输出" class="headerlink" title="1.1 输入、输出"></a>1.1 输入、输出</h3><p><img src="/2021/09/15/rnn/11.JPG" alt></p><h3 id="1-2-是否双向"><a href="#1-2-是否双向" class="headerlink" title="1.2 是否双向"></a>1.2 是否双向</h3><p><img src="/2021/09/15/rnn/22.png" alt></p><h3 id="1-3-是否堆叠"><a href="#1-3-是否堆叠" class="headerlink" title="1.3 是否堆叠"></a>1.3 是否堆叠</h3><p><img src="/2021/09/15/rnn/33.png" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/gaohanjie123/article/details/88699664">https://blog.csdn.net/gaohanjie123/article/details/88699664</a></p><p><a href="https://www.cnblogs.com/Luv-GEM/p/10788849.html">https://www.cnblogs.com/Luv-GEM/p/10788849.html</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 特征提取器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RoBERTa A Robustly Optimized BERT Pretraining Approach</title>
      <link href="/2021/09/15/roberta/"/>
      <url>/2021/09/15/roberta/</url>
      
        <content type="html"><![CDATA[<h2 id="1-和BERT比较"><a href="#1-和BERT比较" class="headerlink" title="1.和BERT比较"></a>1.和BERT比较</h2><p>在结构上和原版BERT没有差异，主要的改动在于：</p><p><img src="/2021/09/15/roberta/55.JPG" alt></p><h2 id="2-改动分析"><a href="#2-改动分析" class="headerlink" title="2.改动分析"></a>2.改动分析</h2><h3 id="2-1-Static-vs-Dynamic-Masking"><a href="#2-1-Static-vs-Dynamic-Masking" class="headerlink" title="2.1 Static vs. Dynamic Masking"></a>2.1 Static vs. Dynamic Masking</h3><p>static masking: 原本的BERT采用的是static mask的方式，就是在create pretraining data中，先对数据进行提前的mask</p><p>dynamic masking: 每一次将训练example喂给模型的时候，才进行随机mask。</p><p>结果对比：</p><p><img src="/2021/09/15/roberta/22.JPG" alt></p><p>结论：动态占优</p><h3 id="2-2-Model-Input-Format-and-Next-Sentence-Prediction"><a href="#2-2-Model-Input-Format-and-Next-Sentence-Prediction" class="headerlink" title="2.2 Model Input Format and Next Sentence Prediction"></a>2.2 Model Input Format and Next Sentence Prediction</h3><p>做了结果对比试验，结果如下：</p><p><img src="/2021/09/15/roberta/33.JPG" alt></p><p>结论：</p><p>Model Input Format: </p><p>​    1.find that using individual sentences hurts performance on downstream tasks</p><p>Next Sentence Prediction: </p><p>​    1.removing the NSP loss matches or slightly improves downstream task performance</p><h3 id="2-3-Training-with-large-batches"><a href="#2-3-Training-with-large-batches" class="headerlink" title="2.3 Training with large batches"></a>2.3 Training with large batches</h3><p><img src="/2021/09/15/roberta/44.JPG" alt></p><h3 id="2-4-Text-Encoding"><a href="#2-4-Text-Encoding" class="headerlink" title="2.4 Text Encoding"></a>2.4 Text Encoding</h3><p>采用BBPE而不是wordpiece</p><h2 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3 常见问题"></a>3 常见问题</h2><p><strong>1 roberta tokenizer 没有token_type_ids？</strong><br>roberta 取消了NSP，所以不需要segment embedding 也就不需要token_type_ids，但是使用的时候发现中文是有token_type_ids的，英文没有token_type_ids的。没有token_type_ids，两句话怎么区别，分隔符sep还是有的，只是没有segment embedding </p><p><strong>2 使用避坑</strong></p><p><a href="https://blog.csdn.net/zwqjoy/article/details/107533184">https://blog.csdn.net/zwqjoy/article/details/107533184</a></p><p><a href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/103205929">https://zhuanlan.zhihu.com/p/103205929</a></p><p><a href="https://zhuanlan.zhihu.com/p/143064748">https://zhuanlan.zhihu.com/p/143064748</a></p><p><a href="https://blog.csdn.net/zwqjoy/article/details/107533184">https://blog.csdn.net/zwqjoy/article/details/107533184</a></p><p><a href="https://hub.fastgit.org/ymcui/Chinese-BERT-wwm">https://hub.fastgit.org/ymcui/Chinese-BERT-wwm</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python环境</title>
      <link href="/2021/09/10/python-env/"/>
      <url>/2021/09/10/python-env/</url>
      
        <content type="html"><![CDATA[<h2 id="1-环境管理工具"><a href="#1-环境管理工具" class="headerlink" title="1.环境管理工具"></a>1.环境管理工具</h2><p>python版本主要为2,3两个大版本</p><p>anaconda管理python和包的版本</p><h2 id="2-包"><a href="#2-包" class="headerlink" title="2.包"></a>2.包</h2><h4 id="2-1-下载包"><a href="#2-1-下载包" class="headerlink" title="2.1 下载包"></a>2.1 下载包</h4><p>下载源有官方源，阿里源，豆瓣源，清华源等</p><h5 id="1-离线下载"><a href="#1-离线下载" class="headerlink" title="1 离线下载"></a>1 离线下载</h5><h5 id="2-在线下载"><a href="#2-在线下载" class="headerlink" title="2 在线下载"></a>2 在线下载</h5><p>下载工具有pip，conda</p><p><strong>更换pip源</strong></p><p>修改文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.pip/pip.conf  # 没有就创建一个，在 ~/.pip/下</span><br></pre></td></tr></table></figure><p>增加内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">index-url=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"># index-url=http://pypi.douban.com/simple/</span><br><span class="line">[install]</span><br><span class="line">trusted-host=pypi.tuna.tsinghua.edu.cn</span><br></pre></td></tr></table></figure><p><strong>查看路径</strong></p><p>which python, which pip ,which conda （~用户主目录， /根目录）</p><h4 id="2-2-使用包"><a href="#2-2-使用包" class="headerlink" title="2.2 使用包"></a>2.2 使用包</h4><p>import </p><p>绝对路径：从工程的最外层开始</p><p>相对路径：利用.（同级）和..（上级）</p><p>怎么添加包的搜路径</p><p><a href="https://blog.csdn.net/weixin_40449300/article/details/79327201">https://blog.csdn.net/weixin_40449300/article/details/79327201</a></p><h2 id="3-ubuntu修改python环境变量"><a href="#3-ubuntu修改python环境变量" class="headerlink" title="3 ubuntu修改python环境变量"></a>3 ubuntu修改python环境变量</h2><p>1.vim ~/.bashrc</p><p>2.添加如下内容</p><p>export PYTHON_HOME=/usr/local/anaconda3/bin</p><p>export PATH=$PYTHON_HOME/bin:$PATH</p><p>export PATH=/home/user_name/anaconda3/bin:$PATH # 指定python路径</p><h2 id="4-ubuntu修改pyton默认版本"><a href="#4-ubuntu修改pyton默认版本" class="headerlink" title="4.ubuntu修改pyton默认版本"></a>4.ubuntu修改pyton默认版本</h2><p><a href="https://blog.csdn.net/White_Idiot/article/details/78240298">https://blog.csdn.net/White_Idiot/article/details/78240298</a></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>降维</title>
      <link href="/2021/09/08/dimension-reduction/"/>
      <url>/2021/09/08/dimension-reduction/</url>
      
        <content type="html"><![CDATA[<h2 id="1-意义"><a href="#1-意义" class="headerlink" title="1.意义"></a>1.意义</h2><p>1.高纬空间样本具有稀疏性，容易欠拟合</p><p>2.可视化</p><p>3.维度过大导致训练时间长，预测慢</p><h2 id="2-分类"><a href="#2-分类" class="headerlink" title="2.分类"></a>2.分类</h2><p>大致分为线形降维度和非线性降维，线形降维包括PCA，LDA等，非线性降维包括LLE，t-SNE，auto encoder等。</p><h2 id="3-PCA系列"><a href="#3-PCA系列" class="headerlink" title="3.PCA系列"></a>3.PCA系列</h2><h3 id="3-1-PCA"><a href="#3-1-PCA" class="headerlink" title="3.1 PCA"></a>3.1 PCA</h3><p>假设矩阵$x\in \mathbb{R}^{ n}$，假设有$M$个样本，将原始数据按列组成$M$ 行$ n $列矩阵$ X\in \mathbb{R}^{M\times n}$，PCA的使用过程为：</p><p>1.计算协方差矩阵$G_t \in \mathbb{R}^{n \times n}$</p><script type="math/tex; mode=display">G_t=\frac{1}{M}\sum_{j=1}^{M}(X-\overline{X})^T(X-\overline{X})</script><p>注意，其中$\overline{X}\in \mathbb{R}^{n}$为列的均值，$X-\overline{X}$表示将$ X$ 的每一列进行零均值化，即减去这一列的均值</p><p>2.求出协方差矩阵的特征值及对应的特征向量</p><script type="math/tex; mode=display">(U,\sum,V)=SVD(G_t)</script><p>3.将特征向量按对应的特征值大小排列，取前 $k$ 列组成矩阵 $P\in \mathbb{R}^{n \times k} $</p><script type="math/tex; mode=display">P=U(:,1:k)</script><p>4.实现数据降维</p><script type="math/tex; mode=display">y_{[1\times k]}=x_{[1\times n ]}P_{[n\times k]}</script><p>局限：</p><p>​    a. PCA只能针对1D的向量，对于2D的矩阵而言，比如图片数据，需要先flatten成向量</p><h3 id="3-2-2DPCA"><a href="#3-2-2DPCA" class="headerlink" title="3.2 2DPCA"></a>3.2 2DPCA</h3><p>将2D的矩阵flatten成向量其实丢失了行列的位置信息，为了直接在2D的矩阵上实现降维，提出了2DPCA。</p><p>假设有原始矩阵$A\in\mathbb{R}^{m \times n }$，使用过程如下：</p><p>1.计算协方差矩阵</p><script type="math/tex; mode=display">G_t=\frac{1}{M}\sum_{j=1}^{M}(A_j-\overline{A})^{T}(A_j-\overline{A})\\\overline{A}=\frac{1}{M}\sum_{j=1}^{M}A_j</script><p>2.求出协方差矩阵的特征值及对应的特征向量</p><script type="math/tex; mode=display">(U,\sum,V)=SVD(G_t)</script><p>3.将特征向量按对应的特征值大小排列，取前 $k$ 列组成矩阵</p><script type="math/tex; mode=display">X=U(:,1:k)</script><p>4.实现数据降维</p><script type="math/tex; mode=display">Y_{[m\times k]}=A_{[m\times n ]}X_{[n\times k]}</script><h3 id="3-3-（2D）2PCA"><a href="#3-3-（2D）2PCA" class="headerlink" title="3.3 （2D）2PCA"></a>3.3 （2D）2PCA</h3><p>作者证明了2DPCA只是在行上工作，然后提出了Alternative 2DPCA可以工作在列上，最后将其结合得到（2D）2PCA，使其可以同时在行列工作</p><p>假设有原始矩阵$A\in\mathbb{R}^{m \times n }$，使用过程如下：</p><p>1.计算协方差矩阵</p><script type="math/tex; mode=display">G_x=\frac{1}{M}\sum_{k=1}^{M}\sum_{i=1}^{m}(A_k^{(i)}-\overline{A}^{(i)})^{T}(A_k^{(i)}-\overline{A}^{(i)})</script><p>其中$A_k=[(A_k^{(1)})^{T} \ (A_k^{(2)})^{T} \ …\ (A_k^{(m)})^{T}]^{T},\overline{A}=[(\overline{A}^{(1)})^{T} \ (\overline{A}^{(2)})^{T} \ …\ (\overline{A}^{(m)})^{T}]^{T}, \ A_k^{(i)}, \overline{A}^{(i)}$表示$A_k,\overline{A}$的第$i$行</p><script type="math/tex; mode=display">\\G_z=\frac{1}{M}\sum_{k=1}^{M}\sum_{j=1}^{n}(A_k^{(j)}-\overline{A}^{(j)})(A_k^{(j)}-\overline{A}^{(j)})^{T}</script><p>其中$A_k=[(A_k^{(1)}) \ (A_k^{(2)}) \ …\ (A_k^{(n)})],\overline{A}=[(\overline{A}^{(1)}) \ (\overline{A}^{(2)}) \ …\ (\overline{A}^{(n)})],A_k^{(j)},\overline{A}^{(j)}$表示$A_k,\overline{A}$的第$j$列</p><p>2.求出协方差矩阵的特征值及对应的特征向量</p><script type="math/tex; mode=display">(U_x,\sum x,V_x)=SVD(G_x)\\(U_z,\sum z,V_z)=SVD(G_z)</script><p>3.将特征向量按对应的特征值大小排列</p><script type="math/tex; mode=display">X=U_x(:,1:k) \in \mathbb{R}^{n\times k}\\Z=U_z(:,1:q) \in \mathbb{R}^{m\times q}</script><p>4.实现数据降维</p><script type="math/tex; mode=display">C_{[q\times k]}=Z^{T}_{[q\times m]}A_{[m\times n ]}X_{[n\times k]}</script><h2 id="4-t-SNE"><a href="#4-t-SNE" class="headerlink" title="4.t-SNE"></a>4.t-SNE</h2><h3 id="4-1-原理"><a href="#4-1-原理" class="headerlink" title="4.1 原理"></a>4.1 原理</h3><p>可以参考 <a href="https://blog.csdn.net/scott198510/article/details/76099700">https://blog.csdn.net/scott198510/article/details/76099700</a></p><h3 id="4-2代码"><a href="#4-2代码" class="headerlink" title="4.2代码"></a>4.2代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">from matplotlib import pylab</span><br><span class="line">import  torch</span><br><span class="line">import  pandas as pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">embdding=torch.load(path1)</span><br><span class="line">words = pd.read_csv(path2)</span><br><span class="line"></span><br><span class="line">words_embedded = TSNE(n_components=2).fit_transform(embdding)</span><br><span class="line"></span><br><span class="line">pylab.figure(figsize=(20, 20))</span><br><span class="line">for i, label in enumerate(words):</span><br><span class="line">  x, y = words_embedded[i, :]</span><br><span class="line">  pylab.scatter(x, y)</span><br><span class="line">  pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords=&#x27;offset points&#x27;,</span><br><span class="line">                 ha=&#x27;right&#x27;, va=&#x27;bottom&#x27;)</span><br><span class="line">pylab.show()</span><br></pre></td></tr></table></figure><h2 id="5-auto-encoder"><a href="#5-auto-encoder" class="headerlink" title="5.auto encoder"></a>5.auto encoder</h2><p><img src="/2021/09/08/dimension-reduction/1.jpg" alt></p><p>AutoEncoder在优化过程中无需使用样本的label，通过最小化重构误差希望学习到样本的抽象特征表示z，这种无监督的优化方式大大提升了模型的通用性。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/68903857">https://zhuanlan.zhihu.com/p/68903857</a></p><p><a href="https://blog.csdn.net/scott198510/article/details/76099700">https://blog.csdn.net/scott198510/article/details/76099700</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 降维 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则化</title>
      <link href="/2021/09/07/regulize/"/>
      <url>/2021/09/07/regulize/</url>
      
        <content type="html"><![CDATA[<p>是机器学习中对原始损失函数引入额外信息，以便防止<strong>过拟合</strong>和提高模型泛化性能的一类方法的统称。</p><h2 id="1-L1正则（Lasso回归）"><a href="#1-L1正则（Lasso回归）" class="headerlink" title="1.L1正则（Lasso回归）"></a>1.L1正则（<strong>Lasso回归</strong>）</h2><p>L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。</p><script type="math/tex; mode=display">\begin{align*}L_{L1}(w)&=L(w)+\lambda\Vert w \Vert_1=L(w)+\lambda\sum_{i=1}^{N}|w_i|\\ \frac{\partial L_{L1}}{\partial w_i}&=\frac{\partial L}{\partial w_i}+\lambda \ sgn(w_i)\\w_i &\rightarrow w_i-\eta(\frac{\partial L}{\partial w_i}+\lambda \ sgn(w_i))\rightarrow w_i-\eta\lambda \ sgn(w_i)-\eta\frac{\partial L}{\partial w_i}\end{align*}</script><p>L1是每次减去一个常数的收敛，所以L1更容易收敛到0。</p><h2 id="2-L2正则（Ridge回归）"><a href="#2-L2正则（Ridge回归）" class="headerlink" title="2.L2正则（Ridge回归）"></a>2.L2正则（<strong>Ridge回归</strong>）</h2><p>L2正则化使得参数平滑。</p><script type="math/tex; mode=display">\begin{align*}L_{L2}(w)&=L(w)+\lambda\Vert w \Vert_2^2=L(w)+\lambda\sum_{i=1}^{N}w_i^2\\ \frac{\partial L_{L2}}{\partial w_i}&=\frac{\partial L}{\partial w_i}+2\lambda w_i\\w_i& \rightarrow w_i-\eta(\frac{\partial L}{\partial w_i}+2\lambda w_i)\rightarrow(1-2\eta\lambda)w_i-\eta \frac{\partial L}{\partial w_i}\end{align*}</script><p>L2是每次乘上一个小于1的倍数进行收敛，所以L2使得参数平滑。</p><h2 id="3-dropout"><a href="#3-dropout" class="headerlink" title="3.dropout"></a>3.dropout</h2><p><img src="/2021/09/07/regulize/1.png" alt></p><p><strong>使用</strong>：在训练时，每个神经单元以概率$p$被保留(Dropout丢弃率为$1−p$)；在预测阶段，每个神经单元都是存在的。</p><p><strong>原理</strong>：神经网络通过Dropout层以一定比例随即的丢弃神经元，使得每次训练的网络模型都不相同，多个Epoch下来相当于训练了多个模型，同时每一个模型都参与了对最终结果的投票，从而提高了模型的泛化能力，类似<strong>bagging</strong>。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/zingp/p/11631913.html">https://www.cnblogs.com/zingp/p/11631913.html</a></p><p><a href="https://blog.csdn.net/b876144622/article/details/81276818">https://blog.csdn.net/b876144622/article/details/81276818</a></p><p><a href="https://www.zhihu.com/question/37096933/answer/70494622">https://www.zhihu.com/question/37096933/answer/70494622</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>损失函数</title>
      <link href="/2021/09/07/loss-func/"/>
      <url>/2021/09/07/loss-func/</url>
      
        <content type="html"><![CDATA[<p>损失函数一般都要用可导函数，因为常用的优化算法，比如梯度下降，牛顿法，都需要导数。</p><h2 id="1-回归损失"><a href="#1-回归损失" class="headerlink" title="1.回归损失"></a>1.回归损失</h2><h3 id="1-1-Mean-Squared-Error"><a href="#1-1-Mean-Squared-Error" class="headerlink" title="1.1 Mean Squared Error"></a>1.1 Mean Squared Error</h3><script type="math/tex; mode=display">L=\frac{1}{N}\sum_{i=1}^{N}(y_i-\hat{y}_i)^2</script><h3 id="1-2-Mean-Absolute-Error"><a href="#1-2-Mean-Absolute-Error" class="headerlink" title="1.2 Mean Absolute Error"></a>1.2 Mean Absolute Error</h3><script type="math/tex; mode=display">L=\frac{1}{N}\sum_{i=1}^{N}|y_i-\hat{y}_i|</script><h3 id="1-3-Huber-Loss-Smooth-Mean-Absolute-Error-Loss"><a href="#1-3-Huber-Loss-Smooth-Mean-Absolute-Error-Loss" class="headerlink" title="1.3 Huber Loss ( Smooth Mean Absolute Error Loss )"></a>1.3 Huber Loss ( Smooth Mean Absolute Error Loss )</h3><script type="math/tex; mode=display">L=\frac{1}{N}\sum_{i=1}^{N}[\mathbb{1}_{|y_i-\hat{y_i}|\le \delta}\frac{(y_i-\hat{y_i})^2}{2}+\mathbb{1}_{|y_i-\hat{y_i}|> \delta} \ (\delta \cdot |y_i-\hat{y_i}|-\frac{1}{2}\delta^{2})]</script><h2 id="2-分类损失"><a href="#2-分类损失" class="headerlink" title="2.分类损失"></a>2.分类损失</h2><h3 id="2-1-Cross-entropy-loss"><a href="#2-1-Cross-entropy-loss" class="headerlink" title="2.1 Cross-entropy loss"></a>2.1 Cross-entropy loss</h3><p><a href="https://zhuanlan.zhihu.com/p/100921909">https://zhuanlan.zhihu.com/p/100921909</a></p><p><img src="/2021/09/07/loss-func/1.JPG" style="zoom:25%;"></p><ul><li>K是种类数量</li><li>y是标签</li><li>p是神经网络的输出，也就是指类别是i的概率</li></ul><h3 id="2-2-Hinge-loss"><a href="#2-2-Hinge-loss" class="headerlink" title="2.2 Hinge loss"></a>2.2 Hinge loss</h3><script type="math/tex; mode=display">L=max(0,1-y\cdot \hat{y})</script><p>SVM模型的损失函数本质上就是 Hinge Loss + L2 正则化</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/m0_38007695/article/details/114983574">https://blog.csdn.net/m0_38007695/article/details/114983574</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> loss </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 损失函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习</title>
      <link href="/2021/09/06/ensemble/"/>
      <url>/2021/09/06/ensemble/</url>
      
        <content type="html"><![CDATA[<p>目前常见的集成学习可以分类为：<strong>1.Bagging 2.Boosting 3.Stacking 4.Blending</strong></p><h2 id="1-Bagging"><a href="#1-Bagging" class="headerlink" title="1.Bagging"></a>1.Bagging</h2><p><strong>bagging是解决variance问题。</strong></p><p><img src="/2021/09/06/ensemble/2.jpg" alt></p><h2 id="2-Boosting"><a href="#2-Boosting" class="headerlink" title="2.Boosting"></a>2.Boosting</h2><p><strong>boosting是解决bias问题。</strong></p><p>Bagging，Boosting二者之间的区别</p><p><a href="https://zhuanlan.zhihu.com/p/81340270">https://zhuanlan.zhihu.com/p/81340270</a></p><p><img src="/2021/09/06/ensemble/4.jpg" alt></p><h2 id="3-Stacking"><a href="#3-Stacking" class="headerlink" title="3.Stacking"></a>3.Stacking</h2><p>stacking和boosting的最大区别在于：boosting的基学习器是一个，stacking的基学习器是多个</p><p><img src="/2021/09/06/ensemble/6.jpg" alt></p><h2 id="4-Blending"><a href="#4-Blending" class="headerlink" title="4.Blending"></a>4.Blending</h2><p>和stacking区别： <a href="https://www.jianshu.com/p/4380cd1def76">https://www.jianshu.com/p/4380cd1def76</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/105038453">https://zhuanlan.zhihu.com/p/105038453</a></p><p><a href="https://zhuanlan.zhihu.com/p/126968534">https://zhuanlan.zhihu.com/p/126968534</a></p><p><a href="https://blog.csdn.net/starter_____/article/details/79328749">https://blog.csdn.net/starter_____/article/details/79328749</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 集成学习 </category>
          
          <category> 集成学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见激活函数</title>
      <link href="/2021/09/06/activate-func/"/>
      <url>/2021/09/06/activate-func/</url>
      
        <content type="html"><![CDATA[<p><strong>作用：激活函数是来向神经网络中引入非线性因素的，通过激活函数，神经网络就可以拟合各种曲线</strong></p><h3 id="1-sigmoid"><a href="#1-sigmoid" class="headerlink" title="1.sigmoid"></a>1.sigmoid</h3><p><img src="/2021/09/06/activate-func/1.jpg" alt></p><p><img src="/2021/09/06/activate-func/2.jpg" alt></p><script type="math/tex; mode=display">\begin{align*}y&=\frac{1}{1+e^{-x}}\\y^{'}&=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})=y(1-y)\end{align*}</script><p>一般应用在二分类的输出层</p><p><strong>缺点</strong>：</p><p>​    1.sigmoid 极容易导致梯度消失问题，可以从导数曲线可以看出，绝大多数的导数值为0</p><p>​    2.Sigmoid 函数的输出不是以零为中心的（non-zero-centered），这会导致神经网络收敛较慢，详细原因请参考 <a href="https://liam.page/2018/04/17/zero-centered-active-function/">https://liam.page/2018/04/17/zero-centered-active-function/</a></p><h3 id="2-softmax"><a href="#2-softmax" class="headerlink" title="2.softmax"></a>2.softmax</h3><script type="math/tex; mode=display">S_i=\frac{e^i}{\sum_je^j}</script><p>和sigmoid关系：Softmax函数是二分类函数Sigmoid在多分类上的推广</p><p><a href="https://zhuanlan.zhihu.com/p/356976844">https://zhuanlan.zhihu.com/p/356976844</a></p><h3 id="3-tanh"><a href="#3-tanh" class="headerlink" title="3.tanh"></a>3.tanh</h3><p><img src="/2021/09/06/activate-func/3.jpg" alt></p><p><img src="/2021/09/06/activate-func/4.jpg" alt></p><script type="math/tex; mode=display">\begin{align*}y&=tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\\y^{'}&=1-(tanh(x))^{2}\end{align*}</script><p><strong>优点</strong>:</p><p>​    1.tanh解决了sigmoid中的 zero-centered 问题</p><p><strong>缺点</strong>： </p><p>​    2.对于梯度消失问题依旧无能为力。</p><h3 id="4-Relu系列"><a href="#4-Relu系列" class="headerlink" title="4.Relu系列"></a>4.Relu系列</h3><h3 id="4-1-Relu"><a href="#4-1-Relu" class="headerlink" title="4.1 Relu"></a>4.1 Relu</h3><p><img src="/2021/09/06/activate-func/5.jpg" alt></p><script type="math/tex; mode=display">\begin{align*}y&=max(0,x)\\y^{'}&=\left\{\begin{array}{cl}1 &  \ x \ge 0 \\0 &  \ x < 0 \\\end{array} \right.\end{align*}</script><p><strong>优点</strong>:</p><p>​    1.可以缓解梯度消失，因为导数在正数部分是恒等于1的</p><p><strong>缺点</strong>：</p><p>​    1.Relu的输出不是zero-centered</p><p>​    2.由于负数部分导数恒为0，会导致一些神经元无法激活，叫做Dead ReLU Problem</p><h3 id="4-2-leaky-Relu"><a href="#4-2-leaky-Relu" class="headerlink" title="4.2 leaky Relu"></a>4.2 leaky Relu</h3><p>leaky Relu就是为了解决Relu的0区间带来的影响，其数学表达为：</p><script type="math/tex; mode=display">\begin{align*}y&=\left\{\begin{array}{cl}x &  \ x \ge 0 \\kx &  \ x < 0 \\\end{array} \right.\\y^{'}&=\left\{\begin{array}{cl}1 &  \ x \ge 0 \\k &  \ x < 0 \\\end{array} \right.\end{align*}</script><p>其中$k$是为超参数，一般数值较小，比如0.01</p><h3 id="4-3-Elu"><a href="#4-3-Elu" class="headerlink" title="4.3 Elu"></a>4.3 Elu</h3><p><img src="/2021/09/06/activate-func/6.png" alt></p><p>Elu激活函数也是为了解决Relu的0区间带来的影响，其数学表达为：</p><script type="math/tex; mode=display">\begin{align*}y&=\left\{\begin{array}{cl}x &  \ x \ge 0 \\\alpha(e^{x}-1) &  \ x < 0 \\\end{array} \right.\\y^{'}&=\left\{\begin{array}{cl}1 &  \ x \ge 0 \\\alpha e^{x} &  \ x < 0 \\\end{array} \right.\end{align*}</script><p>Elu相对于leaky Relu来说，计算要更耗时间一些</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/44398148">https://zhuanlan.zhihu.com/p/44398148</a></p><p><a href="https://liam.page/2018/04/17/zero-centered-active-function/">https://liam.page/2018/04/17/zero-centered-active-function/</a></p><p><a href="https://www.cnblogs.com/tornadomeet/p/3428843.html">https://www.cnblogs.com/tornadomeet/p/3428843.html</a></p><p><a href="https://www.cnblogs.com/chamie/p/8665251.html">https://www.cnblogs.com/chamie/p/8665251.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/33006526?from_voters_page=true">https://zhuanlan.zhihu.com/p/33006526?from_voters_page=true</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>过拟合，欠拟合以及解决办法</title>
      <link href="/2021/09/04/overfit-underfit/"/>
      <url>/2021/09/04/overfit-underfit/</url>
      
        <content type="html"><![CDATA[<h2 id="1-偏差和方差"><a href="#1-偏差和方差" class="headerlink" title="1.偏差和方差"></a>1.偏差和方差</h2><p><img src="/2021/09/04/overfit-underfit/1.jpg" alt></p><script type="math/tex; mode=display">\overline{f}(\textbf{x})=\mathbb{E}_D[f(\textbf{x};D)]</script><p><strong>a.偏差</strong></p><p>期望输出与真实标记的差别称为偏差（bias），即</p><script type="math/tex; mode=display">bias^{2}(\textbf{x})=(\overline{f}(\textbf{x})-y)^{2}</script><p><strong>b.方差</strong></p><script type="math/tex; mode=display">var(\textbf{x})=\mathbb{E}_D[(f(\textbf{x};D)-\overline{f}(x))^2]</script><p><strong>c.噪声</strong></p><script type="math/tex; mode=display">\xi^{2}=\mathbb{E}_D[(y_D-y)^2]</script><p><strong>d.泛化误差（error）</strong></p><script type="math/tex; mode=display">\begin{align*}error&=\mathbb{E}_D[(f(\textbf{x};D)-y_D)^2]\\&=...\\&=(\overline{f}(\textbf{x})-y)^{2}+\mathbb{E}_D[(f(\textbf{x};D)-\overline{f}(x))^2]+\mathbb{E}_D[(y_D-y)^2]\\&=bias^{2}(\textbf{x})+var(\textbf{x})+\xi^{2}\end{align*}</script><h2 id="2-过拟合、欠拟合与偏差、方差的关系"><a href="#2-过拟合、欠拟合与偏差、方差的关系" class="headerlink" title="2.过拟合、欠拟合与偏差、方差的关系"></a>2.过拟合、欠拟合与偏差、方差的关系</h2><p><img src="/2021/09/04/overfit-underfit/2.jpg" alt></p><p><img src="/2021/09/04/overfit-underfit/3.jfif" alt></p><p><strong>欠拟合：模型不能适配训练样本，有一个很大的偏差。</strong></p><p><strong>过拟合：模型很好的适配训练样本，但在测试集上表现很糟，有一个很大的方差。</strong></p><h2 id="3-如何解决过拟合和欠拟合"><a href="#3-如何解决过拟合和欠拟合" class="headerlink" title="3.如何解决过拟合和欠拟合"></a>3.如何解决过拟合和欠拟合</h2><p><strong>a.模型能力（一个模型参数数量不同，不同模型）</strong></p><p><img src="/2021/09/04/overfit-underfit/4.jpg" alt></p><p><strong>b.正则化</strong></p><p>正则化参数出现的目的其实是防止过拟合情形的出现；如果我们的模型已经出现了欠拟合的情形，就可以通过减少正则化参数来消除欠拟合</p><p><strong>c.特征数量</strong></p><p>欠拟合：增加特征项</p><p>过拟合：减少特征项</p><p><strong>d、训练的数据量</strong></p><p>欠拟合：减少数据量</p><p>过拟合：增加数据量</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/38853908">https://zhuanlan.zhihu.com/p/38853908</a></p><p><a href="https://blog.csdn.net/hurry0808/article/details/78148756">https://blog.csdn.net/hurry0808/article/details/78148756</a></p><p><a href="https://blog.csdn.net/cltcj/article/details/119155683">https://blog.csdn.net/cltcj/article/details/119155683</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 过拟合、欠拟合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据不平衡如何解决</title>
      <link href="/2021/09/04/data-imbalance/"/>
      <url>/2021/09/04/data-imbalance/</url>
      
        <content type="html"><![CDATA[<h2 id="1-基于数据"><a href="#1-基于数据" class="headerlink" title="1.基于数据"></a>1.基于数据</h2><p><strong>a.过采样和欠采样</strong></p><p>对少数数据进行有放回的过采样，使原本的数据变的均衡，这样就是对少数数据进行了复制，容易造成过拟合。</p><p>对多数数据进行有放回/无放回的欠采样，这样会丢失一些样本，损失信息，模型只学会整体模式的一部分，容易欠拟合。</p><p><strong>b.SMOTE算法</strong></p><p><strong>c.数据增强</strong></p><p>通过人为或算法增加少数数据的数量</p><h2 id="2-基于loss"><a href="#2-基于loss" class="headerlink" title="2.基于loss"></a>2.基于loss</h2><p>使用代价函数时，可以增加小类样本的权值，降低大类样本的权值</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/62877337">https://zhuanlan.zhihu.com/p/62877337</a></p><p><a href="https://blog.csdn.net/asialee_bird/article/details/83714612">https://blog.csdn.net/asialee_bird/article/details/83714612</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数据构造 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据不平衡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度爆炸、梯度消失和解决方法</title>
      <link href="/2021/09/02/gradient/"/>
      <url>/2021/09/02/gradient/</url>
      
        <content type="html"><![CDATA[<h2 id="1-梯度"><a href="#1-梯度" class="headerlink" title="1.梯度"></a>1.梯度</h2><p>设二元函数$z=f(x,y)$ 在平面区域$D$上具有一阶连续偏导数，则对于每一个点$P(x，y)$的梯度为</p><script type="math/tex; mode=display">grad \ f(x,y)=\nabla f(x,y)=f_x(x,y)\vec{j}+ f_y(x,y)\vec{j}</script><h2 id="2-BP算法图示"><a href="#2-BP算法图示" class="headerlink" title="2.BP算法图示"></a>2.BP算法图示</h2><p><img src="/2021/09/02/gradient/11.JPG" alt></p><h2 id="3-梯度消失和梯度爆炸"><a href="#3-梯度消失和梯度爆炸" class="headerlink" title="3.梯度消失和梯度爆炸"></a>3.梯度消失和梯度爆炸</h2><p>梯度爆炸和梯度消失问题都是因为<strong>网络太深</strong>，<strong>网络权值更新不稳定</strong>造成的，本质上是因为梯度反向传播中的连乘效应。</p><p><img src="/2021/09/02/gradient/22.jpg" alt></p><p>举个例子，现有如上链式连接的网络$(x\rightarrow z \rightarrow y)$</p><script type="math/tex; mode=display">\frac{\partial C }{\partial b_1}=\frac{\partial C }{\partial y_4}\frac{\partial y_4 }{\partial z_4}\frac{\partial z_4 }{\partial x_4}\frac{\partial x_4 }{\partial z_3}\frac{\partial z_3 }{\partial x_3}\frac{\partial x_3 }{\partial z_2}\frac{\partial z_2 }{\partial x_2}\frac{\partial x_2 }{\partial z_1}\frac{\partial z_1 }{\partial b_1}=\frac{\partial C }{\partial y_4}g^{'}(z_4)w_4g^{'}(z_3)w_3g^{'}(z_2)w_2g^{'}(z_1)w_1</script><p>假设$g$为sigmoid，那么$g^{‘}(z)$最大值为$\frac{1}{4}$，而我们初始化的网络权值通常都小于1，所以$g^{‘}(z)w \le \frac{1}{4}$，因此对于上面的链式求导，层数越多，求导结果$\frac{\partial C }{\partial b_1}$越小，因而导致梯度消失的情况出现。</p><p>这样，梯度爆炸问题的出现原因就显而易见了，当$w$比较大的时候或者激活函数的梯度较大，即$g^{‘}(z)w &gt; 1$，层数越多，求导结果$\frac{\partial C }{\partial b_1}$越大，直到爆炸。</p><h2 id="4-梯度消失和梯度爆炸解决方法"><a href="#4-梯度消失和梯度爆炸解决方法" class="headerlink" title="4.梯度消失和梯度爆炸解决方法"></a>4.梯度消失和梯度爆炸解决方法</h2><h3 id="4-1-解决梯度消失"><a href="#4-1-解决梯度消失" class="headerlink" title="4.1 解决梯度消失"></a>4.1 解决梯度消失</h3><p>1.用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。</p><p>2.用Batch Normalization。</p><p>3.LSTM的结构设计也可以改善RNN中的梯度消失问题。</p><p>4.残差网络</p><p>5.合适的初始化权重</p><h3 id="4-2解决梯度爆炸"><a href="#4-2解决梯度爆炸" class="headerlink" title="4.2解决梯度爆炸"></a>4.2解决梯度爆炸</h3><p>1.梯度剪切：对梯度设定阈值</p><p>2.权重正则化(L1 和 L2 )</p><p>3.合适的初始化权重</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/">https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/</a></p><p><a href="https://zhuanlan.zhihu.com/p/25631496">https://zhuanlan.zhihu.com/p/25631496</a></p><p><a href="https://aijishu.com/a/1060000000100195">https://aijishu.com/a/1060000000100195</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 训练技巧 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度爆炸、梯度消失 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统</title>
      <link href="/2021/08/30/recommmend-sys/"/>
      <url>/2021/08/30/recommmend-sys/</url>
      
        <content type="html"><![CDATA[<p>一般推荐系统的结构拆分为：召回-》粗排-》精排-》重排</p><p>大佬总结的干货：    <a href="https://xieyangyi.blog.csdn.net/article/details/123095982">https://xieyangyi.blog.csdn.net/article/details/123095982</a></p><h2 id="0-召回"><a href="#0-召回" class="headerlink" title="0 召回"></a>0 召回</h2><p>缩小规模，减小候选集，不需要十分准确，但不可遗漏</p><p>必须轻量快速低延迟</p><h2 id="1-粗排"><a href="#1-粗排" class="headerlink" title="1 粗排"></a>1 粗排</h2><p>兼顾精准性和低延迟</p><p>一般模型也不能过于复杂</p><h2 id="2-精排"><a href="#2-精排" class="headerlink" title="2 精排"></a>2 精排</h2><p>要求准</p><p>多特征，复杂模型</p><h2 id="3-重排"><a href="#3-重排" class="headerlink" title="3 重排"></a>3 重排</h2><p>业务相关</p><p>规则比较多</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://xieyangyi.blog.csdn.net/article/details/123095982">https://xieyangyi.blog.csdn.net/article/details/123095982</a></p><p><a href="https://www.cnblogs.com/gczr/p/12564617.html">https://www.cnblogs.com/gczr/p/12564617.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>极大似然估计</title>
      <link href="/2021/08/29/MLE/"/>
      <url>/2021/08/29/MLE/</url>
      
        <content type="html"><![CDATA[<h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><p>就是利用已知的样本结果信息，反推最具有可能导致这些样本结果出现的模型参数值。换句话说，即：“<strong>模型已定，结果已知，反推参数</strong>”。</p><h2 id="2-极大似然构造损失函数"><a href="#2-极大似然构造损失函数" class="headerlink" title="2.极大似然构造损失函数"></a>2.极大似然构造损失函数</h2><p><strong>大多数常见的损失函数就是基于极大似然推导的。</strong>例子参考 <a href="https://www.cnblogs.com/hello-ai/p/11000899.html">https://www.cnblogs.com/hello-ai/p/11000899.html</a></p><p><strong>判别模型下的极大似然估计</strong></p><p>最大似然估计很容易扩展到估计条件概率$P\left (y|x;\theta \right)$，从而给定$x$预测$y$。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。如果$X$表示所有的输入，$Y$表示我们观测到的目标，那么条件最大似然估计是：</p><script type="math/tex; mode=display">\theta_{ML} = \mathop\arg\max_{\theta}P\left(Y|X;\theta \right)</script><p>如果假设样本是独立同分布的，那么这可以分解成</p><script type="math/tex; mode=display">\theta_{ML} = \mathop\arg\max_{\theta}\sum_{i=1}^m logP\left(y^{(i)}|x^{(i)};\theta \right)</script><p><strong>生成模型下的极大似然估计</strong></p><p>考虑一组含有m个样本的数据集$X = \left \{ x^{(1)}, …, x^{(m)} \right \}$，由$p_{data}(x)$生成，独立同分布</p><p>对独立同分布的样本，生成样本集$X$的概率如下:</p><script type="math/tex; mode=display">p_{model}(X; \theta)= \prod _{i=1}^m p_{model}\left (x^{(i)}; \theta \right )</script><p>对$\theta$的最大似然估计被定义为：</p><script type="math/tex; mode=display">\theta_{ML} = \mathop{\arg\max}_{\theta}p_{model}\left (X;\theta \right ) = \mathop{\arg\max}_{\theta}\prod _{i=1}^m p_{model}\left (x^{(i)}; \theta \right )</script><p>多个概率的乘积公式会因很多原因不便于计算。例如，计算中很可能会因为多个过小的数值相乘而出现数值下溢。为了得到一个便于计算的等价优化问题，两边取对数：</p><script type="math/tex; mode=display">\theta_{ML} = \mathop{\arg\max}_{\theta}\sum_{i=1}^{m}logp_{model}\left (x^{(i)};\theta\right )</script><p><img src="/2021/08/29/MLE/11.png" alt></p><p>可以发现，使用极大似然估计时，每个样本$x^{(i)}$都希望拉高它所对应的模型概率值$p_{model}(x^{(i)};\theta)$，如上图所示，但是由于所有样本的密度函数$p_{model}(x^{(i)};\theta)$的总和必须是1，所以不可能将所有样本点都拉高到最大的概率，一个样本点的概率密度函数值被拉高将不可避免的使其他点的函数值被拉低，最终的达到一个平衡态。我们也可以将上式除以$m$，便可以看到极大似然法最大化的目标是在经验分布$\widehat{p}_{data}$下样本概率对数的期望值，即</p><script type="math/tex; mode=display">\theta_{ML} = \mathop{\arg\max}_{\theta}E_{x\sim \widehat{p}_{data}}logp_{model}\left (x^{(i)};\theta \right )</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/26614750">https://zhuanlan.zhihu.com/p/26614750</a></p><p><a href="https://www.cnblogs.com/hello-ai/p/11000899.html">https://www.cnblogs.com/hello-ai/p/11000899.html</a></p><p><a href="https://blog.csdn.net/hustqb/article/details/77168436">https://blog.csdn.net/hustqb/article/details/77168436</a></p><p><a href="https://zhuanlan.zhihu.com/p/273246971">https://zhuanlan.zhihu.com/p/273246971</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数学基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 极大似然估计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>先验概率与后验概率</title>
      <link href="/2021/08/29/prior-Posterior/"/>
      <url>/2021/08/29/prior-Posterior/</url>
      
        <content type="html"><![CDATA[<p>$P(X=玩 lol)=0.6；P(X=不玩lol)=0.4$，这个概率是统计得到的,或者你自身依据经验给出的一个概率值，我们称其为<strong>先验概率(prior probability)</strong>；</p><p>$P(X=玩lol|Y=男性)$称之为$X$的<strong>后验概率</strong>，即它获得是在观察到事件$Y=男性$发生后得到的</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/26464206">https://zhuanlan.zhihu.com/p/26464206</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数学基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLNet Generalized Autoregressive Pretraining for Language Understanding</title>
      <link href="/2021/08/27/xlnet/"/>
      <url>/2021/08/27/xlnet/</url>
      
        <content type="html"><![CDATA[<h2 id="1-主要改动"><a href="#1-主要改动" class="headerlink" title="1 主要改动"></a>1 主要改动</h2><p>relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy.</p><p>propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation.  (3) , XLNet integrates ideas from Transformer-XL</p><p>example：[New, York, is, a, city] . select the two tokens [New, York] as the prediction targets and maximize log p （New York | is a city）</p><p>In this case, BERT and XLNet respectively reduce to the following objectives:</p><p><img src="/2021/08/27/xlnet/2.JPG" alt></p><h2 id="2-现有PTM的问题"><a href="#2-现有PTM的问题" class="headerlink" title="2 现有PTM的问题"></a>2 现有PTM的问题</h2><p><strong>1 AR language modeling</strong></p><p>对于给定的句子$\textbf{x}=[x_1,…,x_T]$，AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization</p><script type="math/tex; mode=display">\max \limits_{\theta} \quad logp_{\theta}(\textbf{x})=\sum_{t=1}^{T}logp_{\theta}(x_t|\textbf{x}_{<t})=\sum_{t=1}^{T} log\frac{e^{h_{\theta}(\textbf{x}_{1:t-1})^\top e(x_t)}}{\sum_{x^{'}} e^{h_{\theta}(\textbf{x}_{1:t-1})^\top e(x^{'})}}  \tag{1}</script><p>其中$h_{\theta}(\textbf{x}_{1:t-1})$是考虑上下文的文本表示，$e(x_t)$为$x_t$的词向量</p><p><strong>2 AE anguage modeling</strong></p><p>对于BERT这种AE模型，首先利用$\textbf{x}$构造遮盖的tokens$\overline{\textbf{x}}$和未遮盖的tokens$\hat{\textbf{x}}$，然后the training objective is to reconstruct $\overline{\textbf{x}}$ from $\hat{\textbf{x}}$:</p><script type="math/tex; mode=display">\max \limits_{\theta} \quad logp_{\theta}(\overline{\textbf{x}}\ |\ \hat{\textbf{x}})\approx \sum_{t=1}^{T}m_tlogp_{\theta}(x_t\ |\ \hat{\textbf{x}})=\sum_{t=1}^{T} \ m_t log \frac{e^{H_{\theta}(\hat{\textbf{x}})_t^\top e(x_t)}}{\sum_{x^{'}}e^{H_{\theta}(\hat{\textbf{x}})_t^\top e(x^{'})}} \tag{2}</script><p>其中$m_t=1$表示$x_t$被遮盖了，AR语言模型$t$时刻只能看到之前的时刻，因此记号是$h_{\theta}(\textbf{x}_{1:t-1})$；而AE模型可以同时看到整个句子的所有Token，因此记号是$H_{\theta}(\hat{\textbf{x}})_t$</p><p>这两个模型的优缺点分别为：</p><p><strong>3 对比</strong></p><p>1.AE因为遮盖词只是假设相互独立不是严格相互独立，因此为$\approx$。</p><p>2.AE在预训练时会出现特殊的token为[MASK]，但是它在下游的fine-tuning中不会出现，这就出现了预训练 — finetune的不一致问题。而AR语言模型不会有这个问题。</p><p>3.AR语言模型只能参考一个方向的上下文，而AE可以参考双向的上下文。</p><h2 id="3-改动"><a href="#3-改动" class="headerlink" title="3 改动"></a>3 改动</h2><h3 id="3-1-排列语言模型"><a href="#3-1-排列语言模型" class="headerlink" title="3.1 排列语言模型"></a>3.1 排列语言模型</h3><p>we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional context</p><p>给定长度为$T$的序列，总共有$T!$种排列方法。注意输入顺序是不会变的，因为模型在微调期间只会遇到具有自然顺序的文本序列。作者就是通<strong>Attention Mask</strong>，把其它没有被选到的单词Mask掉，不让它们在预测单词$x_i$的时候发生作用，看着就类似于把这些被选中的单词放到了上文。</p><p>举个例子，如下图，输入序列为$\{x_1,x_2,x_3,x_4\}$，总共有4!，24种情况，作者取了其中4个。假如预测$x_3$，第一个排列为$x_3 \rightarrow x_2 \rightarrow x_4 \rightarrow x_1 $，没有排在$x_3$前面对象，所以只连接了mem，对于真实情况就是输入还是$x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow x_4 $，然后mask掉全部输入，即只利用mem预测$ x_3 $；第二个排列为$x_2 \rightarrow x_4 \rightarrow x_3 \rightarrow x_1 $，$x_2,x_4$排在$x_3$前面，所以连接了$x_2,x_4$对应的向量表示，对于真实情况就是输入还是$x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow x_4 $，然后mask掉$x_1,x_3$，剩余$x_2,x_4$，即利用mem，$x_2,x_4$预测$ x_3 $。</p><p><img src="/2021/08/27/xlnet/22.JPG" alt></p><p>排列语言模型的目标是调整模型参数使得下面的似然概率最大</p><script type="math/tex; mode=display">\max \limits_{\theta} \ \mathbb{E}_{\textbf{z}\sim \mathcal{Z}_T}[\sum_{t=1}^Tlogp_{\theta}(x_{z_t}|\textbf{x}_{\textbf{z}_{<t}})] \tag{3}</script><p>其中$\textbf{z}$为随机变量，表示某个位置排列，$\mathcal{Z}_T$表示全部的排列，$z_t$，$\textbf{z}_{&lt;t}$分别表示某个位置排列的第$t$个元素和与其挨着的前面$t-1$个元素。</p><h3 id="3-2-Two-Stream-Self-Attention"><a href="#3-2-Two-Stream-Self-Attention" class="headerlink" title="3.2 Two-Stream Self-Attention"></a>3.2 Two-Stream Self-Attention</h3><p><strong>Target-Aware Representations</strong></p><p>采用AE原来的表达形式来描述下一个token的分布$p_{\theta}(X_{z_t}|\textbf{x}_{\textbf{z}_{&lt;t}})$如下</p><script type="math/tex; mode=display">p_{\theta}(X_{z_t}=x|\textbf{x}_{\textbf{z}_{<t}})= \frac{e^{ e(x)^\top h_{\theta}(\textbf{x}_{\textbf{z}_{<t}})}}{\sum_{x^{'}} e^{ e(x^{'})^\top h_{\theta}(\textbf{x}_{\textbf{z}_{<t}})}}</script><p>这样表达有一个问题就是没有考虑预测目标词的位置，即没有考虑$ z_t$，这会导致ambiguity in target prediction。证明如下：假设有两个不同的排列$\textbf{z}^{(1)}$和$\textbf{z}^{(2)}$，并且满足如下关系：</p><script type="math/tex; mode=display">\textbf{z}^{(1)}_{<t}=\textbf{z}^{(2)}_{<t}=\textbf{z}_{<t} \ but \ {z}^{(1)}_{t}\neq{z}^{(2)}_{t}</script><p>可以推导出</p><script type="math/tex; mode=display">p_{\theta}(X_{z_t^{(1)}}=x|\textbf{x}_{\textbf{z}_{<t}^{(1)}})=p_{\theta}(X_{z_t^{(2)}}=x|\textbf{x}_{\textbf{z}_{<t}^{(2)}})=\frac{e^{ e(x)^\top h_{\theta}(\textbf{x}_{\textbf{z}_{<t}})}}{\sum_{x^{'}} e^{ e(x^{'})^\top h_{\theta}(\textbf{x}_{\textbf{z}_{<t}})}}</script><p>但是$p_{\theta}(X_{z_t^{(1)}}=x|\textbf{x}_{\textbf{z}_{&lt;t}^{(1)}}),p_{\theta}(X_{z_t^{(2)}}=x|\textbf{x}_{\textbf{z}_{&lt;t}^{(2)}})$应该不一样，因为目标词的位置不同</p><p>为了解决这个问题，提出了Target-Aware Representations，其实就是考虑了目标词的位置</p><script type="math/tex; mode=display">p_{\theta}(X_{z_t}=x|\textbf{x}_{\textbf{z}_{<t}})= \frac{e^{ e(x)^\top g_{\theta}(\textbf{x}_{\textbf{z}_{<t}},z_t)}}{\sum_{x^{'}} e^{ e(x^{'})^\top g_{\theta}(\textbf{x}_{\textbf{z}_{<t}},z_t)}} \tag{4}</script><p><strong>Two-Stream Self-Attention</strong></p><p> contradiction</p><p><img src="/2021/08/27/xlnet/4.JPG" alt></p><p>To resolve such a contradiction，we propose to use two sets of hidden representations instead of one:</p><p><img src="/2021/08/27/xlnet/3.JPG" alt></p><p>假设有self-attention的层号为$m=1,2,…,M$，$g_i^{(0)}=w$，$h_i^{(0)}=e(x_i)$，Two-Stream Self-Attention可以表示为</p><script type="math/tex; mode=display">g_{z_t}^{(m)}\leftarrow Attention(Q=g_{z_t}^{(m-1)},KV=\textbf{h}^{(m-1)}_{z_{<t}};\theta)\\h_{z_t}^{(m)}\leftarrow Attention(Q=h_{z_t}^{(m-1)},KV=\textbf{h}^{(m-1)}_{z_{\le t}};\theta)</script><p>举个例子，如下图</p><p><img src="/2021/08/27/xlnet/11.JPG" alt></p><p>预训练最终使用$g_{z_t}^{(M)}$计算公式（4）,during finetuning, we can simply drop the query stream and use the content stream </p><p>during  pretrain， we can use the last-layer query representation $g_{z_t}^{(M)}$  to compute Eq. (4).</p><p>during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL). </p><h3 id="3-3-Partial-Prediction"><a href="#3-3-Partial-Prediction" class="headerlink" title="3.3 Partial Prediction"></a>3.3 Partial Prediction</h3><p>因为排序很多，计算量很大，所以需要采样。将$z$分隔成$z_{t_\le c}$和 $z_{t_&gt;c}$，$c$为分隔点，我们选择预测后面的词语，因为后面的词语包含的信息更加丰富。引入超参数$K$调整$c$，使得需要预测$\frac{1}{K}$的词（$\frac{|z|-c}{|z|}\approx\frac{1}{K}$），优化目标为:</p><script type="math/tex; mode=display">\max \limits_{\theta}\mathbb{E}_{\textbf{z}\sim \mathcal{Z}_T}[logp_{\theta}(\textbf{x}_{\textbf{z}_{>c}}|\textbf{x}_{\textbf{z}_{ \le c}})]=\mathbb{E}_{\textbf{z}\sim \mathcal{Z}_T}[\sum_{t=c+1}^{|\textbf{z}|}logp_{\theta}(x_{z_t}|\textbf{x}_{\textbf{z}_{<t}})]</script><h3 id="3-4-融合Transformer-XL的思想"><a href="#3-4-融合Transformer-XL的思想" class="headerlink" title="3.4 融合Transformer-XL的思想"></a>3.4 融合Transformer-XL的思想</h3><p>We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism</p><p><strong>Relative Segment Encodings</strong></p><p><strong>recurrence mechanism</strong></p><p><img src="/2021/08/27/xlnet/1.JPG" alt></p><h3 id="3-5-Modeling-Multiple-Segments"><a href="#3-5-Modeling-Multiple-Segments" class="headerlink" title="3.5  Modeling Multiple Segments"></a>3.5  Modeling Multiple Segments</h3><p>the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where “SEP” and “CLS” are two special symbols and “A” and “B” are the two segments. Although we follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction</p><p>BERT that adds an absolute segment embedding，这里采用Relative Segment Encodings</p><p>There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.</p><p>这里有个疑问，对于多于两个seg的情况，比如3个seg，输入格式是否变成[CLS, A, SEP, B, SEP,C,SEP]</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/107350079">https://zhuanlan.zhihu.com/p/107350079</a></p><p><a href="https://blog.csdn.net/weixin_37947156/article/details/93035607">https://blog.csdn.net/weixin_37947156/article/details/93035607</a></p><p><a href="https://www.cnblogs.com/nsw0419/p/12892241.html">https://www.cnblogs.com/nsw0419/p/12892241.html</a></p><p><a href="https://www.cnblogs.com/mantch/archive/2019/09/30/11611554.html">https://www.cnblogs.com/mantch/archive/2019/09/30/11611554.html</a></p><p><a href="https://cloud.tencent.com/developer/article/1492776">https://cloud.tencent.com/developer/article/1492776</a></p><p><a href="https://zhuanlan.zhihu.com/p/96023284">https://zhuanlan.zhihu.com/p/96023284</a></p><p><a href="https://arxiv.org/pdf/1906.08237.pdf">https://arxiv.org/pdf/1906.08237.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XLNet </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ConSERT A Contrastive Framework for Self-Supervised Sentence Representation Transfer</title>
      <link href="/2021/08/27/consert/"/>
      <url>/2021/08/27/consert/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/2105.11741">https://arxiv.org/abs/2105.11741</a></p><p><a href="https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html">https://tech.meituan.com/2021/06/03/acl-2021-consert-bert.html</a></p><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1.背景"></a>1.背景</h2><p>首先，BERT其自身导出的句向量（不经过Fine-tune，对所有词向量求平均）会出现“坍缩（Collapse）”现象，即所有的句子都倾向于编码到一个较小的空间区域内，如图。为了解决这个问题，将对比学习结合到finetune过程，借助无标签数据来提升模型的能力。</p><p><img src="/2021/08/27/consert/11.JPG" alt></p><h2 id="2-原理"><a href="#2-原理" class="headerlink" title="2.原理"></a>2.原理</h2><p>给定一个类似BERT的预训练语言模型$\textbf{M}$，以及从目标领域数据分布中收集的无标签文本语料库$\mathcal{D}$，我们希望通过构建自监督任务在$\mathcal{D}$上对$\textbf{M}$进行Fine-tune，使得Fine-tune后的模型能够在目标任务（文本语义匹配）上表现最好。</p><h3 id="2-1-整体框架"><a href="#2-1-整体框架" class="headerlink" title="2.1 整体框架"></a>2.1 整体框架</h3><p><img src="/2021/08/27/consert/22.JPG" alt></p><p>模型整体结构如上图所示，主要由三个部分组成</p><p>A <strong>data augmentation module</strong> that generates different views for input samples at the token embedding layer.</p><p>A <strong>shared BERT encoder</strong> that computes sentence representations for each input text. During training, we use the average pooling of the token embeddings at the last layer to obtain sentence representations.</p><p>A <strong>contrastive loss layer</strong> on top of the BERT encoder. It maximizes the agreement between one representation and its corresponding version that is augmented from the same sentence while keeping it distant from other sentence representations in the same batch.</p><p>对于任意一个句子输入$x$，得到其对应的两个增强向量$e_i=T_1(x),e_j=T_2(x),e_i,e_j\in \mathbb{R}^{L\times d}$，然后经过shared BERT encoder编码为$r_i,r_j$,其中$T_1,T_2$为不同的数据增强方式，$L$为句子$x$的长度，$d$为隐藏单元的数量。对于每个train step，从$\mathcal{D}$随机选取$N$个样本作为mini-batch，然后得到$2N$个增强样本，使用NT-Xent构造loss为</p><script type="math/tex; mode=display">\mathcal{L}_{i,j}=-log\frac{exp(sim(r_i,r_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k\neq i]}exp(sim(r_i,r_k)/\tau)}\\\mathcal{L}_{con}=\frac{1}{2N}\sum_{(i,j)}\mathcal{L}_{i,j}</script><p>其中$sim(.)$为余弦相似度计算，$\tau$表示temperature，是一个超参数，实验中取0.1,$\mathbb{1}$是指示器，当$k=i$时，值为0。上式分子为正样本，分母为全部（但是基本为负样本，所以可以看成负样本），所以loss变小就是让分子变大，分母变小，也就是让正样本相似度变大，负样本相似度变小</p><h3 id="2-2-数据增强策略"><a href="#2-2-数据增强策略" class="headerlink" title="2.2 数据增强策略"></a>2.2 数据增强策略</h3><p><strong>显式生成增强样本</strong>的方法包括：回译、同义词替换、意译等，然而这些方法一方面不一定能保证语义一致。所以考虑了在Embedding层<strong>隐式生成增强样本</strong>的方法。</p><p><img src="/2021/08/27/consert/44.JPG" alt></p><ul><li><p><strong>对抗攻击（Adversarial Attack）</strong>：这一方法通过梯度反传生成对抗扰动，将该扰动加到原本的Embedding矩阵上，就能得到增强后的样本。由于生成对抗扰动需要梯度反传，因此这一数据增强方法仅适用于有监督训练的场景。</p></li><li><p><strong>打乱词序（Token Shuffling）</strong>：这一方法扰乱输入样本的词序。由于Transformer结构没有“位置”的概念，模型对Token位置的感知全靠Embedding中的Position Ids得到。因此在实现上，我们只需要将Position Ids进行Shuffle即可。</p></li><li><p><strong>裁剪（Cutoff）</strong></p><p>：又可以进一步分为两种：</p><ul><li>Token Cutoff：随机选取Token，将对应Token的Embedding整行置为零。</li><li>Feature Cutoff：随机选取Embedding的Feature，将选取的Feature维度整列置为零。</li></ul></li><li><p><strong>Dropout</strong>：Embedding中的每一个元素都以一定概率置为零，与Cutoff不同的是，该方法并没有按行或者按列的约束。</p></li></ul><h3 id="2-3-融合监督信号"><a href="#2-3-融合监督信号" class="headerlink" title="2.3 融合监督信号"></a>2.3 融合监督信号</h3><p>除了无监督训练以外，作者给出3种进一步融合监督信号的策略，以NLI任务为例：</p><script type="math/tex; mode=display">f=Concat(r_1,r_2,|r_1-r_2|)\\\mathcal{L}_{ce}=CrossEntropy(Wf+b,y)</script><p><strong>Joint training (joint)</strong>:</p><script type="math/tex; mode=display">\mathcal{L}_{joint}=\mathcal{L}_{ce}+\alpha\mathcal{L}_{con}\ \# on\ NLI\ dataset</script><p><strong>Supervised training then unsupervised transfer (sup-unsup)</strong>:</p><p>first train the model with $\mathcal{L}_{ce}$on NLI dataset, then use $\mathcal{L}_{con}$to finetune it on the target dataset.</p><p><strong>Joint training then unsupervised transfer (joint-unsup)</strong>:</p><p>first train the model with the $\mathcal{L}_{joint}$on NLI dataset, then use $\mathcal{L}_{con }$to fine-tune it on the target dataset.</p><h2 id="3-定性分析"><a href="#3-定性分析" class="headerlink" title="3.定性分析"></a>3.定性分析</h2><p>后又发现BERT句向量表示的坍缩和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，坍缩现象可以在一定程度上得到缓解（如图2蓝色曲线所示）。</p><p><img src="/2021/08/27/consert/33.JPG" alt></p><h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4 实验结果"></a>4 实验结果</h2><h3 id="4-1-Unsupervised-Results"><a href="#4-1-Unsupervised-Results" class="headerlink" title="4.1 Unsupervised Results"></a>4.1 Unsupervised Results</h3><p><img src="/2021/08/27/consert/1.JPG" alt></p><h3 id="4-2-Supervised-Results"><a href="#4-2-Supervised-Results" class="headerlink" title="4.2 Supervised Results"></a>4.2 Supervised Results</h3><p><img src="/2021/08/27/consert/2.JPG" alt></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gpt</title>
      <link href="/2021/08/26/gpt/"/>
      <url>/2021/08/26/gpt/</url>
      
        <content type="html"><![CDATA[<p>GPT三部曲宣告NLP的“预训练+微调”时代的崛起和走向辉煌。</p><p>原文分别为：</p><p>《Improving Language Understanding by Generative Pre-Training》</p><p>《Language Models are Unsupervised Multitask Learners》</p><p>《Language Models are Few-Shot Learners》</p><h2 id="1-GPT1"><a href="#1-GPT1" class="headerlink" title="1.GPT1"></a>1.GPT1</h2><p><img src="/2021/08/26/gpt/aa.jpg" alt></p><p><img src="/2021/08/26/gpt/aaa.png" alt="img"></p><p>模型的整体结构如上图所示。使用过程过程分为两步：第一步预训练，利用大量语料学习得到high-capacity的语言模型；第二步是fine_tuning，利用标签数据使其拟合到特定任务。</p><h3 id="1-1-Unsupervised-pre-training"><a href="#1-1-Unsupervised-pre-training" class="headerlink" title="1.1 Unsupervised pre-training"></a>1.1 Unsupervised pre-training</h3><p>作者将transformer decoder中Encoder-Decoder Attention层去掉后作为基本单元，然后多层堆叠作为语言模型的主体，然后将输出经过一个softmax层，来得到目标词的输出分布：</p><script type="math/tex; mode=display">h_0=UW_e+W_p\\h_l=transformer\_block(h_{l-1}),\ \forall l \in [1,n]\\P(u|u_{-k},...,u_{-1})    =softmax(h_nW_e^T)\</script><p>其中$U=\{u_{-k},…,u_{-1}\}$ 是预测词$u $前$k$个token的独热编码序列，$n$是模型的层数，$W_e$是token embedding matrix，$W_p$是position embedding matrix。</p><p>给定一个无监督的语料库$\mathcal{U}$，use a standard language modeling objective to maximize the following likelihood</p><script type="math/tex; mode=display">L_1(\mathcal{U})=\sum_ilog P(u_i|u_{i-k},...,u_{i-1})</script><p>其中$k$ 是上下文窗口大小。</p><h3 id="1-2-Supervised-fine-tuning"><a href="#1-2-Supervised-fine-tuning" class="headerlink" title="1.2 Supervised fine-tuning"></a>1.2 Supervised fine-tuning</h3><p>对于数据集$\mathcal{C}$，有数据$(x^1,x^2,…,x^m,y)$</p><script type="math/tex; mode=display">P(y|x^1,x^2,...,x^m)=softmax(h_l^mW_y)\\L_2(\mathcal{C})=\sum_{(x,y)}log P(y|x^1,x^2,...,x^m)</script><p>其中$W_y$为全连接层的参数</p><p>作者发现，使用语言模型来辅助监督学习进行微调，有两个好处：</p><ol><li>提高监督模型的泛化能力；</li><li>加速收敛。</li></ol><p>所以，最终下游使用的监督模型损失函数为：</p><script type="math/tex; mode=display">L_3(\mathcal{C})=L_2(\mathcal{C})+\lambda*L_1(\mathcal{C})</script><h3 id="1-3-Task-specific-input-transformations"><a href="#1-3-Task-specific-input-transformations" class="headerlink" title="1.3 Task-specific input transformations"></a>1.3 Task-specific input transformations</h3><p><img src="/2021/08/26/gpt/gpt1.JPG" alt></p><p>所有的输入文本都会加上开始和结合token$(s),(e)$</p><p><strong>分类</strong></p><p>分类过程可如上1.2，输入表示为$[(s);Context;(e)]$</p><p><strong>文本蕴含</strong></p><p>将输入拼接成$[(s); premise; ($) ; hypothesis ; (e)]$</p><p><strong>相似度</strong></p><p>由于文本相似度与两个比较文本的前后顺序没有关系，因此将两种文本顺序都考虑进来，如上图所示</p><p><strong>问答与常识推理</strong></p><p>假设文档为$z$，问题为$q$，一系列答案为$\{a_k\}$，将其输入表示为$[(s); z; q; ($);  a_k;(e)]$，然后多个回答组合的形式，如上图。</p><h2 id="2-GPT2"><a href="#2-GPT2" class="headerlink" title="2.GPT2"></a>2.GPT2</h2><p>总结就是：多任务预训练+超大数据集+超大规模模型。通过一个超大数据集涵盖NLP的大多任务，然后使用一个超大规模模型进行多任务预训练，使其无需任何下游任务的finetune就可以做到多个NLP任务的SOTA。举个例子，拿高考为例，人的智力和脑容量可以理解为参数大小，由于个体差异，可以将不同的学生理解为不同参数量的模型，卷子可以理解为数据集，不同的学科可以理解为不同任务。GPT2有点类似学霸，就是有超高的智力和脑容量，然后刷大量不同学科的题目，因此对高考这个多任务的下游任务就可以取得好成绩。</p><p><strong>GPT2相对于GPT1有哪些不同呢？</strong></p><ol><li><p><strong>GPT2去掉了fine-tuning</strong>：不再针对不同任务分别进行微调建模，模型会自动识别出来需要做什么任务。这就好比一个人博览群书，你问他什么类型的问题，他都可以顺手拈来，GPT2就是这样一个博览群书的模型。</p></li><li><p><strong>超大数据集</strong>：WebText，该数据集做了一些简单的数据清理，并且实验结果表明目前模型仍然处于一个欠拟合的情况。</p></li><li><p><strong>增加网络参数</strong>：GPT2将Transformer堆叠的层数增加到48层，隐层的维度为1600，参数量更是达到了15亿。15亿什么概念呢，Bert的参数量也才只有3亿哦~当然，这样的参数量也不是说谁都能达到的，这也得取决于money的多少啊~</p></li><li><p><strong>调整transformer</strong>：将layer normalization放到每个sub-block之前，并在最后一个transformer后再增加一个layer normalization，如下图。</p><p><img src="/2021/08/26/gpt/11.jpg" alt></p></li><li><p><strong>输入表示</strong>：GPT2采用了BPE这种subword的结构作为输入</p></li><li><p><strong>其他</strong>：GPT2将词汇表数量增加到50257个；最大的上下文大小 (context size) 从GPT的512提升到了1024 tokens；batchsize增加到512。</p></li></ol><p><strong>GPT2的输入是完全的文本，什么提示都不加吗？</strong></p><p>当然不是，它也会加入提示词，比如：$TL;DR:$，GPT2模型就会知道是做摘要工作了，输入的格式就是 $文本+TL;DR:$，然后就等待输出就行了~</p><h2 id="3-GPT3"><a href="#3-GPT3" class="headerlink" title="3.GPT3"></a>3.GPT3</h2><p>GPT3，这是一种具有1750亿个参数的超大规模模型，比GPT2大100倍，感觉真是进入算力时代了。距离个人用户太远了，就不深挖了。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/146719974">https://zhuanlan.zhihu.com/p/146719974</a></p><p><a href="https://zhuanlan.zhihu.com/p/125139937">https://zhuanlan.zhihu.com/p/125139937</a></p><p><a href="https://www.cnblogs.com/yifanrensheng/p/13167796.html#_label1_0">https://www.cnblogs.com/yifanrensheng/p/13167796.html#_label1_0</a></p><p><a href="https://www.jianshu.com/p/96c5d5d5c468">https://www.jianshu.com/p/96c5d5d5c468</a></p><p><a href="https://blog.csdn.net/qq_35128926/article/details/111399679">https://blog.csdn.net/qq_35128926/article/details/111399679</a></p><p><a href="https://zhuanlan.zhihu.com/p/96791725">https://zhuanlan.zhihu.com/p/96791725</a></p><p><a href="https://terrifyzhao.github.io/2019/02/18/GPT2.0论文解读.html">https://terrifyzhao.github.io/2019/02/18/GPT2.0%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/56865533">https://zhuanlan.zhihu.com/p/56865533</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN TextRNN TextRCNN</title>
      <link href="/2021/08/23/text-cnn/"/>
      <url>/2021/08/23/text-cnn/</url>
      
        <content type="html"><![CDATA[<h2 id="1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification"><a href="#1-TextCNN-Convolutional-Neural-Networks-for-Sentence-Classification" class="headerlink" title="1.TextCNN (Convolutional Neural Networks for Sentence Classification)"></a>1.TextCNN (Convolutional Neural Networks for Sentence Classification)</h2><p>原文 <a href="https://arxiv.org/abs/1408.5882">https://arxiv.org/abs/1408.5882</a></p><p>调参论文 <a href="https://arxiv.org/abs/1510.03820">https://arxiv.org/abs/1510.03820</a></p><p><img src="/2021/08/23/text-cnn/textcnn1.JPG" alt></p><p><img src="/2021/08/23/text-cnn/1111.JPG" alt></p><p>模型的整体结构如上所示。Feature Map是输入图像经过神经网络卷积产生的结果，filter是卷积核。</p><p><strong>输入表示：</strong></p><p>假设输入文本的长度为$n$，对于长度不够的需要做padding，任意一个单词可以用一个$k$维的向量表示，即$X_i \in \mathbb{R}^{k}$，那么一个句子可以表示为</p><script type="math/tex; mode=display">X_{1:n}=X_1 \oplus X_2\oplus...\oplus X_n</script><p>其中$\oplus$是向量拼接操作，$X_{1:n} \in \mathbb{R}^{nk\times 1}$。</p><p><strong>卷积</strong>：</p><p>对于某个滑窗$X_{i,i+h-1}=\{X_i,X_{i+1},…,X_{i+h-1}\}$经过某个卷积核$W_j$可得</p><script type="math/tex; mode=display">c_{i,j}=f(W_j\cdot X_{i,i+h-1}+b)</script><p>其中$f=tanh(\cdot)$，$W_j\in \mathbb{R}^{ 1\times hk}，c_{i,j} $是标量</p><p>假设卷积通道数为$m$，在NLP中，卷积滑动步伐$k=1$，那么经过卷积层后得到的完整的特征矩阵为</p><script type="math/tex; mode=display">C=[[c_{1,1},c_{2,1},...,c_{n-h+1,1}]^T,[c_{1,2},c_{2,2},...,c_{n-h+1,2}]^T,...,[c_{1,m},c_{2,m},...,c_{n-h+1,m}]^T]</script><p>其中$C \in \mathbb{R}^{(n-h+1)\times m}$</p><p><strong>maxpooling</strong>：</p><script type="math/tex; mode=display">\hat{C}=max\{C\} , \hat{C}\in \mathbb{R}^{m}</script><p><strong>全连接</strong>：</p><p>然后将$\hat{C}$接个全连接，就可以做分类或者回归任务了。</p><h2 id="2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning"><a href="#2-TextRNN-Recurrent-Neural-Network-for-Text-Classification-with-Multi-Task-Learning" class="headerlink" title="2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)"></a>2.TextRNN (Recurrent Neural Network for Text Classification with Multi-Task Learning)</h2><p>原文 <a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf">https://www.ijcai.org/Proceedings/16/Papers/408.pdf</a></p><p><img src="/2021/08/23/text-cnn/textrnn.JPG" alt></p><p><img src="/2021/08/23/text-cnn/11.png" alt></p><p>该文的场景为Recurrent Neural Network for Text Classification with Multi-Task Learning，就是论文的题目。文中给出了三种结构，如上图所示，图中的RNN单元为LSTM。</p><p><strong>Model-I: Uniform-Layer Architecture</strong></p><p>对于任务$m$，输入$\hat X_t$包含两个部分</p><script type="math/tex; mode=display">\hat{X}_t^{(m)}=X_{t}^{(m)}\oplus X_{t}^{(s)}</script><p>其中$X_{t}^{(m)}$表示特定任务的词向量，$X_{t}^{(s)}$表示共享的词向量，$\oplus$表示向量拼接的操作。</p><p><strong>Model-II: Coupled-Layer Architecture</strong></p><script type="math/tex; mode=display">\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来\\\downarrow\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+\sum_{i\in\{m,n\}}g^{(i\longrightarrow m)}U_c^{(i\longrightarrow m)}h_{t-1}^{(i)}) \ \#现在\\g^{(i\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(i)}h_{t-1}^{(i)})</script><p><strong>Model-III: Shared-Layer Architecture</strong></p><script type="math/tex; mode=display">\hat{c}_t=tanh(W_cX_t+U_ch_{t-1}) \ \#原来\\\downarrow\\\hat{c}_t^{(m)}=tanh(W_c^{(m)}X_t+g^{(m)}U_c^{(m)}h_{t-1}^{(m)}+g^{(s\longrightarrow m)}U_c^{(s)}h_{t}^{(s)} \ \#现在\\g^{( m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(m)}h_{t-1}^{(m)}),g^{( s\longrightarrow m)}=\sigma(W_{g}^{(m)}x_t+U_g^{(s\longrightarrow m)}h_{t}^{(s)}),h_t^{(s)}=\overrightarrow{h_t^{(s)}}\oplus\overleftarrow{h_t^{(s)}}</script><h2 id="3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification"><a href="#3-TextRCNN-Recurrent-Convolutional-Neural-Networks-for-Text-Classification" class="headerlink" title="3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)"></a>3.TextRCNN(Recurrent Convolutional Neural Networks for Text Classification)</h2><p>原文 <a href="https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf">https://www.deeplearningitalia.com/wp-content/uploads/2018/03/Recurrent-Convolutional-Neural-Networks-for-Text-Classification.pdf</a></p><p><img src="/2021/08/23/text-cnn/textrcnn1.JPG" alt></p><p>整体结构如上图所示，解释一下为啥叫RCNN，一般的 CNN 网络，都是卷积层 + 池化层，这里是将卷积层换成了双向 RNN，所以结果是，双向 RNN + 池化层。作者原话为：From the perspective of convolutional neural networks, the recurrent structure we previously mentioned is the convolutional layer.</p><p><strong>词语表示</strong></p><p>对于一个词语$w_i$，可以用一个三元组表示为</p><script type="math/tex; mode=display">x_i=[c_l(w_i);e(w_i);c_r(w_i)]</script><p>其中$e(w_i)$表示$w_i$的词向量，$c_l(w_i)$表示$w_i$句子左边的内容的向量表示，$c_r(w_i)$表示$w_i$句子右边的内容的向量表示，用式子表示如下</p><script type="math/tex; mode=display">c_l(w_i)=f(W^{l}c_l(w_{i-1})+W^{(sl)}e(w_{i-1}))\\c_r(w_i)=f(W^{r}c_r(w_{i-1})+W^{(sr)}e(w_{i-1}))</script><p>然后将$x_i$经过全连接得到$y_i^{(2)}$，$y_i^{(2)}$is a latent semantic vector</p><script type="math/tex; mode=display">y_i^{(2)}=tanh(W^{(2)}x_i+b^{(2)})</script><p><strong>语句表示</strong></p><p>获取众多的词语表示后，通过max-pooling得到句子表示</p><script type="math/tex; mode=display">y^{(3)}=\mathop{\max}_{i=1}^{n}y_i^{(2)}</script><p>然后接全连接和softmax</p><script type="math/tex; mode=display">y^{(4)}=W^{(4)}y^{(3)}+b^{(4)}\\p=softmax(y^{(4)})</script><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/wangduo/p/6773601.html">https://www.cnblogs.com/wangduo/p/6773601.html</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT在美团搜索核心排序的探索和实践</title>
      <link href="/2021/08/21/meituan/"/>
      <url>/2021/08/21/meituan/</url>
      
        <content type="html"><![CDATA[<p>很有启发，抱着学习态度，mark一下</p><h2 id="模型层面"><a href="#模型层面" class="headerlink" title="模型层面"></a>模型层面</h2><p>整体结构如下</p><p><img src="/2021/08/21/meituan/1.png" alt></p><p>1 BERT预训练</p><p>2 多任务学习</p><p>​    场景层：根据业务场景进行划分，每个业务场景单独设计网络结构</p><p>3 联合训练</p><p>两个任务分别为：</p><p>​    1 相关性任务：相关性+NER（多任务增强相关性）</p><p>​    2 排序任务</p><p>怎么联合没看出来</p><p>之前是两阶段finetune： 1. 先相关性任务 2 然后排序任务</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html">https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html</a> </p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> 排序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排序学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELMo(Deep contextualized word representations)</title>
      <link href="/2021/08/19/elmo/"/>
      <url>/2021/08/19/elmo/</url>
      
        <content type="html"><![CDATA[<p>引入了新的深度考虑上下文的词语表示，模型考虑了两个方面：（1）词语的复杂特性，包括语法和语义，（2）在语境中的不同含义。模型使用了深度双向语言模型，并且在大预料库上做了预训练。这个模型可以很方便地和现有的模型结合，并且在NLP的6个任务上取得了SOTA。作者还揭露了预训练网络的深层构件是关键，这使得下游模型能够混合不同类型的半监督信号。</p><h2 id="3-ELMo-Embeddings-from-Language-Models"><a href="#3-ELMo-Embeddings-from-Language-Models" class="headerlink" title="3 ELMo: Embeddings from Language Models"></a>3 ELMo: Embeddings from Language Models</h2><p><img src="/2021/08/19/elmo/elmo1.JPG" alt></p><p>模型的整体机构如上所示，由左右两个单向的多层LSTM网络构成，左边为正向，右边为反向。</p><h3 id="3-1-Bidirectional-language-models（预训练）"><a href="#3-1-Bidirectional-language-models（预训练）" class="headerlink" title="3.1 Bidirectional language models（预训练）"></a>3.1 Bidirectional language models（预训练）</h3><p>假定一个句子有$N$个token，分别为$(t_1,t_2,…,t_N)$，正向的语言模型的句子概率为：</p><script type="math/tex; mode=display">p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_1,t_2,...,t_{k-1})</script><p>反向的语言模型的句子概率为：</p><script type="math/tex; mode=display">p(t_1,t_2,...,t_N)=\prod_{k=1}^{N}p(t_k|t_{k+1},t_{k+2},...,t_{N})</script><p>得到正向和反向的语言后，将其结合可以得到双向的语言模型，这里取对数表示为：</p><script type="math/tex; mode=display">\sum_{k=1}^N(log\ p(t_k|t_1,t_2,...,t_{k-1};\Theta_x,\overrightarrow{\Theta}_{LSTM} ,\Theta_s )+log \ p(t_k|t_{k+1},t_{k+2},...,t_{N};\Theta_x,\overleftarrow{\Theta}_{LSTM} ,\Theta_s) )\\</script><p>其中$\Theta_x$为token表示的参数，$\Theta_s$为softmax层的参数，$\overrightarrow{\Theta}_{LSTM}$表示前向语言模型的参数，$\overleftarrow{\Theta}_{LSTM}$表示反向语言模型的参数。</p><h3 id="3-2-ELMo（如何表示词向量）"><a href="#3-2-ELMo（如何表示词向量）" class="headerlink" title="3.2 ELMo（如何表示词向量）"></a>3.2 ELMo（如何表示词向量）</h3><p>得到$L$层的预训练双向深度语言模型后，对于token $t_k$，一共包含了$2L+1$个相关的表示，集合如下</p><script type="math/tex; mode=display">R_k=\{x_{k}^{LM},\overrightarrow{h^{LM}_{k,j}},\overleftarrow{h^{LM}_{k,j}}|j=1,2,...,L \}\\=\{h_{k,j}^{LM} | j=0,...,L\}</script><p>注意$h_{k,0}^{LM}=x_{k}^{LM}，h_{k,j}^{LM}=[\overrightarrow{h^{LM}_{k,j}};\overleftarrow{h^{LM}_{k,j}}]$,其中$x_{k}^{LM}$为token表示，$\overrightarrow{h^{LM}_{k,j}},\overleftarrow{h^{LM}_{k,j}}$分别为正反向语言模型的表示</p><p>对于下游任务，需要将$2L+1$个表示压缩到一个向量$ELmo_k^{task}$，最简单的做法是只取顶层的表示，即</p><script type="math/tex; mode=display">ELmo_k^{task}=E(R_k)=h_{k,L}^{LM}</script><p>更加通用的做法为线形组合输出，如下图，公式表达为</p><script type="math/tex; mode=display">ELmo_k^{task}=E(R_k,\Theta^{task})=\gamma^{task}\sum_{j=0}^{L}s_{j}^{task}h_{k,j}^{LM}</script><p>其中$\gamma^{task}$用于缩放向量，$s_{j}^{task}$表示权重，通过下游任务学习。</p><p><img src="/2021/08/19/elmo/11.jpg" alt></p><h3 id="3-3-Using-biLMs-for-supervised-NLP-tasks（fine-tune）"><a href="#3-3-Using-biLMs-for-supervised-NLP-tasks（fine-tune）" class="headerlink" title="3.3 Using biLMs for supervised NLP tasks（fine tune）"></a>3.3 Using biLMs for supervised NLP tasks（fine tune）</h3><p>对于下游任务模型，可以得到不考虑上下文的静态词向量$x_k$和考虑上下文的向量表示$h_k$</p><p>对于一部分任务，将$x_k$和$ ELMo_k^{task}$ 拼接作为下游任务的特征：$[x_k;ELMo_k^{task}]$</p><p>对于一部分任务，将 $h_k$和 $ ELMo_k^{task}$ 拼接可提升效果：$[h_k;ELMo_k^{task}]$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/linchuhai/article/details/97170541">https://blog.csdn.net/linchuhai/article/details/97170541</a></p><p><a href="https://zhuanlan.zhihu.com/p/63115885">https://zhuanlan.zhihu.com/p/63115885</a></p><p><a href="https://zhuanlan.zhihu.com/p/88993965">https://zhuanlan.zhihu.com/p/88993965</a></p><p><a href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ELMo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSSM双塔模型系列</title>
      <link href="/2021/08/18/dssm/"/>
      <url>/2021/08/18/dssm/</url>
      
        <content type="html"><![CDATA[<p>简单介绍微软出品的DSSM,CNN-DSSM,LSTM-DSSM</p><p>原文分别为：</p><p>《Learning Deep Structured Semantic Models for Web Search using Clickthrough Data》</p><p>《A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval》</p><p>《SEMANTIC MODELLING WITH LONG-SHORT-TERM MEMORY FOR INFORMATION RETRIEVAL》</p><p>首先为什么叫做双塔，query塔做在线serving，doc塔离线计算embeding建索引，推到线上即可。</p><p>注意， DSSM中query和不同的doc是<strong>共享参数</strong>的， <a href="https://flashgene.com/archives/72820.html">https://flashgene.com/archives/72820.html</a></p><h2 id="一-DSSM"><a href="#一-DSSM" class="headerlink" title="一.DSSM"></a>一.DSSM</h2><h3 id="1-1-模型整体结构"><a href="#1-1-模型整体结构" class="headerlink" title="1.1 模型整体结构"></a>1.1 模型整体结构</h3><p><img src="/2021/08/18/dssm/dssm1.JPG" alt></p><p>模型的整体结构如上图所示，$Q$为query，$D_i$为文档。</p><p>文本的初始词袋表示为$x$，因为参数过多，不利于训练，所以降低维度，就提出了word hashing</p><script type="math/tex; mode=display">l_1=W_1x</script><p>word hashing其实就是利于char n-gram分词，然后用向量表示（只是这里依然用词袋表示向量，而不是稠密向量），如下所示</p><p><img src="/2021/08/18/dssm/cnn_dssm4.JPG" alt></p><p>这里有个顾虑为是否存在不同的词使用相同的向量表示。关于这个作者做了实验，结果如下。</p><p><img src="/2021/08/18/dssm/dssm2.JPG" alt></p><p>对于词汇数量500K大小的词表，采用3-gram后，此表压缩到30k，而且重复表示的仅为22个。重复表示率为0.0044%，维度压缩到原来6%，可以说非常有效。</p><p>然后为多层的非线性映射，每层都为全连接网络，得到</p><script type="math/tex; mode=display">l_i=f(W_il_{i-1}+b_{i}),i=2,...,N-1\\</script><p>非线性映射层的最后一层得到语义特征$y$为</p><script type="math/tex; mode=display">y=f(W_Nl_{N-1}+b_N)\\f(x)=tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}</script><p>利用余弦相似度衡量$Q$和$D$相似度得到</p><script type="math/tex; mode=display">R(Q,D)=cosine(y_Q,y_D)=\frac{y_Q^Ty_D}{||y_Q||||y_D||}</script><p>最后的概率输出为</p><script type="math/tex; mode=display">P(D|Q)=\frac{e^{\gamma R(Q,D)}}{\sum_{D^{'}\in \textbf{D}}e^{\gamma R(Q,D^{'})}}</script><p>其中$\gamma$为smoothing factor。</p><h3 id="1-2-训练"><a href="#1-2-训练" class="headerlink" title="1.2 训练"></a>1.2 训练</h3><p>样本集构造，对每个正样本$(Q,D^+)$，搭配4个随机负样本$(Q,D_j^-;j=1,..,4)$</p><p>损失函数为：</p><script type="math/tex; mode=display">L(\wedge)=-log \prod \limits_{(Q,D^+)}P(D^+|Q)</script><p>其中$\wedge$为模型参数。</p><h2 id="二-CNN-DSSM"><a href="#二-CNN-DSSM" class="headerlink" title="二.CNN-DSSM"></a>二.CNN-DSSM</h2><h3 id="2-1-CLSM结构"><a href="#2-1-CLSM结构" class="headerlink" title="2.1 CLSM结构"></a>2.1 CLSM结构</h3><p><img src="/2021/08/18/dssm/cnn_dssm1.JPG" alt></p><p>模型包括几个部分：(1) a word-n-gram layer obtained by running a contextual sliding window over the input word sequence (2) a letter-trigram layer that transforms each word-trigram into a letter-trigram representation vector (3) a convolutional layer that extracts contextual features for each word with its neighboring words defined by a window (4) a max-pooling layer that discovers and combines salient word-n-gram features to form a fixed-length sentence-level feature vector  (5) a semantic layer that extracts a high-level semantic feature vector for the input word sequence.</p><h3 id="2-2-Letter-trigram-based-Word-n-gram-Representation"><a href="#2-2-Letter-trigram-based-Word-n-gram-Representation" class="headerlink" title="2.2 Letter-trigram based Word-n-gram Representation"></a>2.2 Letter-trigram based Word-n-gram Representation</h3><p>在DSSM的Letter-trigram的基础上加了Word-n-gram，Word-n-gram就是对原始输入文本做滑窗，对于第$t$个word-n-gram可以表示为：</p><script type="math/tex; mode=display">l_t=[f^T_{t-d},...,f^T_{t},...,f^T_{t+d}]^T,\ t=1,2,...,T</script><p>其中$n=2d+1,f_t$为的第$t$个词语的letter-trigram。一个letter-trigram的维度为$30K$，那么一个word-n-gram维度为$n\times30K$</p><p>举个例子，如上图，输入文本为$(s) \ online \ auto\ body \ (s)$，滑动窗口大小为n=3，可得$(s)\ online \ auto，\ online \ auto  \ body ，auto\  body \ (s)  $，那么</p><p>$l_1=[f^T((s)),f^T(online ),f^T(auto)]^T,\\l_2=[f^T(online ),f^T(auto),f^T(body)]^T,\\l_3=[f^T(auto),f^T(body),f^T((s))]^T$</p><h3 id="2-3-Modeling-Word-n-gram-Level-Contextual-Features-at-the-Convolutional-Layer"><a href="#2-3-Modeling-Word-n-gram-Level-Contextual-Features-at-the-Convolutional-Layer" class="headerlink" title="2.3 Modeling Word-n-gram-Level Contextual Features at the Convolutional Layer"></a>2.3 Modeling Word-n-gram-Level Contextual Features at the Convolutional Layer</h3><p>语境相关特征向量$h_t$可以表示为：</p><script type="math/tex; mode=display">h_t=tanh(W_c\cdot l_t),\ t=1,...,T</script><p>其中$W_c$为特征转换矩阵，也就是卷积矩阵，对于全部的word n-grams，$W_c$共享。有小伙伴肯定好奇，这不就是全连接吗，和卷积什么关系，俺也疑惑？</p><p>下图为作者做的一个实验。</p><p><img src="/2021/08/18/dssm/cnn_dssm2.JPG" alt></p><h3 id="2-4-Modeling-Sentence-Level-Semantic-Features-Using-Max-Pooling"><a href="#2-4-Modeling-Sentence-Level-Semantic-Features-Using-Max-Pooling" class="headerlink" title="2.4 Modeling Sentence-Level Semantic Features Using Max Pooling"></a>2.4 Modeling Sentence-Level Semantic Features Using Max Pooling</h3><p>获取局部的语境相关的特征向量后，我们需要把它们合在一起组合句子级别的特征向量。由于语句中某些词语不重要，我们可以忽略它，有些词语很重要，要保留。为了达到这个目的，使用了max pooling，用式子描述如下</p><script type="math/tex; mode=display">v(i)= \mathop{\max}_{t=1,..,T} \{h_t(i)\},\ i=1,...,K</script><p>其中$v(i)$表示池化层输出$v$的第$i$个元素，$K$为$v$的维度和$h_t$的维度一样，$h_t(i)$是第$t$个局部语境特征向量的第$i$个元素。举个例子如下，</p><p><img src="/2021/08/18/dssm/cnn_dssm3.JPG" alt></p><h3 id="2-5-Latent-Semantic-Vector-Representations"><a href="#2-5-Latent-Semantic-Vector-Representations" class="headerlink" title="2.5 Latent Semantic Vector Representations"></a>2.5 Latent Semantic Vector Representations</h3><p>语义向量表示$y$，用公式描述如下</p><script type="math/tex; mode=display">y=tanh(W_s\cdot v)</script><h3 id="2-6-Using-the-CLSM-for-IR"><a href="#2-6-Using-the-CLSM-for-IR" class="headerlink" title="2.6 Using the CLSM for IR"></a>2.6 Using the CLSM for IR</h3><p>和DSSM都一样，</p><script type="math/tex; mode=display">R(Q,D)=cosine(y_Q,y_D)=\frac{y_Q^Ty_D}{||y_Q||||y_D||}\\P(D|Q)=\frac{e^{\gamma R(Q,D)}}{\sum_{D^{'}\in \textbf{D}}e^{\gamma R(Q,D^{'})}}</script><h3 id="2-7-损失函数"><a href="#2-7-损失函数" class="headerlink" title="2.7 损失函数"></a>2.7 损失函数</h3><script type="math/tex; mode=display">L(\wedge)=-log \prod \limits_{(Q,D^+)}P(D^+|Q)</script><h2 id="三-LSTM-DSSM"><a href="#三-LSTM-DSSM" class="headerlink" title="三.LSTM-DSSM"></a>三.LSTM-DSSM</h2><p>cnn-dssm只能捕获局部的文本信息，lstm对于长序列的信息捕获能力强于lstm，因此使用lstm改进dssm。</p><h3 id="3-1-模型结构"><a href="#3-1-模型结构" class="headerlink" title="3.1 模型结构"></a>3.1 模型结构</h3><p>整体结构如下图，注意红色的部分为残差传递的方向。</p><p><img src="/2021/08/18/dssm/lstm_dssm1.JPG" alt></p><p>图中的LSTM单元是LSTM的变种，加入了<strong>peep hole</strong>的 LSTM，具体结构如下。</p><p><img src="/2021/08/18/dssm/lstm_dssm2.JPG" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/guoyaohua/p/9229190.html">https://www.cnblogs.com/guoyaohua/p/9229190.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 排序 </category>
          
          <category> 粗排 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSSM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>熵，KL散度，交叉熵，JS散度</title>
      <link href="/2021/08/18/entropy/"/>
      <url>/2021/08/18/entropy/</url>
      
        <content type="html"><![CDATA[<p>GAN需要KL散度和JS散度，所以先预热。</p><h2 id="1-熵"><a href="#1-熵" class="headerlink" title="1.熵"></a>1.熵</h2><p>信息量为：</p><script type="math/tex; mode=display">\begin{align}I(x) &= - \log(p(x)) \tag{1}\end{align}</script><p>熵为信息量的算术平均：</p><script type="math/tex; mode=display">H(x) = - \sum_{i=1}^{n}p(x_i)log(p(x_i)) \tag{2}</script><h2 id="2-交叉熵"><a href="#2-交叉熵" class="headerlink" title="2.交叉熵"></a>2.交叉熵</h2><p>交叉熵为</p><script type="math/tex; mode=display">H(P,Q) = -\sum_{i=1}^np(x_i)logq(x_i)\tag{3}</script><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="3-KL散度"><a href="#3-KL散度" class="headerlink" title="3.KL散度"></a>3.KL散度</h2><p>对于同一个随机变量有两个单独的概率分布，我们可以使用KL散度(Kullback-Leibler divergence)来衡量两个分布的差异。在机器学习的损失函数的计算中，我们可以假设$P$为样本的真实分布，$Q$用来表示模型所预测的分布，使用KL散度来衡量两个分布之间的差异。KL散度等于交叉熵减去熵</p><script type="math/tex; mode=display">\begin{align}D_{KL}(P||Q) &= \sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \notag\\&=\sum_{i=1}^np(x_i)(logp(x_i)-logq(x_i)) \notag\\&=\sum_{i=1}^n[p(x_i)logp(x_i)-p(x_i)logq(x_i)] \notag\\&=\sum_{i=1}^np(x_i)logp(x_i)-\sum_{i=1}^np(x_i)logq(x_i) \\&=-H(P)+H(P,Q)\tag{4}\end{align}</script><p>$P$和$Q$概率分布越接近，$D_{KL}(P||Q)$越小。</p><p><strong>KL散度与交叉熵区别与联系</strong></p><p><a href="https://blog.csdn.net/Dby_freedom/article/details/83374650">https://blog.csdn.net/Dby_freedom/article/details/83374650</a></p><p><strong>KL散度主要有两个性质：</strong></p><p>（1）不对称性</p><p>尽管KL散度从直观上是个距离函数，但它并不是一个真正的度量，因为它不具有对称性，即$D_{KL}(P||Q)\neq D_{KL}(Q||P)$。</p><p>（2）非负性</p><p>即$D_{KL}(P||Q) \geq 0$。</p><h2 id="4-JS散度"><a href="#4-JS散度" class="headerlink" title="4.JS散度"></a>4.JS散度</h2><p>JS散度也是用于度量两个概率分布的相似度，其解决了KL散度不对称的缺点</p><script type="math/tex; mode=display">JS(P||Q) = \frac{1}{2}KL(P||\frac{P+Q}{2})+\frac{1}{2}KL(Q||\frac{P+Q}{2}) \tag{5}</script><p><strong>不同于KL主要在两方面：</strong></p><p>（1）值域范围</p><p>JS散度的值域范围是[0,1]，相同则是0，相反为1。</p><p>（2）对称性</p><p>即$ JS(P||Q)=JS(Q||P)$，从数学表达式中就可以看出。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/Mrfanl/p/11938139.html">https://www.cnblogs.com/Mrfanl/p/11938139.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/346518942">https://zhuanlan.zhihu.com/p/346518942</a></p><p><a href="https://www.w3cschool.cn/article/83016451.html">https://www.w3cschool.cn/article/83016451.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 数学基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> entropy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>排序学习</title>
      <link href="/2021/08/16/L2R/"/>
      <url>/2021/08/16/L2R/</url>
      
        <content type="html"><![CDATA[<p>Literature survey for Learning to rank</p><p><a href="https://www.eecis.udel.edu/~vijay/fall13/snlp/lit-survey/LearningToRank.pdf">https://www.eecis.udel.edu/~vijay/fall13/snlp/lit-survey/LearningToRank.pdf</a></p><p>a short introduction to learning to rank（李航）</p><p><a href="https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en">https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en</a></p><p>Learning to Rank for Information Retrieval  — By Tie-Yan Liu</p><p><a href="http://didawikinf.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/ir/ir13/1_-_learning_to_rank.pdf">http://didawikinf.di.unipi.it/lib/exe/fetch.php/magistraleinformatica/ir/ir13/1_-_learning_to_rank.pdf</a></p><p>Feature Selection For Ranking</p><p><a href="https://dl.acm.org/doi/10.1145/1277741.1277811">https://dl.acm.org/doi/10.1145/1277741.1277811</a></p><p>A Deep Look into Neural Ranking Models for Information Retrieval 中科院</p><p><a href="https://par.nsf.gov/servlets/purl/10277191">https://par.nsf.gov/servlets/purl/10277191</a></p><p>相关参考：</p><p><a href="https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en">https://www.jstage.jst.go.jp/article/transinf/E94.D/10/E94.D_10_1854/_pdf/-char/en</a></p><p><a href="https://blog.csdn.net/anshuai_aw1/article/details/86018105">https://blog.csdn.net/anshuai_aw1/article/details/86018105</a></p><p><a href="https://blog.csdn.net/pearl8899/article/details/102920628">https://blog.csdn.net/pearl8899/article/details/102920628</a></p><p><a href="https://blog.csdn.net/lipengcn/article/details/80373744">https://blog.csdn.net/lipengcn/article/details/80373744</a></p><p>分类</p><p><img src="/2021/08/16/L2R/22.png" alt></p><p><img src="/2021/08/16/L2R/33.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
          <category> 排序 </category>
          
          <category> 排序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> L2R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-trained Models for Natural Language Processing A Survey</title>
      <link href="/2021/08/10/ptm-survey/"/>
      <url>/2021/08/10/ptm-survey/</url>
      
        <content type="html"><![CDATA[<p>原文内容很丰富，慢慢学习更新。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>这篇综述从language representation learning入手，然后全面的阐述Pre-trained Models的原理，结构以及downstream任务，最后还罗列了PTM的未来发展方向。该综述目的旨在为NLP小白，PTM小白做引路人，感人。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>随着深度学习的发展，许多深度学习技术被应用在NLP，比如CNN，RNN，GNN以及attention。</p><p>尽管NLP任务的取得很大成功，但是和CV比较，性能提高可能不是非常明显。这主要是因为NLP任务的数据集都非常小（除了机器翻译），然而深度网络参数非常多，没有足够的数据支撑网络训练会导致过拟合问题。</p><p>最近，大量工作表明，预先训练的模型（PTMs），在大型语料库上可以学习通用语言表示，这有利于下游NLP任务可以避免从零开始训练新模型。随着算力的发展，深度模型（例如，transformer）的出现和训练技巧的不断调高，PTM的结构从浅层发展成深层。<strong>第一代PTM</strong>被用于Non-contextual  word Embedding。由于下游任务不需要这些模型本身，只需要训练好的词向量矩阵，因此对于现在的算力，这些模型非常浅层，比如Skip-Gram和GloVe。虽然这些预训练词向量可以捕获词语的语义，但它们不受上下文限制，无法捕获上下文中的高级含义，某些任务会失效，例如多义词，句法结构，语义角色、回指。<strong>第二代PTM</strong>关注Contextual word embeddings，比如BERT，GPT等。这些编码器任然需要通过下游任务在上下文中表示词语。</p><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2.Background"></a>2.Background</h2><h3 id="2-1-Language-Representation-Learning"><a href="#2-1-Language-Representation-Learning" class="headerlink" title="2.1 Language Representation Learning"></a>2.1 Language Representation Learning</h3><p>The core idea of distributed representation is to describe the meaning of a piece of text by low-dimensional real-valued vectors. And each dimension of the vector has no corresponding sense, while the whole represents a concrete concept.</p><p><img src="/2021/08/10/ptm-survey/ptm1.JPG" alt></p><p><strong>Non-contextual Embeddings</strong></p><p>这一步主要是将分割的字符，比如图中的$x$，变成向量表达$e_x \in \mathbb{R}^{D_e}$，$D_e$是词向量维度。向量化过程就是基于一个离线训练的词向量矩阵$E\in \mathbb{R}^{D_e\times |\mathcal{V}|} $做查找，$\mathcal{V}$是词汇表。</p><p>这个过程主要有两个问题。第一个是这个词向量是静态的，没有考虑上下文含义，无法处理多义词。第二个是oov问题，许多算法可以缓解这个问题，比如基于character level，比如基于subword，subword算法有BPE，CharCNN等。</p><p><strong>Contextual Embeddings</strong></p><p>To address the issue of polysemous and the context-dependent nature of words, we need distinguish the semantics of words in different contexts：</p><script type="math/tex; mode=display">[\textbf{h}_1,\textbf{h}_2,...,\textbf{h}_T]=f_{enc}(x_1,x_2,...,x_T)</script><p>其中$f_{enc}(\cdot)$为深度编码器。$\textbf{h}_t$就是contextual embedding或者dynamical embedding。</p><h3 id="2-2-Neural-Contextual-Encoders"><a href="#2-2-Neural-Contextual-Encoders" class="headerlink" title="2.2 Neural Contextual Encoders"></a>2.2 Neural Contextual Encoders</h3><p><img src="/2021/08/10/ptm-survey/ptm3.JPG" alt></p><p>可以分成两类，sequence models and non-sequence models。</p><h4 id="2-2-1-sequence-models"><a href="#2-2-1-sequence-models" class="headerlink" title="2.2.1 sequence models"></a>2.2.1 sequence models</h4><p>sequence models 分为两类，Convolutional Models和Recurrent Models，见上图。</p><p><strong>Convolutional</strong> </p><p>Convolutional models take the embeddings of words in the input sentence and capture the meaning of a word by aggregating the <strong>local information</strong> from its neighbors by convolution operations</p><p><strong>Recurrent</strong> </p><p>Recurrent models capture the contextual representations of words with short memory, such as LSTMs and GRUs . In practice, bi-directional LSTMs or GRUs are used to collect information from both sides of a word, but its performance is often affected by the <strong>long-term dependency problem</strong>.</p><h4 id="2-2-2-non-sequence-models"><a href="#2-2-2-non-sequence-models" class="headerlink" title="2.2.2 non-sequence models"></a>2.2.2 non-sequence models</h4><p>transformer： model the relation of every two words</p><h4 id="2-2-3-Analysis"><a href="#2-2-3-Analysis" class="headerlink" title="2.2.3 Analysis"></a>2.2.3 Analysis</h4><p><strong>Sequence models：</strong></p><p>1.Sequence models learn the contextual representation of the word with locality bias and are hard to capture the long-range interactions between words. </p><p>2.Nevertheless, sequence models are usually easy to train and get good results for various NLP tasks.</p><p><strong>fully-connected self-attention model：</strong></p><p>1.can directly model the dependency between every two words in a sequence, which is more powerful and suitable to model long range dependency of language</p><p>2.However, due to its heavy structure and less model bias, the Transformer usually requires a large training corpus and is easy to overfit on small or modestly-sized datasets</p><p><strong>结论</strong>：the Transformer has become the mainstream architecture of PTMs due to its powerful capacity.</p><h3 id="2-3-Why-Pre-training"><a href="#2-3-Why-Pre-training" class="headerlink" title="2.3 Why Pre-training?"></a>2.3 Why Pre-training?</h3><ol><li>Pre-training on the huge text corpus can <strong>learn universal language representation</strong>s and help with the downstream tasks.</li><li>Pre-training provides a <strong>better model initialization</strong>,which usually leads to a better generalization performance and speeds up convergence on the target task.</li><li>Pre-training can be <strong>regarded as a kind of regularization</strong> to avoid overfitting on small data</li></ol><h2 id="3-Overview-of-PTMs"><a href="#3-Overview-of-PTMs" class="headerlink" title="3 Overview of PTMs"></a>3 Overview of PTMs</h2><h3 id="3-1-Pre-training-Tasks"><a href="#3-1-Pre-training-Tasks" class="headerlink" title="3.1 Pre-training Tasks"></a>3.1 Pre-training Tasks</h3><p>预训练任务对于学习通用语言表示至关重要。通常，这些预训练任务应具有挑战性，并拥有大量训练数据。在本节中，我们将预训练任务分成三个类别：Supervised learning、Unsupervised learning和Self-Supervised learning。</p><p><strong>Self-Supervised learning</strong>： is a blend of supervised learning and unsupervised learning. The learning paradigm of SSL is entirely the same as supervised learning, but the labels of training data are generated automatically. The key idea of SSL is to predict any part of the input from other parts in some form. For example, the masked language model (MLM) is a self-supervised task that attempts to predict the masked words in a sentence given the rest words.</p><p>接下来基于介绍常用的基于Self-Supervised learning的预训练任务。</p><h4 id="3-1-1-Language-Modeling-LM"><a href="#3-1-1-Language-Modeling-LM" class="headerlink" title="3.1.1 Language Modeling (LM)"></a>3.1.1 Language Modeling (LM)</h4><h4 id="3-1-2-Masked-Language-Modeling-MLM"><a href="#3-1-2-Masked-Language-Modeling-MLM" class="headerlink" title="3.1.2 Masked Language Modeling (MLM)"></a>3.1.2 Masked Language Modeling (MLM)</h4><h4 id="3-1-3-Permuted-Language-Modeling-PLM"><a href="#3-1-3-Permuted-Language-Modeling-PLM" class="headerlink" title="3.1.3 Permuted Language Modeling (PLM)"></a>3.1.3 Permuted Language Modeling (PLM)</h4><h4 id="3-1-4-Denoising-Autoencoder-DAE"><a href="#3-1-4-Denoising-Autoencoder-DAE" class="headerlink" title="3.1.4 Denoising Autoencoder (DAE)"></a>3.1.4 Denoising Autoencoder (DAE)</h4><h4 id="3-1-5-Contrastive-Learning-CTL"><a href="#3-1-5-Contrastive-Learning-CTL" class="headerlink" title="3.1.5 Contrastive Learning (CTL)"></a>3.1.5 Contrastive Learning (CTL)</h4><p>nsp也属于CTL</p><p><a href="https://zhuanlan.zhihu.com/p/360892229">https://zhuanlan.zhihu.com/p/360892229</a></p><h4 id="3-1-6-Others"><a href="#3-1-6-Others" class="headerlink" title="3.1.6 Others"></a>3.1.6 Others</h4><h3 id="3-2-Taxonomy-of-PTMs"><a href="#3-2-Taxonomy-of-PTMs" class="headerlink" title="3.2 Taxonomy of PTMs"></a>3.2 Taxonomy of PTMs</h3><p><img src="/2021/08/10/ptm-survey/ptm5.JPG" alt></p><p>作者从以下四个角度，即Representation Type，Architectures，Pre-Training Task Types，Extensions，对现有的PTM分类，分类结果如上。图和这里有一点不统一，是作者没注意？图里有5个类别，多了Tuning Strategies，而且Representation Type在图中为Contextual?。</p><h3 id="3-3-Model-Analysis"><a href="#3-3-Model-Analysis" class="headerlink" title="3.3 Model Analysis"></a>3.3 Model Analysis</h3><h2 id="4-Extensions-of-PTMs"><a href="#4-Extensions-of-PTMs" class="headerlink" title="4 Extensions of PTMs"></a>4 Extensions of PTMs</h2><h3 id="4-1-Knowledge-Enriched-PTMs"><a href="#4-1-Knowledge-Enriched-PTMs" class="headerlink" title="4.1 Knowledge-Enriched PTMs"></a>4.1 Knowledge-Enriched PTMs</h3><h3 id="4-2-Multilingual-and-Language-Specific-PTMs"><a href="#4-2-Multilingual-and-Language-Specific-PTMs" class="headerlink" title="4.2 Multilingual and Language-Specific PTMs"></a>4.2 Multilingual and Language-Specific PTMs</h3><h3 id="4-3-Multi-Modal-PTMs"><a href="#4-3-Multi-Modal-PTMs" class="headerlink" title="4.3 Multi-Modal PTMs"></a>4.3 Multi-Modal PTMs</h3><h3 id="4-4-Domain-Specific-and-Task-Specific-PTMs"><a href="#4-4-Domain-Specific-and-Task-Specific-PTMs" class="headerlink" title="4.4 Domain-Specific and Task-Specific PTMs"></a>4.4 Domain-Specific and Task-Specific PTMs</h3><h3 id="4-5-Model-Compression"><a href="#4-5-Model-Compression" class="headerlink" title="4.5 Model Compression"></a>4.5 Model Compression</h3><h2 id="5-Adapting-PTMs-to-Downstream-Tasks"><a href="#5-Adapting-PTMs-to-Downstream-Tasks" class="headerlink" title="5 Adapting PTMs to Downstream Tasks"></a>5 Adapting PTMs to Downstream Tasks</h2><p>虽然PTM学习了很多通用知识，但是如何将这些知识有效应用到下游任务是个挑战。</p><h3 id="5-1-Transfer-Learning"><a href="#5-1-Transfer-Learning" class="headerlink" title="5.1 Transfer Learning"></a>5.1 Transfer Learning</h3><p>Transfer learning is to adapt the knowledge from a source task (or domain) to a target task (or domain).如下图。</p><p><img src="/2021/08/10/ptm-survey/ptm6.JPG" alt></p><h3 id="5-2-How-to-Transfer"><a href="#5-2-How-to-Transfer" class="headerlink" title="5.2 How to Transfer?"></a>5.2 How to Transfer?</h3><h4 id="5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus"><a href="#5-2-1-Choosing-appropriate-pre-training-task-model-architecture-and-corpus" class="headerlink" title="5.2.1 Choosing appropriate pre-training task, model architecture and corpus"></a>5.2.1 Choosing appropriate pre-training task, model architecture and corpus</h4><h4 id="5-2-2-Choosing-appropriate-layers"><a href="#5-2-2-Choosing-appropriate-layers" class="headerlink" title="5.2.2 Choosing appropriate layers"></a>5.2.2 Choosing appropriate layers</h4><p>使用哪些层参与下游任务</p><p>选择的层model1+下游任务model2</p><p>对于深度模型的不同层，捕获的知识是不同的，比如说词性标注，句法分析，长期依赖，语义角色，协同引用。对于RNN based的模型，研究表明多层的LSTM编码器的不同层对于不同任务的表现不一样。对于transformer based 的模型，基本的句法理解在网络的浅层出现，然而高级的语义理解在深层出现。</p><p>用$\textbf{H}^{l}(1&lt;=l&lt;=L)$表示PTM的第$l$层的representation，$g(\cdot)$为特定的任务模型。有以下几种方法选择representation:</p><p><strong>a) Embedding Only</strong></p><p>choose only the pre-trained static embeddings，即$g(\textbf{H}^{1})$</p><p><strong>b) Top Layer</strong></p><p>选择顶层的representation，然后接入特定的任务模型，即$g(\textbf{H}^{L})$</p><p><strong>c) All Layers</strong></p><p>输入全部层的representation，让模型自动选择最合适的层次，然后接入特定的任务模型，比如ELMo，式子如下</p><script type="math/tex; mode=display">g(\textbf{r}_t)=g(\gamma \sum_{l=1}^{L}\alpha_l\textbf{H}^{(l)})</script><p>其中$\alpha$ is the softmax-normalized weight for layer $l$ and $\gamma$ is a scalar to scale the vectors output by pre-trained model</p><h4 id="5-2-3-To-tune-or-not-to-tune"><a href="#5-2-3-To-tune-or-not-to-tune" class="headerlink" title="5.2.3 To tune or not to tune?"></a>5.2.3 To tune or not to tune?</h4><p>总共有两种常用的模型迁移方式：<strong>feature extraction</strong> (where the pre-trained parameters are frozen), and <strong>fine-tuning</strong> (where the pre-trained parameters are unfrozen and fine-tuned).</p><p><img src="/2021/08/10/ptm-survey/ptm7.JPG" alt></p><p>选择的层model1参数是否固定，model2一定要训练</p><p>bert 只有top  layer finetune？？？？</p><h3 id="5-3-Fine-Tuning-Strategies"><a href="#5-3-Fine-Tuning-Strategies" class="headerlink" title="5.3 Fine-Tuning Strategies"></a>5.3 Fine-Tuning Strategies</h3><p><strong>Two-stage fine-tuning</strong></p><p>第一阶段为中间任务，第二阶段为目标任务</p><p><strong>Multi-task fine-tuning</strong></p><p>multi-task learning and pre-training are complementary technologies.</p><p><strong>Fine-tuning with extra adaptation modules</strong></p><p>The main drawback of fine-tuning is its parameter ineffciency: every downstream task has its own fine-tuned parameters. Therefore, a better solution is to inject some fine-tunable adaptation modules into PTMs while the original parameters are fixed.</p><p><strong>Others</strong></p><p>self-ensemble ，self-distillation，gradual unfreezing，sequential unfreezing</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/2003.08271v4.pdf">https://arxiv.org/pdf/2003.08271v4.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode常见套路</title>
      <link href="/2021/08/10/leetcode_algorith-tech/"/>
      <url>/2021/08/10/leetcode_algorith-tech/</url>
      
        <content type="html"><![CDATA[<h2 id="一-常见算法"><a href="#一-常见算法" class="headerlink" title="一.常见算法"></a>一.常见算法</h2><p>分治策略，动态规划，回溯，分支限界，贪心策略</p><h2 id="二-巧用数据结构"><a href="#二-巧用数据结构" class="headerlink" title="二.巧用数据结构"></a>二.巧用数据结构</h2><p>普通栈、单调栈</p><p>队列</p><p>堆</p><p>字典树</p><h2 id="三-技巧"><a href="#三-技巧" class="headerlink" title="三.技巧"></a>三.技巧</h2><p>双指针/滑窗，二分查找，排序，快慢指针，取余，位运算，倍增（<a href="https://leetcode.cn/problems/divide-two-integers/">29. 两数相除</a>），递归，时空转化（hashtable），dfs/bfs</p><h2 id="四-套路选择"><a href="#四-套路选择" class="headerlink" title="四 套路选择"></a>四 套路选择</h2><p><a href="https://zhuanlan.zhihu.com/p/358653377">https://zhuanlan.zhihu.com/p/358653377</a></p><p><a href="https://zhuanlan.zhihu.com/p/341176507">https://zhuanlan.zhihu.com/p/341176507</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/358653377">https://zhuanlan.zhihu.com/p/358653377</a></p><p><a href="https://zhuanlan.zhihu.com/p/341176507">https://zhuanlan.zhihu.com/p/341176507</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> leetcode </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Comprehensive Survey on Graph Neural Networks</title>
      <link href="/2021/08/10/graph-nn-survey/"/>
      <url>/2021/08/10/graph-nn-survey/</url>
      
        <content type="html"><![CDATA[<p> there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms.</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>虽然深度学习技术可以捕获欧式空间数据的隐藏模式，但是目前很多应用是基于图的，这是非欧空间的数据。图数据的复杂性给现有的技术带来了很大的挑战。这是因为图数据可以是不规则的，一个图可能有不同数量的无序结点，一个结点可能有不同数量的邻接结点。这会使得一些基本操作，比如卷积，在图领域无法很好的捕获特征。除此之外，目前机器学习算法有一个重要的假设，就是假设各个结点是相互独立的，然而，图中存在很多复杂的连接信息，主要用来表征结点间的互相关性。为了解决以上问题，衍生了很多图神经网络技术。举个例子，比如，图卷积。下图对比了传统的2D卷积和图卷积。二者最大的区别在于邻接结点，一个有序一个无序，一个尺寸固定一个尺寸可变。</p><p><img src="/2021/08/10/graph-nn-survey/gra1.JPG" alt></p><h2 id="2-背景和定义"><a href="#2-背景和定义" class="headerlink" title="2.背景和定义"></a>2.背景和定义</h2><h3 id="A-背景"><a href="#A-背景" class="headerlink" title="A. 背景"></a>A. 背景</h3><p><strong>Graph neural networks vs network embedding</strong></p><p>The main distinction between GNNs and network embedding is that GNNs are a group of neural network models which are designed for various tasks while network embedding covers various kinds of methods targeting the same task.</p><p><strong>Graph neural networks vs graph kernel methods</strong></p><p>The difference is that this mapping function of graph kernel methods is deterministic rather than learnable. </p><p>GNNs are much more efficient than graph kernel methods.</p><h3 id="B-定义"><a href="#B-定义" class="headerlink" title="B. 定义"></a>B. 定义</h3><p><img src="/2021/08/10/graph-nn-survey/gra2.JPG" alt></p><p>上表为本文的notation。</p><p>1.图</p><p>$ {G}=(V,E) $表示一个图。$N(v)=\{u\in V|(v,u)\in E\}$表示结点$v$的邻接结点。$\textbf{A}$是邻接矩阵，如果$A_{ij}=1$,那么表示$e_{ij}\in E$；如果$A_{ij}=0$,那么表示$e_{ij} \notin E$。$\textbf{X} \in \mathbb{R}^{n \times d} $是结点特征矩阵，$\textbf{X}^{e} \in \mathbb{R}^{m \times c}$是边特征矩阵。</p><p>2.有向图</p><p>A graph is undirected if and only if the adjacency matrix is symmetric.</p><p>3.时空图</p><p>A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time.</p><p>$G^{(t)}=(V,E,\textbf{X}^{(t)})，\textbf{X}^{(t)} \in \mathbb{R}^{n \times d}$</p><h2 id="3-分类和框架"><a href="#3-分类和框架" class="headerlink" title="3.分类和框架"></a>3.分类和框架</h2><h3 id="3-1-GNN分类"><a href="#3-1-GNN分类" class="headerlink" title="3.1 GNN分类"></a>3.1 GNN分类</h3><p>作者把GNN分成以下4类，分别为RecGNNs，ConvGNNs , GAEs, STGNNs。</p><p><strong>RecGNNs（Recurrent graph neural networks）</strong></p><p>RecGNNs aim to learn node representations with recurrent neural architectures. They assume a node in a graph constantly exchanges information message with its neighbors until a stable equilibrium is reached.</p><p><strong>ConvGNNs（Convolutional graph neural networks ）</strong> </p><p>The main idea is to generate a node $v$’s representation by aggregating its own features $\textbf{x}_v$ and neighbors’ features $\textbf{x}_u,u\in N(v)$。Different from RecGNNs, ConvGNNs stack multiple graph convolutional layers to extract high-level node representations.</p><p><strong>GAEs（Graph autoencoders）</strong></p><p>are unsupervised learning frameworks which encode nodes/graphs into a latent vector space and reconstruct graph data from the encoded information. GAEs are used to learn network embeddings and<br>graph generative distributions.</p><p><strong>STGNNs（Spatial-temporal graph neural networks）</strong></p><p>aim to learn hidden patterns from spatial-temporal graphs. The key idea of STGNNs is to consider spatial dependency and temporal dependency at the same time.</p><p><img src="/2021/08/10/graph-nn-survey/gra3.JPG" alt></p><p><img src="/2021/08/10/graph-nn-survey/gra4.JPG" alt></p><h3 id="3-2-框架"><a href="#3-2-框架" class="headerlink" title="3.2 框架"></a>3.2 框架</h3><p>With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms:</p><p>Node-level outputs relate to node regression and node classification tasks.</p><p>Edge-level outputs relate to the edge classification and link prediction tasks.</p><p>Graph-level outputs relate to the graph classification task. </p><p><strong>Training Frameworks：</strong></p><p>1.Semi-supervised learning for node-level classification</p><p>2.Supervised learning for graph-level classification</p><p>3.Unsupervised learning for graph embedding</p><h2 id="4-RecGNNs"><a href="#4-RecGNNs" class="headerlink" title="4.RecGNNs"></a>4.RecGNNs</h2><p>RecGNNs apply the <strong>same set of parameters</strong> recurrently over nodes in a graph to extract high-level node representations. 接下来介绍几种RecGNNs 结构。</p><p><strong>GNN*</strong></p><p>Based on an information diffusion mechanism,  GNN* updates nodes’ states by exchanging neighborhood information recurrently until a stable equilibrium is reached.</p><p>结点的hidden state is recurrently updated by</p><script type="math/tex; mode=display">\textbf{h}_v^{(t)}=\sum_{u\in N(v)}f(\textbf{x}_v,\textbf{x}^e_{(v,u)},\textbf{x}_{u},\textbf{h}_{u}^{(t-1)})</script><p>$\textbf{h}_v^0$随机初始化。$f(\cdot)$是 parametric function，must be a contraction mapping, which shrinks the distance between two points after projecting them into a latent space.</p><p>训练过程分为两步，更新结点表示和更新参数，交替进行使得loss收敛。When a convergence criterion is satisfied, the last step node hidden states are forwarded to a readout layer.</p><p><strong>GraphESN</strong></p><p>GraphESN使用ESN提高GNN*的训练效率。GraphESN包含encoder和output output。encoder随机初始化并且不需要训练。It implements a contractive state transition function to recurrently update node states until the global graph state reaches convergence. Afterward, the output layer is trained by taking the fixed node states as inputs.</p><p><strong>Gated Graph Neural Networks (GGNNs)</strong></p><script type="math/tex; mode=display">\textbf{h}_{v}^t=GRU(\textbf{h}_{v}^{t-1},\sum_{u\in N(v)}\textbf{W}h_{u}^t)</script><p>The adavantage is that it no longer needs to constrain parameters to ensure convergence. However, the downside of training by BPTT is that it sacrifices efficiency both in time and memory.</p><p><strong>GGNN</strong></p><p>RecGNNs 利用GRU作为循环函数</p><script type="math/tex; mode=display">\textbf{h}_v^{(t)}=GRU(\textbf{h}_v^{(t-1)},\sum_{u\in N(v)}\textbf{W}\textbf{h}_u^{(t-1)})</script><p>其中$\textbf{h}_v^{(0)}=\textbf{x}_v$。</p><p>GGNN uses the back-propagation through time (BPTT) algorithm to learn the model parameters.</p><p>对于大的图不适用。</p><p><strong>SSE</strong></p><p>proposes a learning algorithm that is more scalable to large graphs</p><script type="math/tex; mode=display">\textbf{h}_{v}^{(t)}=(1-\alpha)\textbf{h}_{v}^{（t-1）}+\alpha \textbf{W}_1 \sigma(\textbf{W}_2[\textbf{x}_v,\sum_{u\in N(v)}[\textbf{h}_u^{t-1},\textbf{x}_u]])</script><p>其中$\alpha$为超参数，$\sigma(\cdot)$为sigmoid函数。</p><h2 id="5-ConvGNNs"><a href="#5-ConvGNNs" class="headerlink" title="5.ConvGNNs"></a>5.ConvGNNs</h2><p><img src="/2021/08/10/graph-nn-survey/gra6.JPG" alt></p><p><img src="/2021/08/10/graph-nn-survey/1.JPG" alt></p><p>ConvGNNs与RecGNNs 主要区别在于上图。</p><p>ConvGNNs fall into two categories, <strong>spectral-based</strong> and <strong>spatial-based</strong>. Spectral based approaches <strong>define graph convolutions by introducing filters</strong> from the perspective of graph signal processing [82] where the graph convolutional operation is interpreted as removing noises from graph signals. Spatial-based approaches inherit ideas from RecGNNs to <strong>define graph convolutions by information propagation</strong>.  spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality.</p><h3 id="5-1-Spectral-based-ConvGNNs"><a href="#5-1-Spectral-based-ConvGNNs" class="headerlink" title="5.1 Spectral-based ConvGNNs"></a>5.1 Spectral-based ConvGNNs</h3><h3 id="5-2-Spatial-based-ConvGNNs"><a href="#5-2-Spatial-based-ConvGNNs" class="headerlink" title="5.2 Spatial-based ConvGNNs"></a>5.2 Spatial-based ConvGNNs</h3><p>罗列几个基本的结构。</p><p><strong>NN4G</strong></p><script type="math/tex; mode=display">\textbf{h}_{v}^{(k)}=f(\textbf{W}^{(k)^T}\textbf{x}_v+\sum_{i=1}^{k-1}\sum_{u\in N(v) }\Theta^{(k)^{T}}\textbf{h}_{u}^{(k-1)})</script><p>其中$f(\cdot)$是激活函数，$\textbf{h}_{v}^{(0)}=0$，可以使用矩阵形式表达为：</p><script type="math/tex; mode=display">\textbf{H}^{(k)}=f(\textbf{X}\textbf{W}^{(k)}+\sum_{i=1}^{k-1}\textbf{A}\textbf{H}^{k-1}\Theta^{(k)})</script><p><strong>DCNN</strong></p><p>regards graph convolutions as a diffusion process.</p><script type="math/tex; mode=display">\textbf{H}^{(k)}=f(\textbf{W}^{(k)}\odot\textbf{P}^k\textbf{X}    )</script><p>其中$f(\cdot)$是激活函数。probability transition matrix $\textbf{P}\in\mathbb{R}^{n\times n},\textbf{P} = \textbf{D}^{-1}\textbf{A}$。</p><p>DCNN concatenates $\textbf{H}^{(1)},\textbf{H}^{(2)},…,\textbf{H}^{(K)}$together as the final model outputs.</p><p><strong>PGC-DGCNN</strong></p><p><strong>MPNN</strong></p><h3 id="5-3-Graph-Pooling-Modules"><a href="#5-3-Graph-Pooling-Modules" class="headerlink" title="5.3 Graph Pooling Modules"></a>5.3 Graph Pooling Modules</h3><p>After a GNN generates node features, we can use them for the final task. But using all these features directly can be computationally challenging, thus, a <strong>down-sampling strategy</strong> is needed. Depending on the objective and the role it plays in the network, different names are given to this strategy: (1) <strong>the pooling operation</strong> aims to reduce the size of parameters by down-sampling the nodes to generate smaller representations and thus avoid overfitting, permutation invariance, and computational complexity issues; (2) <strong>the readout operation</strong> is mainly used to generate graph-level representation based on node representations. <strong>Their mechanism is very similar. In this chapter, we use pooling to refer to all kinds of down-sampling strategies applied to GNNs.</strong></p><p>mean/max/sum pooling is the most <strong>primitive and effective way</strong> ：</p><script type="math/tex; mode=display">\textbf{h}_G=mean/max/sum(\textbf{h}_1^{(K)},\textbf{h}_2^{(K)},...,\textbf{h}_n^{(K)})</script><p>$K$ is the index of the last graph convolutional layer.</p><p> some works [17], [27], [46] also use <strong>attention</strong> mechanisms to enhance the mean/sum pooling.</p><p>[101] propose the <strong>Set2Set</strong> method to generate a memory that increases with the size of the input.</p><p>还有<strong>SortPooling，DiffPool</strong>等</p><h2 id="6-GAEs"><a href="#6-GAEs" class="headerlink" title="6.GAEs"></a>6.GAEs</h2><h2 id="7-STGNNs"><a href="#7-STGNNs" class="headerlink" title="7.STGNNs"></a>7.STGNNs</h2><h2 id="8-APPLICATIONS"><a href="#8-APPLICATIONS" class="headerlink" title="8.APPLICATIONS"></a>8.APPLICATIONS</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1901.00596v4">https://arxiv.org/abs/1901.00596v4</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> GNN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GNN综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KG-BERT BERT for Knowledge Graph Completion</title>
      <link href="/2021/08/06/kg-bert/"/>
      <url>/2021/08/06/kg-bert/</url>
      
        <content type="html"><![CDATA[<p>原文 <a href="https://arxiv.org/pdf/1909.03193.pdf">https://arxiv.org/pdf/1909.03193.pdf</a></p><h2 id="一-背景补充"><a href="#一-背景补充" class="headerlink" title="一.背景补充"></a>一.背景补充</h2><p><img src="/2021/08/06/kg-bert/11.GIF" alt></p><p>知识图谱普遍存在不完备的问题。以上图为例，黑色的箭头表示已经存在的关系，红色的虚线则是缺失的关系。知识图谱补全是基于图谱里已有的关系去推理出缺失的关系。由于BERT在NLP取得的成绩，作者将其迁移到知识图谱补全的应用上。</p><h2 id="二-结构"><a href="#二-结构" class="headerlink" title="二.结构"></a>二.结构</h2><p>作者设计了两种训练方式的KG - BERT, 可以运用到不同的知识图谱补全任务当中。</p><p><strong>2.1 Illustrations of fine-tuning KG-BERT for predicting the plausibility of a triple</strong></p><p><img src="/2021/08/06/kg-bert/kgbert1.JPG" alt></p><p>输入由三部分组成，$Head$，$Relation$，$Tail$。举个例子，$Head$可以是“Steven Paul Jobs was an American business magnate,entrepreneur and investor.” 或者“Steve Jobs”，$Relation$可以是“founded”，$Tail$可以是“Apple Inc. is an American multinational technology company headquartered in Cupertino, California.”或者“Apple Inc.”。用$[SEP]$分隔实体和关系。输入为3个向量的sum，即token, segment 和position embeddings。对于segment，实体的segment Embedding为$e_A$，而关系的segment Embedding为$e_B$。对于position ，相同position的不同token使用相同的position embedding。</p><p>对于输入的三元组$\tau=(h,r,t)$，目标函数为：</p><script type="math/tex; mode=display">S_{\tau}=f(h,r,t)=sigmoid(CW^T)，S_{\tau} \in \mathbb{R}^2,S_{\tau 0},S_{\tau 1} \in [0,1]</script><p>损失函数是$S$和$y$的交叉熵：</p><script type="math/tex; mode=display">L=-\sum_{\tau \in D^{+}\cup D^{-}}(y_{\tau}log(S_{\tau0})+(1-y_{\tau}log(S_{\tau1})))</script><p>其中$y_{\tau}\in \{0,1\}$是标签。</p><p>关于负样本的构造，作者是将正样本的$Head$或者$Tail$变成随机替换成别的，如下</p><script type="math/tex; mode=display">D^{-}=\{(h^{'},r,t)|h^{'}\in E\cap h^{'}\neq h \cap(h^{'},r,t)\notin D^{+} \}\\\cup\{(h,r,t^{'})|t^{'}\in E\cap t^{'}\neq t \cap(h,r,t^{'})\notin D^{+}\}</script><p>其中$E$为实体的集合。</p><p><strong>2.2 Illustrations of fine-tuning KG-BERT for predicting the relation between two entities</strong></p><p><img src="/2021/08/06/kg-bert/kgbert2.JPG" alt></p><p>作者发现直接使用两个实体去预测关系，效果优于使用两个实体和一个随机关系（这里本人认为一个随机的关系本来就是错误特征，感觉肯定会影响预测结果）。这里和2.1结构的差异在于：1.输入从实体加关系的三输入变成基于实体的双输入2.输出从二分类变成多分类</p><p>目标函数为：</p><script type="math/tex; mode=display">S_{\tau}^{'}=f(h,r,t)=softmax(CW^{'T})</script><p>损失函数为$S^{‘}$和$y^{‘}$的交叉熵：</p><script type="math/tex; mode=display">L^{'}=-\sum_{\tau \in D^{+}}\sum_{i=1}^{R}y_{\tau i}^{'}log(s^{'}_{\tau i})</script><h2 id="三-实验"><a href="#三-实验" class="headerlink" title="三.实验"></a>三.实验</h2><p>setting： We choose pre-trained BERT-Base model with 12 layers, 12 self-attention heads and H = 768 as the initialization of KG-BERT, then fine tune KG-BERT with Adam implemented in BERT. </p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://github.com/yao8839836/kg-bert">https://github.com/yao8839836/kg-bert</a></p><p><a href="https://zhuanlan.zhihu.com/p/355391327">https://zhuanlan.zhihu.com/p/355391327</a></p>]]></content>
      
      
      <categories>
          
          <category> 知识图谱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> KG-BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搜索系统</title>
      <link href="/2021/08/06/search-rank-init/"/>
      <url>/2021/08/06/search-rank-init/</url>
      
        <content type="html"><![CDATA[<p>综述</p><p><a href="https://zhuanlan.zhihu.com/p/112719984">https://zhuanlan.zhihu.com/p/112719984</a></p><p><a href="https://zhuanlan.zhihu.com/p/382001982">https://zhuanlan.zhihu.com/p/382001982</a></p><p><a href="https://www.cnblogs.com/davidwang456/articles/10251599.html">https://www.cnblogs.com/davidwang456/articles/10251599.html</a></p><p>DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval</p><p><a href="https://arxiv.org/pdf/1710.05649.pdf">https://arxiv.org/pdf/1710.05649.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 搜索系统 </category>
          
          <category> 搜索系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 搜索系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word2vec</title>
      <link href="/2021/08/04/word2vec/"/>
      <url>/2021/08/04/word2vec/</url>
      
        <content type="html"><![CDATA[<h2 id="一-原理"><a href="#一-原理" class="headerlink" title="一.原理"></a>一.原理</h2><p><strong>两种训练模型</strong></p><ul><li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li><li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li></ul><p><strong>训练技巧</strong></p><p>hierarchical softmax 和 negative sampling</p><h2 id="二-代码"><a href="#二-代码" class="headerlink" title="二.代码"></a>二.代码</h2><p><strong>训练代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure><p><strong>词向量矩阵</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.KeyedVectors.load_word2vec_format(model_path,binary=True)</span><br><span class="line">    print(model.vectors)   ##(779845, 400))</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model.index_to_key)</span><br><span class="line">    print(&quot;\n&quot;)</span><br><span class="line">    print(model[&quot;的&quot;])</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[-1.3980628e+00, -4.6281612e-01,  5.8368486e-01, ...,         5.3952241e-01,  4.4697687e-01,  1.3505782e+00],       [ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, ...,         1.1110669e+00,  2.1992767e-01,  7.0457202e-01],       [-8.5650706e-01,  8.2832746e-02, -8.4218192e-01, ...,         2.1654253e+00,  6.4846051e-01, -5.7714492e-01],       ...,       [ 7.5072781e-03, -1.3543828e-02,  2.3101490e-02, ...,         4.2363801e-03, -5.6749382e-03,  6.3404259e-03],       [-2.6244391e-04, -3.0459568e-02,  5.9752418e-03, ...,         1.7844304e-02, -4.7109672e-04,  7.7916058e-03],       [ 7.2062697e-04, -6.5988898e-03,  1.1346856e-02, ...,        -3.7340564e-03, -1.8825980e-02,  2.7245486e-03]], dtype=float32)</span><br><span class="line"></span><br><span class="line">[&#x27;，&#x27;, &#x27;的&#x27;, &#x27;。&#x27;, &#x27;、&#x27;, &#x27;０&#x27;, &#x27;１&#x27;, &#x27;在&#x27;, &#x27;”&#x27;, &#x27;２&#x27;, &#x27;了&#x27;, &#x27;“&#x27;, &#x27;和&#x27;, &#x27;是&#x27;, &#x27;５&#x27;, ...]</span><br><span class="line"></span><br><span class="line">array([ 4.9143720e-01, -1.4818899e-01, -2.8366420e-01, -3.6405793e-01,        1.0851435e-01,  4.9507666e-02, -7.1219063e-01, -5.4614645e-01,       -1.3581418e+00,  3.0274218e-01,  6.1700332e-01,  3.5553512e-01,        1.6602433e+00,  7.5298291e-01, -1.4151905e-01, -2.1077128e-01,       -2.6325354e-01,  1.6108564e+00, -4.6750236e-01, -1.6261842e+00,        1.3063166e-01,  8.0702168e-01,  4.0011466e-01,  1.2198541e+00,       -6.2879241e-01,  ... 2.1928079e-01,  7.1725255e-01, -2.3430648e-01, -1.2066336e+00,        9.7590965e-01, -1.5906478e-01, -3.5802779e-01, -3.8005975e-01,        1.9056025e-01,  1.1110669e+00,  2.1992767e-01,  7.0457202e-01],      dtype=float32)</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/26306795">https://zhuanlan.zhihu.com/p/26306795</a></p><p><a href="https://arxiv.org/abs/1301.3781v3">https://arxiv.org/abs/1301.3781v3</a></p><p><a href="https://arxiv.org/abs/1405.4053">https://arxiv.org/abs/1405.4053</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Graph Matching Networks for Chinese Short Text Matching</title>
      <link href="/2021/08/04/short-chinese-text-match/"/>
      <url>/2021/08/04/short-chinese-text-match/</url>
      
        <content type="html"><![CDATA[<p><a href="https://aclanthology.org/2020.acl-main.547.pdf">https://aclanthology.org/2020.acl-main.547.pdf</a></p><h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>对于中文短文本匹配，通常基于词粒度而不是字粒度。但是分词结果可能是错误的、模糊的或不一致的，从而损害最终的匹配性能。比如下图：字符序列“南京市长江大桥”经过不同的分词可能表达为不同的意思。</p><p><img src="/2021/08/04/short-chinese-text-match/lattice.JPG" alt></p><p>为了解决这个问题，作者提出了一种基于图神经网络的中文短文本匹配方法。不是将句子分割成一个单词序列，而是保留所有可能的分割路径，形成一个Lattice（segment1，segment2，segment3），如上图所示。</p><h2 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2.问题定义"></a>2.问题定义</h2><p>将两个待匹配中文短文本分别定义为$S_a=\left \{ C_1^a,C_2^a,…,C_{t_a}^a \right \}$，$S_b=\left \{ C_1^b,C_2^b,…,C_{t_b}^b \right \}$，其中$C_i^a$表示句子$a$第$i$个字，$C_j^b$表示句子$b$第$j$个字，$t_a$，$t_b$分别表示两个句子的长度。$f(S_a,S_b)$是目标函数，输出为两个文本的匹配度。词格图用$G=(\nu,\xi)$表示，其中$\nu$是节点集，包括所有字符序列。$\xi$表示边集，如果$\nu$中两个顶点$v_i$和$v_j$相邻，那么就存在一个边为$e_{ij}$。$N_{fw}(v_i)$表示节点$v_i$ 正向的所有可达节点的集合,$N_{bw}(v_i)$表示节点$v_i$ 反向的所有可达节点的集合。句子$a$的词格图为$G^a(\nu_a,\xi_a)$，句子$b$的词格图为$G^b(\nu_b,\xi_b)$。</p><h2 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h2><p><img src="/2021/08/04/short-chinese-text-match/entire1.JPG" alt></p><p>模型分成3个部分，1.语言节点表示 2.图神经匹配 3.相关性分类器</p><h3 id="3-1-语言节点表示"><a href="#3-1-语言节点表示" class="headerlink" title="3.1 语言节点表示"></a>3.1 语言节点表示</h3><p>这一部分基于BERT的结构。BERT的token表示基于字粒度，可以得到$\left \{ [CLS],C_1^a,C_2^a,…,C_{ta}^a,[SEP],C_1^b,C_2^b,…,C_{t_b}^b,[SEP] \right \}$,如上图所示。BERT的输出为各个字的Embedding，$ \left \{\textbf{C}^{CLS},\textbf{C}_1^a,\textbf{C}_2^a,…,\textbf{C}_{t_a}^a,\textbf{C}^{SEP},\textbf{C}_1^b,\textbf{C}_2^b,…,\textbf{C}_{t_b},\textbf{C}^{SEP} \right \}$。</p><h3 id="3-2-图神经匹配"><a href="#3-2-图神经匹配" class="headerlink" title="3.2 图神经匹配"></a>3.2 图神经匹配</h3><p><strong>初始化</strong>：假设节点$v_i$包含$n_i$个连续字符，起始字符位置为$s_i$，即$ \left \{C_{s_i},C_{s_{i+1}},…,C_{s_{i}+n_i-1} \right \}$，这里$v_i$表示句子$a$或者$b$的结点。$V_i=\sum_{k=0}^{n_i-1}\textbf{U}_{s_i+k}\odot\textbf{C}_{s_i+k}$，其中$\odot$表示两个向量对应各个元素相乘。特征识别分数向量$\textbf{U}_{s_i+k}=softmax(FFN(\textbf{C}_{s_i+k}))$，$FFN$为两层。$h$为结点的向量表示，将$h_i^0$等于$V_i$</p><p><strong>Message Propagation</strong> : 对于第$l$次迭代，$G_a$中某个结点$v_i$由如下四个部分组成</p><script type="math/tex; mode=display">m_i^{fw}=\sum_{v_j \in N_{fw}(v_i)}\alpha_{ij}(W^{fw}h_j^{l-1}),\\m_i^{bw}=\sum_{v_k \in N_{bw}(v_i)}\alpha_{ik}(W^{bw}h_k^{l-1}),\\m_i^{b1}=\sum_{v_m \in V^b}\alpha_{im}(W^{fw}h_m^{l-1}),\\m_i^{b2}=\sum_{v_q \in V^b}\alpha_{iq}(W^{bw}h_q^{l-1})，</script><p>其中$\alpha_{ij},\alpha_{ik},\alpha_{im},\alpha_{iq}$是注意力系数，$W^{fw},W^{bw}$是注意力系数参数</p><p>然后定义两种信息为$m_i^{self}\triangleq[m_i^{fw},m_i^{bw}]，m_i^{cross}\triangleq[m_i^{b1},m_i^{b2}]$</p><p><strong>Representation Updating</strong>：得到两种信息后，需要更新结点$ v_i$的向量表示</p><script type="math/tex; mode=display">d_k=cosine(w_k^{cos}\odot m_i^{self},w_k^{cos}\odot m_i^{cross})</script><p>其中$w_k^{cos}$为参数，$d_k$为multi-perspective cosine distance，可以衡量两种信息的距离，$k \in \left \{ 1,2,3,…P\right\}$，$P$是视角的数量。</p><script type="math/tex; mode=display">h_i^l=FFN([m_i^{self},\textbf{d}_i])</script><p>其中$\textbf{d}_i\triangleq[d_1,d_2,…,d_P]$,$FFN$两层。</p><p><strong>句子的图级别表示</strong>：</p><p>总共经历了$L$次迭代（layer），得到$h_i^L$为结点$v_i$最终的向量表示（$h_i^L$includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph)</p><p>最终，两个句子的图级别表示分别为</p><script type="math/tex; mode=display">g^a=attentive pooling(\left \{ h_{1a}^L,h_{2a}^L,...,h_{node-num_a a}^L \right \}),\\g^b=attentive pooling(\left \{ h_{1b}^L,h_{2b}^L,...,h_{node-num_b b}^L \right \})</script><h3 id="3-3-分类器"><a href="#3-3-分类器" class="headerlink" title="3.3 分类器"></a>3.3 分类器</h3><p>得到$g^a,g^b$后，两句子的相似度可以用分类器衡量：</p><script type="math/tex; mode=display">P=FFN([g^a,g^b,g^a \odot g^b,|g^a-g^b|])</script><p>其中$P \in [0,1]$。</p><h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4.实验结果"></a>4.实验结果</h2><p><img src="/2021/08/04/short-chinese-text-match/22.GIF" alt></p><p><img src="/2021/08/04/short-chinese-text-match/33.GIF" alt></p><p>lattice和JIEBA+PKU的区别？</p><p>JIEBA+PKU is a small lattice graph generated by merging two word segmentation results</p><p>lattice：overall lattice，应该是全部的组合</p><p>两者效果差不多是因为Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence).</p><p><img src="/2021/08/04/short-chinese-text-match/11.GIF" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://blog.csdn.net/qq_43390809/article/details/114077216">https://blog.csdn.net/qq_43390809/article/details/114077216</a></p>]]></content>
      
      
      <categories>
          
          <category> GNN </category>
          
          <category> NLP </category>
          
          <category> GNN </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>字典树</title>
      <link href="/2021/08/02/trie-tree/"/>
      <url>/2021/08/02/trie-tree/</url>
      
        <content type="html"><![CDATA[<h2 id="一-核心思想"><a href="#一-核心思想" class="headerlink" title="一.核心思想"></a>一.核心思想</h2><p>Trie tree，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较。Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。字典树的查询时间复杂度是O (L)，L是待查字符串的长度。如果是普通的线性表结构，那么查询效率为O（NL），N为待查数据集的大小。</p><p>假设有b，abc，abd，bcd，abcd，efg，hii 这6个单词,那我们创建字典树如下：</p><p><img src="/2021/08/02/trie-tree/11.png" alt></p><h2 id="二-应用"><a href="#二-应用" class="headerlink" title="二.应用"></a>二.应用</h2><p><strong>目的</strong>：利用汉语拼音缩写还原中文汉字</p><p><strong>准备</strong>：数据集（包含中文汉字以及对应汉语缩写）</p><p><strong>思想</strong>：1.基于汉语拼音缩写<strong>检索</strong>出数据集中对应的所有中文汉字 2.基于中文汉字出现频次排序，将<strong>top1</strong>作为汉语拼音的还原结果</p><p><strong>代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">import  pandas as pd</span><br><span class="line">import pickle</span><br><span class="line">import os</span><br><span class="line">import joblib</span><br><span class="line"></span><br><span class="line">class TrieNode(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Initialize your data structure here.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.data = &#123;&#125;###字母字符</span><br><span class="line">        self.data1=&#123;&#125;###中文</span><br><span class="line">        self.is_word = False###标识是否汉字</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Trie(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.root = TrieNode()</span><br><span class="line"></span><br><span class="line">    def insert(self, word,word1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Inserts a word into the trie.</span><br><span class="line">        :type word: str</span><br><span class="line">        :rtype: void</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in word:</span><br><span class="line">            child = node.data.get(letter)</span><br><span class="line">            if not child:</span><br><span class="line">                node.data[letter] = TrieNode()</span><br><span class="line">            node = node.data[letter]</span><br><span class="line">        node.is_word = True</span><br><span class="line">        if word1 not in node.data1:</span><br><span class="line">            node.data1[word1]=1</span><br><span class="line">        else:</span><br><span class="line">            node.data1[word1]+=1</span><br><span class="line"></span><br><span class="line">    def search(self, word):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns if the word is in the trie.</span><br><span class="line">        :type word: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in word:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            if not node:</span><br><span class="line">                return False</span><br><span class="line">        return node.is_word</span><br><span class="line"></span><br><span class="line">    def starts_with(self, prefix):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns if there is any word in the trie</span><br><span class="line">        that starts with the given prefix.</span><br><span class="line">        :type prefix: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">            if not node:</span><br><span class="line">                return False</span><br><span class="line">        return True</span><br><span class="line"></span><br><span class="line">    def get_start(self, prefix):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Returns words started with prefix</span><br><span class="line">        :param prefix:</span><br><span class="line">        :return: words (list)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        def _get_key(pre, pre_node):</span><br><span class="line">            words_list = []</span><br><span class="line">            if pre_node.is_word:</span><br><span class="line">                words_list.append([pre,pre_node.data1])</span><br><span class="line">            for x in pre_node.data.keys():</span><br><span class="line">                words_list.extend(_get_key(pre + str(x), pre_node.data.get(x)))</span><br><span class="line">            return words_list</span><br><span class="line"></span><br><span class="line">        words = []</span><br><span class="line">        if not self.starts_with(prefix):</span><br><span class="line">            return words</span><br><span class="line">        # if self.search(prefix):</span><br><span class="line">        #     words.append(prefix)</span><br><span class="line">        #     return words</span><br><span class="line">        node = self.root</span><br><span class="line">        for letter in prefix:</span><br><span class="line">            node = node.data.get(letter)</span><br><span class="line">        return _get_key(prefix, node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def find_result(self,string):</span><br><span class="line">        result =self.get_start(string)</span><br><span class="line">        result = sort_by_value(result[0][1])</span><br><span class="line">        result.reverse()</span><br><span class="line">        return result[0]</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">def sort_by_value(d):</span><br><span class="line">    return sorted(d.items(), key=lambda k: k[1])  # k[1] 取到字典的值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def build_tree(data,save_path):</span><br><span class="line"></span><br><span class="line">    trie = Trie()</span><br><span class="line">    for element in data.values:</span><br><span class="line">        trie.insert(element[0], element[1])</span><br><span class="line">    joblib.dump(trie, save_path)</span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line">def load_tree(path):</span><br><span class="line">    trie = joblib.load(path)</span><br><span class="line">    return trie</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    ###</span><br><span class="line">    build_tree(data,save_path)</span><br><span class="line">    ###</span><br><span class="line">    tree=load_tree(save_path)</span><br><span class="line">    print(tree.find_result(&quot;XXXXXXXX&quot;))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/28891541">https://zhuanlan.zhihu.com/p/28891541</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字典树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp中使用预训练的词向量和随机初始化的词向量的区别在哪里？</title>
      <link href="/2021/07/28/word-emb-add/"/>
      <url>/2021/07/28/word-emb-add/</url>
      
        <content type="html"><![CDATA[<p>当你训练数据<strong>不充足</strong>的时候，可以直接使用别人已经预训练好的词向量，也可以根据自己的训练数据微调(fine-tuning)预训练词向量，也可以把词向量和整个模型一块训练，但是通常预训练的词向量我们不会再在训练的过程中进行更新。</p><p>当你的训练数据<strong>比较充足</strong>的时候，并且想让词向量能更好的捕捉自己的训练数据的语义信息时，应该使用随机初始化的词向量。当然，随机初始化的词向量必须要在训练网络的过程中不断进行更新，就和神经网络的权重参数一样进行训练。</p><p>例子：</p><p>1.直观展示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line">###random</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">embeds = nn.Embedding(2, 5) </span><br><span class="line">print(embeds.weight)</span><br><span class="line">###from pretrain</span><br><span class="line">weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])</span><br><span class="line">embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">print(embedding.weight)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.1754,  1.6604, -1.5025, -1.0980, -0.4718],</span><br><span class="line">        [-1.1276,  0.1408, -1.0746, -1.2768, -0.6789]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.7366,  0.0607,  0.6151,  0.2282,  0.3878],</span><br><span class="line">        [-1.1365,  0.1844, -1.1191, -0.8787, -0.5121]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[1.0000, 2.3000, 3.0000],</span><br><span class="line">        [4.0000, 5.1000, 6.3000]])</span><br></pre></td></tr></table></figure><p>2.n-gram</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br></pre></td></tr></table></figure><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://www.zhihu.com/question/337950427">https://www.zhihu.com/question/337950427</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LASERTAGGER</title>
      <link href="/2021/07/27/lasertagger/"/>
      <url>/2021/07/27/lasertagger/</url>
      
        <content type="html"><![CDATA[<h2 id="一-摘要"><a href="#一-摘要" class="headerlink" title="一. 摘要"></a>一. 摘要</h2><p>对于某一些文本生成任务，输入和输出的文本有很多的重叠部分，如果还是采用encoder-decoder的文本生成模型去从零开始生成，其实是很浪费和没必要的，并且会导致两个问题：1：生成模型的幻觉问题(就是模型胡说八道) ；2：出现叠词(部分片段一致)。</p><p>基于上面的考虑，作者提出了lasertagger模型，通过几个常用的操作：keep token、delete token、 add token，给输入序列的每个token打上标签，使得文本生成任务转化为了序列标注任务。</p><p>通过这种方式，相较于encoder-decoder模型的优势有如下：1、推理的速度更快 2、在较小的数据集上性能优于seq2seq baseline，在大数据集上和baseline持平（因为输入和输出的文本有很多的重叠部分，对于这种情况，lasertagger的候选词库比较小，因为对于重叠部分的词，词库只需要添加keep，而传统encoder-decoder的候选词库依然很大，因为对于重叠部分的词，词库需要添加对应的词）</p><h2 id="二-主要贡献"><a href="#二-主要贡献" class="headerlink" title="二.主要贡献"></a>二.主要贡献</h2><p>1、通过输入和输出文本，自动去提取需要add的token</p><p>2、通过输入文本，输出文本和tag集，给训练的输入序列打上标签</p><p>3、提出了两个版本，$LASERTAGGER_{AR}$( bert+transformer decoder )和$LASERTAGGER_{FF}$( bert+desen+softmax )</p><h2 id="三-整体流程"><a href="#三-整体流程" class="headerlink" title="三. 整体流程"></a>三. 整体流程</h2><p><img src="/2021/07/27/lasertagger/entire.JPG" alt></p><p>其实就是两个过程，一.将输入文本变编码成特殊标注，二.将标注解码成文本</p><h2 id="四-文本标注"><a href="#四-文本标注" class="headerlink" title="四. 文本标注"></a>四. 文本标注</h2><h3 id="4-1-Tag集构建（也就是label集构建）"><a href="#4-1-Tag集构建（也就是label集构建）" class="headerlink" title="4.1 Tag集构建（也就是label集构建）"></a>4.1 Tag集构建（也就是label集构建）</h3><p>一般情况，tag分为两个大类： base tag $B$和 add tag $P$。对于base tag，就是$KEEP$或者$DELETE$当前token；对于add tag，就是要添加一个词到token前面，添加的词来源于词表$V$。实际在工程中，将$B$和$P$结合来表示，即$^{P}B$，总的tag数量大约等于$B$的数量乘以$P$的数量，即$2|V|$。对于某些任务可以引入特定的tag，比如对于句子融合，可以引入$SWAP$,如下图。</p><p><img src="/2021/07/27/lasertagger/case.JPG" alt></p><h4 id="4-1-1-词表V的构建"><a href="#4-1-1-词表V的构建" class="headerlink" title="4.1.1 词表V的构建"></a>4.1.1 词表V的构建</h4><p><strong>构建目标：</strong></p><ol><li>最小化词汇表规模；</li><li>最大化目标词语的比例</li></ol><p>限制词汇表的词组数量可以减少相应输出的决策量；最大化目标词语的比例可以防止模型添加无效词。</p><p><strong>构建过程：</strong></p><p>通过$LCS$算法（longest common sequence，最长公共子序列，注意和最长公共子串不是一回事），找出输入和输出序列的最长公共子序列，输出剩下的序列，就是需要$add$的token，添加到词表$V$，词表中的词基于词频排序,然后选择$l$个常用的。</p><p>举个例子：soruce为“12345678”，target为”1264591”</p><p>​                    最长公共子序列为[‘1’, ‘2’, ‘4’, ‘5’]</p><p>​                    需要$add$的token为 [‘6’, ‘91’]</p><p><strong>源码</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def _lcs_table(source, target):</span><br><span class="line">  &quot;&quot;&quot;Returns the Longest Common Subsequence dynamic programming table.&quot;&quot;&quot;</span><br><span class="line">  rows = len(source)</span><br><span class="line">  cols = len(target)</span><br><span class="line">  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]</span><br><span class="line">  for i in range(1, rows + 1):</span><br><span class="line">    for j in range(1, cols + 1):</span><br><span class="line">      if source[i - 1] == target[j - 1]:</span><br><span class="line">        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1</span><br><span class="line">      else:</span><br><span class="line">        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])</span><br><span class="line">  return lcs_table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _backtrack(table, source, target, i, j):</span><br><span class="line">  &quot;&quot;&quot;Backtracks the Longest Common Subsequence table to reconstruct the LCS.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    table: Precomputed LCS table.</span><br><span class="line">    source: List of source tokens.</span><br><span class="line">    target: List of target tokens.</span><br><span class="line">    i: Current row index.</span><br><span class="line">    j: Current column index.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    List of tokens corresponding to LCS.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if i == 0 or j == 0:</span><br><span class="line">    return []</span><br><span class="line">  if source[i - 1] == target[j - 1]:</span><br><span class="line">    # Append the aligned token to output.</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j - 1) + [target[j - 1]]</span><br><span class="line">  if table[i][j - 1] &gt; table[i - 1][j]:</span><br><span class="line">    return _backtrack(table, source, target, i, j - 1)</span><br><span class="line">  else:</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j)</span><br><span class="line"></span><br><span class="line">def _compute_lcs(source, target):</span><br><span class="line">  # s1=&#123;1,3,4,5,6,7,7,8&#125;,s2=&#123;3,5,7,4,8,6,7,8,2&#125; return 35778</span><br><span class="line">  table = _lcs_table(source, target)</span><br><span class="line">  return _backtrack(table, source, target, len(source), len(target))</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def _get_added_phrases(source: Text, target: Text) -&gt; Sequence[Text]:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the phrases that need to be added to the source to get the target.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sep = &#x27;&#x27;</span><br><span class="line">    source_tokens = utils.get_token_list(source.lower())</span><br><span class="line">    target_tokens = utils.get_token_list(target.lower())</span><br><span class="line">    #compute Longest Common Subsequence</span><br><span class="line">    kept_tokens = _compute_lcs(source_tokens, target_tokens)</span><br><span class="line">    added_phrases = []</span><br><span class="line">    kept_idx = 0</span><br><span class="line">    phrase = []</span><br><span class="line">    for token in target_tokens:</span><br><span class="line">        if kept_idx &lt; len(kept_tokens) and token == kept_tokens[kept_idx]:</span><br><span class="line">            kept_idx += 1</span><br><span class="line">            if phrase:</span><br><span class="line">                added_phrases.append(sep.join(phrase))</span><br><span class="line">                phrase = []</span><br><span class="line">        else:</span><br><span class="line">            phrase.append(token)</span><br><span class="line">    if phrase:</span><br><span class="line">        added_phrases.append(sep.join(phrase))</span><br><span class="line">    return added_phrases</span><br></pre></td></tr></table></figure><p>词表位于文件label_map.txt.log，本人基于自己的数据集，内容如下所示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Idx Frequency  Coverage (%)   Phrase</span><br><span class="line">1  19 94.22  址</span><br><span class="line">2  15 95.27  单位</span><br><span class="line">3  8  95.76  地</span><br><span class="line">4  6  96.17  执勤</span><br></pre></td></tr></table></figure><h4 id="4-1-2-tag集"><a href="#4-1-2-tag集" class="headerlink" title="4.1.2 tag集"></a>4.1.2 tag集</h4><p>本人基于自己的数据集，得到的候选tag如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEEP</span><br><span class="line">DELETE</span><br><span class="line">KEEP|址</span><br><span class="line">DELETE|址</span><br><span class="line">KEEP|单位</span><br><span class="line">DELETE|单位</span><br><span class="line">KEEP|地</span><br><span class="line">DELETE|地</span><br><span class="line">KEEP|执勤</span><br><span class="line">DELETE|执勤</span><br></pre></td></tr></table></figure><h3 id="4-2-Converting-Training-Targets-into-Tags"><a href="#4-2-Converting-Training-Targets-into-Tags" class="headerlink" title="4.2 Converting Training Targets into Tags"></a>4.2 Converting Training Targets into Tags</h3><p><strong>paper上的伪代码：</strong></p><p><img src="/2021/07/27/lasertagger/al1.JPG" alt></p><p>采用贪心策略，核心思想就是遍历$t$，先和$s$匹配，匹配上就$keep$，然后$i_t+j$，得到潜在的$add \ phrase \ p=t(i_t:i_t+j-1) $，然后判断$t(i_t+j)==s(i_s)\ and \ p\in V $</p><p><strong>源码</strong>：</p><p>和伪代码有一点不同，差异在于#####之间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">def _compute_single_tag(</span><br><span class="line">        self, source_token, target_token_idx,</span><br><span class="line">        target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes a single tag.</span><br><span class="line"></span><br><span class="line">    The tag may match multiple target tokens (via tag.added_phrase) so we return</span><br><span class="line">    the next unmatched target token.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_token: The token to be tagged.</span><br><span class="line">      target_token_idx: Index of the current target tag.</span><br><span class="line">      target_tokens: List of all target tokens.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A tuple with (1) the computed tag and (2) the next target_token_idx.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    source_token = source_token.lower()</span><br><span class="line">    target_token = target_tokens[target_token_idx].lower()</span><br><span class="line">    if source_token == target_token:</span><br><span class="line">        return tagging.Tag(&#x27;KEEP&#x27;), target_token_idx + 1</span><br><span class="line">    # source_token!=target_token</span><br><span class="line">    added_phrase = &#x27;&#x27;</span><br><span class="line">    for num_added_tokens in range(1, self._max_added_phrase_length + 1):</span><br><span class="line">        if target_token not in self._token_vocabulary:</span><br><span class="line">            break</span><br><span class="line">        added_phrase += (&#x27; &#x27; if added_phrase else &#x27;&#x27;) + target_token</span><br><span class="line">        next_target_token_idx = target_token_idx + num_added_tokens</span><br><span class="line">        if next_target_token_idx &gt;= len(target_tokens):</span><br><span class="line">            break</span><br><span class="line">        target_token = target_tokens[next_target_token_idx].lower()</span><br><span class="line">        if (source_token == target_token and</span><br><span class="line">                added_phrase in self._phrase_vocabulary):</span><br><span class="line">            return tagging.Tag(&#x27;KEEP|&#x27; + added_phrase), next_target_token_idx + 1</span><br><span class="line">    return tagging.Tag(&#x27;DELETE&#x27;), target_token_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        ####################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        ########################################################################################</span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure><p><strong>缺陷</strong>：</p><p>对于一些情况，无法还原，举个例子：</p><p>​        source：证件有效期截止日期  target：证件日期格式</p><p>​        得不到tag结果</p><p>可以补充策略来修复bug</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        #########################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        #######################################################################################</span><br><span class="line"></span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    ####fix bug by lavine</span><br><span class="line"></span><br><span class="line">    ###strategy1</span><br><span class="line">    added_phrase = &quot;&quot;.join(target_tokens[target_token_idx:])</span><br><span class="line">    if added_phrase in self._phrase_vocabulary:</span><br><span class="line">        tags[-1] = tagging.Tag(&#x27;DELETE|&#x27; + added_phrase)</span><br><span class="line">        print(&#x27;&#x27;.join(source_tokens))</span><br><span class="line">        print(&#x27;&#x27;.join(target_tokens))</span><br><span class="line">        print(str([str(tag) for tag in tags] if tags != None else None))</span><br><span class="line">        return tags</span><br><span class="line">    ###strategy2</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure><h3 id="4-3-模型结构"><a href="#4-3-模型结构" class="headerlink" title="4.3 模型结构"></a>4.3 模型结构</h3><p><img src="/2021/07/27/lasertagger/fr.JPG" alt></p><p>模型主要包含两个部分：1.encoder:generates activation vectors for each element in the input sequence 2.decoder：converts encoder activations into tag labels</p><h4 id="4-3-1-encoder"><a href="#4-3-1-encoder" class="headerlink" title="4.3.1 encoder"></a>4.3.1 encoder</h4><p>由于$BERT$在sentence encoding tasks上做到state-of-the-art，所以使用$BERT$ 作为encoder部分。作者选择了$BERT_{base}$,包含12个self-attention层</p><h4 id="4-3-2-decoder"><a href="#4-3-2-decoder" class="headerlink" title="4.3.2 decoder"></a>4.3.2 decoder</h4><p>在$BERT$原文中，对于标注任务采取了非常简单的decoder结构，即采用一层feed-forward作为decoder，把这种组合叫做$LASERTAGGER_{FF}$，这种结构的缺点在于预测的标注词相互独立，没有考虑标注词的关联性。</p><p>为了考虑标注词的关联性，decode使用了Transformer decoder，单向连接，记作$LASERTAGGER_{AR}$，这种encoder和decoder的组合的有点像BERT结合GPT的感觉decoder 和encoder在以下方面交流：(i) through a full attention over the sequence of encoder activations (ii) by directly consuming the encoder activation at the current step</p><h2 id="五-realize"><a href="#五-realize" class="headerlink" title="五.realize"></a>五.realize</h2><p>对于基本的tag，比如$KEEP$，$DELETE$，$ADD$，$realize$就是根据输入和tag直接转换就行；对于特殊的tag，需要一些特定操作，看情况维护规则。</p><h2 id="六-loss"><a href="#六-loss" class="headerlink" title="六 loss"></a>六 loss</h2><p>假设句子长度为n，tag数量为m, loss为n个m分类任务的和</p><h2 id="七-评价指标"><a href="#七-评价指标" class="headerlink" title="七.评价指标"></a>七.评价指标</h2><p>评价指标，不同任务不同评价指标</p><p>1 Sentence Fusion</p><p>Exact score ：percentage of exactly correctly predicted fusions（类似accuracy）</p><p>SARI ：average F1 scores of the added, kept, and deleted n-grams</p><p>2 Split and Rephrase</p><p>SARI</p><p>3 Abstractive Summarization</p><p>ROUGE-L</p><p>4 Grammatical Error Correction (GEC)</p><p>precision and recall, F0:5</p><h2 id="八-实验结果"><a href="#八-实验结果" class="headerlink" title="八.实验结果"></a>八.实验结果</h2><p><strong>baseline</strong>： based on Transformer where both the encoder and decoder replicate the $BERT_{base}$ architecture</p><p><strong>速度</strong>：1.$LASERTAGGER_{AR} $is already 10x faster than comparable-in-accuracy $SEQ2SEQ_{BERT}$ baseline. This difference is due to the former model using a 1-layer decoder (instead of 12 layers) and no encoder-decoder cross attention. 2.$LASERTAGGER_{FF}$ is more than 100x faster</p><p>其余结果参考paper</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/1909.01187.pdf">https://arxiv.org/pdf/1909.01187.pdf</a></p><p><a href="https://github.com/google-research/lasertagger">https://github.com/google-research/lasertagger</a></p><p><a href="https://zhuanlan.zhihu.com/p/348109034">https://zhuanlan.zhihu.com/p/348109034</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本改写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sentence-BERT Sentence Embeddings using Siamese BERT-Networks</title>
      <link href="/2021/07/27/sentence-bert/"/>
      <url>/2021/07/27/sentence-bert/</url>
      
        <content type="html"><![CDATA[<p>paper: <a href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p><p>giit: <a href="https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications">https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications</a></p><h2 id="1-贡献"><a href="#1-贡献" class="headerlink" title="1.贡献"></a>1.贡献</h2><p>基于bert利用孪生结构或者三胞胎结构训练，使得产生在低维空间可用的句子Embedding。对于文本匹配任务，可以离线计算句子Embedding，然后基于句子Embedding在线匹配，可实现快速高精度的匹配。</p><h2 id="2-结构"><a href="#2-结构" class="headerlink" title="2.结构"></a>2.结构</h2><p><img src="/2021/07/27/sentence-bert/s-bert1.JPG" alt></p><p>文章提出三种结构和目标函数，三胞胎结构作者没有画图</p><p>1.Classification Objective Function</p><script type="math/tex; mode=display">loss=cross-entropy(softmax(W_t(u,v,|u-v|)),y_{true})</script><p>2.Regression Objective Function</p><script type="math/tex; mode=display">loss=MSE(cosine-sim(u, v),y_{true})</script><p>3.Triplet Objective Function</p><script type="math/tex; mode=display">loss=max(||s_a-s_p||-||s_a-s_n||+\sigma,0)</script><p>$||.||$计算向量距离，$s_a$为样本本身，$s_p$为正样本，$s_n$为负样本，$\sigma$使得正样本至少比负样本距离样本近$\sigma$。</p><p>对于pooling，文章提出三种策略</p><p>1.Using the output of the CLS-token<br>2.computing the mean of all output vectors (MEAN_strategy)<br>3.computing a max-over-time of the output vectors (MAX_strategy). The default configuration is MEAN.</p><h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h2><h3 id="3-1-Unsupervised-STS"><a href="#3-1-Unsupervised-STS" class="headerlink" title="3.1 Unsupervised STS"></a>3.1 Unsupervised STS</h3><p><img src="/2021/07/27/sentence-bert/11.JPG" alt></p><h3 id="3-2-Supervised-STS"><a href="#3-2-Supervised-STS" class="headerlink" title="3.2 Supervised STS"></a>3.2 Supervised STS</h3><p><img src="/2021/07/27/sentence-bert/22.JPG" alt></p><h3 id="3-3-Argument-Facet-Similarity"><a href="#3-3-Argument-Facet-Similarity" class="headerlink" title="3.3 Argument Facet Similarity"></a>3.3 Argument Facet Similarity</h3><p><img src="/2021/07/27/sentence-bert/33.JPG" alt></p><h3 id="3-4-Wikipedia-Sections-Distinction"><a href="#3-4-Wikipedia-Sections-Distinction" class="headerlink" title="3.4 Wikipedia Sections Distinction"></a>3.4 Wikipedia Sections Distinction</h3><p>We use the Triplet Objective</p><p><img src="/2021/07/27/sentence-bert/44.JPG" alt></p><h2 id="4-代码"><a href="#4-代码" class="headerlink" title="4.代码"></a>4.代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from sentence_bert.sentence_transformers import SentenceTransformer, util</span><br><span class="line"></span><br><span class="line">###load model</span><br><span class="line">model = SentenceTransformer(model_path)</span><br><span class="line"></span><br><span class="line"># Single list of sentences</span><br><span class="line">sentences = [&#x27;The cat sits outside&#x27;,</span><br><span class="line">             &#x27;A man is playing guitar&#x27;,</span><br><span class="line">             &#x27;I love pasta&#x27;,</span><br><span class="line">             &#x27;The new movie is awesome&#x27;,</span><br><span class="line">             &#x27;The cat plays in the garden&#x27;,</span><br><span class="line">             &#x27;A woman watches TV&#x27;,</span><br><span class="line">             &#x27;The new movie is so great&#x27;,</span><br><span class="line">             &#x27;Do you like pizza?&#x27;]</span><br><span class="line"></span><br><span class="line">#Compute embeddings</span><br><span class="line">embeddings = model.encode(sentences, convert_to_tensor=True)</span><br><span class="line"></span><br><span class="line">#Compute cosine-similarities for each sentence with each other sentence</span><br><span class="line">cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)</span><br><span class="line"></span><br><span class="line">#Find the pairs with the highest cosine similarity scores</span><br><span class="line">pairs = []</span><br><span class="line">for i in range(len(cosine_scores)-1):</span><br><span class="line">    for j in range(i+1, len(cosine_scores)):</span><br><span class="line">        pairs.append(&#123;&#x27;index&#x27;: [i, j], &#x27;score&#x27;: cosine_scores[i][j]&#125;)</span><br><span class="line"></span><br><span class="line">#Sort scores in decreasing order</span><br><span class="line">pairs = sorted(pairs, key=lambda x: x[&#x27;score&#x27;], reverse=True)</span><br><span class="line"></span><br><span class="line">for pair in pairs[0:10]:</span><br><span class="line">    i, j = pair[&#x27;index&#x27;]</span><br><span class="line">    print(&quot;&#123;&#125; \t\t &#123;&#125; \t\t Score: &#123;:.4f&#125;&quot;.format(sentences[i], sentences[j], pair[&#x27;score&#x27;]))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">The new movie is awesome  The new movie is so great  Score: 0.9283</span><br><span class="line">The cat sits outside  The cat plays in the garden  Score: 0.6855</span><br><span class="line">I love pasta  Do you like pizza?  Score: 0.5420</span><br><span class="line">I love pasta  The new movie is awesome  Score: 0.2629</span><br><span class="line">I love pasta  The new movie is so great  Score: 0.2268</span><br><span class="line">The new movie is awesome  Do you like pizza?  Score: 0.1885</span><br><span class="line">A man is playing guitar  A woman watches TV  Score: 0.1759</span><br><span class="line">The new movie is so great  Do you like pizza?  Score: 0.1615</span><br><span class="line">The cat plays in the garden  A woman watches TV  Score: 0.1521</span><br><span class="line">The cat sits outside  The new movie is awesome  Score: 0.1475</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id><a href="#" class="headerlink" title=" "></a> </h2>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlpcda-NLP中文数据增强工具，强推</title>
      <link href="/2021/07/27/nlp-data-augment/"/>
      <url>/2021/07/27/nlp-data-augment/</url>
      
        <content type="html"><![CDATA[<p>下载：pip install nlpcda</p><p>工具支持</p><ul><li><a href="https://pypi.org/project/nlpcda/#1随机等价实体替换">1.随机实体替换</a></li><li><a href="https://pypi.org/project/nlpcda/#2随机同义词替换">2.近义词</a></li><li><a href="https://pypi.org/project/nlpcda/#3随机近义字替换">3.近义近音字替换</a></li><li><a href="https://pypi.org/project/nlpcda/#4随机字删除">4.随机字删除（内部细节：数字时间日期片段，内容不会删）</a></li><li><a href="https://pypi.org/project/nlpcda/#5ner命名实体-数据增强">5.NER类 <code>BIO</code> 数据增强</a></li><li><a href="https://pypi.org/project/nlpcda/#6随机置换邻近的字">6.随机置换邻近的字：<strong>研表究明，汉字序顺并不定一影响文字的阅读理解</strong>&lt;&lt;是乱序的</a></li><li><a href="https://pypi.org/project/nlpcda/#7等价字替换">7.中文等价字替换（1 一 壹 ①，2 二 贰 ②）</a></li><li><a href="https://pypi.org/project/nlpcda/#8翻译互转实现的增强">8.翻译互转实现的增强</a></li><li><a href="https://pypi.org/project/nlpcda/#9simbert">9.使用<code>simbert</code>做生成式相似句生成</a></li><li><a href="https://pypi.org/project/nlpcda/#10Cluster2Cluster">10.Cluster2Cluster生成更多样化的新数据</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 小帮手 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 中文数据增强 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本生成评价指标</title>
      <link href="/2021/07/27/text-generate-evaluate/"/>
      <url>/2021/07/27/text-generate-evaluate/</url>
      
        <content type="html"><![CDATA[<h2 id="1-BLEU"><a href="#1-BLEU" class="headerlink" title="1.BLEU"></a>1.BLEU</h2><p>BLEU,全称为bilingual evaluation understudy，一般用于机器翻译和文本生成的评价，比较候选译文和参考译文里的重合程度，重合程度越高就认为译文质量越高，取值范围为[0,1]。</p><p>优点</p><ul><li>它的易于计算且速度快，特别是与人工翻译模型的输出对比；</li><li>它应用范围广泛，这可以让你很轻松将模型与相同任务的基准作对比。</li></ul><p>缺点</p><ul><li>它不考虑语义，句子结构</li><li>不能很好地处理形态丰富的语句（BLEU原文建议大家配备4条翻译参考译文）</li><li>BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强）</li></ul><h3 id="1-1-完整式子"><a href="#1-1-完整式子" class="headerlink" title="1.1 完整式子"></a>1.1 完整式子</h3><p>BLEU完整式子为：</p><script type="math/tex; mode=display">BLEU=BP*e^{\sum_{n=1}^{N}W_nlogP_{n}}</script><h3 id="1-2-BP"><a href="#1-2-BP" class="headerlink" title="1.2 $BP$"></a>1.2 $BP$</h3><p>目的：$n-gram$匹配度可能会随着句子长度的变短而变好，比如，只翻译了一个词且对了，那么匹配度很高,为了避免这种评分的偏向性,引入长度惩罚因子</p><p>Brevity Penalty为长度惩罚因子，其中$l_c$表示机器翻译的译文长度，$l_s$表示参考答案的有效长度</p><script type="math/tex; mode=display">BP=\begin{equation}\left\{\begin{aligned}1 & & if \ \ l_c>l_s \\e^{1-\frac{l_s}{l_c}} &  & l_c<=l_s \end{aligned}\right.\end{equation}</script><h3 id="1-3-P-n"><a href="#1-3-P-n" class="headerlink" title="1.3 $P_n$"></a>1.3 $P_n$</h3><p>人工译文表示为$s_j$，其中$j \in M$,$M$表示共有$M$个参考答案<br>翻译译文表示$c_i$，其中$i \in E$,$E$表示共有$E$个翻译<br>$n-gram$表示$n$个单词长度的词组集合，令$k$ 表示第$k$ 个词组,总共$K$个<br>$h_k(c_i)$表示第$k$个词组在翻译译文$c_i$出现的次数<br>$h_k(s_{j})$表示第$k$个词组在参考答案$s_{j}$出现的次数 </p><script type="math/tex; mode=display">P_n=\frac{\sum_{i=1}^E\sum_{k=1}^Kmin(h_k(c_i),max_{j\in M}h_k(s_{j}))}{\sum_{i=1}^E\sum_{k=1}^Kmin(h_k(c_i))}</script><p>举例如下，例如：</p><p>　　　　原文：今天天气不错</p><p>　　　　机器译文：It is a nice day today</p><p>　　　　人工译文：Today is a nice day</p><p>$1-gram$:</p><p><img src="/2021/07/27/text-generate-evaluate/11.png" alt></p><p>　可以看到机器译文一共6个词，有5个词语都命中的了参考译文,$P_1=\frac{5}{6}$</p><p>$3-gram$:</p><p><img src="/2021/07/27/text-generate-evaluate/22.png" alt></p><p>机器译文一共可以分为4个$3-gram$的词组，其中有2个可以命中参考译文，那么$P_3=\frac{2}{4}$</p><h3 id="1-4-W-n"><a href="#1-4-W-n" class="headerlink" title="1.4 $W_n$"></a>1.4 $W_n$</h3><p>$W_n$表示$P_n$的权重，一般为加权平均，即$W_n=\frac{1}{N}$,其中$N$为$gram$的数量，一般不大于4</p><h2 id="2-ROUGE"><a href="#2-ROUGE" class="headerlink" title="2 ROUGE"></a>2 ROUGE</h2><p>Recall-Oriented Understudy for Gisting Evaluation，可以看做是BLEU 的改进版，专注于召回率而非精度。换句话说，它会查看有多少个参考译句中的 n 元词组出现在了输出之中。</p><p>ROUGE大致分为四种（常用的是前两种）： - ROUGE-N （将BLEU的精确率优化为召回率） - ROUGE-L （将BLEU的n-gram优化为公共子序列） - ROUGE-W （将ROUGE-L的连续匹配给予更高的奖励） - ROUGE-S （允许n-gram出现跳词(skip)）</p><p><img src="/2021/07/27/text-generate-evaluate/3.png" alt></p><p>注意：</p><p>关于rouge包给出三个结果，而论文只有一个值，比如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&#x27;rouge-1&#x27;: &#123;&#x27;r&#x27;: 1.0, &#x27;p&#x27;: 1.0, &#x27;f&#x27;: 0.999999995&#125;, &#x27;rouge-2&#x27;: &#123;&#x27;r&#x27;: 0.0, &#x27;p&#x27;: 0.0, &#x27;f&#x27;: 0.0&#125;, &#x27;rouge-l&#x27;: &#123;&#x27;r&#x27;: 1.0, &#x27;p&#x27;: 1.0, &#x27;f&#x27;: 0.999999995&#125;&#125;]</span><br></pre></td></tr></table></figure><p>用“r”，recall就好了</p><h2 id="3-NIST"><a href="#3-NIST" class="headerlink" title="3 NIST"></a>3 NIST</h2><h2 id="4-METEOR"><a href="#4-METEOR" class="headerlink" title="4 METEOR"></a>4 METEOR</h2><h2 id="5-TER"><a href="#5-TER" class="headerlink" title="5 TER"></a>5 TER</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/qq_36533552/article/details/107444391">https://blog.csdn.net/qq_36533552/article/details/107444391</a></p><p><a href="https://zhuanlan.zhihu.com/p/144182853">https://zhuanlan.zhihu.com/p/144182853</a></p><p><a href="https://arxiv.org/pdf/2006.14799.pdf">https://arxiv.org/pdf/2006.14799.pdf</a></p><p><a href="https://www.cnblogs.com/by-dream/p/7679284.html">https://www.cnblogs.com/by-dream/p/7679284.html</a></p><p><a href="https://blog.csdn.net/qq_30232405/article/details/104219396">https://blog.csdn.net/qq_30232405/article/details/104219396</a></p><p><a href="https://blog.csdn.net/u013521274/article/details/89460322">https://blog.csdn.net/u013521274/article/details/89460322</a></p><p><a href="https://zhuanlan.zhihu.com/p/388720967">https://zhuanlan.zhihu.com/p/388720967</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本生成评价指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ide</title>
      <link href="/2021/07/25/python_ide/"/>
      <url>/2021/07/25/python_ide/</url>
      
        <content type="html"><![CDATA[<h2 id="pycharm"><a href="#pycharm" class="headerlink" title="pycharm"></a>pycharm</h2><h3 id="1-pycharm连接远程，本地无法查看源码，但是可以正常运行"><a href="#1-pycharm连接远程，本地无法查看源码，但是可以正常运行" class="headerlink" title="1 pycharm连接远程，本地无法查看源码，但是可以正常运行"></a>1 pycharm连接远程，本地无法查看源码，但是可以正常运行</h3><p>1 原因</p><p>由于远程的包没有同步到本地，导致无法在本地查看，但是代码运行在远程，服务器上有包，所以可以运行</p><p>2.解决</p><p>将包从远程同步到本地，一般情况重启pycharm会自动同步，或者重新安装</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python ide </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>attention seq2seq</title>
      <link href="/2021/07/24/seq2seq/"/>
      <url>/2021/07/24/seq2seq/</url>
      
        <content type="html"><![CDATA[<h2 id="1-结构"><a href="#1-结构" class="headerlink" title="1.结构"></a>1.结构</h2><p><img src="/2021/07/24/seq2seq/11.png" alt></p><p>左边为encoder，对输入文本编码，右边为decoder，解码并应用。</p><p>整个流程的图解可以参考<a href="https://blog.csdn.net/weixin_44388679/article/details/102575223">https://blog.csdn.net/weixin_44388679/article/details/102575223</a> 中的“四、图解Attention Seq2Seq”，非常详细。</p><h2 id="2-Teacher-Forcing"><a href="#2-Teacher-Forcing" class="headerlink" title="2.Teacher Forcing"></a>2.Teacher Forcing</h2><p>在训练阶段，如果使用Teacher Forcing策略，那么目标句子单词的word embedding使用真值，不适用Teacher Forcing则为使用预测结果；至于预测阶段不能使用Teacher Forcing。</p><h2 id="3-beam-search"><a href="#3-beam-search" class="headerlink" title="3.beam search"></a>3.beam search</h2><p>beam search本质为介于蛮力与贪心之间的策略。对于贪心，每一级的输出只选择top1的结果作为下一级输入，然后top1的结果只是局部最优，不一定是全局最优，精度可能较低。对于蛮力，每级将全部结果输入下级，假设$L$为词表大小，那么最后一级的数据量为$L^{m}$，$m$为decoder 的cell数量，计算效率太低。对于beam search，每级选择top k作为下级输入，综合了效率和精度。</p><h2 id="4-常见问题"><a href="#4-常见问题" class="headerlink" title="4 常见问题"></a>4 常见问题</h2><p>0 为什么rnn based seq2seq不需要额外添加位置信息？</p><p>天然有位置信息（迭代顺序）</p><p>1 为什么rnn based seq2seq输入输出长度可变？</p><p>因为rnn based seq2seq是迭代进行的，所以长度可变</p><p>2 训练的时候要padding吗？</p><p>不用padding</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/47929039">https://zhuanlan.zhihu.com/p/47929039</a></p><p><a href="https://www.cnblogs.com/liuxiaochong/p/14399416.html">https://www.cnblogs.com/liuxiaochong/p/14399416.html</a></p><p><a href="https://blog.csdn.net/thriving_fcl/article/details/74853556">https://blog.csdn.net/thriving_fcl/article/details/74853556</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本生成 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seq2seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>特征提取器</title>
      <link href="/2021/07/24/feature-extractor/"/>
      <url>/2021/07/24/feature-extractor/</url>
      
        <content type="html"><![CDATA[<p>大致分为三类：CNN，LSTM，transformer block</p><h2 id="1-CNN"><a href="#1-CNN" class="headerlink" title="1.CNN"></a>1.CNN</h2><p><img src="/2021/07/24/feature-extractor/22.png" alt></p><p>滑动部分为卷积核</p><h2 id="2-LSTM"><a href="#2-LSTM" class="headerlink" title="2.LSTM"></a>2.LSTM</h2><p><img src="/2021/07/24/feature-extractor/11.png" alt></p><h2 id="3-transformer-block"><a href="#3-transformer-block" class="headerlink" title="3.transformer block"></a>3.transformer block</h2><p><img src="/2021/07/24/feature-extractor/33.png" alt></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/sandwichnlp/p/11612596.html#transformer">https://www.cnblogs.com/sandwichnlp/p/11612596.html#transformer</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 模型结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 特征提取 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见算法总结</title>
      <link href="/2021/07/24/normal-algrithm/"/>
      <url>/2021/07/24/normal-algrithm/</url>
      
        <content type="html"><![CDATA[<p>类别归类：蛮力，分治，动态规划，贪心，回溯，分支限界。</p><p>实现方式有两种，一般为基于迭代和基于递归。</p><p>优化原则：无非时间换空间或者空间换时间 </p><h2 id="一-蛮力Brute-force"><a href="#一-蛮力Brute-force" class="headerlink" title="一.蛮力Brute-force"></a>一.蛮力Brute-force</h2><p>没有什么好说，就是遍历或者枚举。</p><h2 id="二-分治"><a href="#二-分治" class="headerlink" title="二.分治"></a>二.分治</h2><p>分而治之，将大问题拆解成<strong>相互独立且相似</strong>的子问题。</p><p><strong>步骤</strong>：1、先分 2 求解 3 .合并</p><p><strong>实现方式</strong>：一般递归实现</p><h2 id="三-动态规划"><a href="#三-动态规划" class="headerlink" title="三.动态规划"></a>三.动态规划</h2><p>和暴力相比，dp利用了子问题间的依赖关系，减少了一些计算</p><p><strong>适用条件</strong>：1.满足最优子结构性质，即最优解所包含的子问题的解也是最优的 </p><p><strong>构造递推式</strong>：</p><p>0.确定维度，一般一维或者二维</p><p>1.注意含义</p><p>2.选择分隔点</p><p>​    a. 一般和前一步或者两步有关,比如最大子序和，$dp[i]=max(s[i],s[i]+dp[i-1])$</p><p>​    b. 但有时候需要遍历全部分割点，比如单词拆分，$dp[i]=dp[j] \ \&amp; \&amp; \ check(s[j..i−1])$，</p><p>​    c. 有时候动态选择分隔点，比如丑数，$ dp[i]=min(dp[p2]<em>2,dp[p3]</em>3,dp[p5]*5) $,其中$p2,p3,p5$动态变化</p><p>3.注意+， -</p><p>一定是用已有的递推没有的</p><p>举例比如<a href="https://leetcode.cn/problems/unique-paths/">62. 不同路径</a>，$dp[i][j] = dp[i-1][j] + dp[i][j-1]$；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def uniquePaths(self, m: int, n: int) -&gt; int:</span><br><span class="line">        dp = [[1]*n] + [[1]+[0] * (n-1) for _ in range(m-1)]</span><br><span class="line">        #print(dp)</span><br><span class="line">        for i in range(1, m):</span><br><span class="line">            for j in range(1, n):</span><br><span class="line">                dp[i][j] = dp[i-1][j] + dp[i][j-1]</span><br><span class="line">        return dp[-1][-1]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>比如<a href="https://leetcode.cn/problems/longest-palindromic-substring/">5. 最长回文子串</a>，$dp[i][j]=dp[i+1][j-1] \ and \ s[i]==s[j]$</p><p>s = “babad”</p><p>n=5</p><p>（i，j）</p><p>(0,1)  (1,2) (2,3) (3,4) (0,2) (1,3) (2,4) (0,3)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def longestPalindrome(self, s: str) -&gt; str:</span><br><span class="line">        n = len(s)</span><br><span class="line">        if n &lt; 2:</span><br><span class="line">            return s</span><br><span class="line">        </span><br><span class="line">        max_len = 1</span><br><span class="line">        begin = 0</span><br><span class="line">        # dp[i][j] 表示 s[i..j] 是否是回文串</span><br><span class="line">        dp = [[False] * n for _ in range(n)]</span><br><span class="line">        for i in range(n):</span><br><span class="line">            dp[i][i] = True</span><br><span class="line">        </span><br><span class="line">        # 递推开始</span><br><span class="line">        # 先枚举子串长度</span><br><span class="line">        for L in range(2, n + 1):</span><br><span class="line">            # 枚举左边界，左边界的上限设置可以宽松一些</span><br><span class="line">            for i in range(n):</span><br><span class="line">                # 由 L 和 i 可以确定右边界，即 j - i + 1 = L 得</span><br><span class="line">                j = L + i - 1</span><br><span class="line">                # 如果右边界越界，就可以退出当前循环</span><br><span class="line">                if j &gt;= n:</span><br><span class="line">                    break</span><br><span class="line">                    </span><br><span class="line">                if s[i] != s[j]:</span><br><span class="line">                    dp[i][j] = False </span><br><span class="line">                else:</span><br><span class="line">                    if j - i &lt; 3:</span><br><span class="line">                        dp[i][j] = True</span><br><span class="line">                    else:</span><br><span class="line">                        dp[i][j] = dp[i + 1][j - 1]</span><br><span class="line">                </span><br><span class="line">                # 只要 dp[i][L] == true 成立，就表示子串 s[i..L] 是回文，此时记录回文长度和起始位置</span><br><span class="line">                if dp[i][j] and j - i + 1 &gt; max_len:</span><br><span class="line">                    max_len = j - i + 1</span><br><span class="line">                    begin = i</span><br><span class="line">        return s[begin:begin + max_len]</span><br></pre></td></tr></table></figure><p><strong>实现方式</strong></p><p>递归实现：效率不高的原因在于子问题重复计算了，比起暴力哪个快？应该还是这个</p><p>迭代+备忘录：不一定全部都要存储，需要的存着了就可以了，每次子问题计算一次（空间换时间）</p><p>一般采用迭代+备忘录</p><p><strong>解的追踪</strong>：有时候需要借助备忘录搜索解，时间复杂度不超过计算备忘录</p><p><strong>状态压缩动态规划</strong></p><p><a href="https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b">https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b</a></p><p><a href="https://segmentfault.com/a/1190000038223084">https://segmentfault.com/a/1190000038223084</a></p><p><a href="https://blog.csdn.net/wxgaws/article/details/114693162">https://blog.csdn.net/wxgaws/article/details/114693162</a></p><p>Usually, the state of DP can use <strong>limited variables</strong> to represent such a dp[i] , dp[i] [j] ,dp[i] [j] [k].</p><p>However, sometimes, the states of a DP problem may contain <strong>multiple statuses</strong>. In this case, we can think about using the <strong>state compression</strong> approaches to represent the DP state. </p><p>说白了就是在状态很多的时候对状态降维</p><p>目的：可以优化空间复杂度，不知道可以不可以优化时间复杂度</p><h2 id="四-贪心"><a href="#四-贪心" class="headerlink" title="四.贪心"></a>四.贪心</h2><p>短视（少考虑很多，计算量就下去了），局部最优，需要证明</p><h2 id="五-回溯"><a href="#五-回溯" class="headerlink" title="五.回溯"></a>五.回溯</h2><p>回溯算法是暴力求解的一种，它能系统地搜索一个问题的所有解，区别在于回溯会回头（减枝），以此减少计算量</p><p>三步一回头，一般用于可以用树型结构建模的问题。</p><p><strong>实现方式</strong>：利用DFS搜索解空间，一般递归实现</p><p><a href="https://zhuanlan.zhihu.com/p/93530380">https://zhuanlan.zhihu.com/p/93530380</a></p><p><a href="https://zhuanlan.zhihu.com/p/112926891">https://zhuanlan.zhihu.com/p/112926891</a></p><p><strong>优化策略：</strong></p><p>剪枝  <a href="https://blog.csdn.net/arabic1666/article/details/80147606">https://blog.csdn.net/arabic1666/article/details/80147606</a></p><h2 id="六-分支限界"><a href="#六-分支限界" class="headerlink" title="六.分支限界"></a>六.分支限界</h2><p>类似回溯，区别如下</p><p><img src="/2021/07/24/normal-algrithm/算法.JPG" alt></p><p><strong>代价函数</strong></p><p>以极大值问题为例，以当前结点为根的子树得到的可行解的值的最大值，每个结点都有自己的</p><p><strong>界</strong></p><p>极大值问题，当前得到的可行解的值的最大值，就1个</p><p><img src="/2021/07/24/normal-algrithm/1.JPG" alt></p><p>代价函数和界可以用于减枝，对于极大值问题，界大于某结点代价函数，该结点就不需要继续搜索了</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/zm1_1zm/article/details/69224626">https://blog.csdn.net/zm1_1zm/article/details/69224626</a></p><p><a href="https://blog.csdn.net/wxgaws/article/details/114693162">https://blog.csdn.net/wxgaws/article/details/114693162</a></p><p><a href="https://segmentfault.com/a/1190000038223084">https://segmentfault.com/a/1190000038223084</a></p><p><a href="https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b">https://jimmy-shen.medium.com/bitmask-state-compression-dp-39e7ba56978b</a></p>]]></content>
      
      
      <categories>
          
          <category> 基础算法 </category>
          
          <category> 算法导论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法导论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>词语的文本相似度</title>
      <link href="/2021/07/21/word-similarity/"/>
      <url>/2021/07/21/word-similarity/</url>
      
        <content type="html"><![CDATA[<h2 id="一-基于词典"><a href="#一-基于词典" class="headerlink" title="一.基于词典"></a>一.基于词典</h2><p>人为构建，比较主观，不利于维护</p><h3 id="1-1-基于词林"><a href="#1-1-基于词林" class="headerlink" title="1.1 基于词林"></a>1.1 基于词林</h3><h4 id="1-1-1-结构"><a href="#1-1-1-结构" class="headerlink" title="1.1.1 结构"></a>1.1.1 结构</h4><p>扩展版同义词词林分为5层结构，如图，随着级别的递增，词义刻画越来越细，到了第五层，每个分类里词语数量已经不大，很多只有一个词语，已经不可再分，可以称为原子词群、原子类或原子节点。不同级别的分类结果可以为自然语言处理提供不同的服务，例如第四层的分类和第五层的分类在信息检索、文本分类、自动问答等研究领域得到应用。有研究证明，对词义进行有效扩展，或者对关键词做同义词替换可以明显改善信息检索、文本分类和自动问答系统的性能。</p><p><img src="/2021/07/21/word-similarity/cilin.JPG" alt></p><p>下载后的词典文件如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Aa01A01= 人 士 人物 人士 人氏 人选</span><br><span class="line">Aa01A02= 人类 生人 全人类</span><br><span class="line">Aa01A03= 人手 人员 人口 人丁 口 食指</span><br><span class="line">Aa01A04= 劳力 劳动力 工作者</span><br><span class="line">Aa01A05= 匹夫 个人</span><br></pre></td></tr></table></figure><p><img src="/2021/07/21/word-similarity/coder.JPG" alt></p><p>表中的编码位是按照从左到右的顺序排列。第八位的标记有3 种，分别是“=”、“#”、“@”， “=”代表“相等”、“同义”。末尾的“#”代表“不等”、“同类”，属于相关词语。末尾的“@”代表“自我封闭”、“独立”，它在词典中既没有同义词，也没有相关词。</p><p><strong>源码如下</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">class WordSimilarity2010(SimilarBase):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">        本类根据下面的论文方法：</span><br><span class="line">        基于同义词词林的词语相似度计算方法，田久乐, 赵 蔚(东北师范大学 计算机科学与信息技术学院, 长春 130117 )</span><br><span class="line">        计算两个单词所有编码组合的相似度，取最大的一个</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(WordSimilarity2010, self).__init__()</span><br><span class="line">        self.a = 0.65</span><br><span class="line">        self.b = 0.8</span><br><span class="line">        self.c = 0.9</span><br><span class="line">        self.d = 0.96</span><br><span class="line">        self.e = 0.5</span><br><span class="line">        self.f = 0.1</span><br><span class="line">        self.degree = 180</span><br><span class="line">        self.PI = math.pi</span><br><span class="line"></span><br><span class="line">    def similarity(self, w1, w2):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        判断两个词的相似性。</span><br><span class="line">        :param w1: [string]</span><br><span class="line">        :param w2: [string]</span><br><span class="line">        :return: [float]0~1之间。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        code1 = self._data.get(w1, None)</span><br><span class="line">        code2 = self._data.get(w2, None)</span><br><span class="line"></span><br><span class="line">        if not code1 or not code2:</span><br><span class="line">            return 0  # 只要有一个不在库里则代表没有相似性。</span><br><span class="line"></span><br><span class="line">        # 最终返回的最大相似度</span><br><span class="line">        sim_max = 0</span><br><span class="line"></span><br><span class="line">        # 两个词可能对应多个编码</span><br><span class="line">        for c1 in code1:</span><br><span class="line">            for c2 in code2:</span><br><span class="line">                cur_sim = self.sim_by_code(c1, c2)</span><br><span class="line">                # print(c1, c2, &#x27;的相似度为：&#x27;, cur_sim)</span><br><span class="line">                if cur_sim &gt; sim_max:</span><br><span class="line">                    sim_max = cur_sim</span><br><span class="line"></span><br><span class="line">        return sim_max</span><br><span class="line"></span><br><span class="line">    def sim_by_code(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据编码计算相似度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # 先把code的层级信息提取出来</span><br><span class="line">        clayer1 = self._parse_code(c1)</span><br><span class="line">        clayer2 = self._parse_code(c2)</span><br><span class="line"></span><br><span class="line">        common_layer = self.get_common_layer(clayer1,clayer2)</span><br><span class="line">        length = len(common_layer)</span><br><span class="line"></span><br><span class="line">        # 如果有一个编码以&#x27;@&#x27;结尾，那么表示自我封闭，这个编码中只有一个词，直接返回f</span><br><span class="line">        if c1.endswith(&#x27;@&#x27;) or c2.endswith(&#x27;@&#x27;) or 0 == length:</span><br><span class="line">            return self.f</span><br><span class="line"></span><br><span class="line">        cur_sim = 0</span><br><span class="line">        if 6 &lt;= length:</span><br><span class="line">            # 如果前面七个字符相同，则第八个字符也相同，要么同为&#x27;=&#x27;，要么同为&#x27;#&#x27;&#x27;</span><br><span class="line">            if c1.endswith(&#x27;=&#x27;) and c2.endswith(&#x27;=&#x27;):</span><br><span class="line">                cur_sim = 1</span><br><span class="line">            elif c1.endswith(&#x27;#&#x27;) and c2.endswith(&#x27;#&#x27;):</span><br><span class="line">                cur_sim = self.e</span><br><span class="line">        else:</span><br><span class="line">            k = self.get_k(clayer1, clayer2)</span><br><span class="line">            n = self.get_n(common_layer)</span><br><span class="line">            if 1 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.a, n, k)</span><br><span class="line">            elif 2 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.b, n, k)</span><br><span class="line">            elif 3 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.c, n, k)</span><br><span class="line">            elif 4 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.d, n, k)</span><br><span class="line"></span><br><span class="line">        return cur_sim</span><br><span class="line"></span><br><span class="line">    def sim_formula(self, coeff, n, k):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        计算相似度的公式，不同的层系数不同</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return coeff * math.cos(n * self.PI / self.degree) * ((n - k + 1) / n)</span><br><span class="line"></span><br><span class="line">    def get_common_layer(self, ca, cb):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应的layer层</span><br><span class="line">        :param ca:     [list(str)] 分解后的编码。</span><br><span class="line">        :param cb:     [list(str)] 分解后的编码。</span><br><span class="line">        :return:   [list(str)]列表代表相应的根编码。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        common_layer = []</span><br><span class="line"></span><br><span class="line">        for i, j in zip(ca, cb):</span><br><span class="line">            if i == j:</span><br><span class="line">                common_layer.append(i)</span><br><span class="line">            else:</span><br><span class="line">                break</span><br><span class="line">        return common_layer</span><br><span class="line"></span><br><span class="line">    def get_k(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        返回两个编码对应分支的距离，相邻距离为1</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if c1[0] != c2[0]:</span><br><span class="line">            return abs(ord(c1[0]) - ord(c2[0]))</span><br><span class="line">        elif c1[1] != c2[1]:</span><br><span class="line">            return abs(ord(c1[1]) - ord(c2[1]))</span><br><span class="line">        elif c1[2] != c2[2]:</span><br><span class="line">            return abs(int(c1[2]) - int(c2[2]))</span><br><span class="line">        elif c1[3] != c2[3]:</span><br><span class="line">            return abs(ord(c1[3]) - ord(c2[3]))</span><br><span class="line">        else:</span><br><span class="line">            return abs(int(c1[4]) - int(c2[4]))</span><br><span class="line"></span><br><span class="line">    def get_n(self, common_layer):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应结点下有多少个同级子结点。</span><br><span class="line">        :param common_layer:    [listr(str)]相同的结点。</span><br><span class="line">        :return:    int</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        end_node = self._code_tree</span><br><span class="line">        for t_node_name in common_layer:</span><br><span class="line">            end_node = end_node[t_node_name]</span><br><span class="line"></span><br><span class="line">        if not isinstance(end_node, dict):</span><br><span class="line">            return end_node</span><br><span class="line">        return len(end_node.keys())</span><br></pre></td></tr></table></figure><h4 id="1-1-2-使用"><a href="#1-1-2-使用" class="headerlink" title="1.1.2 使用"></a>1.1.2 使用</h4><p>环境准备：pip install WordSimilarity</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from word_similarity import WordSimilarity2010</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">ws_tool = WordSimilarity2010()</span><br><span class="line">start = time.time()</span><br><span class="line">b_a = &quot;联系方式&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">b_a = &quot;手机&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 0</span><br><span class="line">运行时间：5.793571472167969e-05</span><br><span class="line">手机 电话 相似度为 0.30484094213212237</span><br><span class="line">运行时间：0.0001442432403564453</span><br></pre></td></tr></table></figure><h3 id="1-2-基于知网与词林的词语语义相似度计算"><a href="#1-2-基于知网与词林的词语语义相似度计算" class="headerlink" title="1.2 基于知网与词林的词语语义相似度计算"></a>1.2 基于知网与词林的词语语义相似度计算</h3><h4 id="1-2-1-原理"><a href="#1-2-1-原理" class="headerlink" title="1.2.1 原理"></a>1.2.1 原理</h4><p>综合了词林cilin与知网hownet的相似度计算方法，采用混合策略，混合策略具体可以参考源码，如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">from hownet.howNet import How_Similarity</span><br><span class="line">from cilin.V3.ciLin import CilinSimilarity</span><br><span class="line"></span><br><span class="line">class HybridSim():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    混合相似度计算策略。使用了词林与知网词汇量的并集。扩大了词汇覆盖范围。</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    ci_lin = CilinSimilarity()  # 实例化词林相似度计算对象</span><br><span class="line">    how_net = How_Similarity()  # 实例化知网相似度计算对象</span><br><span class="line">    Common = ci_lin.vocab &amp; how_net.vocab</span><br><span class="line">    A = how_net.vocab - ci_lin.vocab</span><br><span class="line">    B = ci_lin.vocab - how_net.vocab</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def get_Final_sim(cls, w1, w2):</span><br><span class="line">        lin = cls.ci_lin.sim2018(w1, w2) if w1 in cls.ci_lin.vocab and w2 in cls.ci_lin.vocab else 0</span><br><span class="line">        how = cls.how_net.calc(w1, w2) if w1 in cls.how_net.vocab and w2 in cls.how_net.vocab else 0</span><br><span class="line"></span><br><span class="line">        if w1 in cls.Common and w2 in cls.Common:  # 两个词都被词林和知网共同收录。</span><br><span class="line">            # print(&#x27;两个词都被词林和知网共同收录。&#x27;, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(w1, w2, &#x27;词林改进版相似度：&#x27;, lin, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(&#x27;知网相似度结果为：&#x27;, how, end=&#x27;\t&#x27;)</span><br><span class="line">            return lin * 1 + how * 0  # 可以调节两者的权重，以获取更优结果！！</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.A:  # 两个词都只被知网收录。</span><br><span class="line">            return how</span><br><span class="line">        if w1 in cls.B and w2 in cls.B:  # 两个词都只被词林收录。</span><br><span class="line">            return lin</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.B:  # 一个只被词林收录，另一个只被知网收录。</span><br><span class="line">            print(&#x27;触发策略三，左词为知网，右词为词林&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.B:</span><br><span class="line">            print(&#x27;触发策略三，左词为词林，右词为知网&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（左知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（右知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.B and w2 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（左词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.B and w1 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（右词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        print(&#x27;对不起，词语可能未收录，无法计算相似度！&#x27;)</span><br><span class="line">        return -1</span><br></pre></td></tr></table></figure><h4 id="1-2-2-使用"><a href="#1-2-2-使用" class="headerlink" title="1.2.2 使用"></a>1.2.2 使用</h4><p>参考<a href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from Hybrid_Sim import HybridSim</span><br><span class="line">from Pearson import *</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    print(&#x27;词林词汇量&#x27;, len(HybridSim.ci_lin.vocab ),&#x27;\t知网词汇量&#x27;, len(HybridSim.how_net.vocab))</span><br><span class="line">    print(&#x27;两者总词汇量&#x27;,len(HybridSim.ci_lin.vocab | HybridSim.how_net.vocab),&#x27;\t重叠词汇量&#x27;, len(HybridSim.Common))</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">词林词汇量 77498 知网词汇量 53336</span><br><span class="line">两者总词汇量 85817 重叠词汇量 45017</span><br><span class="line">对不起，词语可能未收录，无法计算相似度！</span><br><span class="line">联系方式 电话相似度为： -1</span><br><span class="line">运行时间：3.504753112792969e-05</span><br><span class="line">手机 电话相似度为： 1.0</span><br><span class="line">运行时间：0.019332408905029297</span><br></pre></td></tr></table></figure><h2 id="二-基于词向量"><a href="#二-基于词向量" class="headerlink" title="二.基于词向量"></a>二.基于词向量</h2><p>基于样本构建，利于维护</p><h3 id="2-1-基于word2vec"><a href="#2-1-基于word2vec" class="headerlink" title="2.1 基于word2vec"></a>2.1 基于word2vec</h3><h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><p>word2vec的原理和词向量获取过程不在此赘述，在本部分主要讲解基于word2vec的词向量如何计算词语相似度。源码如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def similarity(self, w1, w2):</span><br><span class="line">    &quot;&quot;&quot;Compute cosine similarity between two keys.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    w1 : str</span><br><span class="line">        Input key.</span><br><span class="line">    w2 : str</span><br><span class="line">        Input key.</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    float</span><br><span class="line">        Cosine similarity between `w1` and `w2`.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))</span><br></pre></td></tr></table></figure><h4 id="2-2-2-使用"><a href="#2-2-2-使用" class="headerlink" title="2.2.2 使用"></a>2.2.2 使用</h4><p>训练</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure><p>使用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.Word2Vec.load(model_path)</span><br><span class="line">    start = time.time()</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    start = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 -0.014857853</span><br><span class="line">运行时间：-4.76837158203125e-07</span><br><span class="line">手机 电话 相似度为 0.1771852</span><br><span class="line">运行时间：0.0004227161407470703</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/sinat_33741547/article/details/80016713">https://blog.csdn.net/sinat_33741547/article/details/80016713</a></p><p><a href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本匹配 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本匹配 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bert(Pre-training of Deep Bidirectional Transformers for Language Understanding)</title>
      <link href="/2021/07/20/bert/"/>
      <url>/2021/07/20/bert/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p><h2 id="1-结构"><a href="#1-结构" class="headerlink" title="1 结构"></a>1 结构</h2><p><img src="/2021/07/20/bert/111.JPG" alt></p><p>整体结构如上图，基本单元为Transformer 的encoder部分。作者对结构的描述为：BERT’s model architecture is a multi-layer bidirectional Transformer encoder。</p><h2 id="2-Input-Output-Representations"><a href="#2-Input-Output-Representations" class="headerlink" title="2 Input/Output Representations"></a>2 Input/Output Representations</h2><p><img src="/2021/07/20/bert/bert_input.JPG" alt></p><p>[CLS]表征句子开始，[SEP]表示句子结束以及分割两个句子</p><p>Token Embedding为词向量的表示，Position Embedding为位置信息，Segment Embedding表示A，B两句话，最后的输入向量为三者相加。比起transformer多一个Segment Embedding。</p><p>具体例子：<a href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p><h2 id="3-预训练任务"><a href="#3-预训练任务" class="headerlink" title="3 预训练任务"></a>3 预训练任务</h2><p><img src="/2021/07/20/bert/bert_pre.JPG" alt></p><p><strong>1 Masked LM</strong></p><p>standard conditional language models can only be trained left-to-right or right-to-left ,   since bidirectional conditioning would allow each word to indirectly “see itself”.In order to train a deep bidirectional representation,MLM</p><p>The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time    (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time.</p><p><strong>2 Next Sentence Prediction (NSP)</strong></p><p> In order to train a model that understands sentence relationships</p><p>choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).</p><h2 id="4-Fine-tuning-BERT"><a href="#4-Fine-tuning-BERT" class="headerlink" title="4 Fine-tuning BERT"></a>4 Fine-tuning BERT</h2><p><img src="/2021/07/20/bert/bert_finetune.JPG" alt></p><p>For each task, we simply plug in the task specific inputs and outputs into BERT and finetune all the parameters end-to-end.</p><p>输入: 可以为句子对或者单句，取决于特定任务</p><p>输出：At the output, the token representations are fed into an output layer for token level tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis.</p><h2 id="5-常见问题"><a href="#5-常见问题" class="headerlink" title="5 常见问题"></a>5 常见问题</h2><p><strong>1 bert为什么双向，gpt单向？</strong></p><p>1.结构的不同</p><p>因为BERT用了transformer的encoder，在编码某个token的时候同时利用了其上下文的token，但是gptT用了transformer的decoder，只能利用上文</p><p>2.预训练任务的不同</p><p><strong>2 为什么bert长度固定？</strong></p><p>因为bert是基于transformer encoder的，不同位置的词语都是并行的，所以长度要提前固定，不可变</p><p>bert的输入输出长度为max_length,大于截断，小于padding，max_length的最大值为512</p><p><strong>3 为什么bert需要补充位置信息？</strong></p><p>因为是并行，不像迭代，没有天然的位置信息，需要补充position embedding。</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> PTM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tokenization</title>
      <link href="/2021/07/20/tokenization/"/>
      <url>/2021/07/20/tokenization/</url>
      
        <content type="html"><![CDATA[<p>对于中文和英文而言，由于语言差异导致算法也有差异。对于中文，存在字粒度和词粒度。对于英文，存在三个级别的粒度，character level，subword level，word level。下面主要阐述中文的词粒度和英文的subword level。</p><h2 id="一、中文"><a href="#一、中文" class="headerlink" title="一、中文"></a>一、中文</h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 原理</h3><p><a href="https://zhuanlan.zhihu.com/p/146792308">https://zhuanlan.zhihu.com/p/146792308</a></p><h3 id="1-2-常见中文分词工具"><a href="#1-2-常见中文分词工具" class="headerlink" title="1.2 常见中文分词工具"></a>1.2 常见中文分词工具</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># #####stanfordcorenlp</span><br><span class="line">from timeit import default_timer as timer</span><br><span class="line">from stanfordcorenlp import StanfordCoreNLP</span><br><span class="line">tic = timer()</span><br><span class="line">path=&quot;XXXXX&quot;</span><br><span class="line">nlp_zh = StanfordCoreNLP(path,lang=&#x27;zh&#x27;)#模型文件路径</span><br><span class="line">sentence = &quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;</span><br><span class="line">tic = timer()</span><br><span class="line">print (&#x27;Tokenize:&#x27;, nlp_zh.word_tokenize(sentence))</span><br><span class="line">toc = timer()</span><br><span class="line">print(toc - tic) # 输出的时间，秒为单位</span><br><span class="line">#########thulac</span><br><span class="line">import thulac</span><br><span class="line">thu1 = thulac.thulac(seg_only=True)  #默认模式</span><br><span class="line">tic = timer()</span><br><span class="line">text = thu1.cut(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;, text=True).split(&quot; &quot;)  #进行一句话分词</span><br><span class="line">toc = timer()</span><br><span class="line">print(text)</span><br><span class="line">print(toc - tic)</span><br><span class="line">####jieba</span><br><span class="line">import jieba</span><br><span class="line">tic = timer()</span><br><span class="line">print(jieba.lcut(str(&quot;搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来&quot;)))</span><br><span class="line">toc = timer()</span><br><span class="line">print(toc - tic)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Tokenize: [&#x27;搜索&#x27;, &#x27;引擎&#x27;, &#x27;会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串都&#x27;, &#x27;记录&#x27;, &#x27;下来&#x27;]</span><br><span class="line">运行时间：22.68650701455772</span><br><span class="line">[&#x27;搜索引擎会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每&#x27;, &#x27;次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串&#x27;, &#x27;都&#x27;, &#x27;记录&#x27;, &#x27;下&#x27;, &#x27;来&#x27;]</span><br><span class="line">运行时间：0.0016864966601133347</span><br><span class="line">[&#x27;搜索引擎&#x27;, &#x27;会&#x27;, &#x27;通过&#x27;, &#x27;日志&#x27;, &#x27;文件&#x27;, &#x27;把&#x27;, &#x27;用户&#x27;, &#x27;每次&#x27;, &#x27;检索&#x27;, &#x27;使用&#x27;, &#x27;的&#x27;, &#x27;所有&#x27;, &#x27;检索&#x27;, &#x27;串&#x27;, &#x27;都&#x27;, &#x27;记录下来&#x27;]</span><br><span class="line">运行时间：0.9094752036035061</span><br></pre></td></tr></table></figure><p>观察结果，可以看出thulac分词效率最高，jieba分词的精度和效率比较平衡，stanfordcorenlp分词粒度很细，但是速度慢</p><h2 id="二、英文"><a href="#二、英文" class="headerlink" title="二、英文"></a>二、英文</h2><p>SubWord算法如今已成为一个重要的NLP模型的提升算法。其主要优势如下：</p><p>1.word level存在OOV问题，一旦碰到就是back off to a dictionary，无法很好地处理未知和罕见词汇<br>2.Character level可以解决OOV，但是相比于 word-level , Character-level 的输入句子变长，使得数据变得稀疏，而且对于远距离的依赖难以学到，训练速度降低。<br>常见的SubWord算法有：BPE，WordPiece，Unigram Language Model等</p><h3 id="2-1-BPE"><a href="#2-1-BPE" class="headerlink" title="2.1 BPE"></a>2.1 BPE</h3><p>全称为Byte Pair Encoding，算法来自paper《Neural Machine Translation of Rare Words with Subword Units》。</p><h4 id="2-1-1-构建BPE-subword词表"><a href="#2-1-1-构建BPE-subword词表" class="headerlink" title="2.1.1  构建BPE subword词表"></a>2.1.1  构建BPE subword词表</h4><p><strong>原理</strong></p><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分为字符序列并在末尾添加后缀“ &lt;/ w&gt;”并且统计单词频率。停止符”&lt;/w&gt;”的意义在于表示subword是词后缀。具体来说，不加”&lt;/w&gt;”可以出现在词首，加了”&lt;/w&gt;”只能位于词尾。例如，“ low”的频率为5，那么我们将其改写为“ l o w &lt;/ w&gt;”：5</li><li>统计每一个连续字节对的出现频率，选择最高频者合并成新的subword</li><li>重复第4步直到达到第2步设定的subword词表大小或下一个最高频的字节对出现频率为1</li></ol><p>注意，每次合并后词表可能出现3种变化：</p><ul><li>+1，表明加入合并后的新字词，同时原来的2个子词还保留（2个字词都不是完全随着另一个字词的出现而紧跟着出现）</li><li>+0，表明加入合并后的新字词，同时原来的2个子词中一个保留，一个被消解（只有一个字词完全随着另一个字词的出现而紧跟着出现）</li><li>-1，表明加入合并后的新字词，同时原来的2个子词都被消解（2个字词同时连续出现）</li></ul><p><strong>例子：</strong></p><p>训练语料为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w e s t &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d e s t &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><p>Iter 1, 最高频连续字节对”e”和”s”出现了6+3=9次，合并成”es”，输出：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w es t &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d es t &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><p>Iter 2, 最高频连续字节对”es”和”t”出现了6+3=9次, 合并成”est”，输出：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w est &lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d est &lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><p>Iter 3, 以此类推，最高频连续字节对为”est”和”&lt;/w&gt;” ，合并后输出：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l o w &lt;/w&gt;&#x27;: <span class="number">5</span>, &#x27;l o w e r &lt;/w&gt;&#x27;: <span class="number">2</span>, &#x27;n e w est&lt;/w&gt;&#x27;: <span class="number">6</span>, &#x27;w i d est&lt;/w&gt;&#x27;: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><p>……</p><p>Iter n, 继续迭代直到达到预设的subword词表大小或下一个最高频的字节对出现频率为1。</p><p><strong>代码</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import re, collections</span><br><span class="line">def get_stats(vocab):</span><br><span class="line">    pairs = collections.defaultdict(int)</span><br><span class="line">    for word, freq in vocab.items():</span><br><span class="line">        symbols = word.split()</span><br><span class="line">        for i in range(len(symbols)-1):</span><br><span class="line">            pairs[symbols[i],symbols[i+1]] += freq</span><br><span class="line">    return pairs</span><br><span class="line">def merge_vocab(pair, v_in):</span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(&#x27; &#x27;.join(pair))</span><br><span class="line">    p = re.compile(r&#x27;(?&lt;!\S)&#x27; + bigram + r&#x27;(?!\S)&#x27;)</span><br><span class="line">    for word in v_in:</span><br><span class="line">        w_out = p.sub(&#x27;&#x27;.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line">    return v_out</span><br><span class="line">vocab = &#123;&#x27;l o w &lt;/w&gt;&#x27; : 5, &#x27;l o w e r &lt;/w&gt;&#x27; : 2,</span><br><span class="line">&#x27;n e w e s t &lt;/w&gt;&#x27;:6, &#x27;w i d e s t &lt;/w&gt;&#x27;:3&#125;</span><br><span class="line">num_merges = 10</span><br><span class="line">for i in range(num_merges):</span><br><span class="line">    pairs = get_stats(vocab)</span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br><span class="line">    print(best)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(&#x27;e&#x27;, &#x27;s&#x27;)</span><br><span class="line">(&#x27;es&#x27;, &#x27;t&#x27;)</span><br><span class="line">(&#x27;est&#x27;, &#x27;&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;l&#x27;, &#x27;o&#x27;)</span><br><span class="line">(&#x27;lo&#x27;, &#x27;w&#x27;)</span><br><span class="line">(&#x27;n&#x27;, &#x27;e&#x27;)</span><br><span class="line">(&#x27;ne&#x27;, &#x27;w&#x27;)</span><br><span class="line">(&#x27;new&#x27;, &#x27;est&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;low&#x27;, &#x27;&lt;/w&gt;&#x27;)</span><br><span class="line">(&#x27;w&#x27;, &#x27;i&#x27;)</span><br></pre></td></tr></table></figure><h4 id="2-1-2-编解码"><a href="#2-1-2-编解码" class="headerlink" title="2.1.2 编解码"></a>2.1.2 编解码</h4><p><strong>编码</strong></p><p>1.将subword词表按照子词长度由大到小排序。</p><p>2.对于每个单词，遍历排好序的subword词表，寻找是否有token是当前单词的子字符串。最终，我们将迭代所有token，并将所有子字符串替换为token。 </p><p>3.如果仍然有子字符串没被替换但所有token都已迭代完毕，则将剩余的子字符串替换为特殊token，如<unk>。</unk></p><p>例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;highest&lt;/w&gt;&quot; -&gt; [&quot;high&quot;, &quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;mountain&lt;/w&gt;&quot; -&gt; [&quot;moun&quot;, &quot;tain&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>编码的计算量很大。对于已知数据，我们可以pre-tokenize所有单词，并在词典中保存单词和tokenize的结果。如果存在字典中不存在的未知单词，可以应用上述编码方法对单词进行tokenize，然后将新单词以及tokenize的结果添加到字典中备用。</p><p><strong>解码</strong></p><p>将所有的subword拼在一起。</p><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码序列</span></span><br><span class="line">[[“the&lt;/w&gt;”], [“high”, “est&lt;/w&gt;”], [“moun”, “tain&lt;/w&gt;”]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码序列</span></span><br><span class="line">[“the&lt;/w&gt;”, “highest&lt;/w&gt;”, “mountain&lt;/w&gt;”]</span><br></pre></td></tr></table></figure><h4 id="2-1-3-和embedding结合"><a href="#2-1-3-和embedding结合" class="headerlink" title="2.1.3 和embedding结合"></a>2.1.3 和embedding结合</h4><p>1.构建词表，假设有subword词表：[“errrr&lt;/w&gt;”, “tain&lt;/w&gt;”, “moun”, “est&lt;/w&gt;”, “high”, “the&lt;/w&gt;”, “a&lt;/w&gt;”]</p><p>2.编码，词语”highest”编码成[“high”, “est&lt;/w&gt;”]</p><p>3.向量表示，$[E_{high},\ E_{est(/w)}]$]</p><p><a href="https://www.cnblogs.com/d0main/p/10447853.html">https://www.cnblogs.com/d0main/p/10447853.html</a></p><h3 id="2-2-WordPiece"><a href="#2-2-WordPiece" class="headerlink" title="2.2 WordPiece"></a>2.2 WordPiece</h3><p>算法来自于《Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation》</p><p>WordPiece算法可以看作是BPE的变种。不同点在于，WordPiece基于概率生成新的subword而不是下一最高频字节对。</p><h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><ol><li>准备足够大的训练语料</li><li>确定期望的subword词表大小</li><li>将单词拆分成字符序列</li><li>基于第3步数据训练语言模型</li><li>从所有可能的subword单元中选择加入语言模型后能最大程度地增加训练数据概率的单元作为新的单元</li><li>重复第5步直到达到第2步设定的subword词表大小或概率增量低于某一阈值</li></ol><h3 id="2-3-ULM"><a href="#2-3-ULM" class="headerlink" title="2.3 ULM"></a>2.3 ULM</h3><h3 id="2-4-char-n-gram"><a href="#2-4-char-n-gram" class="headerlink" title="2.4 char n-gram"></a>2.4 char n-gram</h3><p><a href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p><h3 id="2-5-Byte-Level-BPE"><a href="#2-5-Byte-Level-BPE" class="headerlink" title="2.5 Byte-Level BPE"></a>2.5 Byte-Level BPE</h3><p>《Neural Machine Translation with Byte-Level Subwords》</p><p><img src="/2021/07/20/tokenization/1.JPG" alt></p><h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3.总结"></a>3.总结</h2><p>我们在进行中文NLP任务的时候，目前基本都是字粒度；英文的话大多数是使用subword的wordpiece。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/112444056">https://zhuanlan.zhihu.com/p/112444056</a></p><p><a href="https://arxiv.org/pdf/1508.07909.pdf">https://arxiv.org/pdf/1508.07909.pdf</a></p><p><a href="https://zhuanlan.zhihu.com/p/38130825">https://zhuanlan.zhihu.com/p/38130825</a></p><p><a href="https://zhuanlan.zhihu.com/p/86965595">https://zhuanlan.zhihu.com/p/86965595</a></p><p><a href="https://blog.csdn.net/zhangxiaolinxin/article/details/107052054?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&amp;spm=1001.2101.3001.4242">https://blog.csdn.net/zhangxiaolinxin/article/details/107052054?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&amp;spm=1001.2101.3001.4242</a></p><p><a href="https://www.cnblogs.com/mj-selina/p/13687291.html">https://www.cnblogs.com/mj-selina/p/13687291.html</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> Tokenization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tokenization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow，pytorch，cuda，cudnn，显卡驱动之间的区别以及对应关系</title>
      <link href="/2021/07/20/torch-cuda/"/>
      <url>/2021/07/20/torch-cuda/</url>
      
        <content type="html"><![CDATA[<h2 id="一-概念理解"><a href="#一-概念理解" class="headerlink" title="一.概念理解"></a>一.概念理解</h2><p>显卡驱动连接操作系统与底层硬件。</p><p>CUDA和NVIDIA的显卡驱动程序完全是两个不同的概念。CUDA是NVIDIA推出的用于自家GPU的并行计算框架，也就是说CUDA只能在NVIDIA的GPU上运行，而且只有当要解决的计算问题是可以大量并行计算的时候才能发挥CUDA的作用。CUDA的本质是一个工具包（ToolKit）。</p><p>cuDNN是一个SDK，是一个专门用于神经网络的加速包，注意，它跟我们的CUDA没有一一对应的关系，即每一个版本的CUDA可能有好几个版本的cuDNN与之对应，但一般有一个最新版本的cuDNN版本与CUDA对应更好。</p><p>TensorFlow为谷歌推出的深度学习框架，pytorch是Facebook 推出的深度学习框架。</p><h2 id="二-版本对应关系"><a href="#二-版本对应关系" class="headerlink" title="二.版本对应关系"></a>二.版本对应关系</h2><p>深度学习框架基于GPU运算效率远高于CPU，但是需要满足框架的版本和cuda，cudnn以及显卡驱动版本匹配才可以正常工作。</p><h3 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h3><p><img src="/2021/07/20/torch-cuda/11.png" alt></p><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><p><img src="/2021/07/20/torch-cuda/torch.JPG" alt></p><h3 id="cuDNN与CUDA"><a href="#cuDNN与CUDA" class="headerlink" title="cuDNN与CUDA"></a>cuDNN与CUDA</h3><p><img src="/2021/07/20/torch-cuda/cuda.JPG" alt></p><h3 id="CUDA和NVIDIA显卡驱动关系"><a href="#CUDA和NVIDIA显卡驱动关系" class="headerlink" title="CUDA和NVIDIA显卡驱动关系"></a>CUDA和NVIDIA显卡驱动关系</h3><p><img src="/2021/07/20/torch-cuda/22.png" alt></p><h2 id="三-常用命令"><a href="#三-常用命令" class="headerlink" title="三.常用命令"></a>三.常用命令</h2><p>查看GPU型号</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i nvidia</span><br></pre></td></tr></table></figure><p>查看NVIDIA驱动版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/driver/nvidia/version</span><br></pre></td></tr></table></figure><p>Python 查看pytorch版本、判断CUDA是否可用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch </span><br><span class="line">print(torch.__version__) </span><br><span class="line">print(torch.cuda.is_available())</span><br></pre></td></tr></table></figure><p>查看cuda版本</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/local/cuda/version.txt</span><br><span class="line">conda list | grep cuda</span><br></pre></td></tr></table></figure><p>Tensorflow中查看GPU是否可用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.test.is_gpu_available()</span><br></pre></td></tr></table></figure><h2 id="四-参考文献"><a href="#四-参考文献" class="headerlink" title="四.参考文献"></a>四.参考文献</h2><p><a href="https://blog.csdn.net/caiguanhong/article/details/112184290">https://blog.csdn.net/caiguanhong/article/details/112184290</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 深度学习框架 </category>
          
          <category> 深度学习框架 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 框架依赖 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本表示</title>
      <link href="/2021/07/19/word-representation/"/>
      <url>/2021/07/19/word-representation/</url>
      
        <content type="html"><![CDATA[<p>文本表示的表示形式可以是单一数值（基本没人用），可以是向量（目前主流），好奇有没有高纬tensor表示的？下文是基于向量表示的。</p><h2 id="1-词语表示"><a href="#1-词语表示" class="headerlink" title="1.词语表示"></a>1.词语表示</h2><h3 id="1-1-one-hot"><a href="#1-1-one-hot" class="headerlink" title="1.1 one hot"></a>1.1 one hot</h3><p>举个例子，有样本如下：</p><p>​    Jane wants to go to Shenzhen.</p><p>​    Bob wants to go to Shanghai.</p><p>基于上述两个文档中出现的单词，构建如下一个词典：</p><p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p><p>那么wants 可以表示为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0,1,0,0,0,0,0]</span><br></pre></td></tr></table></figure><h3 id="1-2-word-embedding"><a href="#1-2-word-embedding" class="headerlink" title="1.2 word embedding"></a>1.2 word embedding</h3><p>词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度的向量空间当中，使得语意相似的词在向量空间上也会比较相近，举个例子，如</p><p><img src="/2021/07/19/word-representation/11.jpg" alt></p><p>上表为词向量矩阵，其中行表示不同特征，列表示不同词，Man可以表示为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[-1,0.01,0.03,0.09]</span><br></pre></td></tr></table></figure><p>性质：$emb_{Man}-emb_{Women}\approx  emb_{King}-emb_{Queen}$</p><p>常见的词向量矩阵构建方法有，word2vec，GloVe</p><h2 id="2-句子表示"><a href="#2-句子表示" class="headerlink" title="2.句子表示"></a>2.句子表示</h2><h3 id="2-1-词袋模型"><a href="#2-1-词袋模型" class="headerlink" title="2.1 词袋模型"></a>2.1 词袋模型</h3><p>词袋模型不考虑文本中词与词之间的上下文关系，仅仅只考虑所有词的权重。而权重与词在文本中出现的频率有关。</p><p>例句:</p><p>​    Jane wants to go to Shenzhen.</p><p>​    Bob wants to go to Shanghai.</p><p>基于上述两个文档中出现的单词，构建如下一个词典：</p><p>Vocabulary=  [Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p><p>那么上面两个例句就可以用以下两个向量表示，其值为该词语出现的次数：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1,1,2,1,1,0,0]</span><br><span class="line">[0,1,2,1,0,1,1]</span><br></pre></td></tr></table></figure><h3 id="2-2-Sentence-Embedding"><a href="#2-2-Sentence-Embedding" class="headerlink" title="2.2 Sentence Embedding"></a>2.2 Sentence Embedding</h3><h4 id="2-2-1-评价工具"><a href="#2-2-1-评价工具" class="headerlink" title="2.2.1 评价工具"></a>2.2.1 评价工具</h4><p>SentEval  is a popular toolkit to evaluate the quality of sentence embeddings.</p><h4 id="2-2-2-常见方法"><a href="#2-2-2-常见方法" class="headerlink" title="2.2.2 常见方法"></a>2.2.2 常见方法</h4><p>sentence BERT</p><p>BERT-flow</p><p><a href="https://zhuanlan.zhihu.com/p/444346578">https://zhuanlan.zhihu.com/p/444346578</a></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/353187575">https://zhuanlan.zhihu.com/p/353187575</a></p><p><a href="https://www.jianshu.com/p/0587bc01e414">https://www.jianshu.com/p/0587bc01e414</a></p><p><a href="https://www.cnblogs.com/chenyusheng0803/p/10978883.html">https://www.cnblogs.com/chenyusheng0803/p/10978883.html</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本表示 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fasttext</title>
      <link href="/2021/07/19/fasttext/"/>
      <url>/2021/07/19/fasttext/</url>
      
        <content type="html"><![CDATA[<h2 id="1、文本分类"><a href="#1、文本分类" class="headerlink" title="1、文本分类"></a>1、文本分类</h2><h3 id="1-1-n-gram"><a href="#1-1-n-gram" class="headerlink" title="1.1 n-gram"></a>1.1 n-gram</h3><p>由于Bag of words不考虑词语的顺序，因此引入bag of n-gram。针对英文，词内的是char n-gram，用于词向量；词之间的是word n-gram，用于分类；对于中文，存在词粒度和字粒度。</p><p>举个例子，句子A为”今天天气真不错”，这里以词粒度举例，先分词为[“今天”，”天气”，”真“，”不错“]</p><p>uni-gram：今天   天气   真   不错</p><p>2-gram为：今天/天气    天气/真    真/不错</p><p>3-gram为：今天/天气/真      天气/真/不错</p><p>由于n-gram的量远比word大的多，完全存下所有的n-gram也不现实。FastText采用了hashing trick的方式，如下图所示：</p><p><img src="/2021/07/19/fasttext/1.png" alt></p><p>用哈希的方式既能保证查找时O(1)的效率，又可能把内存消耗控制在O(buckets * dim)范围内。不过这种方法潜在的问题是存在哈希冲突，不同的n-gram可能会共享同一个embedding。如果buckets取的足够大，这种影响会很小。</p><p>代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">def build_dataset(config, ues_word):</span><br><span class="line">    if ues_word:</span><br><span class="line">        tokenizer = lambda x: x.split(&#x27; &#x27;)  # word-level</span><br><span class="line">    else:</span><br><span class="line">        tokenizer = lambda x: [y for y in x]  # char-level</span><br><span class="line">    if os.path.exists(config.vocab_path):</span><br><span class="line">        vocab = pkl.load(open(config.vocab_path, &#x27;rb&#x27;))</span><br><span class="line">    else:</span><br><span class="line">        vocab = build_vocab(config.train_path, tokenizer=tokenizer, max_size=MAX_VOCAB_SIZE, min_freq=1)</span><br><span class="line">        pkl.dump(vocab, open(config.vocab_path, &#x27;wb&#x27;))</span><br><span class="line">    print(f&quot;Vocab size: &#123;len(vocab)&#125;&quot;)</span><br><span class="line"></span><br><span class="line">    def biGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        return (t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def triGramHash(sequence, t, buckets):</span><br><span class="line">        t1 = sequence[t - 1] if t - 1 &gt;= 0 else 0</span><br><span class="line">        t2 = sequence[t - 2] if t - 2 &gt;= 0 else 0</span><br><span class="line">        return (t2 * 14918087 * 18408749 + t1 * 14918087) % buckets</span><br><span class="line"></span><br><span class="line">    def load_dataset(path, pad_size=32):</span><br><span class="line">        contents = []</span><br><span class="line">        with open(path, &#x27;r&#x27;, encoding=&#x27;UTF-8&#x27;) as f:</span><br><span class="line">            for line in tqdm(f):</span><br><span class="line">                lin = line.strip()</span><br><span class="line">                if not lin:</span><br><span class="line">                    continue</span><br><span class="line">                content, label = lin.split(&#x27;\t&#x27;)</span><br><span class="line">                words_line = []</span><br><span class="line">                token = tokenizer(content)</span><br><span class="line">                seq_len = len(token)</span><br><span class="line">                if pad_size:</span><br><span class="line">                    if len(token) &lt; pad_size:</span><br><span class="line">                        token.extend([PAD] * (pad_size - len(token)))</span><br><span class="line">                    else:</span><br><span class="line">                        token = token[:pad_size]</span><br><span class="line">                        seq_len = pad_size</span><br><span class="line">                # word to id</span><br><span class="line">                for word in token:</span><br><span class="line">                    words_line.append(vocab.get(word, vocab.get(UNK)))</span><br><span class="line"></span><br><span class="line">                # fasttext ngram</span><br><span class="line">                buckets = config.n_gram_vocab</span><br><span class="line">                bigram = []</span><br><span class="line">                trigram = []</span><br><span class="line">                # ------ngram------</span><br><span class="line">                for i in range(pad_size):</span><br><span class="line">                    bigram.append(biGramHash(words_line, i, buckets))</span><br><span class="line">                    trigram.append(triGramHash(words_line, i, buckets))</span><br><span class="line">                # -----------------</span><br><span class="line">                contents.append((words_line, int(label), seq_len, bigram, trigram))</span><br><span class="line">        return contents  # [([...], 0), ([...], 1), ...]</span><br><span class="line"></span><br><span class="line">    train = load_dataset(config.train_path, config.pad_size)</span><br><span class="line">    dev = load_dataset(config.dev_path, config.pad_size)</span><br><span class="line">    test = load_dataset(config.test_path, config.pad_size)</span><br><span class="line">    return vocab, train, dev, test</span><br></pre></td></tr></table></figure><h3 id="1-2-网络结构"><a href="#1-2-网络结构" class="headerlink" title="1.2 网络结构"></a>1.2 网络结构</h3><p><img src="/2021/07/19/fasttext/fasttext.JPG" alt="fasttext"></p><p>模型结构上word2vec的cbow模型很像</p><p>输入层：举个例子，输入文本”今天天气真不错”，词粒度的2-gram为</p><script type="math/tex; mode=display">x_2=\begin{bmatrix}emb_{今天/天气}，emb_{天气/真}，emb_{ 真/不错} \end{bmatrix},emb为词向量矩阵\\x_{1},x_{2},...,x_{N}最后输入到中间层的形式为:mean(\begin{bmatrix}x_1 \\ x_2 \\...\\x_N  \end{bmatrix}),其中mean为对每个x的列求平均</script><p>中间层：线形层+relu作为激活函数</p><p>输出层：为简单的线形层</p><p>代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">class Model(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super(Model, self).__init__()</span><br><span class="line">        if config.embedding_pretrained is not None:</span><br><span class="line">            self.embedding = nn.Embedding.from_pretrained(config.embedding_pretrained, freeze=False)</span><br><span class="line">        else:</span><br><span class="line">            self.embedding = nn.Embedding(config.n_vocab, config.embed, padding_idx=config.n_vocab - 1)</span><br><span class="line">        self.embedding_ngram2 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.embedding_ngram3 = nn.Embedding(config.n_gram_vocab, config.embed)</span><br><span class="line">        self.dropout = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc1 = nn.Linear(config.embed * 3, config.hidden_size)</span><br><span class="line">        # self.dropout2 = nn.Dropout(config.dropout)</span><br><span class="line">        self.fc2 = nn.Linear(config.hidden_size, config.num_classes)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line"></span><br><span class="line">        out_word = self.embedding(x[0])</span><br><span class="line">        out_bigram = self.embedding_ngram2(x[2])</span><br><span class="line">        out_trigram = self.embedding_ngram3(x[3])</span><br><span class="line">        out = torch.cat((out_word, out_bigram, out_trigram), -1)</span><br><span class="line"></span><br><span class="line">        out = out.mean(dim=1)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line">        out = self.fc1(out)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = self.fc2(out)</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure><h3 id="1-3-分层softmax"><a href="#1-3-分层softmax" class="headerlink" title="1.3 分层softmax"></a>1.3 分层softmax</h3><p>对于分类问题，神经网络的输出结果需要经过softmax将其转为概率分布后才可以利用交叉熵计算loss</p><p>由于普通softmax的计算效率比较低，计算效率为$O(Kd)$使用分层的softmax时间复杂度可以达到$dlogK$，$K$为分类的数量，$d$为向量的维度</p><h4 id="1-3-1-普通softmax"><a href="#1-3-1-普通softmax" class="headerlink" title="1.3.1 普通softmax"></a>1.3.1 普通softmax</h4><p>假设输出为$Y_{pred}=[y_1,y_2,…,y_K]$,则$P_{y_i}$为</p><script type="math/tex; mode=display">P_{y_i}=\frac{e_{y_i}}{\sum_{j=0}^Ke^{y_j}}</script><p>其中$y_i$的维度为$d$，从公式可以看出计算效率为$O(Kd)$</p><h4 id="1-3-2-分层softmax"><a href="#1-3-2-分层softmax" class="headerlink" title="1.3.2 分层softmax"></a>1.3.2 分层softmax</h4><p>霍夫曼树可以参考 <a href="https://zhuanlan.zhihu.com/p/154356949">https://zhuanlan.zhihu.com/p/154356949</a></p><p>为什么要霍夫曼，普通的不行？</p><p>分层softmax核心思想为利用训练样本构建霍夫曼树，如下</p><p><img src="/2021/07/19/fasttext/11.png" alt="fasttext"></p><p>树的结构是根据不同类在样本中出现的频次构造的，即频次越大的节点距离根节点越近。$K$个不同的类组成所有的叶子节点，$K-1个$内部节点作为参数。从根节点到某个叶子节点$y_i$经过的节点和边形成一条路径，路径长度表示为 $L_{y_i}$,$n_{(y_i,j)}$表示路径上的节点，那么</p><script type="math/tex; mode=display">P_{y_i}=\prod \limits_{j=1}^{L_{y_i}}P_{(n(y_{i},j),left\ or\ right)}\\=\prod \limits_{j=0}^{L_{y_i}-1}\sigma(f(n(y_i,j+1)==LC(n(y_i,j))){\theta_{n(y_i,j)}^T} Y)\\其中LC(n(y_i,j)表示n(y_i,j)的左孩子，\sigma 为SIGMOD函数，f(m)=\begin{equation}\left\{\begin{aligned}1 && if \ m==true \\-1 & & \ else \\\end{aligned}\right.\end{equation}</script><p>从公式可以看出时间复杂度降低至$dlogK$。</p><p>以图中$y_2$为例：</p><script type="math/tex; mode=display">P_{y_2}=P_{(n(y_{2},1),left)}\cdot P_{(n(y_{2},2),left)}\cdot P_{(n(y_{2},3),right)}\\=\sigma({\theta_{n(y_2,1)}^T} Y)\cdot \sigma({\theta_{n(y_2,2)}^T} Y)\cdot \sigma({-\theta_{n(y_2,3)}^T} Y)</script><p>从根节点走到叶子节点 $y_2$ ，实际上是在做了3次逻辑回归。</p><h2 id="2-训练词向量"><a href="#2-训练词向量" class="headerlink" title="2.训练词向量"></a>2.训练词向量</h2><p><a href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a></p><p><a href="https://zhuanlan.zhihu.com/p/32965521">https://zhuanlan.zhihu.com/p/32965521</a></p><p><a href="https://blog.csdn.net/qq_27009517/article/details/80676022">https://blog.csdn.net/qq_27009517/article/details/80676022</a></p><p><a href="http://alex.smola.org/papers/2009/Weinbergeretal09.pdf">http://alex.smola.org/papers/2009/Weinbergeretal09.pdf</a></p><p><a href="https://arxiv.org/abs/1607.04606">https://arxiv.org/abs/1607.04606</a></p><p>fasttext工具 <a href="https://github.com/facebookresearch/fastText">https://github.com/facebookresearch/fastText</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 文本分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformer(attention is all your need)</title>
      <link href="/2021/07/18/transformer/"/>
      <url>/2021/07/18/transformer/</url>
      
        <content type="html"><![CDATA[<p>1.We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. </p><p>2.Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.</p><p><img src="/2021/07/18/transformer/transformer.JPG" alt></p><h2 id="1-Positional-Encoding"><a href="#1-Positional-Encoding" class="headerlink" title="1 Positional Encoding"></a>1 Positional Encoding</h2><p>in order for the model to <strong>make use of the order of the sequence</strong>, we must inject some information about the relative or absolute position of the tokens in the sequence. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>详细可参考 <a href="https://wmathor.com/index.php/archives/1453/">https://wmathor.com/index.php/archives/1453/</a></p><h2 id="2-Attention"><a href="#2-Attention" class="headerlink" title="2 Attention"></a>2 Attention</h2><p><img src="/2021/07/18/transformer/11.jpg" alt></p><p>其中不同颜色表示不同head，颜色深浅表示词的关联程度。</p><p>不同head表示不同应用场景 ，单一head表示某个场景下，各个字之间的关联程度</p><h3 id="1-Scaled-Dot-Product-Attention"><a href="#1-Scaled-Dot-Product-Attention" class="headerlink" title="1 Scaled Dot-Product Attention"></a>1 Scaled Dot-Product Attention</h3><p><img src="/2021/07/18/transformer/self-attention.JPG" alt></p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_{k}}})V</script><p>$d_{k}$  ： keys of dimension</p><p>为什么scale？We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.</p><p><strong>Mask</strong></p><p>可以分为两类：Attention Mask和Padding Mask，接下来具体讲解。</p><p>1.Attention Mask</p><p>ensures that the predictions for position i can depend only on the known outputs at positions less than i.</p><p>sotfmax前要mask，上三角mask掉</p><p><img src="/2021/07/18/transformer/11.jpeg" alt></p><p>2.Padding Mask</p><p>Padding位置上的信息是无效的，所以需要丢弃。</p><p>过程如下图示：</p><p><img src="/2021/07/18/transformer/44.jpg" alt></p><p><img src="/2021/07/18/transformer/66.jpg" alt></p><h3 id="2-Multi-Head-Attention"><a href="#2-Multi-Head-Attention" class="headerlink" title="2 Multi-Head Attention"></a>2 Multi-Head Attention</h3><p><img src="/2021/07/18/transformer/self-attention2.JPG" alt></p><p><strong>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.</strong></p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head1,...,head_n)W^O\\ where  \ head_{i}=Attetion(QW_{i}^{Q},kW_{i}^{K},VW_{i}^{V})</script><h3 id="3-Applications-of-Attention-in-our-Model"><a href="#3-Applications-of-Attention-in-our-Model" class="headerlink" title="3 Applications of Attention in our Model"></a>3 Applications of Attention in our Model</h3><p><strong>1.encoder-decoder attention layers</strong></p><p>结构：queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder.</p><p>目的：This allows every position in the decoder to attend over all positions in the input sequence.</p><p><strong>2.encoder contains self-attention layers</strong></p><p>结构：keys, values and queries come from the same place</p><p>目的：Each position in the encoder can attend to all positions in the previous layer of the encoder.</p><p><strong>3.self-attention layers in the decoder</strong></p><p>结构：keys, values and queries come from the same place</p><p>目的：allow each position in the decoder to attend to all positions in the decoder up to and including that position</p><h2 id="3-Encoder-and-Decoder-Stacks"><a href="#3-Encoder-and-Decoder-Stacks" class="headerlink" title="3 Encoder and Decoder Stacks"></a>3 Encoder and Decoder Stacks</h2><h3 id="1-encoder"><a href="#1-encoder" class="headerlink" title="1 encoder"></a>1 encoder</h3><p>1).Input Embedding与Positional Encoding</p><script type="math/tex; mode=display">X = \text{Input Embedding}+ \text{Positional Encoding}\\</script><p>2). multi-head attention</p><script type="math/tex; mode=display">Q = \text{Linear}_q(X) = XW_{Q}\\K = \text{Linear}_k(X) = XW_{K}\\V = \text{Linear}_v(X) = XW_{V}\\X_{attention} = \text{Attention}(Q,K,V)</script><p>3). 残差连接与 Layer Normalization</p><script type="math/tex; mode=display">X_{attention} = X + X_{attention}\\X_{attention} = \text{LayerNorm}(X_{attention})</script><p>4). FeedForward</p><script type="math/tex; mode=display">X_{hidden} = \text{Linear}(\text{ReLU}(\text{Linear}(X_{attention})))</script><p>5). 残差连接与 Layer Normalization</p><script type="math/tex; mode=display">X_{hidden} = X_{attention} + X_{hidden}\\X_{hidden} = \text{LayerNorm}(X_{hidden})</script><p>其中$ X_{hidden} \in \mathbb{R}^{batch_size  \ <em> \  seq_len \  </em> \  embed_dim} $</p><h3 id="2-decoder"><a href="#2-decoder" class="headerlink" title="2 decoder"></a>2 decoder</h3><p>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：</p><ul><li>Masked Multi-Head Self-Attention</li><li>Multi-Head Encoder-Decoder Attention</li><li>FeedForward Network</li></ul><h2 id="4-常见问题"><a href="#4-常见问题" class="headerlink" title="4 常见问题"></a>4 常见问题</h2><p><strong>1 并行化</strong></p><p>训练encoder，decoder都并行，测试encoder并行，decoder不是并行</p><p><a href="https://zhuanlan.zhihu.com/p/368592551">https://zhuanlan.zhihu.com/p/368592551</a></p><p><strong>2 self-attention和普通attention的区别</strong></p><p>取决于query和key是否在一个地方</p><p><strong>3 Why Self-Attention</strong></p><p>Motivating our use of self-attention we consider three desiderata.</p><p>1.One is the total <strong>computational complexity</strong> per layer.</p><p>2.Another is the amount of computation that can be <strong>parallelized</strong>, as measured by the minimum number of sequential operations required.</p><p>3.The third is the path length between <strong>long-range dependencies</strong> in the network</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p><p>大佬详解： <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
          <category> 特征提取器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo_intro</title>
      <link href="/2021/07/18/hexo-intro/"/>
      <url>/2021/07/18/hexo-intro/</url>
      
        <content type="html"><![CDATA[<h2 id="1-部署"><a href="#1-部署" class="headerlink" title="1.部署"></a>1.部署</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo g </span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure><h2 id="2-创建文章"><a href="#2-创建文章" class="headerlink" title="2.创建文章"></a>2.创建文章</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;XXX&quot;</span><br></pre></td></tr></table></figure><h2 id="3-常见问题"><a href="#3-常见问题" class="headerlink" title="3.常见问题"></a>3.常见问题</h2><p><strong>Error: pandoc exited with code 7: pandoc: Unknown extension: smart</strong></p><p>解决：卸载pandoc</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm un hexo-renderer-pandoc —save</span><br></pre></td></tr></table></figure><p><strong>error：spawn failed</strong></p><p>1.删除<code>.deploy_git</code>文件夹</p><p>2.执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global core.autocrlf false</span><br></pre></td></tr></table></figure><p><strong>hexo 图片显示问题</strong></p><p>1、在_config.yml设置post_asset_folder为true</p><p>hexo new “paper_name”时会创建paper_name.md和paper_name的文件夹，将图片放在paper_name的文件夹</p><p>2、安装插件asset-image<br>npm install <a href="https://github.com/CodeFalling/hexo-asset-image">https://github.com/CodeFalling/hexo-asset-image</a><br>3、设置图片为相对路径</p><p><img src="/2021/07/18/hexo-intro/11.png" alt></p><p>注意修改图片路径中的 \ 为 / ,并且不带 . 或者 . /</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类任务的衡量指标</title>
      <link href="/2021/07/18/classify-performance/"/>
      <url>/2021/07/18/classify-performance/</url>
      
        <content type="html"><![CDATA[<h2 id="一、二分类"><a href="#一、二分类" class="headerlink" title="一、二分类"></a>一、二分类</h2><h3 id="1-1-confusion-matrix"><a href="#1-1-confusion-matrix" class="headerlink" title="1.1 confusion matrix"></a>1.1 confusion matrix</h3><p><img src="/2021/07/18/classify-performance/22.png" alt></p><p><img src="/2021/07/18/classify-performance/11.png" alt></p><h3 id="1-2-accuracy"><a href="#1-2-accuracy" class="headerlink" title="1.2 accuracy"></a>1.2 accuracy</h3><script type="math/tex; mode=display">accuracy={\frac{TP+TN}{TP+TN+FP+FN}}</script><p>accuracy 衡量全局分类正确的数量占总样本的比例</p><h3 id="1-3-precision"><a href="#1-3-precision" class="headerlink" title="1.3 precision"></a>1.3 precision</h3><script type="math/tex; mode=display">precision={\frac{TP}{TP+FP}}</script><p>precision为预测正确正样本数占预测的全部正样本数的比例，即系统判定为正样本的正确率。通俗地说，假如医生给病人检查，医生判断病人有疾病，然后医生判断的正确率有多少。</p><h3 id="1-4-recall"><a href="#1-4-recall" class="headerlink" title="1.4 recall"></a>1.4 recall</h3><script type="math/tex; mode=display">recall={\frac{TP}{TP+FN}}</script><p>recall为预测正确的正样本数量占真实正样本数量的比例，即衡量正样本的召回比例。通俗说，假如有一批病人，医生能从中找出病人的比例</p><h3 id="1-5-F1"><a href="#1-5-F1" class="headerlink" title="1.5 F1"></a>1.5 F1</h3><p>由于precision和recall往往是矛盾的，因此为了综合考虑二者，引入F1，即为precision和recall的调和平均</p><script type="math/tex; mode=display">F_{1}={2\frac{precision\cdot recall}{precision+recall}}</script><p>当$precision$和$recall$的任一个值为0，$F_1$都为0</p><p>之所以采用调和平均，是因为调和平均数受极端值影响较大，更适合评价不平衡数据的分类问题</p><p>通用的F值表达式：</p><script type="math/tex; mode=display">F_{\beta}={(1+\beta^2)\frac{precision\cdot recall}{\beta^2\cdot precision+recall}}</script><p>除了$F_1$分数之外，$F_2$ 分数和$F_{0.5}$分数在统计学中也得到大量的应用。其中，$F_2$分数中，召回率的权重高于精确率，而$F_{0.5}$分数中，精确率的权重高于召回率。</p><h3 id="1-6-ROC"><a href="#1-6-ROC" class="headerlink" title="1.6 ROC"></a>1.6 ROC</h3><p><img src="/2021/07/18/classify-performance/11.jfif" alt></p><p><img src="/2021/07/18/classify-performance/22.jpg" alt></p><p>roc曲线：接收者操作特征(receiver operating characteristic), roc曲线上每个点反映某个阈值下的FPR和TPR的组合。</p><p>横轴：$FPR$，叫做假正类率，表示预测为正例但真实情况为反例的占所有真实情况中反例的比率，公式为$FPR=\frac{FP}{TN+FP}$。</p><p>纵轴：$TPR$ ，叫做真正例率，表示预测为正例且真实情况为正例的占所有真实情况中正例的比率，公式为​</p><p>$TPR=\frac{TP}{TP+FN}$。</p><h3 id="1-7-AUC"><a href="#1-7-AUC" class="headerlink" title="1.7 AUC"></a>1.7 AUC</h3><p>$AUC$(Area under Curve)：ROC曲线下的面积，数值可以直观评价分类器的好坏，值越大越好，对于二分类，结果介于0.5和1之间，1为完美分类器，0.5是因为二分类分类效果最差也是0.5。</p><h2 id="二、多分类"><a href="#二、多分类" class="headerlink" title="二、多分类"></a>二、多分类</h2><h3 id="2-1-混淆矩阵"><a href="#2-1-混淆矩阵" class="headerlink" title="2.1 混淆矩阵"></a>2.1 混淆矩阵</h3><p><img src="/2021/07/18/classify-performance/11.jpg" alt></p><h3 id="2-2-accuracy"><a href="#2-2-accuracy" class="headerlink" title="2.2 accuracy"></a>2.2 accuracy</h3><script type="math/tex; mode=display">accuracy=\frac{分类正确的样本数,即对角线上的数}{总样本数，即矩阵全部元素相加}</script><h3 id="2-3-某个类别的precision，recall，F1"><a href="#2-3-某个类别的precision，recall，F1" class="headerlink" title="2.3 某个类别的precision，recall，F1"></a>2.3 某个类别的precision，recall，F1</h3><p>与二分类公式一样</p><p><img src="/2021/07/18/classify-performance/case1.JPG" alt></p><script type="math/tex; mode=display">precision_{pig}=\frac{20}{20+(10+40)}=\frac{2}{7}\\recall_{pig}=\frac{20}{20+10}=\frac{2}{3}\\F_{1pig}={2\frac{precision_{pig}\cdot recall_{pig}}{precision_{pig}+recall_{pig}}}</script><h3 id="2-4-系统的precision，recall，F1"><a href="#2-4-系统的precision，recall，F1" class="headerlink" title="2.4 系统的precision，recall，F1"></a>2.4 系统的precision，recall，F1</h3><p>系统的precision，recall，$F_1$需要综合考虑所有类别，即同时考虑猫、狗、猪的precision，recall，$F_1$。有如下几种方案：</p><h4 id="2-4-1-Macro-average"><a href="#2-4-1-Macro-average" class="headerlink" title="2.4.1 Macro average"></a>2.4.1 Macro average</h4><script type="math/tex; mode=display">Macro-precision=\frac{precision_{cat}+precision_{dog}+precision_{pig}}{3}\\Macro-recall=\frac{recall{cat}+recall{dog}+recall{pig}}{3}\\Macro-F_{1}=\frac{F_{1cat}+F_{1dog}+F_{1pig}}{3}</script><h4 id="2-4-2-Weighted-average"><a href="#2-4-2-Weighted-average" class="headerlink" title="2.4.2 Weighted average"></a>2.4.2 Weighted average</h4><p>对macro的推广</p><script type="math/tex; mode=display">Weighted-precision=W_{cat}\cdot precision_{cat}+W_{dog}\cdot precision_{dog}+W_{pig}\cdot precision_{pig}\\Weighted-recall=W_{cat}\cdot recall{cat}+W_{dog}\cdot recall{dog}+W_{pig}\cdot recall{pig}\\Weighted-F_{1}=W_{cat}\cdot F_{1cat}+W_{dog}\cdot F_{1dog}+W_{pig}\cdot F_{1pig}\\W_{cat}:W_{dog}:W_{pig}=N_{cat}:N_{dog}:N_{pig},其中N为样本数量，W为权重</script><h4 id="2-4-3-Micro-average"><a href="#2-4-3-Micro-average" class="headerlink" title="2.4.3 Micro average"></a>2.4.3 Micro average</h4><script type="math/tex; mode=display">Micro-precision={\frac{TP_{总}}{TP_{总}+FP_{总}}}={\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FP_{i}}}\\Micro-recall={\frac{TP_{总}}{TP_{总}+FN_{总}}}={\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FN_{i}}}\\Micro-F_{1}={2\frac{Micro-precision\cdot Micro-recall}{Micro-precision+Micro-recall}}</script><h3 id="2-5-ROC"><a href="#2-5-ROC" class="headerlink" title="2.5 ROC"></a>2.5 ROC</h3><p><img src="/2021/07/18/classify-performance/33.jpg" alt></p><p>对于多分类分类器整体效果的ROC如上micro或者macro曲线，其余3条描述单个类别的分类效果。对于多分类，ROC上的点，同样是某个阈值下的FPR和TPR的组合。</p><p>对于多分类的$FPR$,$TPR$，有几种计算方式</p><p>a. micro average</p><script type="math/tex; mode=display">FPR_{micro } =\frac{FP_总}{TN_总+FP_总}=\frac{\sum_{i=1}^{n}FP_{i}}{\sum_{i=1}^{n}TN_{i}+\sum_{i=1}^{n}FP_{i}}\\TPR_{micro }=\frac{TP_总}{TP_总+FN_总}=\frac{\sum_{i=1}^{n}TP_{i}}{\sum_{i=1}^{n}TP_{i}+\sum_{i=1}^{n}FN_{i}}\\n表示类别数量，FP_i，TN_i，TP_i，FN_i为某个类别的FP，TN，TP，FN</script><p>b. macro average</p><script type="math/tex; mode=display">FPR_{macro}=\frac{1}{n}\sum_{i=1}^{n}FPR_{i}\\TPR_{macro}=\frac{1}{n}\sum_{i=1}^{n}TPR_{i}，其中FPR_i，TPR_i为某个类别的FPR和TPR</script><h3 id="2-6-AUC"><a href="#2-6-AUC" class="headerlink" title="2.6 AUC"></a>2.6 AUC</h3><p>$AUC$依旧为ROC曲线下的面积，对于多分类个人认为取值范围为[0,1]。</p><h2 id="三-代码"><a href="#三-代码" class="headerlink" title="三.代码"></a>三.代码</h2><p><strong>accuracy，precision，recall，F1</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import precision_recall_fscore_support, accuracy_score</span><br><span class="line">def eval_acc_f1(y_true, y_pred):</span><br><span class="line">    acc = accuracy_score(y_true, y_pred)</span><br><span class="line">    prf = precision_recall_fscore_support(y_true, y_pred, average=&quot;macro&quot;)</span><br><span class="line">    return acc, prf</span><br></pre></td></tr></table></figure><p><strong>ROC和AUC</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"># 引入必要的库</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from itertools import cycle</span><br><span class="line">from sklearn import svm, datasets</span><br><span class="line">from sklearn.metrics import roc_curve, auc</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import label_binarize</span><br><span class="line">from sklearn.multiclass import OneVsRestClassifier</span><br><span class="line">from scipy import interp</span><br><span class="line"></span><br><span class="line"># 加载数据</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"># 将标签二值化</span><br><span class="line">y = label_binarize(y, classes=[0, 1, 2])</span><br><span class="line"># 设置种类</span><br><span class="line">n_classes = y.shape[1]</span><br><span class="line"></span><br><span class="line"># 训练模型并预测</span><br><span class="line">random_state = np.random.RandomState(0)</span><br><span class="line">n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line"># shuffle and split training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)</span><br><span class="line"></span><br><span class="line"># Learn to predict each class against the other</span><br><span class="line">classifier = OneVsRestClassifier(svm.SVC(kernel=&#x27;linear&#x27;, probability=True,</span><br><span class="line">                                 random_state=random_state))</span><br><span class="line">y_score = classifier.fit(X_train, y_train).decision_function(X_test)</span><br><span class="line"></span><br><span class="line"># 计算每一类的ROC</span><br><span class="line">fpr = dict()</span><br><span class="line">tpr = dict()</span><br><span class="line">roc_auc = dict()</span><br><span class="line">for i in range(n_classes):</span><br><span class="line">    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])</span><br><span class="line">    roc_auc[i] = auc(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line"># Compute micro-average ROC curve and ROC area（方法二）</span><br><span class="line">fpr[&quot;micro&quot;], tpr[&quot;micro&quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())</span><br><span class="line">roc_auc[&quot;micro&quot;] = auc(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;])</span><br><span class="line"></span><br><span class="line"># Compute macro-average ROC curve and ROC area（方法一）</span><br><span class="line"># First aggregate all false positive rates</span><br><span class="line">all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))</span><br><span class="line"># Then interpolate all ROC curves at this points</span><br><span class="line">mean_tpr = np.zeros_like(all_fpr)</span><br><span class="line">for i in range(n_classes):</span><br><span class="line">    mean_tpr += interp(all_fpr, fpr[i], tpr[i])</span><br><span class="line"># Finally average it and compute AUC</span><br><span class="line">mean_tpr /= n_classes</span><br><span class="line">fpr[&quot;macro&quot;] = all_fpr</span><br><span class="line">tpr[&quot;macro&quot;] = mean_tpr</span><br><span class="line">roc_auc[&quot;macro&quot;] = auc(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;])</span><br><span class="line"></span><br><span class="line"># Plot all ROC curves</span><br><span class="line">lw=2</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(fpr[&quot;micro&quot;], tpr[&quot;micro&quot;],</span><br><span class="line">         label=&#x27;micro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span><br><span class="line">               &#x27;&#x27;.format(roc_auc[&quot;micro&quot;]),</span><br><span class="line">         color=&#x27;deeppink&#x27;, linestyle=&#x27;:&#x27;, linewidth=4)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr[&quot;macro&quot;], tpr[&quot;macro&quot;],</span><br><span class="line">         label=&#x27;macro-average ROC curve (area = &#123;0:0.2f&#125;)&#x27;</span><br><span class="line">               &#x27;&#x27;.format(roc_auc[&quot;macro&quot;]),</span><br><span class="line">         color=&#x27;navy&#x27;, linestyle=&#x27;:&#x27;, linewidth=4)</span><br><span class="line"></span><br><span class="line">colors = cycle([&#x27;aqua&#x27;, &#x27;darkorange&#x27;, &#x27;cornflowerblue&#x27;])</span><br><span class="line">for i, color in zip(range(n_classes), colors):</span><br><span class="line">    plt.plot(fpr[i], tpr[i], color=color, lw=lw,</span><br><span class="line">             label=&#x27;ROC curve of class &#123;0&#125; (area = &#123;1:0.2f&#125;)&#x27;</span><br><span class="line">             &#x27;&#x27;.format(i, roc_auc[i]))</span><br><span class="line"></span><br><span class="line">plt.plot([0, 1], [0, 1], &#x27;k--&#x27;, lw=lw)</span><br><span class="line">plt.xlim([0.0, 1.0])</span><br><span class="line">plt.ylim([0.0, 1.05])</span><br><span class="line">plt.xlabel(&#x27;False Positive Rate&#x27;)</span><br><span class="line">plt.ylabel(&#x27;True Positive Rate&#x27;)</span><br><span class="line">plt.title(&#x27;Some extension of Receiver operating characteristic to multi-class&#x27;)</span><br><span class="line">plt.legend(loc=&quot;lower right&quot;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p><a href="https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839">https://blog.csdn.net/Orange_Spotty_Cat/article/details/80520839</a></p><p><a href="https://zhuanlan.zhihu.com/p/147663370">https://zhuanlan.zhihu.com/p/147663370</a></p><p><a href="https://zhuanlan.zhihu.com/p/81202617">https://zhuanlan.zhihu.com/p/81202617</a></p><p><a href="https://zhuanlan.zhihu.com/p/266386193">https://zhuanlan.zhihu.com/p/266386193</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 评价指标 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类任务的衡量指标 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
