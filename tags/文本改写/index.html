<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: 文本改写 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">文本改写</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-09-30  <a class="commentCountImg" href="/2021/09/30/felix/#comment-container"><span class="display-none-class">439b8b1d934392a5881d2cae5276da81</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="439b8b1d934392a5881d2cae5276da81">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>4 m  <i class="fas fa-pencil-alt"> </i>0.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/09/30/felix/">Felix Flexible Text Editing Through Tagging and Insertion</a></h1><div class="content"><p>google继lasertagger之后的又一篇text edit paper</p>
<p>In contrast to conventional sequence-to-sequence (seq2seq) models, FELIX is efficient in <strong>low-resource settings</strong> and <strong>fast</strong> at inference time, while being <strong>capable</strong> of modeling flexible input-output transformations. We achieve this by decomposing the text-editing task into two sub-tasks: <strong>tagging</strong> to decide on the subset of input tokens and their order in the output text and <strong>insertion</strong> to in-fill the missing tokens in the output not present in the input.</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p><img src="/2021/09/30/felix/11.JPG" alt></p>
<p>In particular, we have designed FELIX with the following requirements in mind: Sample efficiency, Fast inference time, Flexible text editing</p>
<h2 id="2-Model-description"><a href="#2-Model-description" class="headerlink" title="2 Model description"></a>2 Model description</h2><p>FELIX decomposes the conditional probability of generating an output sequence $y$ from an input<br>$x$ as follows:</p>
<script type="math/tex; mode=display">
p(\textbf{y}|\textbf{x})=p_{ins}(\textbf{y}|\textbf{y}^m)p_{tag}(\textbf{y}^t,\pi|\textbf{x})</script><h3 id="2-1-Tagging-Model"><a href="#2-1-Tagging-Model" class="headerlink" title="2.1 Tagging Model"></a>2.1 Tagging Model</h3><p>trained to optimize both the tagging and pointing loss:</p>
<script type="math/tex; mode=display">
\mathcal{L}=\mathcal{L}_{pointing  }+\lambda\mathcal{L}_{tagging   }</script><p><strong>Tagging</strong> :</p>
<p>tag sequence $\textbf{y}^t$由3种tag组成：$KEEP$，$DELETE$，$INSERT (INS)$</p>
<p>Tags are predicted by applying a single feedforward layer $f$ to the output of the encoder $\textbf{h}^L$ (the source sentence is first encoded using a 12-layer BERT-base model). $\textbf{y}^t_i=argmax(f(\textbf{h}^L_i))$</p>
<p><strong>Pointing</strong>:</p>
<p><img src="/2021/09/30/felix/33.JPG" alt></p>
<p>Given a sequence $\textbf{x}$ and the predicted tags $\textbf{y}^t$ , the re-ordering model generates a permutation $\pi$ so that from $\pi$and  $\textbf{y}^t$ we can reconstruct the insertion model input $\textbf{y}^m$. Thus we have: </p>
<script type="math/tex; mode=display">
p(\textbf{y}^m|\textbf{x}) \approx \prod \limits_{i}p(\pi(i)|\textbf{x},\textbf{y}^t,i)p(\textbf{y}_i^t|\textbf{x})</script><p>Our implementation is based on a <strong>pointer network</strong>. The output of this model is a series of predicted pointers (source token → next target token)</p>
<p>The input to the Pointer layer at position $i$:</p>
<script type="math/tex; mode=display">
\textbf{h}^{L+1}_{i}=f([\textbf{h}^{L}_{i};e(\textbf{y}_i^t);e(\textbf{p}_i)])</script><p>其中$e(\textbf{y}_i^t)$is the embedding of the predicted tag，$e(\textbf{p}_i)$ is the positional embedding</p>
<p>The pointer network attends over all hidden states, as such:</p>
<script type="math/tex; mode=display">
p(\pi(i)|\textbf{h}_i^{L+1})=attention(\textbf{h}_i^{L+1},\textbf{h}_{\pi(i)}^{L+1})</script><p>其中$\textbf{h}_i^{L+1}$ as $Q $, $\textbf{h}_{\pi(i)}^{L+1}$ as $K$</p>
<p>When realizing the pointers, we use a constrained beam search</p>
<h3 id="2-2-Insertion-Model"><a href="#2-2-Insertion-Model" class="headerlink" title="2.2 Insertion Model"></a>2.2 Insertion Model</h3><p><img src="/2021/09/30/felix/22.JPG" alt></p>
<p>To represent masked token spans we consider two options: <strong>masking</strong> and <strong>infilling</strong>. In the former case the tagging model predicts how many tokens need to be inserted by specializing the $INSERT$ tag into $INS_k$, where $k$ translates the span into $ k$  $MASK$ tokens. For the infilling case the tagging model predicts a generic $INS$ tag. </p>
<p>Note that we preserve the deleted​ span in the input to the insertion model by enclosing it between $[REPL]$ and $[/REPL]$ tags.</p>
<p>our insertion model is also based on a 12-layer BERT-base and we can directly take advantage of the BERT-style pretrained checkpoints.</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.findings-emnlp.111.pdf">https://aclanthology.org/2020.findings-emnlp.111.pdf</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/">文本改写</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-27  <a class="commentCountImg" href="/2021/07/27/lasertagger/#comment-container"><span class="display-none-class">cb752746ba01f81f9394f0e9d77bdc74</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="cb752746ba01f81f9394f0e9d77bdc74">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>18 m  <i class="fas fa-pencil-alt"> </i>2.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/27/lasertagger/">LASERTAGGER</a></h1><div class="content"><h2 id="一-摘要"><a href="#一-摘要" class="headerlink" title="一. 摘要"></a>一. 摘要</h2><p>对于某一些文本生成任务，输入和输出的文本有很多的重叠部分，如果还是采用encoder-decoder的文本生成模型去从零开始生成，其实是很浪费和没必要的，并且会导致两个问题：1：生成模型的幻觉问题(就是模型胡说八道) ；2：出现叠词(部分片段一致)。</p>
<p>基于上面的考虑，作者提出了lasertagger模型，通过几个常用的操作：keep token、delete token、 add token，给输入序列的每个token打上标签，使得文本生成任务转化为了序列标注任务。</p>
<p>通过这种方式，相较于encoder-decoder模型的优势有如下：1、推理的速度更快 2、在较小的数据集上性能优于seq2seq baseline，在大数据集上和baseline持平（因为输入和输出的文本有很多的重叠部分，对于这种情况，lasertagger的候选词库比较小，因为对于重叠部分的词，词库只需要添加keep，而传统encoder-decoder的候选词库依然很大，因为对于重叠部分的词，词库需要添加对应的词）</p>
<h2 id="二-主要贡献"><a href="#二-主要贡献" class="headerlink" title="二.主要贡献"></a>二.主要贡献</h2><p>1、通过输入和输出文本，自动去提取需要add的token</p>
<p>2、通过输入文本，输出文本和tag集，给训练的输入序列打上标签</p>
<p>3、提出了两个版本，$LASERTAGGER_{AR}$( bert+transformer decoder )和$LASERTAGGER_{FF}$( bert+desen+softmax )</p>
<h2 id="三-整体流程"><a href="#三-整体流程" class="headerlink" title="三. 整体流程"></a>三. 整体流程</h2><p><img src="/2021/07/27/lasertagger/entire.JPG" alt></p>
<p>其实就是两个过程，一.将输入文本变编码成特殊标注，二.将标注解码成文本</p>
<h2 id="四-文本标注"><a href="#四-文本标注" class="headerlink" title="四. 文本标注"></a>四. 文本标注</h2><h3 id="4-1-Tag集构建（也就是label集构建）"><a href="#4-1-Tag集构建（也就是label集构建）" class="headerlink" title="4.1 Tag集构建（也就是label集构建）"></a>4.1 Tag集构建（也就是label集构建）</h3><p>一般情况，tag分为两个大类： base tag $B$和 add tag $P$。对于base tag，就是$KEEP$或者$DELETE$当前token；对于add tag，就是要添加一个词到token前面，添加的词来源于词表$V$。实际在工程中，将$B$和$P$结合来表示，即$^{P}B$，总的tag数量大约等于$B$的数量乘以$P$的数量，即$2|V|$。对于某些任务可以引入特定的tag，比如对于句子融合，可以引入$SWAP$,如下图。</p>
<p><img src="/2021/07/27/lasertagger/case.JPG" alt></p>
<h4 id="4-1-1-词表V的构建"><a href="#4-1-1-词表V的构建" class="headerlink" title="4.1.1 词表V的构建"></a>4.1.1 词表V的构建</h4><p><strong>构建目标：</strong></p>
<ol>
<li>最小化词汇表规模；</li>
<li>最大化目标词语的比例</li>
</ol>
<p>限制词汇表的词组数量可以减少相应输出的决策量；最大化目标词语的比例可以防止模型添加无效词。</p>
<p><strong>构建过程：</strong></p>
<p>通过$LCS$算法（longest common sequence，最长公共子序列，注意和最长公共子串不是一回事），找出输入和输出序列的最长公共子序列，输出剩下的序列，就是需要$add$的token，添加到词表$V$，词表中的词基于词频排序,然后选择$l$个常用的。</p>
<p>举个例子：soruce为“12345678”，target为”1264591”</p>
<p>​                    最长公共子序列为[‘1’, ‘2’, ‘4’, ‘5’]</p>
<p>​                    需要$add$的token为 [‘6’, ‘91’]</p>
<p><strong>源码</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def _lcs_table(source, target):</span><br><span class="line">  &quot;&quot;&quot;Returns the Longest Common Subsequence dynamic programming table.&quot;&quot;&quot;</span><br><span class="line">  rows = len(source)</span><br><span class="line">  cols = len(target)</span><br><span class="line">  lcs_table = [[0] * (cols + 1) for _ in range(rows + 1)]</span><br><span class="line">  for i in range(1, rows + 1):</span><br><span class="line">    for j in range(1, cols + 1):</span><br><span class="line">      if source[i - 1] == target[j - 1]:</span><br><span class="line">        lcs_table[i][j] = lcs_table[i - 1][j - 1] + 1</span><br><span class="line">      else:</span><br><span class="line">        lcs_table[i][j] = max(lcs_table[i - 1][j], lcs_table[i][j - 1])</span><br><span class="line">  return lcs_table</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _backtrack(table, source, target, i, j):</span><br><span class="line">  &quot;&quot;&quot;Backtracks the Longest Common Subsequence table to reconstruct the LCS.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    table: Precomputed LCS table.</span><br><span class="line">    source: List of source tokens.</span><br><span class="line">    target: List of target tokens.</span><br><span class="line">    i: Current row index.</span><br><span class="line">    j: Current column index.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    List of tokens corresponding to LCS.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  if i == 0 or j == 0:</span><br><span class="line">    return []</span><br><span class="line">  if source[i - 1] == target[j - 1]:</span><br><span class="line">    # Append the aligned token to output.</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j - 1) + [target[j - 1]]</span><br><span class="line">  if table[i][j - 1] &gt; table[i - 1][j]:</span><br><span class="line">    return _backtrack(table, source, target, i, j - 1)</span><br><span class="line">  else:</span><br><span class="line">    return _backtrack(table, source, target, i - 1, j)</span><br><span class="line"></span><br><span class="line">def _compute_lcs(source, target):</span><br><span class="line">  # s1=&#123;1,3,4,5,6,7,7,8&#125;,s2=&#123;3,5,7,4,8,6,7,8,2&#125; return 35778</span><br><span class="line">  table = _lcs_table(source, target)</span><br><span class="line">  return _backtrack(table, source, target, len(source), len(target))</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">def _get_added_phrases(source: Text, target: Text) -&gt; Sequence[Text]:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Computes the phrases that need to be added to the source to get the target.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    sep = &#x27;&#x27;</span><br><span class="line">    source_tokens = utils.get_token_list(source.lower())</span><br><span class="line">    target_tokens = utils.get_token_list(target.lower())</span><br><span class="line">    #compute Longest Common Subsequence</span><br><span class="line">    kept_tokens = _compute_lcs(source_tokens, target_tokens)</span><br><span class="line">    added_phrases = []</span><br><span class="line">    kept_idx = 0</span><br><span class="line">    phrase = []</span><br><span class="line">    for token in target_tokens:</span><br><span class="line">        if kept_idx &lt; len(kept_tokens) and token == kept_tokens[kept_idx]:</span><br><span class="line">            kept_idx += 1</span><br><span class="line">            if phrase:</span><br><span class="line">                added_phrases.append(sep.join(phrase))</span><br><span class="line">                phrase = []</span><br><span class="line">        else:</span><br><span class="line">            phrase.append(token)</span><br><span class="line">    if phrase:</span><br><span class="line">        added_phrases.append(sep.join(phrase))</span><br><span class="line">    return added_phrases</span><br></pre></td></tr></table></figure>
<p>词表位于文件label_map.txt.log，本人基于自己的数据集，内容如下所示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Idx Frequency  Coverage (%)   Phrase</span><br><span class="line">1  19 94.22  址</span><br><span class="line">2  15 95.27  单位</span><br><span class="line">3  8  95.76  地</span><br><span class="line">4  6  96.17  执勤</span><br></pre></td></tr></table></figure>
<h4 id="4-1-2-tag集"><a href="#4-1-2-tag集" class="headerlink" title="4.1.2 tag集"></a>4.1.2 tag集</h4><p>本人基于自己的数据集，得到的候选tag如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEEP</span><br><span class="line">DELETE</span><br><span class="line">KEEP|址</span><br><span class="line">DELETE|址</span><br><span class="line">KEEP|单位</span><br><span class="line">DELETE|单位</span><br><span class="line">KEEP|地</span><br><span class="line">DELETE|地</span><br><span class="line">KEEP|执勤</span><br><span class="line">DELETE|执勤</span><br></pre></td></tr></table></figure>
<h3 id="4-2-Converting-Training-Targets-into-Tags"><a href="#4-2-Converting-Training-Targets-into-Tags" class="headerlink" title="4.2 Converting Training Targets into Tags"></a>4.2 Converting Training Targets into Tags</h3><p><strong>paper上的伪代码：</strong></p>
<p><img src="/2021/07/27/lasertagger/al1.JPG" alt></p>
<p>采用贪心策略，核心思想就是遍历$t$，先和$s$匹配，匹配上就$keep$，然后$i_t+j$，得到潜在的$add \ phrase \ p=t(i_t:i_t+j-1) $，然后判断$t(i_t+j)==s(i_s)\ and \ p\in V $</p>
<p><strong>源码</strong>：</p>
<p>和伪代码有一点不同，差异在于#####之间。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">def _compute_single_tag(</span><br><span class="line">        self, source_token, target_token_idx,</span><br><span class="line">        target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes a single tag.</span><br><span class="line"></span><br><span class="line">    The tag may match multiple target tokens (via tag.added_phrase) so we return</span><br><span class="line">    the next unmatched target token.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_token: The token to be tagged.</span><br><span class="line">      target_token_idx: Index of the current target tag.</span><br><span class="line">      target_tokens: List of all target tokens.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A tuple with (1) the computed tag and (2) the next target_token_idx.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    source_token = source_token.lower()</span><br><span class="line">    target_token = target_tokens[target_token_idx].lower()</span><br><span class="line">    if source_token == target_token:</span><br><span class="line">        return tagging.Tag(&#x27;KEEP&#x27;), target_token_idx + 1</span><br><span class="line">    # source_token!=target_token</span><br><span class="line">    added_phrase = &#x27;&#x27;</span><br><span class="line">    for num_added_tokens in range(1, self._max_added_phrase_length + 1):</span><br><span class="line">        if target_token not in self._token_vocabulary:</span><br><span class="line">            break</span><br><span class="line">        added_phrase += (&#x27; &#x27; if added_phrase else &#x27;&#x27;) + target_token</span><br><span class="line">        next_target_token_idx = target_token_idx + num_added_tokens</span><br><span class="line">        if next_target_token_idx &gt;= len(target_tokens):</span><br><span class="line">            break</span><br><span class="line">        target_token = target_tokens[next_target_token_idx].lower()</span><br><span class="line">        if (source_token == target_token and</span><br><span class="line">                added_phrase in self._phrase_vocabulary):</span><br><span class="line">            return tagging.Tag(&#x27;KEEP|&#x27; + added_phrase), next_target_token_idx + 1</span><br><span class="line">    return tagging.Tag(&#x27;DELETE&#x27;), target_token_idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        ####################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        ########################################################################################</span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<p><strong>缺陷</strong>：</p>
<p>对于一些情况，无法还原，举个例子：</p>
<p>​        source：证件有效期截止日期  target：证件日期格式</p>
<p>​        得不到tag结果</p>
<p>可以补充策略来修复bug</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def _compute_tags_fixed_order(self, source_tokens, target_tokens):</span><br><span class="line">    &quot;&quot;&quot;Computes tags when the order of sources is fixed.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      source_tokens: List of source tokens.</span><br><span class="line">      target_tokens: List of tokens to be obtained via edit operations.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      List of tagging.Tag objects. If the source couldn&#x27;t be converted into the</span><br><span class="line">      target via tagging, returns an empty list.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    tags = [tagging.Tag(&#x27;DELETE&#x27;) for _ in source_tokens]</span><br><span class="line">    # Indices of the tokens currently being processed.</span><br><span class="line">    source_token_idx = 0</span><br><span class="line">    target_token_idx = 0</span><br><span class="line">    while target_token_idx &lt; len(target_tokens):</span><br><span class="line">        tags[source_token_idx], target_token_idx = self._compute_single_tag(</span><br><span class="line">            source_tokens[source_token_idx], target_token_idx, target_tokens)</span><br><span class="line">        #########################################################################################</span><br><span class="line">        # If we&#x27;re adding a phrase and the previous source token(s) were deleted,</span><br><span class="line">        # we could add the phrase before a previously deleted token and still get</span><br><span class="line">        # the same realized output. For example:</span><br><span class="line">        #    [DELETE, DELETE, KEEP|&quot;what is&quot;]</span><br><span class="line">        # and</span><br><span class="line">        #    [DELETE|&quot;what is&quot;, DELETE, KEEP]</span><br><span class="line">        # Would yield the same realized output. Experimentally, we noticed that</span><br><span class="line">        # the model works better / the learning task becomes easier when phrases</span><br><span class="line">        # are always added before the first deleted token. Also note that in the</span><br><span class="line">        # current implementation, this way of moving the added phrase backward is</span><br><span class="line">        # the only way a DELETE tag can have an added phrase, so sequences like</span><br><span class="line">        # [DELETE|&quot;What&quot;, DELETE|&quot;is&quot;] will never be created.</span><br><span class="line">        if tags[source_token_idx].added_phrase:</span><br><span class="line">            # # the learning task becomes easier when phrases are always added before the first deleted token</span><br><span class="line">            first_deletion_idx = self._find_first_deletion_idx(</span><br><span class="line">                source_token_idx, tags)</span><br><span class="line">            if first_deletion_idx != source_token_idx:</span><br><span class="line">                tags[first_deletion_idx].added_phrase = (</span><br><span class="line">                    tags[source_token_idx].added_phrase)</span><br><span class="line">                tags[source_token_idx].added_phrase = &#x27;&#x27;</span><br><span class="line">        #######################################################################################</span><br><span class="line"></span><br><span class="line">        source_token_idx += 1</span><br><span class="line">        if source_token_idx &gt;= len(tags):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">    # If all target tokens have been consumed, we have found a conversion and</span><br><span class="line">    # can return the tags. Note that if there are remaining source tokens, they</span><br><span class="line">    # are already marked deleted when initializing the tag list.</span><br><span class="line">    if target_token_idx &gt;= len(target_tokens):  # all target tokens have been consumed</span><br><span class="line">        return tags</span><br><span class="line">    ####fix bug by lavine</span><br><span class="line"></span><br><span class="line">    ###strategy1</span><br><span class="line">    added_phrase = &quot;&quot;.join(target_tokens[target_token_idx:])</span><br><span class="line">    if added_phrase in self._phrase_vocabulary:</span><br><span class="line">        tags[-1] = tagging.Tag(&#x27;DELETE|&#x27; + added_phrase)</span><br><span class="line">        print(&#x27;&#x27;.join(source_tokens))</span><br><span class="line">        print(&#x27;&#x27;.join(target_tokens))</span><br><span class="line">        print(str([str(tag) for tag in tags] if tags != None else None))</span><br><span class="line">        return tags</span><br><span class="line">    ###strategy2</span><br><span class="line">    return []  # TODO</span><br></pre></td></tr></table></figure>
<h3 id="4-3-模型结构"><a href="#4-3-模型结构" class="headerlink" title="4.3 模型结构"></a>4.3 模型结构</h3><p><img src="/2021/07/27/lasertagger/fr.JPG" alt></p>
<p>模型主要包含两个部分：1.encoder:generates activation vectors for each element in the input sequence 2.decoder：converts encoder activations into tag labels</p>
<h4 id="4-3-1-encoder"><a href="#4-3-1-encoder" class="headerlink" title="4.3.1 encoder"></a>4.3.1 encoder</h4><p>由于$BERT$在sentence encoding tasks上做到state-of-the-art，所以使用$BERT$ 作为encoder部分。作者选择了$BERT_{base}$,包含12个self-attention层</p>
<h4 id="4-3-2-decoder"><a href="#4-3-2-decoder" class="headerlink" title="4.3.2 decoder"></a>4.3.2 decoder</h4><p>在$BERT$原文中，对于标注任务采取了非常简单的decoder结构，即采用一层feed-forward作为decoder，把这种组合叫做$LASERTAGGER_{FF}$，这种结构的缺点在于预测的标注词相互独立，没有考虑标注词的关联性。</p>
<p>为了考虑标注词的关联性，decode使用了Transformer decoder，单向连接，记作$LASERTAGGER_{AR}$，这种encoder和decoder的组合的有点像BERT结合GPT的感觉decoder 和encoder在以下方面交流：(i) through a full attention over the sequence of encoder activations (ii) by directly consuming the encoder activation at the current step</p>
<h2 id="五-realize"><a href="#五-realize" class="headerlink" title="五.realize"></a>五.realize</h2><p>对于基本的tag，比如$KEEP$，$DELETE$，$ADD$，$realize$就是根据输入和tag直接转换就行；对于特殊的tag，需要一些特定操作，看情况维护规则。</p>
<h2 id="六-loss"><a href="#六-loss" class="headerlink" title="六 loss"></a>六 loss</h2><p>假设句子长度为n，tag数量为m, loss为n个m分类任务的和</p>
<h2 id="七-评价指标"><a href="#七-评价指标" class="headerlink" title="七.评价指标"></a>七.评价指标</h2><p>评价指标，不同任务不同评价指标</p>
<p>1 Sentence Fusion</p>
<p>Exact score ：percentage of exactly correctly predicted fusions（类似accuracy）</p>
<p>SARI ：average F1 scores of the added, kept, and deleted n-grams</p>
<p>2 Split and Rephrase</p>
<p>SARI</p>
<p>3 Abstractive Summarization</p>
<p>ROUGE-L</p>
<p>4 Grammatical Error Correction (GEC)</p>
<p>precision and recall, F0:5</p>
<h2 id="八-实验结果"><a href="#八-实验结果" class="headerlink" title="八.实验结果"></a>八.实验结果</h2><p><strong>baseline</strong>： based on Transformer where both the encoder and decoder replicate the $BERT_{base}$ architecture</p>
<p><strong>速度</strong>：1.$LASERTAGGER_{AR} $is already 10x faster than comparable-in-accuracy $SEQ2SEQ_{BERT}$ baseline. This difference is due to the former model using a 1-layer decoder (instead of 12 layers) and no encoder-decoder cross attention. 2.$LASERTAGGER_{FF}$ is more than 100x faster</p>
<p>其余结果参考paper</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.01187.pdf">https://arxiv.org/pdf/1909.01187.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/google-research/lasertagger">https://github.com/google-research/lasertagger</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/348109034">https://zhuanlan.zhihu.com/p/348109034</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/">文本生成</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/">文本改写</a></div><hr></div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">405</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">142</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">386</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:58:45.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/gray-test/">灰度测试</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:53:30.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/abtest/">abtest</a></p><p class="categories"><a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> / <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:51:45.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/diff/">diff评测</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:44:10.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/seach-evluation/">搜索系统评价指标</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-10T16:29:10.000Z">2024-04-11</time></p><p class="title"><a href="/2024/04/11/nlp_evalution/">NLP评价指标</a></p><p class="categories"><a href="/categories/NLP/">NLP</a> / <a href="/categories/NLP/NLP/">NLP</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DIEN/"><span class="tag">DIEN</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>