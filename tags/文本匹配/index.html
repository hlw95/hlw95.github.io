<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: 文本匹配 - Lavine Hu</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Lavine Hu"><meta name="msapplication-TileImage" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lavine Hu"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Lavine Hu"><meta property="og:url" content="https://github.com/hlw95?tab=following"><meta property="og:site_name" content="Lavine Hu"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><meta property="article:author" content="Lavine Hu"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/avatar.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Lavine Hu","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Lavine Hu"},"description":""}</script><link rel="icon" href="https://cdn.jsdelivr.net/gh/removeif/removeif-demo@latest/img/favicon.png"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Lavine Hu</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/message">留言</a></div><div class="navbar-end"><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">文本匹配</a></li></ul></nav></div></div><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-10-26  <a class="commentCountImg" href="/2021/10/26/text-matching/#comment-container"><span class="display-none-class">4f0c1edbb24c7f5c8f422d9a846dcbba</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="4f0c1edbb24c7f5c8f422d9a846dcbba">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>10 m  <i class="fas fa-pencil-alt"> </i>1.5 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/26/text-matching/">文本匹配</a></h1><div class="content"><h2 id="1-无监督"><a href="#1-无监督" class="headerlink" title="1.无监督"></a>1.无监督</h2><h3 id="1-1-编辑距离"><a href="#1-1-编辑距离" class="headerlink" title="1.1 编辑距离"></a>1.1 编辑距离</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>编辑距离，英文名字为Levenshtein distance，通过描述一个字符串A需要多少次基本操作可以变成字符串B，来衡量两个字符串的相似度。</p>
<p>基本操作包括：增、删、改</p>
<p>增：字符串A为“AS”，字符串B为“ ASD“，字符串A-&gt;字符串B需要增加一个字符“D”</p>
<p>删：字符串A为“ASD”，字符串B为“ AS“，字符串A-&gt;字符串B需要删除一个字符“D”</p>
<p>改：字符串A为“ASX”，字符串B为“ ASD“，字符串A-&gt;字符串B需要将字符“X”变成字符“D”</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h4><p>实现过程使用动态规划，递推公式为</p>
<script type="math/tex; mode=display">
lev_{a,b}(i,j)=
\begin{equation}
f(x)=\left\{
\begin{aligned}
max(i,j) &  & if\ \min(i,j)=0
\\
min\left\{
\begin{aligned}
lev_{a,b}(i-1,j)+1 
\\
lev_{a,b}(i,j-1)+1 
\\
lev_{a,b}(i-1,j-1)+1_{(a_i\neq b_j)}
\end{aligned}
\right.
\end{aligned}
\right.
\end{equation}</script><p>$i$和$j$分别表示字符串$a$和字符串$b$的下标，$lev_{a,b}(i,j)$表示子串$a[:i]$到子串$b[:j]$的编辑距离。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def lev(str_a,str_b):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    ED距离，用来衡量单词之间的相似度</span><br><span class="line">    :param str_a:</span><br><span class="line">    :param str_b:</span><br><span class="line">    :return:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    str_a=str_a.lower()</span><br><span class="line">    str_b=str_b.lower()</span><br><span class="line">    matrix_ed=np.zeros((len(str_a)+1,len(str_b)+1),dtype=np.int)</span><br><span class="line">    matrix_ed[0]=np.arange(len(str_b)+1)</span><br><span class="line">    matrix_ed[:,0] = np.arange(len(str_a) + 1)</span><br><span class="line">    for i in range(1,len(str_a)+1):</span><br><span class="line">        for j in range(1,len(str_b)+1):</span><br><span class="line">            # 表示删除a_i</span><br><span class="line">            dist_1 = matrix_ed[i - 1, j] + 1</span><br><span class="line">            # 表示插入b_i</span><br><span class="line">            dist_2 = matrix_ed[i, j - 1] + 1</span><br><span class="line">            # 表示替换b_i</span><br><span class="line">            dist_3 = matrix_ed[i - 1, j - 1] + (1 if str_a[i - 1] != str_b[j - 1] else 0)</span><br><span class="line">            #取最小距离</span><br><span class="line">            matrix_ed[i,j]=np.min([dist_1, dist_2, dist_3])</span><br><span class="line">    print(matrix_ed)</span><br><span class="line">    return matrix_ed[-1,-1]</span><br></pre></td></tr></table></figure>
<h3 id="1-2-TF-IDF"><a href="#1-2-TF-IDF" class="headerlink" title="1.2 TF-IDF"></a>1.2 TF-IDF</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>（1）TF</p>
<p>针对某个文本</p>
<p>$TF_{word}=\frac{word在文本中出现的次数}{文本中所有词的总数}$</p>
<p>（2）IDF</p>
<p>针对语料库</p>
<p>$IDF_{word}=log(\frac{语料库的文本总数}{包含该word的文本数+1})$</p>
<p>（3）TF-IDF</p>
<p>$TF-IDF_{word}=TF_{word}*IDF_{word}$</p>
<p>（4）TF-IDF VEC</p>
<p>现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”]</p>
<p> $VEC_{A}=[TF-IDF_{今天},TF-IDF_{天气}，TF-IDF_{真好},0,0]$</p>
<p>（5）计算两句话的文本相似度</p>
<p>假设词库包含[“今天”,”天气”,”真好”,”天气”,”不错呀”],现有句子A：”今天天气真好”，对句子A做分词得到[“今天”,”天气”,”真好”],句子B:”天气不错呀”，分词后[“天气”,”不错呀”]</p>
<p>利用（3）得到句子A的TF-IDF VEC $VEC_{A}$，句子B的TF-IDF VEC $VEC_B$，利用余弦相似度计算文本相似度</p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import jieba</span><br><span class="line">import numpy  as np</span><br><span class="line">from sklearn.externals import joblib</span><br><span class="line">from scipy.linalg import norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TF_IDF_Model(object):</span><br><span class="line">    def __init__(self, corpus_list):</span><br><span class="line"></span><br><span class="line">        self.documents_list = corpus_list</span><br><span class="line">        self.documents_number = len(corpus_list)</span><br><span class="line">        self.get_idf()</span><br><span class="line"></span><br><span class="line">    def get_idf(self):</span><br><span class="line">        df = &#123;&#125;</span><br><span class="line">        self.idf = &#123;&#125;</span><br><span class="line">        tf = []</span><br><span class="line">        for document in self.documents_list:</span><br><span class="line">            temp = &#123;&#125;</span><br><span class="line">            for word in document:</span><br><span class="line">                temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">            tf.append(temp)</span><br><span class="line">            for key in temp.keys():</span><br><span class="line">                df[key] = df.get(key, 0) + 1</span><br><span class="line">        for key, value in df.items():</span><br><span class="line">            self.idf[key] = np.log10(self.documents_number / (value + 1))</span><br><span class="line"></span><br><span class="line">    def get_tf(self, document):</span><br><span class="line">        document = list(jieba.cut(document))</span><br><span class="line">        # tf = []</span><br><span class="line">        temp = &#123;&#125;</span><br><span class="line">        for word in document:</span><br><span class="line">            temp[word] = temp.get(word, 0) + 1 / len(document)</span><br><span class="line">        # tf.append(temp)</span><br><span class="line">        return temp</span><br><span class="line"></span><br><span class="line">    def tf_idf_vec(self, text):</span><br><span class="line">        tf = self.get_tf(text)</span><br><span class="line">        word = list(self.idf.keys())</span><br><span class="line">        vec = [0] * len(self.idf)</span><br><span class="line">        text = list(jieba.cut(text))</span><br><span class="line">        for ele in text:</span><br><span class="line">            if ele in word:</span><br><span class="line">                vec[word.index(ele)] = tf[ele] * self.idf[ele]</span><br><span class="line">        return vec</span><br><span class="line"></span><br><span class="line">    def cal_similarty(self, sentence1, sentence2):</span><br><span class="line">        vec1 = self.tf_idf_vec(sentence1)</span><br><span class="line">        vec2 = self.tf_idf_vec(sentence2)</span><br><span class="line">        similarty = np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))</span><br><span class="line">        return similarty</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train_model():</span><br><span class="line">    #####bulid corpus</span><br><span class="line">    corpus = pd.read_csv(corpus_path)</span><br><span class="line">    corpus_list = corpus[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    # corpus_list = corpus1[&quot;name&quot;].get_values().tolist()</span><br><span class="line">    corpus_list = [list(jieba.cut(str(doc))) for doc in corpus_list]</span><br><span class="line">    tf_idf_model = TF_IDF_Model(corpus_list)</span><br><span class="line">    joblib.dump(tf_idf_model, model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_model(path):</span><br><span class="line">    tf_idf_model = joblib.load(path)</span><br><span class="line">    return tf_idf_model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    from supercat.data_qualifier.tf_idf import TF_IDF_Model</span><br><span class="line">    ####</span><br><span class="line">    train_model()</span><br><span class="line">    ######</span><br><span class="line">    tf_idf_model = load_model(model_path)</span><br><span class="line">    sentence1=&quot;XXXX&quot;</span><br><span class="line">    sentence2=&quot;XXXX&quot;</span><br><span class="line">    print(tf_idf_model.get_tf(sentence1))</span><br><span class="line">    print(tf_idf_model.idf)</span><br><span class="line">    print(tf_idf_model.tf_idf_vec(sentence1))</span><br><span class="line">    print(tf_idf_model.cal_similarty(sentence1,sentence2))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-有监督"><a href="#2-有监督" class="headerlink" title="2.有监督"></a>2.有监督</h2><p><strong>基于表示的匹配方法</strong>：使用深度学习模型分别表征Query和Doc，通过计算向量相似度来作为语义匹配分数。微软的DSSM[26]及其扩展模型属于基于表示的语义匹配方法，美团搜索借鉴DSSM的双塔结构思想，左边塔输入Query信息，右边塔输入POI、品类信息，生成Query和Doc的高阶文本相关性、高阶品类相关性特征，应用于排序模型中取得了很好的效果。此外，比较有代表性的表示匹配模型还有百度提出 SimNet[27]，中科院提出的多视角循环神经网络匹配模型（MV-LSTM）[28]等。</p>
<p><strong>基于交互的匹配方法</strong>：这种方法不直接学习Query和Doc的语义表示向量，而是在神经网络底层就让Query和Doc提前交互，从而获得更好的文本向量表示，最后通过一个MLP网络获得语义匹配分数。代表性模型有华为提出的基于卷积神经网络的匹配模型ARC-II[29]，中科院提出的基于矩阵匹配的的层次化匹配模型MatchPyramid[30]。</p>
<p>基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量，缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。</p>
<p>匹配不同于排序，匹配是1对1的，排序是1对多</p>
<h3 id="2-1基于表示"><a href="#2-1基于表示" class="headerlink" title="2.1基于表示"></a>2.1基于表示</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138864580">https://zhuanlan.zhihu.com/p/138864580</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27590277/article/details/121391770">https://blog.csdn.net/qq_27590277/article/details/121391770</a></p>
<h3 id="2-2-基于交互"><a href="#2-2-基于交互" class="headerlink" title="2.2.基于交互"></a>2.2.基于交互</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/guofei_fly/article/details/107501276">https://blog.csdn.net/guofei_fly/article/details/107501276</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-08-04  <a class="commentCountImg" href="/2021/08/04/short-chinese-text-match/#comment-container"><span class="display-none-class">e42e73069e4b8f9b20abc430afce93b1</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="e42e73069e4b8f9b20abc430afce93b1">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>8 m  <i class="fas fa-pencil-alt"> </i>1.2 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/08/04/short-chinese-text-match/">Neural Graph Matching Networks for Chinese Short Text Matching</a></h1><div class="content"><p><a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.547.pdf">https://aclanthology.org/2020.acl-main.547.pdf</a></p>
<h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>对于中文短文本匹配，通常基于词粒度而不是字粒度。但是分词结果可能是错误的、模糊的或不一致的，从而损害最终的匹配性能。比如下图：字符序列“南京市长江大桥”经过不同的分词可能表达为不同的意思。</p>
<p><img src="/2021/08/04/short-chinese-text-match/lattice.JPG" alt></p>
<p>为了解决这个问题，作者提出了一种基于图神经网络的中文短文本匹配方法。不是将句子分割成一个单词序列，而是保留所有可能的分割路径，形成一个Lattice（segment1，segment2，segment3），如上图所示。</p>
<h2 id="2-问题定义"><a href="#2-问题定义" class="headerlink" title="2.问题定义"></a>2.问题定义</h2><p>将两个待匹配中文短文本分别定义为$S_a=\left \{ C_1^a,C_2^a,…,C_{t_a}^a \right \}$，$S_b=\left \{ C_1^b,C_2^b,…,C_{t_b}^b \right \}$，其中$C_i^a$表示句子$a$第$i$个字，$C_j^b$表示句子$b$第$j$个字，$t_a$，$t_b$分别表示两个句子的长度。$f(S_a,S_b)$是目标函数，输出为两个文本的匹配度。词格图用$G=(\nu,\xi)$表示，其中$\nu$是节点集，包括所有字符序列。$\xi$表示边集，如果$\nu$中两个顶点$v_i$和$v_j$相邻，那么就存在一个边为$e_{ij}$。$N_{fw}(v_i)$表示节点$v_i$ 正向的所有可达节点的集合,$N_{bw}(v_i)$表示节点$v_i$ 反向的所有可达节点的集合。句子$a$的词格图为$G^a(\nu_a,\xi_a)$，句子$b$的词格图为$G^b(\nu_b,\xi_b)$。</p>
<h2 id="3-模型结构"><a href="#3-模型结构" class="headerlink" title="3.模型结构"></a>3.模型结构</h2><p><img src="/2021/08/04/short-chinese-text-match/entire1.JPG" alt></p>
<p>模型分成3个部分，1.语言节点表示 2.图神经匹配 3.相关性分类器</p>
<h3 id="3-1-语言节点表示"><a href="#3-1-语言节点表示" class="headerlink" title="3.1 语言节点表示"></a>3.1 语言节点表示</h3><p>这一部分基于BERT的结构。BERT的token表示基于字粒度，可以得到$\left \{ [CLS],C_1^a,C_2^a,…,C_{ta}^a,[SEP],C_1^b,C_2^b,…,C_{t_b}^b,[SEP] \right \}$,如上图所示。BERT的输出为各个字的Embedding，$ \left \{\textbf{C}^{CLS},\textbf{C}_1^a,\textbf{C}_2^a,…,\textbf{C}_{t_a}^a,\textbf{C}^{SEP},\textbf{C}_1^b,\textbf{C}_2^b,…,\textbf{C}_{t_b},\textbf{C}^{SEP} \right \}$。</p>
<h3 id="3-2-图神经匹配"><a href="#3-2-图神经匹配" class="headerlink" title="3.2 图神经匹配"></a>3.2 图神经匹配</h3><p><strong>初始化</strong>：假设节点$v_i$包含$n_i$个连续字符，起始字符位置为$s_i$，即$ \left \{C_{s_i},C_{s_{i+1}},…,C_{s_{i}+n_i-1} \right \}$，这里$v_i$表示句子$a$或者$b$的结点。$V_i=\sum_{k=0}^{n_i-1}\textbf{U}_{s_i+k}\odot\textbf{C}_{s_i+k}$，其中$\odot$表示两个向量对应各个元素相乘。特征识别分数向量$\textbf{U}_{s_i+k}=softmax(FFN(\textbf{C}_{s_i+k}))$，$FFN$为两层。$h$为结点的向量表示，将$h_i^0$等于$V_i$</p>
<p><strong>Message Propagation</strong> : 对于第$l$次迭代，$G_a$中某个结点$v_i$由如下四个部分组成</p>
<script type="math/tex; mode=display">
m_i^{fw}=\sum_{v_j \in N_{fw}(v_i)}\alpha_{ij}(W^{fw}h_j^{l-1}),
\\m_i^{bw}=\sum_{v_k \in N_{bw}(v_i)}\alpha_{ik}(W^{bw}h_k^{l-1}),
\\m_i^{b1}=\sum_{v_m \in V^b}\alpha_{im}(W^{fw}h_m^{l-1}),
\\m_i^{b2}=\sum_{v_q \in V^b}\alpha_{iq}(W^{bw}h_q^{l-1})，</script><p>其中$\alpha_{ij},\alpha_{ik},\alpha_{im},\alpha_{iq}$是注意力系数，$W^{fw},W^{bw}$是注意力系数参数</p>
<p>然后定义两种信息为$m_i^{self}\triangleq[m_i^{fw},m_i^{bw}]，m_i^{cross}\triangleq[m_i^{b1},m_i^{b2}]$</p>
<p><strong>Representation Updating</strong>：得到两种信息后，需要更新结点$ v_i$的向量表示</p>
<script type="math/tex; mode=display">
d_k=cosine(w_k^{cos}\odot m_i^{self},w_k^{cos}\odot m_i^{cross})</script><p>其中$w_k^{cos}$为参数，$d_k$为multi-perspective cosine distance，可以衡量两种信息的距离，$k \in \left \{ 1,2,3,…P\right\}$，$P$是视角的数量。</p>
<script type="math/tex; mode=display">
h_i^l=FFN([m_i^{self},\textbf{d}_i])</script><p>其中$\textbf{d}_i\triangleq[d_1,d_2,…,d_P]$,$FFN$两层。</p>
<p><strong>句子的图级别表示</strong>：</p>
<p>总共经历了$L$次迭代（layer），得到$h_i^L$为结点$v_i$最终的向量表示（$h_i^L$includes not only the information from its reachable nodes but also information of pairwise comparison with all nodes in another graph)</p>
<p>最终，两个句子的图级别表示分别为</p>
<script type="math/tex; mode=display">
g^a=attentive pooling(\left \{ h_{1a}^L,h_{2a}^L,...,h_{node-num_a a}^L \right \}),
\\g^b=attentive pooling(\left \{ h_{1b}^L,h_{2b}^L,...,h_{node-num_b b}^L \right \})</script><h3 id="3-3-分类器"><a href="#3-3-分类器" class="headerlink" title="3.3 分类器"></a>3.3 分类器</h3><p>得到$g^a,g^b$后，两句子的相似度可以用分类器衡量：</p>
<script type="math/tex; mode=display">
P=FFN([g^a,g^b,g^a \odot g^b,|g^a-g^b|])</script><p>其中$P \in [0,1]$。</p>
<h2 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4.实验结果"></a>4.实验结果</h2><p><img src="/2021/08/04/short-chinese-text-match/22.GIF" alt></p>
<p><img src="/2021/08/04/short-chinese-text-match/33.GIF" alt></p>
<p>lattice和JIEBA+PKU的区别？</p>
<p>JIEBA+PKU is a small lattice graph generated by merging two word segmentation results</p>
<p>lattice：overall lattice，应该是全部的组合</p>
<p>两者效果差不多是因为Compared with the tiny graph, the overall lattice has more noisy nodes (i.e. invalid words in the corresponding sentence).</p>
<p><img src="/2021/08/04/short-chinese-text-match/11.GIF" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43390809/article/details/114077216">https://blog.csdn.net/qq_43390809/article/details/114077216</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/GNN/GNN/">GNN</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2021-07-21  <a class="commentCountImg" href="/2021/07/21/word-similarity/#comment-container"><span class="display-none-class">b208022762ac5cc544ef49ae50f65ab2</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="b208022762ac5cc544ef49ae50f65ab2">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>18 m  <i class="fas fa-pencil-alt"> </i>2.7 k</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/21/word-similarity/">词语的文本相似度</a></h1><div class="content"><h2 id="一-基于词典"><a href="#一-基于词典" class="headerlink" title="一.基于词典"></a>一.基于词典</h2><p>人为构建，比较主观，不利于维护</p>
<h3 id="1-1-基于词林"><a href="#1-1-基于词林" class="headerlink" title="1.1 基于词林"></a>1.1 基于词林</h3><h4 id="1-1-1-结构"><a href="#1-1-1-结构" class="headerlink" title="1.1.1 结构"></a>1.1.1 结构</h4><p>扩展版同义词词林分为5层结构，如图，随着级别的递增，词义刻画越来越细，到了第五层，每个分类里词语数量已经不大，很多只有一个词语，已经不可再分，可以称为原子词群、原子类或原子节点。不同级别的分类结果可以为自然语言处理提供不同的服务，例如第四层的分类和第五层的分类在信息检索、文本分类、自动问答等研究领域得到应用。有研究证明，对词义进行有效扩展，或者对关键词做同义词替换可以明显改善信息检索、文本分类和自动问答系统的性能。</p>
<p><img src="/2021/07/21/word-similarity/cilin.JPG" alt></p>
<p>下载后的词典文件如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Aa01A01= 人 士 人物 人士 人氏 人选</span><br><span class="line">Aa01A02= 人类 生人 全人类</span><br><span class="line">Aa01A03= 人手 人员 人口 人丁 口 食指</span><br><span class="line">Aa01A04= 劳力 劳动力 工作者</span><br><span class="line">Aa01A05= 匹夫 个人</span><br></pre></td></tr></table></figure>
<p><img src="/2021/07/21/word-similarity/coder.JPG" alt></p>
<p>表中的编码位是按照从左到右的顺序排列。第八位的标记有3 种，分别是“=”、“#”、“@”， “=”代表“相等”、“同义”。末尾的“#”代表“不等”、“同类”，属于相关词语。末尾的“@”代表“自我封闭”、“独立”，它在词典中既没有同义词，也没有相关词。</p>
<p><strong>源码如下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line">class WordSimilarity2010(SimilarBase):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">        本类根据下面的论文方法：</span><br><span class="line">        基于同义词词林的词语相似度计算方法，田久乐, 赵 蔚(东北师范大学 计算机科学与信息技术学院, 长春 130117 )</span><br><span class="line">        计算两个单词所有编码组合的相似度，取最大的一个</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(WordSimilarity2010, self).__init__()</span><br><span class="line">        self.a = 0.65</span><br><span class="line">        self.b = 0.8</span><br><span class="line">        self.c = 0.9</span><br><span class="line">        self.d = 0.96</span><br><span class="line">        self.e = 0.5</span><br><span class="line">        self.f = 0.1</span><br><span class="line">        self.degree = 180</span><br><span class="line">        self.PI = math.pi</span><br><span class="line"></span><br><span class="line">    def similarity(self, w1, w2):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        判断两个词的相似性。</span><br><span class="line">        :param w1: [string]</span><br><span class="line">        :param w2: [string]</span><br><span class="line">        :return: [float]0~1之间。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        code1 = self._data.get(w1, None)</span><br><span class="line">        code2 = self._data.get(w2, None)</span><br><span class="line"></span><br><span class="line">        if not code1 or not code2:</span><br><span class="line">            return 0  # 只要有一个不在库里则代表没有相似性。</span><br><span class="line"></span><br><span class="line">        # 最终返回的最大相似度</span><br><span class="line">        sim_max = 0</span><br><span class="line"></span><br><span class="line">        # 两个词可能对应多个编码</span><br><span class="line">        for c1 in code1:</span><br><span class="line">            for c2 in code2:</span><br><span class="line">                cur_sim = self.sim_by_code(c1, c2)</span><br><span class="line">                # print(c1, c2, &#x27;的相似度为：&#x27;, cur_sim)</span><br><span class="line">                if cur_sim &gt; sim_max:</span><br><span class="line">                    sim_max = cur_sim</span><br><span class="line"></span><br><span class="line">        return sim_max</span><br><span class="line"></span><br><span class="line">    def sim_by_code(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据编码计算相似度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">        # 先把code的层级信息提取出来</span><br><span class="line">        clayer1 = self._parse_code(c1)</span><br><span class="line">        clayer2 = self._parse_code(c2)</span><br><span class="line"></span><br><span class="line">        common_layer = self.get_common_layer(clayer1,clayer2)</span><br><span class="line">        length = len(common_layer)</span><br><span class="line"></span><br><span class="line">        # 如果有一个编码以&#x27;@&#x27;结尾，那么表示自我封闭，这个编码中只有一个词，直接返回f</span><br><span class="line">        if c1.endswith(&#x27;@&#x27;) or c2.endswith(&#x27;@&#x27;) or 0 == length:</span><br><span class="line">            return self.f</span><br><span class="line"></span><br><span class="line">        cur_sim = 0</span><br><span class="line">        if 6 &lt;= length:</span><br><span class="line">            # 如果前面七个字符相同，则第八个字符也相同，要么同为&#x27;=&#x27;，要么同为&#x27;#&#x27;&#x27;</span><br><span class="line">            if c1.endswith(&#x27;=&#x27;) and c2.endswith(&#x27;=&#x27;):</span><br><span class="line">                cur_sim = 1</span><br><span class="line">            elif c1.endswith(&#x27;#&#x27;) and c2.endswith(&#x27;#&#x27;):</span><br><span class="line">                cur_sim = self.e</span><br><span class="line">        else:</span><br><span class="line">            k = self.get_k(clayer1, clayer2)</span><br><span class="line">            n = self.get_n(common_layer)</span><br><span class="line">            if 1 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.a, n, k)</span><br><span class="line">            elif 2 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.b, n, k)</span><br><span class="line">            elif 3 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.c, n, k)</span><br><span class="line">            elif 4 == length:</span><br><span class="line">                cur_sim = self.sim_formula(self.d, n, k)</span><br><span class="line"></span><br><span class="line">        return cur_sim</span><br><span class="line"></span><br><span class="line">    def sim_formula(self, coeff, n, k):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        计算相似度的公式，不同的层系数不同</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return coeff * math.cos(n * self.PI / self.degree) * ((n - k + 1) / n)</span><br><span class="line"></span><br><span class="line">    def get_common_layer(self, ca, cb):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应的layer层</span><br><span class="line">        :param ca:     [list(str)] 分解后的编码。</span><br><span class="line">        :param cb:     [list(str)] 分解后的编码。</span><br><span class="line">        :return:   [list(str)]列表代表相应的根编码。</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        common_layer = []</span><br><span class="line"></span><br><span class="line">        for i, j in zip(ca, cb):</span><br><span class="line">            if i == j:</span><br><span class="line">                common_layer.append(i)</span><br><span class="line">            else:</span><br><span class="line">                break</span><br><span class="line">        return common_layer</span><br><span class="line"></span><br><span class="line">    def get_k(self, c1, c2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        返回两个编码对应分支的距离，相邻距离为1</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if c1[0] != c2[0]:</span><br><span class="line">            return abs(ord(c1[0]) - ord(c2[0]))</span><br><span class="line">        elif c1[1] != c2[1]:</span><br><span class="line">            return abs(ord(c1[1]) - ord(c2[1]))</span><br><span class="line">        elif c1[2] != c2[2]:</span><br><span class="line">            return abs(int(c1[2]) - int(c2[2]))</span><br><span class="line">        elif c1[3] != c2[3]:</span><br><span class="line">            return abs(ord(c1[3]) - ord(c2[3]))</span><br><span class="line">        else:</span><br><span class="line">            return abs(int(c1[4]) - int(c2[4]))</span><br><span class="line"></span><br><span class="line">    def get_n(self, common_layer):</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line">        返回相应结点下有多少个同级子结点。</span><br><span class="line">        :param common_layer:    [listr(str)]相同的结点。</span><br><span class="line">        :return:    int</span><br><span class="line">        &#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">        end_node = self._code_tree</span><br><span class="line">        for t_node_name in common_layer:</span><br><span class="line">            end_node = end_node[t_node_name]</span><br><span class="line"></span><br><span class="line">        if not isinstance(end_node, dict):</span><br><span class="line">            return end_node</span><br><span class="line">        return len(end_node.keys())</span><br></pre></td></tr></table></figure>
<h4 id="1-1-2-使用"><a href="#1-1-2-使用" class="headerlink" title="1.1.2 使用"></a>1.1.2 使用</h4><p>环境准备：pip install WordSimilarity</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from word_similarity import WordSimilarity2010</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">ws_tool = WordSimilarity2010()</span><br><span class="line">start = time.time()</span><br><span class="line">b_a = &quot;联系方式&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">b_a = &quot;手机&quot;</span><br><span class="line">b_b = &quot;电话&quot;</span><br><span class="line">sim_b = ws_tool.similarity(b_a, b_b)</span><br><span class="line">print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">end = time.time()</span><br><span class="line">print(&quot;运行时间：&quot;+str(end-start))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 0</span><br><span class="line">运行时间：5.793571472167969e-05</span><br><span class="line">手机 电话 相似度为 0.30484094213212237</span><br><span class="line">运行时间：0.0001442432403564453</span><br></pre></td></tr></table></figure>
<h3 id="1-2-基于知网与词林的词语语义相似度计算"><a href="#1-2-基于知网与词林的词语语义相似度计算" class="headerlink" title="1.2 基于知网与词林的词语语义相似度计算"></a>1.2 基于知网与词林的词语语义相似度计算</h3><h4 id="1-2-1-原理"><a href="#1-2-1-原理" class="headerlink" title="1.2.1 原理"></a>1.2.1 原理</h4><p>综合了词林cilin与知网hownet的相似度计算方法，采用混合策略，混合策略具体可以参考源码，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">from hownet.howNet import How_Similarity</span><br><span class="line">from cilin.V3.ciLin import CilinSimilarity</span><br><span class="line"></span><br><span class="line">class HybridSim():</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    混合相似度计算策略。使用了词林与知网词汇量的并集。扩大了词汇覆盖范围。</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    ci_lin = CilinSimilarity()  # 实例化词林相似度计算对象</span><br><span class="line">    how_net = How_Similarity()  # 实例化知网相似度计算对象</span><br><span class="line">    Common = ci_lin.vocab &amp; how_net.vocab</span><br><span class="line">    A = how_net.vocab - ci_lin.vocab</span><br><span class="line">    B = ci_lin.vocab - how_net.vocab</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def get_Final_sim(cls, w1, w2):</span><br><span class="line">        lin = cls.ci_lin.sim2018(w1, w2) if w1 in cls.ci_lin.vocab and w2 in cls.ci_lin.vocab else 0</span><br><span class="line">        how = cls.how_net.calc(w1, w2) if w1 in cls.how_net.vocab and w2 in cls.how_net.vocab else 0</span><br><span class="line"></span><br><span class="line">        if w1 in cls.Common and w2 in cls.Common:  # 两个词都被词林和知网共同收录。</span><br><span class="line">            # print(&#x27;两个词都被词林和知网共同收录。&#x27;, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(w1, w2, &#x27;词林改进版相似度：&#x27;, lin, end=&#x27;\t&#x27;)</span><br><span class="line">            # print(&#x27;知网相似度结果为：&#x27;, how, end=&#x27;\t&#x27;)</span><br><span class="line">            return lin * 1 + how * 0  # 可以调节两者的权重，以获取更优结果！！</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.A:  # 两个词都只被知网收录。</span><br><span class="line">            return how</span><br><span class="line">        if w1 in cls.B and w2 in cls.B:  # 两个词都只被词林收录。</span><br><span class="line">            return lin</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.B:  # 一个只被词林收录，另一个只被知网收录。</span><br><span class="line">            print(&#x27;触发策略三，左词为知网，右词为词林&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.B:</span><br><span class="line">            print(&#x27;触发策略三，左词为词林，右词为知网&#x27;)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return 0.2</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.A and w2 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（左知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.A and w1 in cls.Common:</span><br><span class="line">            print(&#x27;策略四（右知网）：知网相似度结果为：&#x27;, how)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return how</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * how + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w1 in cls.B and w2 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（左词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w1][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w2) for word in same_words]</span><br><span class="line">            print(w1, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        if w2 in cls.B and w1 in cls.Common:</span><br><span class="line">            print(w1, w2, &#x27;策略五（右词林）：词林改进版相似度：&#x27;, lin)</span><br><span class="line">            same_words = cls.ci_lin.code_word[cls.ci_lin.word_code[w2][0]]</span><br><span class="line">            if not same_words:</span><br><span class="line">                return lin</span><br><span class="line">            all_sims = [cls.how_net.calc(word, w1) for word in same_words]</span><br><span class="line">            print(w2, &#x27;词林同义词有：&#x27;, same_words, all_sims, end=&#x27;\t&#x27;)</span><br><span class="line">            return 0.6 * lin + 0.4 * max(all_sims)</span><br><span class="line"></span><br><span class="line">        print(&#x27;对不起，词语可能未收录，无法计算相似度！&#x27;)</span><br><span class="line">        return -1</span><br></pre></td></tr></table></figure>
<h4 id="1-2-2-使用"><a href="#1-2-2-使用" class="headerlink" title="1.2.2 使用"></a>1.2.2 使用</h4><p>参考<a target="_blank" rel="noopener" href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from Hybrid_Sim import HybridSim</span><br><span class="line">from Pearson import *</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    print(&#x27;词林词汇量&#x27;, len(HybridSim.ci_lin.vocab ),&#x27;\t知网词汇量&#x27;, len(HybridSim.how_net.vocab))</span><br><span class="line">    print(&#x27;两者总词汇量&#x27;,len(HybridSim.ci_lin.vocab | HybridSim.how_net.vocab),&#x27;\t重叠词汇量&#x27;, len(HybridSim.Common))</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    start = time.time()</span><br><span class="line">    hybrid = HybridSim.get_Final_sim(b_a, b_a)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a+&quot; &quot;+b_b+&quot;相似度为：&quot;, hybrid)</span><br><span class="line">    print(&quot;运行时间：&quot;+str(end-start))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">词林词汇量 77498 	知网词汇量 53336</span><br><span class="line">两者总词汇量 85817 	重叠词汇量 45017</span><br><span class="line">对不起，词语可能未收录，无法计算相似度！</span><br><span class="line">联系方式 电话相似度为： -1</span><br><span class="line">运行时间：3.504753112792969e-05</span><br><span class="line">手机 电话相似度为： 1.0</span><br><span class="line">运行时间：0.019332408905029297</span><br></pre></td></tr></table></figure>
<h2 id="二-基于词向量"><a href="#二-基于词向量" class="headerlink" title="二.基于词向量"></a>二.基于词向量</h2><p>基于样本构建，利于维护</p>
<h3 id="2-1-基于word2vec"><a href="#2-1-基于word2vec" class="headerlink" title="2.1 基于word2vec"></a>2.1 基于word2vec</h3><h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><p>word2vec的原理和词向量获取过程不在此赘述，在本部分主要讲解基于word2vec的词向量如何计算词语相似度。源码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def similarity(self, w1, w2):</span><br><span class="line">    &quot;&quot;&quot;Compute cosine similarity between two keys.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    w1 : str</span><br><span class="line">        Input key.</span><br><span class="line">    w2 : str</span><br><span class="line">        Input key.</span><br><span class="line"></span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    float</span><br><span class="line">        Cosine similarity between `w1` and `w2`.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return dot(matutils.unitvec(self[w1]), matutils.unitvec(self[w2]))</span><br></pre></td></tr></table></figure>
<h4 id="2-2-2-使用"><a href="#2-2-2-使用" class="headerlink" title="2.2.2 使用"></a>2.2.2 使用</h4><p>训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models.word2vec import Word2Vec</span><br><span class="line">import  pandas as pd</span><br><span class="line">from gensim import models</span><br><span class="line">import jieba</span><br><span class="line">###train</span><br><span class="line">data=pd.read_csv(data_path)</span><br><span class="line">sentences=data.tolist()</span><br><span class="line">model= Word2Vec()</span><br><span class="line">model.build_vocab(sentences)</span><br><span class="line">model.train(sentences,total_examples = model.corpus_count,epochs = 5)</span><br><span class="line">model.save(model_path)</span><br></pre></td></tr></table></figure>
<p>使用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from gensim import models</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    model=models.Word2Vec.load(model_path)</span><br><span class="line">    start = time.time()</span><br><span class="line">    b_a = &quot;联系方式&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    start = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br><span class="line">    b_a = &quot;手机&quot;</span><br><span class="line">    b_b = &quot;电话&quot;</span><br><span class="line">    sim_b = model.wv.n_similarity(b_a, b_b)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(b_a, b_b, &#x27;相似度为&#x27;, sim_b)</span><br><span class="line">    print(&quot;运行时间：&quot; + str(end - start))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">联系方式 电话 相似度为 -0.014857853</span><br><span class="line">运行时间：-4.76837158203125e-07</span><br><span class="line">手机 电话 相似度为 0.1771852</span><br><span class="line">运行时间：0.0004227161407470703</span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_33741547/article/details/80016713">https://blog.csdn.net/sinat_33741547/article/details/80016713</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/yaleimeng/Final_word_Similarity">https://github.com/yaleimeng/Final_word_Similarity</a></p>
</div><div class="index-category-tag"><div class="level-item"><i class="fas fa-folder-open has-text-grey"> </i><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/">NLP</a><span> </span><a class="article-more button is-small link-muted index-categories" href="/categories/NLP/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div>  <div class="level-item"><i class="fas fa-tags has-text-grey"> </i><a class="article-more button is-small link-muted index-tags" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/">文本匹配</a></div><hr></div></article></div><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><!--!--><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://avatars.githubusercontent.com/u/50144751?v=4" alt="Lavine Hu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Lavine Hu</p><p class="is-size-6 is-block">我能卖你生瓜蛋子？</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>杭州</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">435</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">135</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">414</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hlw95" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/hlw95"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Weibo" href="/null"><i class="fab fa-weibo"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="/18829272646@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Next" href="/null"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"From《"+data.from+"》</p><p>Provider-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-16T15:26:32.000Z">2024-04-16</time></p><p class="title"><a href="/2024/04/16/zhihu/">知乎搜索</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-16T15:19:47.000Z">2024-04-16</time></p><p class="title"><a href="/2024/04/16/deepmatch/">deepmatch</a></p><p class="categories"><a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a> / <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-16T14:54:16.000Z">2024-04-16</time></p><p class="title"><a href="/2024/04/16/recommend-rank/">排序</a></p><p class="categories"><a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/">推荐系统</a> / <a href="/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%8E%92%E5%BA%8F/">排序</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-16T14:39:51.000Z">2024-04-16</time></p><p class="title"><a href="/2024/04/16/code-reconstruct/">重构</a></p><p class="categories"><a href="/categories/c/">c++</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-15T15:09:25.000Z">2024-04-15</time></p><p class="title"><a href="/2024/04/15/vector-retrieval/">向量检索</a></p><p class="categories"><a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/">搜索系统</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/">召回</a> / <a href="/categories/%E6%90%9C%E7%B4%A2%E7%B3%BB%E7%BB%9F/%E5%8F%AC%E5%9B%9E/%E5%90%91%E9%87%8F/">向量</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">11</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/GNN/GCN/"><span class="level-start"><span class="level-item">GCN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/GNN/"><span class="level-start"><span class="level-item">GNN</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/GNN/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/LTR/LTR/"><span class="level-start"><span class="level-item">LTR</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/listwise/"><span class="level-start"><span class="level-item">listwise</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pairwise/"><span class="level-start"><span class="level-item">pairwise</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/pointwise/"><span class="level-start"><span class="level-item">pointwise</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/LTR/%E5%B0%8F%E5%B8%AE%E6%89%8B/"><span class="level-start"><span class="level-item">小帮手</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/04/"><span class="level-start"><span class="level-item">April 2024</span></span><span class="level-end"><span class="level-item tag">48</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2024/03/"><span class="level-start"><span class="level-item">March 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/04/"><span class="level-start"><span class="level-item">April 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2022/10/"><span class="level-start"><span class="level-item">October 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/PTM/"><span class="tag">PTM</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">文本表示</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%8C%B9%E9%85%8D/"><span class="tag">文本匹配</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%98%E5%8C%96/"><span class="tag">优化</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%AC%E5%9B%9E/"><span class="tag">召回</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F/"><span class="tag">排序</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"><span class="tag">文本分类</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E6%9C%AC%E6%94%B9%E5%86%99/"><span class="tag">文本改写</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Atlas/"><span class="tag">Atlas</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AutoTokenizer/"><span class="tag">AutoTokenizer</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Azkaban/"><span class="tag">Azkaban</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BertGCN/"><span class="tag">BertGCN</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bert%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/"><span class="tag">Bert文本表示</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CDC%EF%BC%88Change-Data-Capture%EF%BC%89%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94/"><span class="tag">CDC（Change Data Capture）工具对比</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CRF%E5%92%8CHMM/"><span class="tag">CRF和HMM</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ChineseBERT-Chinese-Pretraining-Enhanced-by-Glyph-and-Pinyin-Information/"><span class="tag">ChineseBERT Chinese Pretraining Enhanced by Glyph and Pinyin Information</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DAG/"><span class="tag">DAG</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL/"><span class="tag">DGL</span><span class="tag is-grey-lightest">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DGL-notice/"><span class="tag">DGL notice</span><span class="tag is-grey-lightest">1</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><!--!--></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Lavine Hu</a><p class="size-small"><span>&copy; 2024 Lavine Hu</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作,<br />&nbsp;&nbsp;&nbsp;&nbsp;方便于网友与自己学习交流，如有侵权，请<a href="/message" target="_blank">留言</a>，立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️Site from <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> has existed <strong>" + dnum + "</strong> d <strong>" + hnum + "</strong> h <strong>" + mnum + "</strong> m <strong>" + snum + "</strong> s！❤️";
        }var now = new Date();setInterval("createTime('2021/07/17 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️Thx <strong><span id="busuanzi_value_site_uv">99+</span></strong> users <strong><span id="busuanzi_value_site_pv">99+</span></strong> visited！❤️</span></div></p></div><div class="level-end"><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="2364053447" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);})</script><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('a0eb7de1c4dc4908943b','b466946cbbe843a4a6ef249eb54ae5956d2f91c3','hlw95','hlw95.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginModelPath":"assets/","model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"display":null,"superSample":2,"width":250,"height":500,"position":"right","hOffset":0,"vOffset":-20,"jsonPath":"/live2dw/assets/hijiki.model.json"},"mobile":{"show":true,"scale":0.5,"react":null,"opacityDefault":0.7,"opacityOnHover":0.2},"log":false,"pluginJsPath":"lib/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>